{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b188fd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " * Copyright (c) 2018 Radhamadhab Dalai\n",
    " *\n",
    " * Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    " * of this software and associated documentation files (the \"Software\"), to deal\n",
    " * in the Software without restriction, including without limitation the rights\n",
    " * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    " * copies of the Software, and to permit persons to whom the Software is\n",
    " * furnished to do so, subject to the following conditions:\n",
    " *\n",
    " * The above copyright notice and this permission notice shall be included in\n",
    " * all copies or substantial portions of the Software.\n",
    " *\n",
    " * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    " * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    " * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    " * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    " * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    " * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
    " * THE SOFTWARE.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29327a1",
   "metadata": {},
   "source": [
    "# Extreme Learning Machine (ELM)\n",
    "\n",
    "Extreme learning machine (ELM) is a machine learning algorithm specifically designed for single-hidden-layer feedforward neural networks (SLFNs). This algorithm is notable for its ability to randomly select hidden nodes and analytically determine the output weights of SLFNs, resulting in fast training speeds and good generalization performance. Here, we'll dive into the key concepts and implementation of ELM.\n",
    "\n",
    "## Key Concepts of Extreme Learning Machine (ELM)\n",
    "\n",
    "### Single-Hidden-Layer Feedforward Neural Networks (SLFNs)\n",
    "SLFNs are a type of neural network architecture with one hidden layer between the input and output layers. The hidden layer transforms the input data through nonlinear activation functions, enabling the network to learn complex patterns.\n",
    "\n",
    "### Random Hidden Nodes\n",
    "In ELM, the hidden nodes' parameters (weights and biases) are randomly generated and fixed. This randomization simplifies the learning process by reducing it to a linear problem, which can be solved analytically.\n",
    "\n",
    "### Analytical Determination of Output Weights\n",
    "The output weights are computed using a closed-form solution. Given the fixed hidden node parameters, the output weights can be determined by minimizing the error between the network's predictions and the actual target values using a least-squares solution.\n",
    "\n",
    "## Mathematical Formulation\n",
    "\n",
    "Given a training set \\( \\{(x_i, y_i) | x_i \\in \\mathbb{R}^n, y_i \\in \\mathbb{R}^m, i = 1, \\ldots, N\\} \\), where \\( x_i \\) is the input and \\( y_i \\) is the target output, an SLFN with \\( N \\) hidden nodes can be represented as:\n",
    "\n",
    "$$\n",
    "h(x) = \\sum_{i=1}^N \\beta_i g(w_i \\cdot x + b_i)\n",
    "$$\n",
    "\n",
    "where \\( \\beta_i \\) is the output weight, \\( w_i \\) and \\( b_i \\) are the randomly generated parameters of the hidden node, and \\( g(\\cdot) \\) is the activation function.\n",
    "\n",
    "The objective is to find the output weights \\( \\beta \\) that minimize the error:\n",
    "\n",
    "$$\n",
    "\\min_{\\beta} \\| H \\beta - Y \\|\n",
    "$$\n",
    "\n",
    "where \\( H \\) is the hidden layer output matrix, and \\( Y \\) is the target matrix. The matrix \\( H \\) is constructed as:\n",
    "\n",
    "$$\n",
    "H = \n",
    "\\begin{bmatrix}\n",
    "g(w_1 \\cdot x_1 + b_1) & \\cdots & g(w_N \\cdot x_1 + b_N) \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "g(w_1 \\cdot x_N + b_1) & \\cdots & g(w_N \\cdot x_N + b_N)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The output weights \\( \\beta \\) can be computed analytically using the Moore-Penrose generalized inverse of \\( H \\):\n",
    "\n",
    "$$\n",
    "\\beta = H^\\dagger Y\n",
    "$$\n",
    "\n",
    "## Implementation in Python\n",
    "\n",
    "Here is a simple implementation of the Extreme Learning Machine (ELM) in Python:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd359a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:\n",
      " [[-0.95906357]\n",
      " [ 0.10913444]\n",
      " [ 0.34502531]\n",
      " [-0.63861083]\n",
      " [-0.61727189]\n",
      " [-0.75399938]\n",
      " [-0.02450225]\n",
      " [-0.6646245 ]\n",
      " [-0.72546154]\n",
      " [-0.5249342 ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ExtremeLearningMachine:\n",
    "    def __init__(self, input_dim, hidden_dim, activation='sigmoid'):\n",
    "        \"\"\"\n",
    "        Initialize the Extreme Learning Machine.\n",
    "\n",
    "        Parameters:\n",
    "        - input_dim: The dimension of the input features.\n",
    "        - hidden_dim: The number of hidden neurons.\n",
    "        - activation: The activation function to use ('sigmoid', 'tanh', 'relu').\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.activation = activation\n",
    "        \n",
    "        # Randomly initialize weights and biases\n",
    "        self.W = np.random.randn(hidden_dim, input_dim)\n",
    "        self.b = np.random.randn(hidden_dim, 1)\n",
    "    \n",
    "    def _activation(self, X):\n",
    "        \"\"\"\n",
    "        Apply activation function.\n",
    "\n",
    "        Parameters:\n",
    "        - X: The input data.\n",
    "\n",
    "        Returns:\n",
    "        - Activated output.\n",
    "        \"\"\"\n",
    "        if self.activation == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-X))\n",
    "        elif self.activation == 'tanh':\n",
    "            return np.tanh(X)\n",
    "        elif self.activation == 'relu':\n",
    "            return np.maximum(0, X)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function\")\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train the ELM model.\n",
    "\n",
    "        Parameters:\n",
    "        - X: The input features (shape: [num_samples, input_dim]).\n",
    "        - Y: The target values (shape: [num_samples, output_dim]).\n",
    "        \"\"\"\n",
    "        # Compute hidden layer output matrix H\n",
    "        H = self._activation(np.dot(X, self.W.T) + self.b.T)\n",
    "        \n",
    "        # Compute output weights using the Moore-Penrose generalized inverse\n",
    "        H_pseudo_inv = np.linalg.pinv(H)\n",
    "        self.beta = np.dot(H_pseudo_inv, Y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the output for given input data.\n",
    "\n",
    "        Parameters:\n",
    "        - X: The input features (shape: [num_samples, input_dim]).\n",
    "\n",
    "        Returns:\n",
    "        - Predicted output values.\n",
    "        \"\"\"\n",
    "        H = self._activation(np.dot(X, self.W.T) + self.b.T)\n",
    "        return np.dot(H, self.beta)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate synthetic data\n",
    "    np.random.seed(0)\n",
    "    num_samples = 100\n",
    "    input_dim = 10\n",
    "    hidden_dim = 20\n",
    "    output_dim = 1\n",
    "\n",
    "    X_train = np.random.rand(num_samples, input_dim)\n",
    "    Y_train = np.sin(np.sum(X_train, axis=1, keepdims=True))  # Example target: sin(sum of inputs)\n",
    "\n",
    "    # Create and train ELM\n",
    "    elm = ExtremeLearningMachine(input_dim=input_dim, hidden_dim=hidden_dim, activation='relu')\n",
    "    elm.fit(X_train, Y_train)\n",
    "\n",
    "    # Predict\n",
    "    X_test = np.random.rand(10, input_dim)\n",
    "    Y_pred = elm.predict(X_test)\n",
    "\n",
    "    print(\"Predictions:\\n\", Y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0d94bf",
   "metadata": {},
   "source": [
    "### Random Hidden Nodes and SLFNs\n",
    "\n",
    "**Definition 7.7 (Piecewise Continuous):** A function is said to be piecewise continuous if it has only a finite number of discontinuities in any interval and its left and right limits are defined (not necessarily equal) at each discontinuity.\n",
    "\n",
    "**Definition 7.8 (Randomly Generated):** The function sequence \\(\\{g_n = g(w_n^T x + b_n)\\}\\) or \\(\\{g_n = g(\\|w_n - x\\| / b_n)\\}\\) is said to be randomly generated if the corresponding parameters are randomly generated from or based on a continuous sampling distribution probability.\n",
    "\n",
    "**Definition 7.9 (Random Node):** A node is called a random node if its parameters \\((w, b)\\) are randomly generated based on a continuous sampling distribution probability.\n",
    "\n",
    "### SLFN Architectures\n",
    "\n",
    "Single-Layer Feedforward Networks (SLFNs) have two main network architectures:\n",
    "- SLFNs with additive hidden nodes\n",
    "- SLFNs with Radial Basis Function (RBF) networks, which apply RBF nodes in the hidden layer.\n",
    "\n",
    "The network function of an SLFN with \\(d\\) hidden nodes can be represented by:\n",
    "\n",
    "$$\n",
    "f_d(x) = \\sum_{i=1}^{d} \\beta_i g_i(x), \\quad x \\in \\mathbb{R}^n, \\; \\beta_i \\in \\mathbb{R}\n",
    "$$\n",
    "\n",
    "where \\(g_i\\) denotes the \\(i\\)-th hidden node output function.\n",
    "\n",
    "Two commonly used \\(i\\)-th hidden node output functions \\(g_i\\) are defined as:\n",
    "\n",
    "1. For additive nodes:\n",
    "   $$\n",
    "   g_i(x) = g(w_i^T x + b_i), \\quad w_i \\in \\mathbb{R}^n, \\; b_i \\in \\mathbb{R}\n",
    "   $$\n",
    "\n",
    "2. For RBF nodes:\n",
    "   $$\n",
    "   g_i(x) = g\\left(\\frac{\\|x - b_i\\|}{a_i}\\right), \\quad a_i \\in \\mathbb{R}^n, \\; b_i \\in \\mathbb{R}^+\n",
    "   $$\n",
    "\n",
    "where \\(a_i\\) and \\(b_i\\) are the center and impact factor of the \\(i\\)-th RBF node, respectively, and \\(R^+\\) indicates the set of all positive real values.\n",
    "\n",
    "In other words, the output function of an SLFN with \\(d\\) additive nodes and \\(d\\) RBF nodes can be, respectively, given by:\n",
    "\n",
    "1. For additive nodes:\n",
    "   $$\n",
    "   f_d(x) = \\sum_{i=1}^{d} \\beta_i g\\left(w_i^T x + b_i\\right) \\in \\mathbb{R}\n",
    "   $$\n",
    "\n",
    "2. For RBF nodes:\n",
    "   $$\n",
    "   f_d(x) = \\sum_{i=1}^{d} \\beta_i g\\left(\\frac{\\|x - b_i\\|}{a_i}\\right), \\quad a_i \\in \\mathbb{R}^n \\in \\mathbb{R}\n",
    "   $$\n",
    "\n",
    "### Theorems\n",
    "\n",
    "**Theorem 7.2 ([70]):** Suppose we are given a standard SLFN with \\(N\\) hidden nodes and activation function \\(g: \\mathbb{R} \\to \\mathbb{R}\\) which is infinitely differentiable in any interval. For \\(N\\) arbitrary distinct samples \\((x_i, y_i)\\), \\(i = 1, \\ldots, N\\), where \\(x_i \\in \\mathbb{R}^n\\) and \\(y_i \\in \\mathbb{R}^m\\), then for any \\(w_i\\) and \\(b_i\\) randomly chosen, respectively, from any intervals of \\(\\mathbb{R}^n\\) and \\(\\mathbb{R}\\) according to any continuous probability distribution, the hidden layer output matrix \\(H\\) of the SLFN, with probability one, is invertible and \\(\\|HB - Y\\| = 0\\).\n",
    "\n",
    "**Theorem 7.3 ([70]):** If we are given any small positive value \\(\\epsilon > 0\\) and activation function \\(g: \\mathbb{R} \\to \\mathbb{R}\\) which is infinitely differentiable in any interval, and there exists \\(d \\leq N\\) such that for \\(N\\) arbitrary distinct samples \\(x_1, \\ldots, x_N\\) with \\(x_i \\in \\mathbb{R}^n\\) and \\(y_i \\in \\mathbb{R}^m\\), for any \\(w_i\\) and \\(b_i\\) randomly chosen, respectively, from any intervals of \\(\\mathbb{R}^n\\) and \\(\\mathbb{R}\\) according to any continuous probability distribution, then with probability one \\(\\|H_{N \\times d} B_{d \\times m} - Y_{N \\times m}\\| < \\epsilon\\).\n",
    "\n",
    "Infinitely differentiable activation functions include the sigmoidal functions as well as the radial basis, sine, cosine, exponential, and many other nonregular functions, as shown by Huang and Babri [69].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8836b829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.3320863 ]\n",
      " [0.56467043]\n",
      " [0.61434423]\n",
      " [0.86048069]\n",
      " [0.62249718]\n",
      " [0.43570646]\n",
      " [0.45177932]\n",
      " [0.54242468]\n",
      " [0.49766405]\n",
      " [0.53969698]\n",
      " [0.4911353 ]\n",
      " [0.38984049]\n",
      " [0.84046841]\n",
      " [0.50098494]\n",
      " [0.3539762 ]\n",
      " [0.57933475]\n",
      " [0.42608073]\n",
      " [0.49432112]\n",
      " [0.41588136]\n",
      " [0.50363753]\n",
      " [0.49464266]\n",
      " [0.39507703]\n",
      " [0.26448625]\n",
      " [0.53011184]\n",
      " [0.37992645]\n",
      " [0.48581851]\n",
      " [0.44260452]\n",
      " [0.37835892]\n",
      " [0.4876137 ]\n",
      " [0.5558591 ]\n",
      " [0.42615518]\n",
      " [0.52496465]\n",
      " [0.54480534]\n",
      " [0.53305071]\n",
      " [0.55871666]\n",
      " [0.43328693]\n",
      " [0.51952578]\n",
      " [0.42249565]\n",
      " [0.55568979]\n",
      " [0.56492972]\n",
      " [0.44802283]\n",
      " [0.5937965 ]\n",
      " [0.76035292]\n",
      " [0.46304483]\n",
      " [0.5056868 ]\n",
      " [0.39306417]\n",
      " [0.75910231]\n",
      " [0.41467363]\n",
      " [0.67028621]\n",
      " [0.37284126]\n",
      " [0.52823741]\n",
      " [0.44631063]\n",
      " [0.55532268]\n",
      " [0.62182567]\n",
      " [0.56997007]\n",
      " [0.32093247]\n",
      " [0.28885814]\n",
      " [0.40313601]\n",
      " [0.69415569]\n",
      " [0.78958184]\n",
      " [0.63326574]\n",
      " [0.55571037]\n",
      " [0.61334342]\n",
      " [0.57922728]\n",
      " [0.50387058]\n",
      " [0.28639762]\n",
      " [0.50246076]\n",
      " [0.4734542 ]\n",
      " [0.60367878]\n",
      " [0.62678803]\n",
      " [0.4952507 ]\n",
      " [0.50494657]\n",
      " [0.4635662 ]\n",
      " [0.50108343]\n",
      " [0.39942063]\n",
      " [0.44297375]\n",
      " [0.35236075]\n",
      " [0.4651397 ]\n",
      " [0.45539077]\n",
      " [0.89986789]\n",
      " [0.47027125]\n",
      " [0.49181115]\n",
      " [0.4852094 ]\n",
      " [0.40760444]\n",
      " [0.52591359]\n",
      " [0.42159732]\n",
      " [0.41810862]\n",
      " [0.40460009]\n",
      " [0.3148939 ]\n",
      " [0.70564334]\n",
      " [0.44928871]\n",
      " [0.5358546 ]\n",
      " [0.4465465 ]\n",
      " [0.45855092]\n",
      " [0.53733074]\n",
      " [0.72222   ]\n",
      " [0.53813534]\n",
      " [0.56055096]\n",
      " [0.41398486]\n",
      " [0.40898001]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SLFNAdditive:\n",
    "    def __init__(self, input_dim, hidden_nodes):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.beta = None\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "\n",
    "    def _activation_function(self, x):\n",
    "        # Sigmoid activation function\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Randomly initialize weights and biases\n",
    "        self.W = np.random.randn(self.hidden_nodes, self.input_dim)\n",
    "        self.b = np.random.randn(self.hidden_nodes)\n",
    "        \n",
    "        # Compute hidden layer output matrix H\n",
    "        H = self._activation_function(X @ self.W.T + self.b)\n",
    "        \n",
    "        # Compute output weights beta\n",
    "        self.beta = np.linalg.pinv(H) @ y\n",
    "\n",
    "    def predict(self, X):\n",
    "        H = self._activation_function(X @ self.W.T + self.b)\n",
    "        return H @ self.beta\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Create synthetic data\n",
    "    np.random.seed(0)\n",
    "    X = np.random.rand(100, 5)\n",
    "    y = np.random.rand(100, 1)\n",
    "    \n",
    "    model = SLFNAdditive(input_dim=5, hidden_nodes=10)\n",
    "    model.fit(X, y)\n",
    "    predictions = model.predict(X)\n",
    "    print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e843358b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.54733402]\n",
      " [0.56880901]\n",
      " [0.54205627]\n",
      " [0.57327753]\n",
      " [0.4468781 ]\n",
      " [0.56994786]\n",
      " [0.46543569]\n",
      " [0.52951433]\n",
      " [0.47680798]\n",
      " [0.6886485 ]\n",
      " [0.52394805]\n",
      " [0.45211762]\n",
      " [0.44195841]\n",
      " [0.40600525]\n",
      " [0.29348904]\n",
      " [0.37435315]\n",
      " [0.53798543]\n",
      " [0.51448145]\n",
      " [0.48200389]\n",
      " [0.37903319]\n",
      " [0.56579035]\n",
      " [0.42729645]\n",
      " [0.4134629 ]\n",
      " [0.56745583]\n",
      " [0.53069523]\n",
      " [0.63515795]\n",
      " [0.60664738]\n",
      " [0.5832641 ]\n",
      " [0.36590251]\n",
      " [0.38090445]\n",
      " [0.36955207]\n",
      " [0.53714864]\n",
      " [0.43674267]\n",
      " [0.63265826]\n",
      " [0.59630768]\n",
      " [0.54618567]\n",
      " [0.46356991]\n",
      " [0.5953703 ]\n",
      " [0.52944968]\n",
      " [0.62476833]\n",
      " [0.44399545]\n",
      " [0.53607809]\n",
      " [0.74766782]\n",
      " [0.50029257]\n",
      " [0.44934862]\n",
      " [0.28730394]\n",
      " [0.48350463]\n",
      " [0.58831402]\n",
      " [0.57628978]\n",
      " [0.52093793]\n",
      " [0.56855986]\n",
      " [0.58275491]\n",
      " [0.48551603]\n",
      " [0.62234231]\n",
      " [0.3439165 ]\n",
      " [0.20985124]\n",
      " [0.36293891]\n",
      " [0.44487683]\n",
      " [0.53068413]\n",
      " [0.6024756 ]\n",
      " [0.49205961]\n",
      " [0.44346988]\n",
      " [0.6559669 ]\n",
      " [0.6412266 ]\n",
      " [0.52984572]\n",
      " [0.51889914]\n",
      " [0.44464269]\n",
      " [0.66143614]\n",
      " [0.52972259]\n",
      " [0.54077514]\n",
      " [0.63735679]\n",
      " [0.5405922 ]\n",
      " [0.46987302]\n",
      " [0.53426077]\n",
      " [0.62556392]\n",
      " [0.53955737]\n",
      " [0.34049192]\n",
      " [0.41261275]\n",
      " [0.41848086]\n",
      " [0.72725635]\n",
      " [0.39834574]\n",
      " [0.58391895]\n",
      " [0.35902187]\n",
      " [0.52998166]\n",
      " [0.28665217]\n",
      " [0.36770282]\n",
      " [0.52104945]\n",
      " [0.56448484]\n",
      " [0.37688623]\n",
      " [0.70225461]\n",
      " [0.30296184]\n",
      " [0.47294183]\n",
      " [0.56395044]\n",
      " [0.51550761]\n",
      " [0.50099052]\n",
      " [0.50200013]\n",
      " [0.41355673]\n",
      " [0.56992996]\n",
      " [0.54685401]\n",
      " [0.53577336]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SLFNRBF:\n",
    "    def __init__(self, input_dim, hidden_nodes):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.beta = None\n",
    "        self.centers = None\n",
    "        self.sigma = None\n",
    "\n",
    "    def _rbf_function(self, X, centers, sigma):\n",
    "        # Radial Basis Function (RBF) Gaussian Kernel\n",
    "        return np.exp(-np.linalg.norm(X[:, np.newaxis] - centers, axis=2) ** 2 / (2 * sigma ** 2))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Randomly initialize centers and sigma\n",
    "        self.centers = X[np.random.choice(X.shape[0], self.hidden_nodes, replace=False)]\n",
    "        self.sigma = np.mean(np.linalg.norm(X[:, np.newaxis] - self.centers, axis=2))\n",
    "        \n",
    "        # Compute hidden layer output matrix H\n",
    "        H = self._rbf_function(X, self.centers, self.sigma)\n",
    "        \n",
    "        # Compute output weights beta\n",
    "        self.beta = np.linalg.pinv(H) @ y\n",
    "\n",
    "    def predict(self, X):\n",
    "        H = self._rbf_function(X, self.centers, self.sigma)\n",
    "        return H @ self.beta\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Create synthetic data\n",
    "    np.random.seed(0)\n",
    "    X = np.random.rand(100, 5)\n",
    "    y = np.random.rand(100, 1)\n",
    "    \n",
    "    model = SLFNRBF(input_dim=5, hidden_nodes=10)\n",
    "    model.fit(X, y)\n",
    "    predictions = model.predict(X)\n",
    "    print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea13f9fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
