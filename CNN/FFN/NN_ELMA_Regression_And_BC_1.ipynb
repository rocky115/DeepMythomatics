{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ad0400",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " * Copyright (c) 2018 Radhamadhab Dalai\n",
    " *\n",
    " * Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    " * of this software and associated documentation files (the \"Software\"), to deal\n",
    " * in the Software without restriction, including without limitation the rights\n",
    " * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    " * copies of the Software, and to permit persons to whom the Software is\n",
    " * furnished to do so, subject to the following conditions:\n",
    " *\n",
    " * The above copyright notice and this permission notice shall be included in\n",
    " * all copies or substantial portions of the Software.\n",
    " *\n",
    " * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    " * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    " * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    " * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    " * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    " * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
    " * THE SOFTWARE.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcb95ce",
   "metadata": {},
   "source": [
    "## Extreme learning machine (ELM) algorithm\n",
    "The offline solution of (7.11.7) is given by\n",
    "\n",
    "$$\n",
    "B = H^\\dagger Y,\n",
    "$$\n",
    "\n",
    "(7.11.11)\n",
    "\n",
    "where \\( H^\\dagger \\) is the Mooreâ€“Penrose generalized inverse of the hidden layer output matrix \\( H \\).\n",
    "\n",
    "Based on the minimum norm LS solution \\( B = H^\\dagger Y \\), Huang et al. [70] proposed an extreme learning machine (ELM) as a simple learning algorithm for SLFNs, as shown in Algorithm 7.5.\n",
    "\n",
    "**Algorithm 7.5 Extreme learning machine (ELM) algorithm**\n",
    "\n",
    "1. **Input**: A training set \\( \\{(x_i , y_i )|x_i \\in \\mathbb{R}^n , y_i \\in \\mathbb{R}^m , i = 1, \\ldots , N \\} \\), activation function \\( g(x) \\), and hidden node number \\( d \\).\n",
    "\n",
    "2. **Initialization**: Randomly assign input weight \\( w_i \\) and bias \\( b_i \\), \\( i = 1, \\ldots , d \\).\n",
    "\n",
    "3. **Learning step**:\n",
    "\n",
    "   3.1. Use (7.11.8) to calculate the hidden layer output matrix \\( H \\).\n",
    "\n",
    "   3.2. Use (7.11.10) to construct the training output matrix \\( Y \\).\n",
    "\n",
    "   3.3. Calculate the minimum norm least squares weight matrix \\( B = H^\\dagger Y \\), and get \\( \\beta_i \\) from \\( B^T = [\\beta_1, \\ldots , \\beta_d] \\).\n",
    "\n",
    "4. **Testing step**: For given testing sample \\( x \\in \\mathbb{R}^n \\), the output of SLFNs is given by\n",
    "\n",
    "$$\n",
    "y = \\sum_{i=1}^d \\beta_i g(w_i^T x + b_i).\n",
    "$$\n",
    "\n",
    "It is shown [71] that the ELM can use a wide type of feature mappings (hidden layer output functions), including random hidden nodes and kernels. With this extension, the unified ELM solution can be obtained for feedforward neural networks, RBF network, LS-SVM, and PSVM.\n",
    "\n",
    "In ELM, the hidden layer need not be tuned. For one output node case, the output function of ELM for generalized SLFNs is given by\n",
    "\n",
    "$$\n",
    "f_d (x) = \\sum_{j=1}^d \\beta_j h_j (x) = h^T (x) \\beta,\n",
    "$$\n",
    "\n",
    "(7.11.12)\n",
    "\n",
    "where \\( h(x) = [h_1 (x), \\ldots , h_d (x)]^T \\) is the output vector of the hidden layer with respect to the input \\( x \\), and \\( \\beta = [\\beta_1 , \\ldots , \\beta_d ]^T \\) is the vector of the output weights between the hidden layer of \\( d \\) nodes.\n",
    "\n",
    "The output vector \\( h(x) \\) is a feature mapping: it actually maps the data from the \\( n \\)-dimensional input space to the \\( d \\)-dimensional hidden layer feature space (ELM feature space) \\( H \\). For the binary classification problem, the decision function of ELM is\n",
    "\n",
    "$$\n",
    "f_d (x) = \\text{sign} (h^T (x) \\beta).\n",
    "$$\n",
    "\n",
    "(7.11.13)\n",
    "\n",
    "An ELM with a single-output node is used to generate \\( N \\) output samples:\n",
    "\n",
    "$$\n",
    "\\sum_{j=1}^d \\beta_j h_j (x_i ) = y_i ,\n",
    "$$\n",
    "\n",
    "which can be rewritten as the form\n",
    "\n",
    "$$\n",
    "h^T (x_i ) \\beta = y_i , \\quad i = 1, \\ldots , N\n",
    "$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\n",
    "H \\beta = y,\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "H = \\begin{bmatrix}\n",
    "h_1 (x_1 ) & \\cdots & h_d (x_1) \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "h_1 (x_N ) & \\cdots & h_d (x_N )\n",
    "\\end{bmatrix},\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\beta = \\begin{bmatrix}\n",
    "\\beta_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\beta_d\n",
    "\\end{bmatrix},\n",
    "$$\n",
    "\n",
    "$$\n",
    "y = \\begin{bmatrix}\n",
    "y_1 \\\\\n",
    "\\vdots \\\\\n",
    "y_N\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "h^T (x_1) \\\\\n",
    "\\vdots \\\\\n",
    "h^T (x_N)\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "The constrained optimization problem for ELM regression and binary classification with a single-output node can be formulated as [71]:\n",
    "\n",
    "$$\n",
    "\\min_{\\beta, \\xi_i} L_{PELM} = \\frac{1}{2} \\|\\beta\\|_2^2 + \\frac{C}{2} \\sum_{i=1}^N \\xi_i^2,\n",
    "$$\n",
    "\n",
    "subject to \\( h^T (x_i ) \\beta = y_i - \\xi_i, \\quad i = 1, \\ldots , N. \\)\n",
    "\n",
    "The dual unconstrained optimization problem is\n",
    "\n",
    "$$\n",
    "\\min_{\\beta, \\xi_i, \\alpha_i} L_{DELM} (\\beta, \\xi_i, \\alpha_i) = \\frac{1}{2} \\|\\beta\\|_2^2 + \\frac{C}{2} \\sum_{i=1}^N \\xi_i^2 - \\sum_{i=1}^N \\alpha_i (h^T (x_i) \\beta - y_i + \\xi_i)\n",
    "$$\n",
    "\n",
    "with the Lagrange multipliers \\( \\alpha_i \\geq 0, \\quad i = 1, \\ldots , N. \\)\n",
    "\n",
    "From the optimality conditions one has\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L_{DELM}}{\\partial \\beta} = 0 \\Rightarrow\n",
    "$$\n",
    "\n",
    "where \\( \\alpha = [\\alpha_1, \\ldots , \\alpha_N]^T. \\)\n",
    "\n",
    "$$\n",
    "\\beta = \\sum_{i=1}^N \\alpha_i h(x_i) = H^T \\alpha = H^T \\alpha,\n",
    "$$\n",
    "\n",
    "which can be rewritten as the form\n",
    "\n",
    "$$\n",
    "h^T (x_i ) \\beta = y_i , \\quad i = 1, \\ldots , N\n",
    "$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\n",
    "H \\beta = y,\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "H = \\begin{bmatrix}\n",
    "h_1 (x_1 ) & \\cdots & h_d (x_1) \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "h_1 (x_N ) & \\cdots & h_d (x_N )\n",
    "\\end{bmatrix},\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\beta = \\begin{bmatrix}\n",
    "\\beta_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\beta_d\n",
    "\\end{bmatrix},\n",
    "$$\n",
    "\n",
    "$$\n",
    "y = \\begin{bmatrix}\n",
    "y_1 \\\\\n",
    "\\vdots \\\\\n",
    "y_N\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "h^T (x_1) \\\\\n",
    "\\vdots \\\\\n",
    "h^T (x_N)\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "The constrained optimization problem for ELM regression and binary classification with a single-output node can be formulated as [71]:\n",
    "\n",
    "$$\n",
    "\\min_{\\beta, \\xi_i} L_{PELM} = \\frac{1}{2} \\|\\beta\\|_2^2 + \\frac{C}{2} \\sum_{i=1}^N \\xi_i^2,\n",
    "$$\n",
    "\n",
    "subject to \\( h^T (x_i ) \\beta = y_i - \\xi_i, \\quad i = 1, \\ldots , N. \\)\n",
    "\n",
    "The dual unconstrained optimization problem is\n",
    "\n",
    "$$\n",
    "\\min_{\\beta, \\xi_i, \\alpha_i} L_{DELM} (\\beta, \\xi_i, \\alpha_i) = \\frac{1}{2} \\|\\beta\\|_2^2 + \\frac{C}{2} \\sum_{i=1}^N \\xi_i^2 - \\sum_{i=1}^N \\alpha_i (h^T (x_i) \\beta - y_i + \\xi_i)\n",
    "$$\n",
    "\n",
    "with the Lagrange multipliers \\( \\alpha_i \\geq 0, \\quad i = 1, \\ldots , N. \\)\n",
    "\n",
    "From the optimality conditions one has\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L_{DELM}}{\\partial \\beta} = 0 \\Rightarrow\n",
    "$$\n",
    "\n",
    "where \\( \\alpha = [\\alpha_1, \\ldots , \\alpha_N]^T. \\)\n",
    "\n",
    "$$\n",
    "\\beta = \\sum_{i=1}^N \\alpha_i h(x_i) = H^T \\alpha = H^T \\alpha,\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73078271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:\n",
      " [[-0.18704703]\n",
      " [ 0.22153287]\n",
      " [ 0.27996201]\n",
      " [ 0.77352065]\n",
      " [ 0.4413394 ]\n",
      " [ 0.46376024]\n",
      " [ 0.0402454 ]\n",
      " [ 0.53954023]\n",
      " [ 0.29939952]\n",
      " [ 0.52385948]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import pinv\n",
    "\n",
    "class ExtremeLearningMachine:\n",
    "    def __init__(self, n_hidden_units, activation_function):\n",
    "        self.n_hidden_units = n_hidden_units\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "    def _add_bias(self, X):\n",
    "        # Add bias term to input data\n",
    "        bias = np.ones((X.shape[0], 1))\n",
    "        return np.hstack([X, bias])\n",
    "\n",
    "    def _init_weights(self, n_features):\n",
    "        # Initialize input weights and biases randomly\n",
    "        self.input_weights = np.random.randn(n_features, self.n_hidden_units)\n",
    "        self.biases = np.random.randn(1, self.n_hidden_units)\n",
    "\n",
    "    def _compute_hidden_layer_output(self, X):\n",
    "        # Compute hidden layer output\n",
    "        Z = np.dot(X, self.input_weights) + self.biases\n",
    "        H = self.activation_function(Z)\n",
    "        return H\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        # Add bias term to input data\n",
    "        X_train = self._add_bias(X_train)\n",
    "        \n",
    "        # Initialize weights\n",
    "        n_features = X_train.shape[1]\n",
    "        self._init_weights(n_features)\n",
    "        \n",
    "        # Compute hidden layer output\n",
    "        H = self._compute_hidden_layer_output(X_train)\n",
    "        \n",
    "        # Compute output weights using Moore-Penrose pseudoinverse\n",
    "        H_pinv = pinv(H)\n",
    "        self.output_weights = np.dot(H_pinv, y_train)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        # Add bias term to input data\n",
    "        X_test = self._add_bias(X_test)\n",
    "        \n",
    "        # Compute hidden layer output\n",
    "        H = self._compute_hidden_layer_output(X_test)\n",
    "        \n",
    "        # Compute output\n",
    "        y_pred = np.dot(H, self.output_weights)\n",
    "        return y_pred\n",
    "\n",
    "# Activation function: Sigmoid\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate some random data for demonstration\n",
    "    np.random.seed(0)\n",
    "    X_train = np.random.rand(100, 10)\n",
    "    y_train = np.random.rand(100, 1)\n",
    "    X_test = np.random.rand(10, 10)\n",
    "\n",
    "    # Create ELM instance\n",
    "    elm = ExtremeLearningMachine(n_hidden_units=50, activation_function=sigmoid)\n",
    "\n",
    "    # Train ELM\n",
    "    elm.fit(X_train, y_train)\n",
    "\n",
    "    # Predict with ELM\n",
    "    y_pred = elm.predict(X_test)\n",
    "    print(\"Predictions:\\n\", y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d32130",
   "metadata": {},
   "source": [
    "Substituting (7.11.22) into (7.11.15) and using (7.11.18), we get the equation\n",
    "$$\n",
    "H^T H \\alpha = y,\n",
    "$$\n",
    "(7.11.23)\n",
    "where\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "h^T(x_1) \\\\\n",
    "\\vdots \\\\\n",
    "h^T(x_N)\n",
    "\\end{bmatrix}^T\n",
    "H = \\begin{bmatrix}\n",
    "h(x_1), \\ldots, h(x_N)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "H^T H = \\begin{bmatrix}\n",
    "h^T(x_1)h(x_1) & \\cdots & h^T(x_1)h(x_N) \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "h^T(x_N)h(x_1) & \\cdots & h^T(x_N)h(x_N)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "(7.11.24)\n",
    "The LS solution of Eq. (7.11.23) is given by\n",
    "$$\n",
    "\\alpha = (H^T H)^\\dagger y.\n",
    "$$\n",
    "(7.11.25)\n",
    "If a feature mapping $h(x)$ is unknown to users, one can apply Mercer's conditions\n",
    "on ELM, and hence a kernel matrix for ELM can be defined as follows [71]:\n",
    "$$\n",
    "K(x, x_i) = h^T(x)h(x_i).\n",
    "$$\n",
    "(7.11.26)\n",
    "If using (7.11.26), then (7.11.24) can be rewritten as in the following kernel form:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "K(x_1, x_1) & \\cdots & K(x_1, x_N) \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "K(x_N, x_1) & \\cdots & K(x_N, x_N)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "(7.11.27)\n",
    "whose $(i, j)$th entry is $H^T H_{ij} = K(x_i, x_j)$ for $i = 1, \\ldots, N; j = 1, \\ldots, N$.\n",
    "This shows that the feature mapping $h(x)$ need not be used; instead, it is enough\n",
    "to use the corresponding kernel $K(x, x_i)$, e.g., $K(x, x_i) = \\exp(-\\gamma \\|x - x_i\\|)$.\n",
    "Finally, from (7.11.22) it follows that the EML regression function is\n",
    "$$\n",
    "\\hat{y} = h^T(x) \\beta = h^T(x) \\sum_{i=1}^N \\alpha_i h(x_i) = \\sum_{i=1}^N \\alpha_i K(x, x_i),\n",
    "$$\n",
    "while the decision function for EML binary classification is\n",
    "$$\n",
    "\\text{class of } x = \\text{sign} \\left( \\sum_{i=1}^N \\alpha_i K(x, x_i) \\right).\n",
    "$$\n",
    "(7.11.28)\n",
    "(7.11.29)\n",
    "Algorithm 7.6 shows the ELM algorithm for regression and binary classification.\n",
    "Algorithm 7.6 ELM algorithm for regression and binary classification [71]\n",
    "\n",
    "1. **Input**: A training set $\\{(x_i, y_i) | x_i \\in \\mathbb{R}^n, y_i \\in \\mathbb{R}, i = 1, \\ldots, N\\}$, hidden node number $d$ and the kernel function $K(u, v) = \\exp(-\\gamma \\|u - v\\|)$.\n",
    "2. **Initialization**: $y = [y_1, \\ldots, y_N]^T$.\n",
    "3. **Learning step**:\n",
    "    1. Use the kernel function to construct the matrix $H^T H_{ij} = K(x_i, x_j)$, $i, j = 1, \\ldots, N$.\n",
    "    2. Calculate the minimum norm least squares solution $\\alpha = (H^T H)^\\dagger y$.\n",
    "4. **Testing step**: For given testing sample $x \\in \\mathbb{R}^n$, the ELM regression is $\\sum_{i=1}^N \\alpha_i K(x, x_i)$, while the ELM binary classification is $\\text{class of } x = \\text{sign} \\left( \\sum_{i=1}^N \\alpha_i K(x, x_i) \\right)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a340e25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:\n",
      " [[ 0.3175779 ]\n",
      " [ 0.40938498]\n",
      " [ 0.28007594]\n",
      " [ 0.84347293]\n",
      " [ 0.50490296]\n",
      " [ 0.51523179]\n",
      " [-0.0439662 ]\n",
      " [ 0.67854794]\n",
      " [ 0.24910511]\n",
      " [ 0.01155326]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import pinv\n",
    "\n",
    "class ELM:\n",
    "    def __init__(self, n_hidden_units, kernel_function):\n",
    "        self.n_hidden_units = n_hidden_units\n",
    "        self.kernel_function = kernel_function\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        N = X_train.shape[0]\n",
    "        \n",
    "        # Construct the kernel matrix\n",
    "        H = np.zeros((N, N))\n",
    "        for i in range(N):\n",
    "            for j in range(N):\n",
    "                H[i, j] = self.kernel_function(X_train[i], X_train[j])\n",
    "        \n",
    "        # Calculate the minimum norm least squares solution\n",
    "        H_pinv = pinv(H)\n",
    "        self.alpha = np.dot(H_pinv, y_train)\n",
    "        \n",
    "        self.X_train = X_train\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        N = self.X_train.shape[0]\n",
    "        M = X_test.shape[0]\n",
    "        \n",
    "        # Compute the kernel matrix between test data and training data\n",
    "        K = np.zeros((M, N))\n",
    "        for i in range(M):\n",
    "            for j in range(N):\n",
    "                K[i, j] = self.kernel_function(X_test[i], self.X_train[j])\n",
    "        \n",
    "        # Compute the predictions\n",
    "        y_pred = np.dot(K, self.alpha)\n",
    "        return y_pred\n",
    "\n",
    "# Example kernel function: RBF (Gaussian)\n",
    "def rbf_kernel(x, y, gamma=1.0):\n",
    "    return np.exp(-gamma * np.linalg.norm(x - y) ** 2)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate some random data for demonstration\n",
    "    np.random.seed(0)\n",
    "    X_train = np.random.rand(100, 10)\n",
    "    y_train = np.random.rand(100, 1)\n",
    "    X_test = np.random.rand(10, 10)\n",
    "\n",
    "    # Create ELM instance\n",
    "    elm = ELM(n_hidden_units=50, kernel_function=rbf_kernel)\n",
    "\n",
    "    # Train ELM\n",
    "    elm.fit(X_train, y_train)\n",
    "\n",
    "    # Predict with ELM\n",
    "    y_pred = elm.predict(X_test)\n",
    "    print(\"Predictions:\\n\", y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb93c79",
   "metadata": {},
   "source": [
    "## Extreme Learning Machine Algorithm for Multiclass Classification\n",
    "For multiclass applications, let ELM have multi-output nodes instead of a single-output node. For a \\(k\\)-class classifier, we are given \\(N\\) training set \\(\\{(x_i, y_i) | x_i \\in \\mathbb{R}^n, y_i \\in \\mathbb{R}^d\\}, i = 1, \\ldots, N\\). If the original class label is \\(p\\), then the \\(j\\)-th entry of the expected output vector of the \\(d\\) output nodes, \\(y_i = [y_{i1}, \\ldots, y_{id}]^T \\in \\mathbb{R}^d\\), is denoted as\n",
    "\n",
    "$$\n",
    "y_{ij} =\n",
    "\\begin{cases} \n",
    "1, & \\text{if } j = p, \\\\\n",
    "-1, & \\text{if } j \\neq p,\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where \\(p \\in \\{1, \\ldots, k\\}\\). That is, only the \\(p\\)-th entry of \\(y_i\\) is +1, while the rest of the entries are set to -1.\n",
    "\n",
    "The primal classification problem of the \\(m\\)-th class for ELM with multi-output nodes can be formulated as [71]:\n",
    "\n",
    "$$\n",
    "\\min_{\\beta^m, b^m, \\xi_{m,i}} \\mathcal{L}_{\\text{PELM}} =\n",
    "\\frac{1}{2} \\|\\beta^m\\|_2^2 + b^m +\n",
    "\\frac{C}{2} \\sum_{i=1}^{N} \\xi_{m,i}^2\n",
    "$$\n",
    "\n",
    "subject to\n",
    "\n",
    "$$\n",
    "\\begin{cases} \n",
    "(h^m(x_1))^T \\beta^m + b^m = 1 - \\xi_{m,1}, \\\\\n",
    "\\vdots \\\\\n",
    "(h^m(x_N))^T \\beta^m + b^m = 1 - \\xi_{m,N}.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The corresponding dual unconstrained optimization problem is given by\n",
    "\n",
    "$$\n",
    "\\min_{\\beta^m, b^m, \\xi_{m,i}, \\alpha_{m,i}} \\mathcal{L}_{\\text{DELM}} =\n",
    "\\frac{1}{2} \\|\\beta^m\\|_2^2 + b^m +\n",
    "\\frac{C}{2} \\sum_{i=1}^{N} \\xi_{m,i}^2 -\n",
    "\\sum_{i=1}^{N} \\alpha_{m,i} y_i^{(m)} \\left( (h^m(x_i))^T \\beta^m + b^m - 1 + \\xi_{m,i} \\right)\n",
    "$$\n",
    "\n",
    "From the optimality conditions one has\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}_{\\text{DELM}}}{\\partial \\beta^m} = 0 \\Rightarrow \\beta^m = \\sum_{i=1}^{N} \\alpha_{m,i} y_i^{(m)} h^m(x_i),\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}_{\\text{DELM}}}{\\partial b^m} = 0 \\Rightarrow b^m = \\sum_{i=1}^{N} \\alpha_{m,i} y_i^{(m)},\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}_{\\text{DELM}}}{\\partial \\xi_{m,i}} = 0 \\Rightarrow \\xi_{m,i} = C^{-1} \\alpha_{m,i},\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}_{\\text{DELM}}}{\\partial \\alpha_{m,i}} = 0 \\Rightarrow y_i^{(m)} \\left( (h^m(x_i))^T \\beta^m + b^m - 1 + \\xi_{m,i} \\right) = 0,\n",
    "$$\n",
    "\n",
    "for \\(i = 1, \\ldots, N\\).\n",
    "\n",
    "Eliminating \\(\\beta^m\\) and \\(\\xi_{m,i}\\) in the above equations yields the KKT equation\n",
    "\n",
    "$$\n",
    "b^m = \\sum_{i=1}^{N} \\alpha_{m,i} y_i^{(m)} = (\\alpha^m)^T y^m\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\left( C^{-1} I + H^m (H^m)^T + y^m (y^m)^T \\right) \\alpha^m = 1,\n",
    "$$\n",
    "\n",
    "where \\(I\\) is an \\(N \\times N\\) identity matrix, \\(1\\) is an \\(N \\times 1\\) summing vector with all entries equal to 1, and\n",
    "\n",
    "$$\n",
    "H^m = \\left[ y_1^{(m)} h^m(x_1), \\ldots, y_N^{(m)} h^m(x_N) \\right],\n",
    "$$\n",
    "\n",
    "$$\n",
    "y^m = \\left[ y_1^{(m)}, \\ldots, y_N^{(m)} \\right]^T,\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\alpha^m = \\left[ \\alpha_{m,1}, \\ldots, \\alpha_{m,N} \\right]^T.\n",
    "$$\n",
    "\n",
    "It easily follows that the \\((i,j)\\)-th entry of the matrix \\(H^m (H^m)^T\\) can be represented as\n",
    "\n",
    "$$\n",
    "\\left( H^m (H^m)^T \\right)_{ij} = y_i^{(m)} y_j^{(m)} (h^m(x_i))^T h^m(x_j) = y_i^{(m)} y_j^{(m)} K^m(x_i, x_j),\n",
    "$$\n",
    "\n",
    "where \\(K^m(x_i, x_j) = (h^m(x_i))^T h^m(x_j)\\) is the kernel function for the \\(m\\)-th ELM classifier.\n",
    "\n",
    "Algorithm 7.7 shows the ELM multiclass classification algorithm.\n",
    "\n",
    "### Algorithm 7.7 ELM multiclass classification algorithm [71]\n",
    "\n",
    "1. **Input:** A training set \\(\\{(x_i, y_i) | x_i \\in \\mathbb{R}^n, y_i \\in \\mathbb{R}^d, i = 1, \\ldots, N\\}\\), hidden node number \\(d\\), the number \\(k\\) of classes and the kernel function \\(K^m(u, v)\\) for the \\(m\\)-th classifier \\(m = 1, \\ldots, k\\), such as \\(K^m(u, v) = \\exp(-\\gamma \\|u - v\\|^2)\\).\n",
    "2. **Initialization:** Reconstruct the \\(m\\)-th classâ€™s output vector \\(y^{(m)} = \\left[ y_1^{(m)}, \\ldots, y_N^{(m)} \\right]^T\\) with\n",
    "   $$\n",
    "   y_i^{(m)} =\n",
    "   \\begin{cases} \n",
    "   1, & \\text{if } y_i = m; \\\\\n",
    "   -1, & \\text{if } y_i \\neq m;\n",
    "   \\end{cases}\n",
    "   $$\n",
    "   for \\(m = 1, \\ldots, k\\); \\(i = 1, \\ldots, N\\).\n",
    "3. **Learning step:**\n",
    "   1. while \\(m = 1, \\ldots, k\\)\n",
    "   2. Use the kernel function \\(K^m(x_i, x_j)\\) and the \\(m\\)-th classâ€™s output vector \\(y^{(m)}\\) to construct the matrix \\((H^m)^T H^m\\) where\n",
    "      $$\n",
    "      \\left( (H^m)^T H^m \\right)_{ij} = y_i^{(m)} y_j^{(m)} K^m(x_i, x_j), \\quad i, j = 1, \\ldots, N.\n",
    "      $$\n",
    "   3. Calculate the minimum norm least square solution\n",
    "      $$\n",
    "      \\alpha^m = \\left( (H^m)^T H^m + y^m (y^m)^T + C^{-1} I \\right)^{-1} 1.\n",
    "      $$\n",
    "   4. Calculate\n",
    "      $$\n",
    "      b^m = (\\alpha^m)^T y^m.\n",
    "      $$\n",
    "   5. endwhile\n",
    "4. **Testing step:** For a given testing sample \\(x \\in \\mathbb{R}^n\\), the decision function of the ELM multiclass classifier is given by\n",
    "   $$\n",
    "   \\text{class of } x = \\arg \\max_{m=1, \\ldots, k} \\left( \\sum_{i=1}^{N} \\alpha_{m,i} y_i^{(m)} K^m(x, x_i) + b^m \\right).\n",
    "   $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6eb18fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class labels: [3 1 3 2 2 2 1 1 2 3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import solve\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "\n",
    "class ELM_Multiclass:\n",
    "    def __init__(self, hidden_nodes, C, gamma):\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.C = C\n",
    "        self.gamma = gamma\n",
    "        self.k = None\n",
    "        self.beta = None\n",
    "        self.b = None\n",
    "        self.X = None\n",
    "\n",
    "    def _construct_target_vector(self, y, m):\n",
    "        return np.where(y == m, 1, -1)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        N, n = X.shape\n",
    "        self.k = len(np.unique(y))\n",
    "        H = np.random.rand(self.hidden_nodes, n)\n",
    "        b = np.random.rand(self.hidden_nodes)\n",
    "\n",
    "        H = np.tanh(np.dot(X, H.T) + b)\n",
    "        \n",
    "        self.X = X  # Store the training data for use in the predict method\n",
    "        self.beta = []\n",
    "        self.b = []\n",
    "        \n",
    "        for m in range(1, self.k + 1):\n",
    "            y_m = self._construct_target_vector(y, m)\n",
    "            \n",
    "            K = rbf_kernel(X, X, gamma=self.gamma)\n",
    "            Hm = y_m[:, None] * K\n",
    "            \n",
    "            identity_matrix = np.eye(N)\n",
    "            one_vector = np.ones(N)\n",
    "            \n",
    "            HmHTm = np.dot(Hm, Hm.T)\n",
    "            A = HmHTm + np.outer(y_m, y_m) + (1 / self.C) * identity_matrix\n",
    "            alpha_m = solve(A, one_vector)\n",
    "            \n",
    "            beta_m = np.dot(Hm.T, alpha_m)\n",
    "            b_m = np.sum(alpha_m * y_m)\n",
    "            \n",
    "            self.beta.append(beta_m)\n",
    "            self.b.append(b_m)\n",
    "\n",
    "    def predict(self, X):\n",
    "        N = X.shape[0]\n",
    "        H = np.random.rand(self.hidden_nodes, X.shape[1])\n",
    "        b = np.random.rand(self.hidden_nodes)\n",
    "\n",
    "        H = np.tanh(np.dot(X, H.T) + b)\n",
    "        \n",
    "        scores = np.zeros((N, self.k))\n",
    "        \n",
    "        for m in range(1, self.k + 1):\n",
    "            beta_m = self.beta[m - 1]\n",
    "            b_m = self.b[m - 1]\n",
    "            K = rbf_kernel(X, self.X, gamma=self.gamma)\n",
    "            scores[:, m - 1] = np.dot(K, beta_m) + b_m\n",
    "        \n",
    "        return np.argmax(scores, axis=1) + 1  # Class labels start from 1\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Generating random data for demonstration\n",
    "    np.random.seed(0)\n",
    "    X_train = np.random.rand(100, 20)\n",
    "    y_train = np.random.randint(1, 4, size=100)  # 3 classes\n",
    "    X_test = np.random.rand(10, 20)\n",
    "    \n",
    "    elm = ELM_Multiclass(hidden_nodes=50, C=1.0, gamma=0.5)\n",
    "    elm.fit(X_train, y_train)\n",
    "    y_pred = elm.predict(X_test)\n",
    "    \n",
    "    print(\"Predicted class labels:\", y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77386fc9",
   "metadata": {},
   "source": [
    "We first introduce the definition of the basic concepts in graph embedding. Suppose we are given a graph \\( G = (V, E) \\), where \\( v \\in V \\) is a vertex or node and \\( e \\in E \\) is an edge. \\( G \\) is associated with a node type mapping function \\( f_v: V \\to T_v \\) and an edge type mapping function \\( f_e: E \\to T_e \\), where \\( T_v \\) and \\( T_e \\) denote the set of node types and edge types, respectively. Each node \\( v_i \\in V \\) belongs to one particular type, i.e., \\( f_v(v_i) \\in T_v \\). Similarly, for \\( e_{ij} \\in E \\), \\( f_e(e_{ij}) \\in T_e \\).\n",
    "\n",
    "Graph learning is closely related to graph proximities and graph embedding. Graph learning tasks can be broadly abstracted into the following four categories:\n",
    "- **Node classification** aims at determining the label of nodes (a.k.a. vertices) based on other labeled nodes and the topology of the network.\n",
    "- **Link prediction** refers to the task of predicting missing links or links that are likely to occur in the future.\n",
    "- **Clustering** is used to find subsets of similar nodes and group them together.\n",
    "- **Visualization** helps in providing insights into the structure of the network.\n",
    "\n",
    "The most basic measure for both dimension reduction and structure preservation of a graph is the graph proximity. Proximity measures are usually adopted to quantify the graph property to be preserved in the embedded space.\n",
    "\n",
    "The microscopic structures of a graph can be described by its first-order proximity and second-order proximity. The first-order proximity between the vertices is their local pairwise similarity between only the nodes connected by edges.\n",
    "\n",
    "**Definition 7.10 (First-Order Proximity [144])** The first-order proximity is the observed pairwise proximity between two nodes \\( v_i \\) and \\( v_j \\), denoted as \\( S_{ij}^{(1)} = s_{ij} \\), where \\( s_{ij} \\) is the edge weight between the two nodes. If no edge is observed between nodes \\( i \\) and \\( j \\), then their first-order proximity \\( S_{ij}^{(1)} = 0 \\).\n",
    "\n",
    "The first-order proximity is the first and foremost measure of similarity between two nodes. The first-order proximity implies that two nodes in real-world networks are always similar if they are linked by an observed edge. For example, if a paper cites another paper, they should contain some common topic or keywords. However, it is not sufficient to only capture the first-order proximity, and it is also necessary to introduce the second-order proximity to capture the global network structure.\n",
    "\n",
    "**Definition 7.11 (Second-Order Proximity [144])** Let \\( \\mathbf{s}_i^{(1)} = [S_{i,1}^{(1)}, \\ldots, S_{i,n}^{(1)}]^T \\) and \\( \\mathbf{s}_i^{(2)} = [S_{i,1}^{(2)}, \\ldots, S_{i,n}^{(2)}]^T \\) be the first-order and the second-order proximity vectors between node \\( i \\) and other nodes, respectively. Then the second-order proximity \\( S_{ij}^{(2)} \\) is determined by the similarity of \\( \\mathbf{s}_i^{(1)} \\) and \\( \\mathbf{s}_j^{(1)} \\). If no vertex is linked from/to both \\( i \\) and \\( j \\), then the second-order proximity between \\( v_i \\) and \\( v_j \\) is zero, i.e., \\( S_{ij}^{(2)} = 0 \\).\n",
    "\n",
    "The second-order proximity \\( S_{ij}^{(2)} \\) is the similarity between \\( v_i \\)â€™s neighborhood \\( \\mathbf{s}_i^{(1)} \\) and \\( v_j \\)â€™s neighborhood \\( \\mathbf{s}_j^{(1)} \\).\n",
    "\n",
    "- The first-order proximity compares the similarity between the nodes \\( i \\) and \\( j \\). The more similar two nodes are, the larger the first-order proximity value between them.\n",
    "- The second-order proximity compares the similarity between the nodesâ€™ neighborhood structures. The more similar two nodesâ€™ neighborhoods are, the larger the second-order proximity value between them.\n",
    "\n",
    "Similarly, we can define the higher-order proximity \\( S_{ij}^{(k)} \\) (where \\( k \\geq 3 \\)) between a pair of vertices \\( (i, j) \\) in a graph.\n",
    "\n",
    "**Definition 7.12 (k-Order Proximity)** Let \\( \\mathbf{s}_i^{(k)} = [S_{i,1}^{(k)}, \\ldots, S_{i,n}^{(k)}]^T \\) be the \\( k \\)-th order proximity vector between node \\( i \\) and other nodes. Then the \\( k \\)-th order proximity \\( S_{ij}^{(k)} \\) is determined by the similarity of \\( \\mathbf{s}_i^{(k-1)} \\) and \\( \\mathbf{s}_j^{(k-1)} \\).\n",
    "\n",
    "In particular, when \\( k \\geq 3 \\), the \\( k \\)-th order proximity is generally referred to as the higher-order proximity. The matrix \\( S^{(k)} = [S_{ij}^{(k)}] \\) is known as the \\( k \\)-order proximity matrix. When \\( k \\geq 3 \\), \\( S^{(k)} \\) is called the higher-order proximity matrix. The higher-order proximity matrices are also defined using some other metrics, e.g., Katz Index, Rooted PageRank, Adamic-Adar, etc. that will be discussed in Sect. 7.13.3.\n",
    "\n",
    "By Definitions 7.10, 7.11, and 7.12, the first-order, second-order, and third-order proximity matrices \\( S^{(k)} = [S_{ij}^{(k)}] \\in \\mathbb{R}^{n \\times n} \\) (where \\( k = 1, 2, 3 \\)) are nonnegative matrices, respectively.\n",
    "\n",
    "If considering the cosine similarity as the \\( k \\)-order proximity, then for nodes \\( v_i \\) and \\( v_j \\), we have the following results:\n",
    "1. The first-order proximity\n",
    "$$\n",
    "S_{ij}^{(1)} = s_{ij}\n",
    "$$\n",
    "where \\( s_{ij} \\) is the edge weight between the two nodes.\n",
    "\n",
    "2. The second-order proximity\n",
    "$$\n",
    "S_{ij}^{(2)} = \\frac{\\mathbf{s}_i^{(1)} \\cdot \\mathbf{s}_j^{(1)}}{\\| \\mathbf{s}_i^{(1)} \\|_2 \\| \\mathbf{s}_j^{(1)} \\|_2} = \\frac{\\sum_{l=1}^{n} S_{i,l}^{(1)} S_{j,l}^{(1)}}{\\sqrt{\\sum_{l=1}^{n} (S_{i,l}^{(1)})^2} \\sqrt{\\sum_{l=1}^{n} (S_{j,l}^{(1)})^2}}\n",
    "$$\n",
    "In this way, the second-order proximity is between \\([0, 1]\\).\n",
    "\n",
    "3. The third-order proximity\n",
    "$$\n",
    "S_{ij}^{(3)} = \\frac{\\| \\mathbf{s}_i^{(2)} \\|_2 \\| \\mathbf{s}_j^{(2)} \\|_2}{\\sum_{l=1}^{n} (S_{i,l}^{(2)})^2 \\sum_{l=1}^{n} (S_{j,l}^{(2)})^2}\n",
    "$$\n",
    "\n",
    "**Definition 7.13 (Graph Embedding [14, 41])** Given the inputs of a graph \\( G = (V, E) \\), and a predefined dimensionality of the embedding \\( d \\) (\\( d \\ll |V| \\)), the graph embedding is to convert \\( G \\) into a \\( d \\)-dimensional space \\( \\mathbb{R}^d \\). In this space, the graph properties (such as the first-order, second-order, and higher-order proximities) are preserved as much as possible. The graph is represented as either a \\( d \\)-dimensional vector (for a whole graph) or a set of \\( d \\)-dimensional vectors with each vector representing the embedding of part of the graph (e.g., node, edge, substructure). Therefore, a graph embedding maps each node of graph \\( G(E, V) \\) to a low-dimensional feature vector \\( \\mathbf{y}_i \\) and tries to preserve the connection strengths between vertices. For example, a graph embedding preserving first-order proximity might be obtained by minimizing\n",
    "$$\n",
    "\\sum_{i,j} s_{ij} \\| \\mathbf{y}_i - \\mathbf{y}_j \\|^2_2\n",
    "$$\n",
    "\n",
    "Graph embedding is an important method for learning low-dimensional representations of vertices in networks, aiming to capture and preserve the network structure. Learning network representations faces the following great challenges [154]:\n",
    "1. **High nonlinearity:** The underlying structure of a graph or network is highly nonlinear. Therefore, designing a model to capture the highly nonlinear structure is rather difficult.\n",
    "2. **Topology structure-preserving:** To support applications in analyzing networks, network embedding is required to preserve the network structure. However, the underlying structure of the network is very complex. The similarity of vertices is dependent on both the local and global network structures. Therefore, how to simultaneously preserve the local\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17e21825",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'networkx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5887/2180222366.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# graph_embedding.py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnetworkx\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnode2vec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNode2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'networkx'"
     ]
    }
   ],
   "source": [
    "# graph_embedding.py\n",
    "\n",
    "import networkx as nx\n",
    "from node2vec import Node2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Create a graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes and edges\n",
    "G.add_edges_from([\n",
    "    (1, 2), (1, 3), (2, 4), (3, 4),\n",
    "    (4, 5), (5, 6), (6, 7), (7, 5)\n",
    "])\n",
    "\n",
    "# Optionally, add node attributes or edge weights\n",
    "nx.set_node_attributes(G, {1: 'A', 2: 'B', 3: 'C', 4: 'D', 5: 'E', 6: 'F', 7: 'G'}, 'label')\n",
    "\n",
    "# Apply Node2Vec\n",
    "node2vec = Node2Vec(G, dimensions=64, walk_length=30, num_walks=200, workers=4)\n",
    "model = node2vec.fit(window=10, min_count=1, sg=1)\n",
    "\n",
    "# Get node embeddings\n",
    "embeddings = {node: model.wv[node] for node in G.nodes()}\n",
    "\n",
    "# Convert embeddings to a matrix\n",
    "embedding_matrix = [embeddings[node] for node in G.nodes()]\n",
    "\n",
    "# Perform PCA to reduce to 2 dimensions for visualization\n",
    "pca = PCA(n_components=2)\n",
    "reduced_embeddings = pca.fit_transform(embedding_matrix)\n",
    "\n",
    "# Create a DataFrame for plotting\n",
    "df = pd.DataFrame(reduced_embeddings, index=G.nodes(), columns=['x', 'y'])\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(df['x'], df['y'])\n",
    "\n",
    "for node, (x, y) in df.iterrows():\n",
    "    plt.text(x, y, str(node), fontsize=12)\n",
    "\n",
    "plt.title('Node Embeddings Visualization')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76f8b62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
