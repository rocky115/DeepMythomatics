{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e76a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " * Copyright (c) 2018 Radhamadhab Dalai\n",
    " *\n",
    " * Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    " * of this software and associated documentation files (the \"Software\"), to deal\n",
    " * in the Software without restriction, including without limitation the rights\n",
    " * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    " * copies of the Software, and to permit persons to whom the Software is\n",
    " * furnished to do so, subject to the following conditions:\n",
    " *\n",
    " * The above copyright notice and this permission notice shall be included in\n",
    " * all copies or substantial portions of the Software.\n",
    " *\n",
    " * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    " * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    " * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    " * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    " * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    " * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
    " * THE SOFTWARE.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc81b65c",
   "metadata": {},
   "source": [
    "## Extreme learning machine (ELM) algorithm\n",
    "The offline solution of (7.11.7) is given by\n",
    "\n",
    "$$\n",
    "B = H^\\dagger Y,\n",
    "$$\n",
    "\n",
    "(7.11.11)\n",
    "\n",
    "where \\( H^\\dagger \\) is the Mooreâ€“Penrose generalized inverse of the hidden layer output matrix \\( H \\).\n",
    "\n",
    "Based on the minimum norm LS solution \\( B = H^\\dagger Y \\), Huang et al. [70] proposed an extreme learning machine (ELM) as a simple learning algorithm for SLFNs, as shown in Algorithm 7.5.\n",
    "\n",
    "**Algorithm 7.5 Extreme learning machine (ELM) algorithm**\n",
    "\n",
    "1. **Input**: A training set \\( \\{(x_i , y_i )|x_i \\in \\mathbb{R}^n , y_i \\in \\mathbb{R}^m , i = 1, \\ldots , N \\} \\), activation function \\( g(x) \\), and hidden node number \\( d \\).\n",
    "\n",
    "2. **Initialization**: Randomly assign input weight \\( w_i \\) and bias \\( b_i \\), \\( i = 1, \\ldots , d \\).\n",
    "\n",
    "3. **Learning step**:\n",
    "\n",
    "   3.1. Use (7.11.8) to calculate the hidden layer output matrix \\( H \\).\n",
    "\n",
    "   3.2. Use (7.11.10) to construct the training output matrix \\( Y \\).\n",
    "\n",
    "   3.3. Calculate the minimum norm least squares weight matrix \\( B = H^\\dagger Y \\), and get \\( \\beta_i \\) from \\( B^T = [\\beta_1, \\ldots , \\beta_d] \\).\n",
    "\n",
    "4. **Testing step**: For given testing sample \\( x \\in \\mathbb{R}^n \\), the output of SLFNs is given by\n",
    "\n",
    "$$\n",
    "y = \\sum_{i=1}^d \\beta_i g(w_i^T x + b_i).\n",
    "$$\n",
    "\n",
    "It is shown [71] that the ELM can use a wide type of feature mappings (hidden layer output functions), including random hidden nodes and kernels. With this extension, the unified ELM solution can be obtained for feedforward neural networks, RBF network, LS-SVM, and PSVM.\n",
    "\n",
    "In ELM, the hidden layer need not be tuned. For one output node case, the output function of ELM for generalized SLFNs is given by\n",
    "\n",
    "$$\n",
    "f_d (x) = \\sum_{j=1}^d \\beta_j h_j (x) = h^T (x) \\beta,\n",
    "$$\n",
    "\n",
    "(7.11.12)\n",
    "\n",
    "where \\( h(x) = [h_1 (x), \\ldots , h_d (x)]^T \\) is the output vector of the hidden layer with respect to the input \\( x \\), and \\( \\beta = [\\beta_1 , \\ldots , \\beta_d ]^T \\) is the vector of the output weights between the hidden layer of \\( d \\) nodes.\n",
    "\n",
    "The output vector \\( h(x) \\) is a feature mapping: it actually maps the data from the \\( n \\)-dimensional input space to the \\( d \\)-dimensional hidden layer feature space (ELM feature space) \\( H \\). For the binary classification problem, the decision function of ELM is\n",
    "\n",
    "$$\n",
    "f_d (x) = \\text{sign} (h^T (x) \\beta).\n",
    "$$\n",
    "\n",
    "(7.11.13)\n",
    "\n",
    "An ELM with a single-output node is used to generate \\( N \\) output samples:\n",
    "\n",
    "$$\n",
    "\\sum_{j=1}^d \\beta_j h_j (x_i ) = y_i ,\n",
    "$$\n",
    "\n",
    "which can be rewritten as the form\n",
    "\n",
    "$$\n",
    "h^T (x_i ) \\beta = y_i , \\quad i = 1, \\ldots , N\n",
    "$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\n",
    "H \\beta = y,\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "H = \\begin{bmatrix}\n",
    "h_1 (x_1 ) & \\cdots & h_d (x_1) \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "h_1 (x_N ) & \\cdots & h_d (x_N )\n",
    "\\end{bmatrix},\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\beta = \\begin{bmatrix}\n",
    "\\beta_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\beta_d\n",
    "\\end{bmatrix},\n",
    "$$\n",
    "\n",
    "$$\n",
    "y = \\begin{bmatrix}\n",
    "y_1 \\\\\n",
    "\\vdots \\\\\n",
    "y_N\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "h^T (x_1) \\\\\n",
    "\\vdots \\\\\n",
    "h^T (x_N)\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "The constrained optimization problem for ELM regression and binary classification with a single-output node can be formulated as [71]:\n",
    "\n",
    "$$\n",
    "\\min_{\\beta, \\xi_i} L_{PELM} = \\frac{1}{2} \\|\\beta\\|_2^2 + \\frac{C}{2} \\sum_{i=1}^N \\xi_i^2,\n",
    "$$\n",
    "\n",
    "subject to \\( h^T (x_i ) \\beta = y_i - \\xi_i, \\quad i = 1, \\ldots , N. \\)\n",
    "\n",
    "The dual unconstrained optimization problem is\n",
    "\n",
    "$$\n",
    "\\min_{\\beta, \\xi_i, \\alpha_i} L_{DELM} (\\beta, \\xi_i, \\alpha_i) = \\frac{1}{2} \\|\\beta\\|_2^2 + \\frac{C}{2} \\sum_{i=1}^N \\xi_i^2 - \\sum_{i=1}^N \\alpha_i (h^T (x_i) \\beta - y_i + \\xi_i)\n",
    "$$\n",
    "\n",
    "with the Lagrange multipliers \\( \\alpha_i \\geq 0, \\quad i = 1, \\ldots , N. \\)\n",
    "\n",
    "From the optimality conditions one has\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L_{DELM}}{\\partial \\beta} = 0 \\Rightarrow\n",
    "$$\n",
    "\n",
    "where \\( \\alpha = [\\alpha_1, \\ldots , \\alpha_N]^T. \\)\n",
    "\n",
    "$$\n",
    "\\beta = \\sum_{i=1}^N \\alpha_i h(x_i) = H^T \\alpha = H^T \\alpha,\n",
    "$$\n",
    "\n",
    "which can be rewritten as the form\n",
    "\n",
    "$$\n",
    "h^T (x_i ) \\beta = y_i , \\quad i = 1, \\ldots , N\n",
    "$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\n",
    "H \\beta = y,\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "H = \\begin{bmatrix}\n",
    "h_1 (x_1 ) & \\cdots & h_d (x_1) \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "h_1 (x_N ) & \\cdots & h_d (x_N )\n",
    "\\end{bmatrix},\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\beta = \\begin{bmatrix}\n",
    "\\beta_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\beta_d\n",
    "\\end{bmatrix},\n",
    "$$\n",
    "\n",
    "$$\n",
    "y = \\begin{bmatrix}\n",
    "y_1 \\\\\n",
    "\\vdots \\\\\n",
    "y_N\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "h^T (x_1) \\\\\n",
    "\\vdots \\\\\n",
    "h^T (x_N)\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "The constrained optimization problem for ELM regression and binary classification with a single-output node can be formulated as [71]:\n",
    "\n",
    "$$\n",
    "\\min_{\\beta, \\xi_i} L_{PELM} = \\frac{1}{2} \\|\\beta\\|_2^2 + \\frac{C}{2} \\sum_{i=1}^N \\xi_i^2,\n",
    "$$\n",
    "\n",
    "subject to \\( h^T (x_i ) \\beta = y_i - \\xi_i, \\quad i = 1, \\ldots , N. \\)\n",
    "\n",
    "The dual unconstrained optimization problem is\n",
    "\n",
    "$$\n",
    "\\min_{\\beta, \\xi_i, \\alpha_i} L_{DELM} (\\beta, \\xi_i, \\alpha_i) = \\frac{1}{2} \\|\\beta\\|_2^2 + \\frac{C}{2} \\sum_{i=1}^N \\xi_i^2 - \\sum_{i=1}^N \\alpha_i (h^T (x_i) \\beta - y_i + \\xi_i)\n",
    "$$\n",
    "\n",
    "with the Lagrange multipliers \\( \\alpha_i \\geq 0, \\quad i = 1, \\ldots , N. \\)\n",
    "\n",
    "From the optimality conditions one has\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L_{DELM}}{\\partial \\beta} = 0 \\Rightarrow\n",
    "$$\n",
    "\n",
    "where \\( \\alpha = [\\alpha_1, \\ldots , \\alpha_N]^T. \\)\n",
    "\n",
    "$$\n",
    "\\beta = \\sum_{i=1}^N \\alpha_i h(x_i) = H^T \\alpha = H^T \\alpha,\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28f79295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:\n",
      " [[-0.18704703]\n",
      " [ 0.22153287]\n",
      " [ 0.27996201]\n",
      " [ 0.77352065]\n",
      " [ 0.4413394 ]\n",
      " [ 0.46376024]\n",
      " [ 0.0402454 ]\n",
      " [ 0.53954023]\n",
      " [ 0.29939952]\n",
      " [ 0.52385948]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import pinv\n",
    "\n",
    "class ExtremeLearningMachine:\n",
    "    def __init__(self, n_hidden_units, activation_function):\n",
    "        self.n_hidden_units = n_hidden_units\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "    def _add_bias(self, X):\n",
    "        # Add bias term to input data\n",
    "        bias = np.ones((X.shape[0], 1))\n",
    "        return np.hstack([X, bias])\n",
    "\n",
    "    def _init_weights(self, n_features):\n",
    "        # Initialize input weights and biases randomly\n",
    "        self.input_weights = np.random.randn(n_features, self.n_hidden_units)\n",
    "        self.biases = np.random.randn(1, self.n_hidden_units)\n",
    "\n",
    "    def _compute_hidden_layer_output(self, X):\n",
    "        # Compute hidden layer output\n",
    "        Z = np.dot(X, self.input_weights) + self.biases\n",
    "        H = self.activation_function(Z)\n",
    "        return H\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        # Add bias term to input data\n",
    "        X_train = self._add_bias(X_train)\n",
    "        \n",
    "        # Initialize weights\n",
    "        n_features = X_train.shape[1]\n",
    "        self._init_weights(n_features)\n",
    "        \n",
    "        # Compute hidden layer output\n",
    "        H = self._compute_hidden_layer_output(X_train)\n",
    "        \n",
    "        # Compute output weights using Moore-Penrose pseudoinverse\n",
    "        H_pinv = pinv(H)\n",
    "        self.output_weights = np.dot(H_pinv, y_train)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        # Add bias term to input data\n",
    "        X_test = self._add_bias(X_test)\n",
    "        \n",
    "        # Compute hidden layer output\n",
    "        H = self._compute_hidden_layer_output(X_test)\n",
    "        \n",
    "        # Compute output\n",
    "        y_pred = np.dot(H, self.output_weights)\n",
    "        return y_pred\n",
    "\n",
    "# Activation function: Sigmoid\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate some random data for demonstration\n",
    "    np.random.seed(0)\n",
    "    X_train = np.random.rand(100, 10)\n",
    "    y_train = np.random.rand(100, 1)\n",
    "    X_test = np.random.rand(10, 10)\n",
    "\n",
    "    # Create ELM instance\n",
    "    elm = ExtremeLearningMachine(n_hidden_units=50, activation_function=sigmoid)\n",
    "\n",
    "    # Train ELM\n",
    "    elm.fit(X_train, y_train)\n",
    "\n",
    "    # Predict with ELM\n",
    "    y_pred = elm.predict(X_test)\n",
    "    print(\"Predictions:\\n\", y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a23951e",
   "metadata": {},
   "source": [
    "Substituting (7.11.22) into (7.11.15) and using (7.11.18), we get the equation\n",
    "$$\n",
    "H^T H \\alpha = y,\n",
    "$$\n",
    "(7.11.23)\n",
    "where\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "h^T(x_1) \\\\\n",
    "\\vdots \\\\\n",
    "h^T(x_N)\n",
    "\\end{bmatrix}^T\n",
    "H = \\begin{bmatrix}\n",
    "h(x_1), \\ldots, h(x_N)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "H^T H = \\begin{bmatrix}\n",
    "h^T(x_1)h(x_1) & \\cdots & h^T(x_1)h(x_N) \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "h^T(x_N)h(x_1) & \\cdots & h^T(x_N)h(x_N)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "(7.11.24)\n",
    "The LS solution of Eq. (7.11.23) is given by\n",
    "$$\n",
    "\\alpha = (H^T H)^\\dagger y.\n",
    "$$\n",
    "(7.11.25)\n",
    "If a feature mapping $h(x)$ is unknown to users, one can apply Mercer's conditions\n",
    "on ELM, and hence a kernel matrix for ELM can be defined as follows [71]:\n",
    "$$\n",
    "K(x, x_i) = h^T(x)h(x_i).\n",
    "$$\n",
    "(7.11.26)\n",
    "If using (7.11.26), then (7.11.24) can be rewritten as in the following kernel form:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "K(x_1, x_1) & \\cdots & K(x_1, x_N) \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "K(x_N, x_1) & \\cdots & K(x_N, x_N)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "(7.11.27)\n",
    "whose $(i, j)$th entry is $H^T H_{ij} = K(x_i, x_j)$ for $i = 1, \\ldots, N; j = 1, \\ldots, N$.\n",
    "This shows that the feature mapping $h(x)$ need not be used; instead, it is enough\n",
    "to use the corresponding kernel $K(x, x_i)$, e.g., $K(x, x_i) = \\exp(-\\gamma \\|x - x_i\\|)$.\n",
    "Finally, from (7.11.22) it follows that the EML regression function is\n",
    "$$\n",
    "\\hat{y} = h^T(x) \\beta = h^T(x) \\sum_{i=1}^N \\alpha_i h(x_i) = \\sum_{i=1}^N \\alpha_i K(x, x_i),\n",
    "$$\n",
    "while the decision function for EML binary classification is\n",
    "$$\n",
    "\\text{class of } x = \\text{sign} \\left( \\sum_{i=1}^N \\alpha_i K(x, x_i) \\right).\n",
    "$$\n",
    "(7.11.28)\n",
    "(7.11.29)\n",
    "Algorithm 7.6 shows the ELM algorithm for regression and binary classification.\n",
    "Algorithm 7.6 ELM algorithm for regression and binary classification [71]\n",
    "\n",
    "1. **Input**: A training set $\\{(x_i, y_i) | x_i \\in \\mathbb{R}^n, y_i \\in \\mathbb{R}, i = 1, \\ldots, N\\}$, hidden node number $d$ and the kernel function $K(u, v) = \\exp(-\\gamma \\|u - v\\|)$.\n",
    "2. **Initialization**: $y = [y_1, \\ldots, y_N]^T$.\n",
    "3. **Learning step**:\n",
    "    1. Use the kernel function to construct the matrix $H^T H_{ij} = K(x_i, x_j)$, $i, j = 1, \\ldots, N$.\n",
    "    2. Calculate the minimum norm least squares solution $\\alpha = (H^T H)^\\dagger y$.\n",
    "4. **Testing step**: For given testing sample $x \\in \\mathbb{R}^n$, the ELM regression is $\\sum_{i=1}^N \\alpha_i K(x, x_i)$, while the ELM binary classification is $\\text{class of } x = \\text{sign} \\left( \\sum_{i=1}^N \\alpha_i K(x, x_i) \\right)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dbefb5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:\n",
      " [[ 0.3175779 ]\n",
      " [ 0.40938498]\n",
      " [ 0.28007594]\n",
      " [ 0.84347293]\n",
      " [ 0.50490296]\n",
      " [ 0.51523179]\n",
      " [-0.0439662 ]\n",
      " [ 0.67854794]\n",
      " [ 0.24910511]\n",
      " [ 0.01155326]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import pinv\n",
    "\n",
    "class ELM:\n",
    "    def __init__(self, n_hidden_units, kernel_function):\n",
    "        self.n_hidden_units = n_hidden_units\n",
    "        self.kernel_function = kernel_function\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        N = X_train.shape[0]\n",
    "        \n",
    "        # Construct the kernel matrix\n",
    "        H = np.zeros((N, N))\n",
    "        for i in range(N):\n",
    "            for j in range(N):\n",
    "                H[i, j] = self.kernel_function(X_train[i], X_train[j])\n",
    "        \n",
    "        # Calculate the minimum norm least squares solution\n",
    "        H_pinv = pinv(H)\n",
    "        self.alpha = np.dot(H_pinv, y_train)\n",
    "        \n",
    "        self.X_train = X_train\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        N = self.X_train.shape[0]\n",
    "        M = X_test.shape[0]\n",
    "        \n",
    "        # Compute the kernel matrix between test data and training data\n",
    "        K = np.zeros((M, N))\n",
    "        for i in range(M):\n",
    "            for j in range(N):\n",
    "                K[i, j] = self.kernel_function(X_test[i], self.X_train[j])\n",
    "        \n",
    "        # Compute the predictions\n",
    "        y_pred = np.dot(K, self.alpha)\n",
    "        return y_pred\n",
    "\n",
    "# Example kernel function: RBF (Gaussian)\n",
    "def rbf_kernel(x, y, gamma=1.0):\n",
    "    return np.exp(-gamma * np.linalg.norm(x - y) ** 2)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate some random data for demonstration\n",
    "    np.random.seed(0)\n",
    "    X_train = np.random.rand(100, 10)\n",
    "    y_train = np.random.rand(100, 1)\n",
    "    X_test = np.random.rand(10, 10)\n",
    "\n",
    "    # Create ELM instance\n",
    "    elm = ELM(n_hidden_units=50, kernel_function=rbf_kernel)\n",
    "\n",
    "    # Train ELM\n",
    "    elm.fit(X_train, y_train)\n",
    "\n",
    "    # Predict with ELM\n",
    "    y_pred = elm.predict(X_test)\n",
    "    print(\"Predictions:\\n\", y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bbb156",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extreme Learning Machine Algorithm for Multiclass Classification\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
