{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5170d131",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " * Copyright (c) 2018 Radhamadhab Dalai\n",
    " *\n",
    " * Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    " * of this software and associated documentation files (the \"Software\"), to deal\n",
    " * in the Software without restriction, including without limitation the rights\n",
    " * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    " * copies of the Software, and to permit persons to whom the Software is\n",
    " * furnished to do so, subject to the following conditions:\n",
    " *\n",
    " * The above copyright notice and this permission notice shall be included in\n",
    " * all copies or substantial portions of the Software.\n",
    " *\n",
    " * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    " * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    " * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    " * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    " * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    " * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
    " * THE SOFTWARE.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cafa3cf",
   "metadata": {},
   "source": [
    "## Dropout Spherical K-Means\n",
    "\n",
    "A major goal in machine learning or neural networks is to learn deep hierarchies of features for other tasks. Many algorithms are available to learn deep hierarchies of features from unlabeled data, especially text documents and images. It has been found [17, 18, 21] that using K-means clustering as the unsupervised learning module in these types of \"feature learning\" pipelines can lead to excellent results, often rivaling the state-of-the-art systems.\n",
    "\n",
    "The classic K-means clustering algorithm finds cluster centroids that minimize the distance between data points and the nearest centroid. K-means is also called \"vector quantization\" (VQ) in the sense that K-means can be viewed as a way of constructing a \"dictionary\" \\(D \\in \\mathbb{R}^{n \\times K}\\) of \\(K\\) column vectors \\(d^{(j)}\\), \\(j = 1, \\ldots, K\\) so that a data vector \\(x^{(i)} \\in \\mathbb{R}^n\\) (i = 1, \\ldots, m) can be mapped to a code vector \\(s^{(i)}\\) that minimizes the error in reconstruction. A popular modified version of K-means is known as \"gain shape vector quantization\" [169] or \"spherical K-means\" [21].\n",
    "\n",
    "Given \\(m\\) data vectors \\(x^{(i)} \\in \\mathbb{R}^n\\), \\(i = 1, \\ldots, m\\). The VQ of the data vector \\(x^{(i)}\\) can be described by the model:\n",
    "\n",
    "$$\n",
    "x^{(i)} = D s^{(i)}, \\quad i = 1, \\ldots, m,\n",
    "$$\n",
    "\n",
    "where \\(s^{(i)}\\) is a quantization coefficient vector or \"code vector\" associated with the input \\(x^{(i)}\\), \\(D \\in \\mathbb{R}^{K \\times n}\\) denotes the codebook matrix or \"dictionary\" whose \\(j\\)-th column is denoted by \\(d^{(j)}\\). The goal of spherical K-means is equivalently to find \\(D\\) and \\(s^{(i)}\\) from the input \\(x^{(i)}\\) according to [18]:\n",
    "\n",
    "$$\n",
    "\\min_{D, s} \\sum_{i=1}^m \\| D s^{(i)} - x^{(i)} \\|_2^2,\n",
    "$$\n",
    "\n",
    "subject to \\(\\|s^{(i)}\\|_0 \\leq 1, \\, \\forall i = 1, \\ldots, m\\) and \\(\\|d^{(j)}\\|_2 = 1, \\, \\forall j = 1, \\ldots, K\\).\n",
    "\n",
    "In the above equation, the code vector \\(s^{(i)}\\) can be thought of as a \"feature representation\" of each example \\(x^{(i)}\\) that satisfies several criteria:\n",
    "- Given \\(s^{(i)}\\) and \\(D\\), the original example \\(x^{(i)}\\) should be able to be reconstructed well.\n",
    "- The first constraint \\(\\|s^{(i)}\\|_0 \\leq 1\\) means that each \\(s^{(i)}\\) should have at most one nonzero entry (associated with vector quantization). Thus, a new representation of \\(x^{(i)}\\) should preserve it as well as possible, but also be a very simple or parsimonious representation.\n",
    "- The second constraint \\(\\|d^{(j)}\\|_2 = 1\\) requires that each dictionary column have unit length, preventing them from becoming arbitrarily large or small.\n",
    "\n",
    "The full K-means algorithm for learning feature representation can be summarized below [18]:\n",
    "\n",
    "1. **Normalize inputs**:\n",
    "\n",
    "$$\n",
    "x^{(i)} \\leftarrow \\frac{x^{(i)} - \\text{mean}(x^{(i)})}{\\sqrt{\\text{var}(x^{(i)}) + \\epsilon_{\\text{norm}}}}, \\quad \\forall i = 1, \\ldots, m.\n",
    "$$   7.9.23\n",
    "\n",
    "2. **Eigenvalue decomposition and estimate inputs**:\n",
    "\n",
    "$$\n",
    "\\text{cov}(x^{(i)}) = V D V^T, \\quad i = 1, \\ldots, m,\n",
    "$$\n",
    "\n",
    "$$\n",
    "x^{(i)} \\leftarrow V (D + \\epsilon_{\\text{norm}} I)^{-1/2} V^T x^{(i)}, \\quad i = 1, \\ldots, m.\n",
    "$$\n",
    "\n",
    "3. **Loop until convergence (typically 10 iterations is enough)**:\n",
    "\n",
    "$$\n",
    "s_j^{(i)} = \n",
    "\\begin{cases} \n",
    "(d^{(j)})^T x^{(i)}, & \\text{if } j = \\arg \\max_l \\|D^{(l)} x^{(i)}\\|_1, \\\\\n",
    "0, & \\text{otherwise},\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$$\n",
    "d^{(j)} \\leftarrow X s_j + d^{(j)},\n",
    "$$\n",
    "\n",
    "$$\n",
    "d^{(j)} \\leftarrow \\frac{d^{(j)}}{\\|d^{(j)}\\|_2},\n",
    "$$\n",
    "\n",
    "where \\(j = 1, \\ldots, K, \\, i = 1, \\ldots, m\\); while \\(X = [x^{(1)}, \\ldots, x^{(m)}] \\in \\mathbb{R}^{n \\times m}\\) and \\(s_j = [s_j, \\ldots, s_j]^T \\in \\mathbb{R}^{m \\times 1}\\) are the data (or example) matrix and the code vectors, respectively.\n",
    "\n",
    "When dropout is applied to the outputs of a dictionary, the spherical K-means is extended to the dropout spherical K-means [173]. The optimization problem of the dropout spherical K-means can be described as:\n",
    "\n",
    "$$\n",
    "\\min_{D, s} \\sum_{i=1}^m \\| (M \\odot D) s^{(i)} - x^{(i)} \\|_2^2,\n",
    "$$\n",
    "\n",
    "subject to \\(\\|s^{(i)}\\|_0 \\leq 1, \\, \\forall i = 1, \\ldots, m\\) and \\(\\|d^{(j)}\\|_2 = 1, \\, \\forall j = 1, \\ldots, K\\),\n",
    "\n",
    "where \\(M\\) is a binary mask matrix with the same size as \\(D\\), and each column is drawn independently from \\(M_j \\sim \\text{Bernoulli}(p)\\). Given a dictionary \\(D\\) and a dropout mask matrix \\(M\\), then \\(M \\odot D\\) can be viewed as a \"thinned dictionary\" \\(D_{\\text{thin}} = M \\odot D\\), where \\(D_{\\text{thin}} \\in D\\). Hence, Eq. (7.9.24) becomes:\n",
    "\n",
    "$$\n",
    "s_j^{(i)} = \n",
    "\\begin{cases} \n",
    "(d_{\\text{thin}}^{(j)})^T x^{(i)}, & \\text{if } j = \\arg \\max_l \\|D_{\\text{thin}}^{(l)} x^{(i)}\\|_1, \\\\\n",
    "0, & \\text{otherwise},\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "for all \\(j = 1, \\ldots, K; i = 1, \\ldots, m\\).\n",
    "\n",
    "After the code vector \\(s^{(i)}\\) is obtained, new centroids can be computed according to:\n",
    "\n",
    "$$\n",
    "D_{\\text{new}} = \\arg \\min_{D_{\\text{thin}}} \\|D_{\\text{thin}} S - X\\|_2^2 + \\|D_{\\text{thin}} - D_{\\text{old}}\\|_2^2,\n",
    "$$\n",
    "\n",
    "$$\n",
    "D_{\\text{thin}} = \\left(S S^T + I\\right)^{-1} \\left(X S^T + \\alpha D_{\\text{thin}}\\right).\n",
    "$$\n",
    "\n",
    "The full dropout K-means algorithm [173] for learning feature representation can be summarized below:\n",
    "\n",
    "1. **Normalize inputs**: Use Eq. (7.9.21) to compute the normalized input vectors \\(x^{(i)}\\).\n",
    "2. **Eigenvalue decomposition and estimate inputs**: Use Eq. (7.9.22) to compute the EVD of the variance matrix \\(\\text{cov}(x^{(i)})\\), then estimate the inputs \\(x^{(i)}\\) via Eq. (7.9.23).\n",
    "3. **Loop until convergence**: Calculate the code vectors \\(s^{(i)}\\) by using Eq. (7.9.29), and update \\(D_{\\text{new}} = X S^T + D_{\\text{thin}}\\). Finally, make the normalization \\(d^{(j)} = \\frac{d^{(j)}}{\\|d^{(j)}\\|_2}\\).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6aecd743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary D:\n",
      " [[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan]]\n",
      "Code vectors S:\n",
      " [[nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/radha/anaconda3/envs/cv37/lib/python3.7/site-packages/ipykernel_launcher.py:73: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def normalize_inputs(X):\n",
    "    \"\"\"\n",
    "    Normalize input data X.\n",
    "    \"\"\"\n",
    "    X_mean = np.mean(X, axis=0)\n",
    "    X_std = np.std(X, axis=0)\n",
    "    X_norm = (X - X_mean) / (X_std + 1e-8)\n",
    "    return X_norm\n",
    "\n",
    "def eigenvalue_decomposition(X):\n",
    "    \"\"\"\n",
    "    Perform eigenvalue decomposition on the covariance matrix of X.\n",
    "    \"\"\"\n",
    "    cov_matrix = np.cov(X, rowvar=False)\n",
    "    eig_values, eig_vectors = np.linalg.eigh(cov_matrix)\n",
    "    return eig_values, eig_vectors\n",
    "\n",
    "def estimate_inputs(X, eig_values, eig_vectors):\n",
    "    \"\"\"\n",
    "    Estimate inputs using eigenvalue decomposition results.\n",
    "    \"\"\"\n",
    "    D_inv_sqrt = np.diag(1.0 / np.sqrt(eig_values + 1e-8))\n",
    "    X_estimated = eig_vectors @ D_inv_sqrt @ eig_vectors.T @ X.T\n",
    "    return X_estimated.T\n",
    "\n",
    "def spherical_kmeans(X, K, max_iters=10):\n",
    "    \"\"\"\n",
    "    Spherical K-means clustering.\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    D = np.random.randn(K, n_features)\n",
    "    D = D / np.linalg.norm(D, axis=1, keepdims=True)\n",
    "\n",
    "    for _ in range(max_iters):\n",
    "        # Compute code vectors\n",
    "        S = np.zeros((n_samples, K))\n",
    "        for i in range(n_samples):\n",
    "            dot_products = np.dot(D, X[i])\n",
    "            j = np.argmax(dot_products)\n",
    "            S[i, j] = dot_products[j]\n",
    "\n",
    "        # Update dictionary\n",
    "        D_new = np.dot(S.T, X)\n",
    "        D_new = D_new / np.linalg.norm(D_new, axis=1, keepdims=True)\n",
    "        D = D_new\n",
    "\n",
    "    return D, S\n",
    "\n",
    "def dropout_spherical_kmeans(X, K, dropout_rate=0.5, max_iters=10):\n",
    "    \"\"\"\n",
    "    Dropout Spherical K-means clustering.\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    D = np.random.randn(K, n_features)\n",
    "    D = D / np.linalg.norm(D, axis=1, keepdims=True)\n",
    "\n",
    "    for _ in range(max_iters):\n",
    "        # Apply dropout\n",
    "        dropout_mask = np.random.binomial(1, 1-dropout_rate, D.shape)\n",
    "        D_thin = D * dropout_mask\n",
    "\n",
    "        # Compute code vectors\n",
    "        S = np.zeros((n_samples, K))\n",
    "        for i in range(n_samples):\n",
    "            dot_products = np.dot(D_thin, X[i])\n",
    "            j = np.argmax(dot_products)\n",
    "            S[i, j] = dot_products[j]\n",
    "\n",
    "        # Update dictionary\n",
    "        D_new = np.dot(S.T, X)\n",
    "        D_new = D_new / np.linalg.norm(D_new, axis=1, keepdims=True)\n",
    "        D = D_new\n",
    "\n",
    "    return D, S\n",
    "\n",
    "def main():\n",
    "    # Example usage\n",
    "    np.random.seed(0)\n",
    "    X = np.random.randn(100, 20)  # 100 samples, 20 features\n",
    "\n",
    "    # Normalize inputs\n",
    "    X_norm = normalize_inputs(X)\n",
    "\n",
    "    # Eigenvalue decomposition\n",
    "    eig_values, eig_vectors = eigenvalue_decomposition(X_norm)\n",
    "\n",
    "    # Estimate inputs\n",
    "    X_estimated = estimate_inputs(X_norm, eig_values, eig_vectors)\n",
    "\n",
    "    # Perform Dropout Spherical K-means\n",
    "    K = 10  # Number of clusters\n",
    "    D, S = dropout_spherical_kmeans(X_estimated, K, dropout_rate=0.5, max_iters=10)\n",
    "\n",
    "    print(\"Dictionary D:\\n\", D)\n",
    "    print(\"Code vectors S:\\n\", S)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54691e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exercise - Rectify and modify it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21dec3f",
   "metadata": {},
   "source": [
    "## Definition 7.5 (DropConnect Network [153])\n",
    "Let \\(\\{x_1, \\ldots, x_l\\}\\) with labels \\(\\{y_1, \\ldots, y_l\\}\\) be the data set \\(S\\) of \\(l\\) entries. A DropConnect network is defined as a mixture model:\n",
    "\n",
    "$$\n",
    "o = \\sum_{m} p(M) f(x; \\theta, M) = \\mathbb{E}_m \\{f(x; \\theta, M)\\},\n",
    "$$\n",
    "\n",
    "where \\(m\\) is the DropConnect layer mask, \\(\\theta = \\{W_s, W, W_g\\}\\) are network parameters: \\(W_s\\) are the softmax layer parameters, \\(W\\) are the DropConnect layer parameters, and \\(W_g\\) are the feature extractor parameters. Each network \\(f(x; \\theta, M)\\) has weights \\(p(M)\\) such that \\(M_{ij} \\sim \\text{Bernoulli}(p)\\).\n",
    "\n",
    "When each element of \\(M\\) has equal probability of being on and off (\\(p = 0.5\\)), the mixture model has equal weights for all sub-models \\(f(x; \\theta, M)\\), otherwise, the mixture model has larger weights in some sub-models than others.\n",
    "\n",
    "A standard DropConnect model architecture comprises the following basic steps [152]:\n",
    "\n",
    "1. **Feature Extractor**:\n",
    "\n",
    "$$\n",
    "v = g(x; W_g),\n",
    "$$\n",
    "\n",
    "where \\(v \\in \\mathbb{R}^{n \\times 1}\\) is the output feature vector, \\(x \\in \\mathbb{R}^{I \\times 1}\\) is the input data vector to the overall model, and \\(W_g \\in \\mathbb{R}^{n \\times I}\\) is the parameter matrix for the feature extractor. \\(g(\\cdot)\\) is chosen to be a multilayered convolutional neural network (CNN), with \\(W_g\\) being the convolutional filters (and biases) of the CNN.\n",
    "\n",
    "2. **DropConnect Layer**:\n",
    "\n",
    "$$\n",
    "r = f(u) = f((M \\odot W)v) \\in \\mathbb{R}^{d \\times 1},\n",
    "$$\n",
    "\n",
    "where \\(v\\) is the output of the feature extractor, \\(W \\in \\mathbb{R}^{d \\times n}\\) is a fully connected weight matrix, \\(f\\) is a nonlinear activation function, and \\(M \\in \\mathbb{R}^{d \\times n}\\) is the binary mask matrix.\n",
    "\n",
    "3. **Softmax Classification Layer**:\n",
    "\n",
    "$$\n",
    "o_i = \\text{softmax}(r; W_s) = \\frac{\\exp(w_{s,i}^T r)}{1 + \\sum_{j} \\exp(w_{s,j}^T r)},\n",
    "$$\n",
    "\n",
    "where \\(W_s \\in \\mathbb{R}^{k \\times d}\\) is the weight matrix of the softmax classification layer which takes \\(r\\) as its input and uses \\(W_s\\) to map this input to a \\(k\\)-dimensional output vector \\(o\\) (\\(k\\) is the number of classes).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2a3458f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-30 09:12:15.553996: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-30 09:12:20.084294: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-07-30 09:12:20.084338: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-07-30 09:12:20.511633: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-30 09:12:31.424638: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-07-30 09:12:31.424844: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-07-30 09:12:31.424861: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2024-07-30 09:12:49.287174: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2024-07-30 09:12:49.299619: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2024-07-30 09:12:49.299700: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (radha-Vostro-15-3568): /proc/driver/nvidia/version does not exist\n",
      "2024-07-30 09:12:49.312107: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-30 09:12:51.133480: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 188160000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 156s 82ms/step - loss: 0.1637 - accuracy: 0.9509 - val_loss: 0.0904 - val_accuracy: 0.9706\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 147s 78ms/step - loss: 0.0667 - accuracy: 0.9793 - val_loss: 0.0658 - val_accuracy: 0.9785\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 143s 76ms/step - loss: 0.0476 - accuracy: 0.9849 - val_loss: 0.0665 - val_accuracy: 0.9772\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 137s 73ms/step - loss: 0.0347 - accuracy: 0.9891 - val_loss: 0.0639 - val_accuracy: 0.9814\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 152s 81ms/step - loss: 0.0259 - accuracy: 0.9912 - val_loss: 0.0789 - val_accuracy: 0.9784\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 140s 75ms/step - loss: 0.0200 - accuracy: 0.9935 - val_loss: 0.0734 - val_accuracy: 0.9802\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 139s 74ms/step - loss: 0.0141 - accuracy: 0.9957 - val_loss: 0.0921 - val_accuracy: 0.9757\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 143s 76ms/step - loss: 0.0120 - accuracy: 0.9959 - val_loss: 0.0772 - val_accuracy: 0.9828\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 150s 80ms/step - loss: 0.0113 - accuracy: 0.9959 - val_loss: 0.0801 - val_accuracy: 0.9814\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 139s 74ms/step - loss: 0.0080 - accuracy: 0.9972 - val_loss: 0.0825 - val_accuracy: 0.9813\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.0825 - accuracy: 0.9813\n",
      "Test accuracy: 0.9812999963760376\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer, Dense, Conv2D, Flatten, Softmax\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "\n",
    "class DropConnect(Layer):\n",
    "    def __init__(self, units, drop_prob=0.5):\n",
    "        super(DropConnect, self).__init__()\n",
    "        self.units = units\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(name='kernel',\n",
    "                                      shape=(input_shape[-1], self.units),\n",
    "                                      initializer='glorot_uniform',\n",
    "                                      trainable=True)\n",
    "        self.bias = self.add_weight(name='bias',\n",
    "                                    shape=(self.units,),\n",
    "                                    initializer='zeros',\n",
    "                                    trainable=True)\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        if training:\n",
    "            binary_tensor = tf.random.uniform(self.kernel.shape) > self.drop_prob\n",
    "            kernel = self.kernel * tf.cast(binary_tensor, dtype=tf.float32)\n",
    "        else:\n",
    "            kernel = self.kernel * (1 - self.drop_prob)\n",
    "        output = tf.matmul(inputs, kernel) + self.bias\n",
    "        return output\n",
    "\n",
    "class DropConnectNetwork(Model):\n",
    "    def __init__(self, input_shape, num_classes, drop_prob=0.5):\n",
    "        super(DropConnectNetwork, self).__init__()\n",
    "        self.conv1 = Conv2D(32, (3, 3), activation='relu', input_shape=input_shape)\n",
    "        self.flatten = Flatten()\n",
    "        self.drop_connect = DropConnect(128, drop_prob)\n",
    "        self.dense = Dense(num_classes)\n",
    "        self.softmax = Softmax()\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.flatten(x)\n",
    "        x = self.drop_connect(x, training=training)\n",
    "        x = self.dense(x)\n",
    "        return self.softmax(x)\n",
    "\n",
    "# Define constants\n",
    "input_shape = (28, 28, 1)  # Example for MNIST dataset\n",
    "num_classes = 10\n",
    "\n",
    "# Instantiate the model\n",
    "model = DropConnectNetwork(input_shape, num_classes)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Load and preprocess data\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(f'Test accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bebab8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
