{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553aaf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " * Copyright (c) 2018 Radhamadhab Dalai\n",
    " *\n",
    " * Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    " * of this software and associated documentation files (the \"Software\"), to deal\n",
    " * in the Software without restriction, including without limitation the rights\n",
    " * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    " * copies of the Software, and to permit persons to whom the Software is\n",
    " * furnished to do so, subject to the following conditions:\n",
    " *\n",
    " * The above copyright notice and this permission notice shall be included in\n",
    " * all copies or substantial portions of the Software.\n",
    " *\n",
    " * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    " * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    " * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    " * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    " * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    " * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
    " * THE SOFTWARE.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fff7756",
   "metadata": {},
   "source": [
    "## Generative Adversarial Network (GAN) Framework\n",
    "\n",
    "Generative Adversarial Networks (GANs) have emerged as a powerful framework for learning generative models of complex data distributions. GANs play an adversarial game with two linked models: the Generative Model and the Discriminative Model. The basic structure of GANs is shown in the following figure:\n",
    "\n",
    "$$\n",
    "\\begin{array}{c}\n",
    "\\text{Generator} \\\\\n",
    "\\text{Synthetic data} \\\\\n",
    "G \\\\\n",
    "G(z) \\\\\n",
    "\\downarrow \\\\\n",
    "\\text{Discriminator} \\\\\n",
    "D \\\\\n",
    "\\text{True / Fake} \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "- **Generator**: The generator \\( G \\) uses random noise \\( z \\) to generate synthetic data \\( \\hat{x} = G(z) \\).\n",
    "- **Discriminator**: The discriminator \\( D \\) tries to identify whether the synthesized data \\( \\hat{x} \\) is real or fake.\n",
    "\n",
    "In this setup, the generator and the discriminator are closely connected, which is the essence of Goodfellowâ€™s GANs. The generator aims to produce data that is indistinguishable from real data, while the discriminator tries to differentiate between real and synthetic data.\n",
    "\n",
    "The framework of GANs is as follows:\n",
    "\n",
    "- **Generator**: Defined by a differentiable function \\( G \\) that takes \\( z \\) as input and uses parameters \\( \\theta_G \\). When \\( z \\) is sampled from some simple prior distribution, \\( G(z) = G(z; \\theta) \\) yields samples \\( x_i \\) drawn from the model distribution \\( P_{model} = P_G \\). The generator's objective is to find the maximum likelihood estimate of \\( \\theta_G \\) as follows:\n",
    "\n",
    "$$\n",
    "\\theta^*_G = \\arg \\max_{\\theta} \\sum_{i=1}^m \\log P_G(x_i; \\theta)\n",
    "$$\n",
    "\n",
    "Approximating the expectation:\n",
    "\n",
    "$$\n",
    "\\theta^*_G = \\arg \\max_{\\theta} \\mathbb{E}_{x \\sim P_{data}}[\\log P_G(x; \\theta)]\n",
    "$$\n",
    "\n",
    "Breaking it down:\n",
    "\n",
    "$$\n",
    "\\theta^*_G = \\arg \\max_{\\theta} \\left( \\int_{x} P_{data}(x) \\log P_G(x; \\theta) \\, dx - \\int_{x} P_{data}(x) \\log P_{data}(x) \\, dx \\right)\n",
    "$$\n",
    "\n",
    "Since the second term is independent of \\( \\theta \\):\n",
    "\n",
    "$$\n",
    "\\theta^*_G = \\arg \\max_{\\theta} \\int_{x} P_{data}(x) \\log P_G(x; \\theta) \\, dx - \\text{constant}\n",
    "$$\n",
    "\n",
    "Therefore, the optimization can be framed as minimizing the Kullback-Leibler (KL) divergence between \\( P_{data} \\) and \\( P_G \\):\n",
    "\n",
    "$$\n",
    "\\theta^*_G = \\arg \\min_{\\theta} \\text{KL}(P_{data}(x) \\| P_G(x; \\theta))\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eac5d51",
   "metadata": {},
   "source": [
    "The generator \\( G \\) in a GAN aims to maximize the likelihood of generating data that resembles the true data distribution. The objective function for the generator is formulated as follows:\n",
    "\n",
    "$$\n",
    "\\theta^*_G = \\arg \\max_{\\theta} \\sum_{i=1}^m \\log P_G(x_i; \\theta)\n",
    "$$\n",
    "\n",
    "Approximating the expectation:\n",
    "\n",
    "$$\n",
    "\\theta^*_G = \\arg \\max_{\\theta} \\mathbb{E}_{x \\sim P_{data}}[\\log P_G(x; \\theta)]\n",
    "$$\n",
    "\n",
    "Breaking it down:\n",
    "\n",
    "$$\n",
    "\\theta^*_G = \\arg \\max_{\\theta} \\left( \\int_{x} P_{data}(x) \\log P_G(x; \\theta) \\, dx - \\int_{x} P_{data}(x) \\log P_{data}(x) \\, dx \\right)\n",
    "$$\n",
    "\n",
    "Since the second term is independent of \\( \\theta \\), it simplifies to:\n",
    "\n",
    "$$\n",
    "\\theta^*_G = \\arg \\max_{\\theta} \\int_{x} P_{data}(x) \\log P_G(x; \\theta) \\, dx - \\text{constant}\n",
    "$$\n",
    "\n",
    "Therefore, the optimization can be framed as minimizing the Kullback-Leibler (KL) divergence between \\( P_{data} \\) and \\( P_G \\):\n",
    "\n",
    "$$\n",
    "\\theta^*_G = \\arg \\min_{\\theta} \\text{KL}(P_{data}(x) \\| P_G(x; \\theta))\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4c1ddd",
   "metadata": {},
   "source": [
    "### Generative Adversarial Network Framework\n",
    "\n",
    "The Generative Adversarial Network (GAN) consists of two models:\n",
    "- **Generator \\( G \\)**: Tries to create data samples from a random noise \\( z \\), aiming to make them indistinguishable from real data.\n",
    "- **Discriminator \\( D \\)**: Tries to distinguish between real data samples and those generated by \\( G \\).\n",
    "\n",
    "The generator can be thought of as a counterfeiter trying to make fake money, while the discriminator acts as the police trying to detect counterfeit money.\n",
    "\n",
    "#### GAN Framework\n",
    "\n",
    "The generator \\( G \\) aims to fool the discriminator by producing samples that closely resemble real data. The optimization problem for the generator can be framed as follows:\n",
    "\n",
    "$$\n",
    "\\theta^*_G = \\arg \\max_{\\theta} \\sum_{i=1}^m \\log P_G(x_i; \\theta)\n",
    "$$\n",
    "\n",
    "Approximating the expectation:\n",
    "\n",
    "$$\n",
    "\\theta^*_G = \\arg \\max_{\\theta} \\mathbb{E}_{x \\sim P_{data}}[\\log P_G(x; \\theta)]\n",
    "$$\n",
    "\n",
    "Rewriting it:\n",
    "\n",
    "$$\n",
    "\\theta^*_G = \\arg \\max_{\\theta} \\left( \\int_{x} P_{data}(x) \\log P_G(x; \\theta) \\, dx - \\int_{x} P_{data}(x) \\log P_{data}(x) \\, dx \\right)\n",
    "$$\n",
    "\n",
    "Since the second term is independent of \\( \\theta \\), this simplifies to:\n",
    "\n",
    "$$\n",
    "\\theta^*_G = \\arg \\max_{\\theta} \\int_{x} P_{data}(x) \\log P_G(x; \\theta) \\, dx - \\text{constant}\n",
    "$$\n",
    "\n",
    "Thus, the generator minimizes the Kullback-Leibler (KL) divergence:\n",
    "\n",
    "$$\n",
    "\\theta^*_G = \\arg \\min_{\\theta} \\text{KL}(P_{data}(x) \\| P_G(x; \\theta))\n",
    "$$\n",
    "\n",
    "#### Optimal Discriminator\n",
    "\n",
    "For a fixed generator \\( G \\), the optimal discriminator \\( D^*_G(x) \\) is given by:\n",
    "\n",
    "$$\n",
    "D^*_G(x) = \\frac{P_{data}(x)}{P_{data}(x) + P_G(x; \\theta)}\n",
    "$$\n",
    "\n",
    "The discriminator is trained to maximize the log-probability of correctly classifying real data, while the generator is trained to minimize the log-probability of the discriminator being mistaken:\n",
    "\n",
    "$$\n",
    "\\min_G \\max_D V(D, G) = \\mathbb{E}_{x \\sim P_{data}(x)} [\\log D(x)] + \\mathbb{E}_{z \\sim P_z(z)} [\\log (1 - D(G(z)))]\n",
    "$$\n",
    "\n",
    "#### Minibatch Stochastic Gradient Descent Training of GANs\n",
    "\n",
    "Algorithm 7.12 for minibatch stochastic gradient descent training:\n",
    "\n",
    "1. **Input**: The number \\( k \\) of steps to apply to the discriminator.\n",
    "2. **For** number of training iterations do:\n",
    "   1. **For** \\( k \\) steps do:\n",
    "      1. Sample a mini-batch of \\( m \\) noise samples \\( \\{z_1, \\ldots, z_m\\} \\) from noise prior \\( P_z(z) \\).\n",
    "      2. Sample a mini-batch of \\( m \\) examples \\( \\{x_1, \\ldots, x_m\\} \\) from the data generating distribution \\( P_{data}(x) \\).\n",
    "      3. Update the discriminator by ascending its stochastic gradient:\n",
    "      $$\n",
    "      \\nabla_{\\theta_D} \\frac{1}{m} \\sum_{i=1}^m \\left[\\log D(x_i) + \\log (1 - D(G(z_i)))\\right]\n",
    "      $$\n",
    "   2. Sample a mini-batch of \\( m \\) noise samples \\( \\{z_1, \\ldots, z_m\\} \\) from noise prior \\( P_z(z) \\).\n",
    "   3. Update the generator by descending its stochastic gradient:\n",
    "   $$\n",
    "   \\nabla_{\\theta_G} \\frac{1}{m} \\sum_{i=1}^m \\log (1 - D(G(z_i)))\n",
    "   $$\n",
    "3. **End for**\n",
    "\n",
    "#### Reformulation of the Minimax Game\n",
    "\n",
    "The discriminator's objective function can be interpreted as:\n",
    "\n",
    "$$\n",
    "C(G) = \\mathbb{E}_{x \\sim P_{data}(x)} \\left[ \\log \\frac{P_{data}(x)}{P_{data}(x) + P_G(x; \\theta)} \\right] + \\mathbb{E}_{x \\sim P_G(x; \\theta)} \\left[ \\log \\frac{P_{data}(x)}{P_{data}(x) + P_G(x; \\theta)} \\right]\n",
    "$$\n",
    "\n",
    "Thus, the minimax game can be reformulated as:\n",
    "\n",
    "$$\n",
    "\\min_G C(G) = \\min_G \\max_D V(D, G)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3dd1da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 6s 3ms/step\n",
      "0/100 [D loss: 0.5023618713021278 | D accuracy: 59.375] [G loss: 5.665816307067871]\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "1/100 [D loss: 0.0008865520940162241 | D accuracy: 100.0] [G loss: 8.528217315673828]\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/100 [D loss: 4.967133645550348e-05 | D accuracy: 100.0] [G loss: 10.451444625854492]\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "3/100 [D loss: 8.892960750017664e-06 | D accuracy: 100.0] [G loss: 11.859798431396484]\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "4/100 [D loss: 2.5628761690654756e-06 | D accuracy: 100.0] [G loss: 12.795978546142578]\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "5/100 [D loss: 1.2066299692747862e-06 | D accuracy: 100.0] [G loss: 13.540592193603516]\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "6/100 [D loss: 6.238304770533887e-07 | D accuracy: 100.0] [G loss: 14.012605667114258]\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "7/100 [D loss: 4.3458758014480736e-07 | D accuracy: 100.0] [G loss: 14.29888916015625]\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "8/100 [D loss: 3.7532952634755645e-07 | D accuracy: 100.0] [G loss: 14.466861724853516]\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "9/100 [D loss: 3.4146812386004477e-07 | D accuracy: 100.0] [G loss: 14.673149108886719]\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "10/100 [D loss: 3.3954375236455725e-07 | D accuracy: 100.0] [G loss: 14.603830337524414]\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "11/100 [D loss: 4.748374506369224e-07 | D accuracy: 100.0] [G loss: 14.395450592041016]\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "12/100 [D loss: 6.667283176942096e-07 | D accuracy: 100.0] [G loss: 14.151418685913086]\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "13/100 [D loss: 6.095514321069473e-07 | D accuracy: 100.0] [G loss: 14.148832321166992]\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "14/100 [D loss: 9.620729848336396e-07 | D accuracy: 100.0] [G loss: 13.759363174438477]\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "15/100 [D loss: 1.144234122507637e-06 | D accuracy: 100.0] [G loss: 13.442492485046387]\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "16/100 [D loss: 1.937941672802701e-06 | D accuracy: 100.0] [G loss: 13.186004638671875]\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "17/100 [D loss: 2.6597336974035213e-06 | D accuracy: 100.0] [G loss: 12.77297592163086]\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "18/100 [D loss: 3.8040972710137733e-06 | D accuracy: 100.0] [G loss: 12.190417289733887]\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "19/100 [D loss: 4.964423048370303e-06 | D accuracy: 100.0] [G loss: 11.861112594604492]\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "20/100 [D loss: 1.0522346813202077e-05 | D accuracy: 100.0] [G loss: 11.677103042602539]\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "21/100 [D loss: 2.092266276520604e-05 | D accuracy: 100.0] [G loss: 11.4920015335083]\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "22/100 [D loss: 2.2127259697940493e-05 | D accuracy: 100.0] [G loss: 10.788414001464844]\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "23/100 [D loss: 3.931396755132294e-05 | D accuracy: 100.0] [G loss: 10.579882621765137]\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "24/100 [D loss: 4.8591297076669426e-05 | D accuracy: 100.0] [G loss: 10.046167373657227]\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "25/100 [D loss: 9.644331178638305e-05 | D accuracy: 100.0] [G loss: 9.410028457641602]\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "26/100 [D loss: 0.0001266170900074831 | D accuracy: 100.0] [G loss: 9.139195442199707]\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "27/100 [D loss: 0.00014187430816381408 | D accuracy: 100.0] [G loss: 8.961108207702637]\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "28/100 [D loss: 0.00030901104394286463 | D accuracy: 100.0] [G loss: 8.56912612915039]\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "29/100 [D loss: 0.00036301319740724564 | D accuracy: 100.0] [G loss: 8.299703598022461]\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "30/100 [D loss: 0.0007307608608296823 | D accuracy: 100.0] [G loss: 7.803613662719727]\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "31/100 [D loss: 0.0008395599525486633 | D accuracy: 100.0] [G loss: 7.81707763671875]\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "32/100 [D loss: 0.0010835466894885577 | D accuracy: 100.0] [G loss: 7.149321556091309]\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "33/100 [D loss: 0.0018711879283223182 | D accuracy: 100.0] [G loss: 6.854073524475098]\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "34/100 [D loss: 0.0013394686463522112 | D accuracy: 100.0] [G loss: 7.056844711303711]\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "35/100 [D loss: 0.001968416871453691 | D accuracy: 100.0] [G loss: 6.581004619598389]\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "36/100 [D loss: 0.0024525178923035552 | D accuracy: 100.0] [G loss: 6.377767562866211]\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "37/100 [D loss: 0.0028151536599435173 | D accuracy: 100.0] [G loss: 6.436695575714111]\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "38/100 [D loss: 0.002751268927489548 | D accuracy: 100.0] [G loss: 6.230350494384766]\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "39/100 [D loss: 0.002742701345940013 | D accuracy: 100.0] [G loss: 6.476436614990234]\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "40/100 [D loss: 0.0038648422487630035 | D accuracy: 100.0] [G loss: 6.460648536682129]\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "41/100 [D loss: 0.002135982282908293 | D accuracy: 100.0] [G loss: 6.144505977630615]\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "42/100 [D loss: 0.002319632107311639 | D accuracy: 100.0] [G loss: 6.402884483337402]\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "43/100 [D loss: 0.002449102240480391 | D accuracy: 100.0] [G loss: 6.4999308586120605]\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "44/100 [D loss: 0.00162551135056066 | D accuracy: 100.0] [G loss: 6.6146087646484375]\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "45/100 [D loss: 0.0016015118670097469 | D accuracy: 100.0] [G loss: 6.631828784942627]\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "46/100 [D loss: 0.0014477035825088422 | D accuracy: 100.0] [G loss: 6.555530071258545]\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "47/100 [D loss: 0.0017681968294911863 | D accuracy: 100.0] [G loss: 6.502243995666504]\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "48/100 [D loss: 0.0015118503555996143 | D accuracy: 100.0] [G loss: 6.3095855712890625]\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "49/100 [D loss: 0.0012804926494510574 | D accuracy: 100.0] [G loss: 6.438077926635742]\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "50/100 [D loss: 0.0015967154189519466 | D accuracy: 100.0] [G loss: 6.726436614990234]\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "51/100 [D loss: 0.0013462940708372237 | D accuracy: 100.0] [G loss: 6.729179382324219]\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "52/100 [D loss: 0.0011097656134745892 | D accuracy: 100.0] [G loss: 6.550752639770508]\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "53/100 [D loss: 0.0010334712015263037 | D accuracy: 100.0] [G loss: 6.849071502685547]\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "54/100 [D loss: 0.0009472891021640208 | D accuracy: 100.0] [G loss: 6.904531478881836]\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "55/100 [D loss: 0.0009687629078251527 | D accuracy: 100.0] [G loss: 6.851644515991211]\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "56/100 [D loss: 0.0009997464321567229 | D accuracy: 100.0] [G loss: 6.733766078948975]\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "57/100 [D loss: 0.001183273744049691 | D accuracy: 100.0] [G loss: 6.730710983276367]\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "58/100 [D loss: 0.000976836090008256 | D accuracy: 100.0] [G loss: 6.705848217010498]\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "59/100 [D loss: 0.0008548319498831863 | D accuracy: 100.0] [G loss: 6.801575660705566]\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "60/100 [D loss: 0.0009630512549020087 | D accuracy: 100.0] [G loss: 7.112064838409424]\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "61/100 [D loss: 0.001020122807562614 | D accuracy: 100.0] [G loss: 6.724672317504883]\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "62/100 [D loss: 0.0009995034602395883 | D accuracy: 100.0] [G loss: 6.833573341369629]\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "63/100 [D loss: 0.0010427900783227528 | D accuracy: 100.0] [G loss: 6.747490882873535]\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "64/100 [D loss: 0.0010594821093358755 | D accuracy: 100.0] [G loss: 6.598271369934082]\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "65/100 [D loss: 0.0009867199793019412 | D accuracy: 100.0] [G loss: 6.580695152282715]\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "66/100 [D loss: 0.0009393257922610801 | D accuracy: 100.0] [G loss: 6.433055400848389]\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "67/100 [D loss: 0.0009310640204436798 | D accuracy: 100.0] [G loss: 6.703426361083984]\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "68/100 [D loss: 0.0010445360118269361 | D accuracy: 100.0] [G loss: 6.699738025665283]\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "69/100 [D loss: 0.0009929602977773496 | D accuracy: 100.0] [G loss: 6.521219730377197]\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "70/100 [D loss: 0.0010120165552578483 | D accuracy: 100.0] [G loss: 6.5118231773376465]\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "71/100 [D loss: 0.0009804819740438292 | D accuracy: 100.0] [G loss: 6.285118103027344]\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "72/100 [D loss: 0.0012340765392787265 | D accuracy: 100.0] [G loss: 6.4192423820495605]\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "73/100 [D loss: 0.0012325376746262378 | D accuracy: 100.0] [G loss: 6.423679351806641]\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "74/100 [D loss: 0.0011132534349885606 | D accuracy: 100.0] [G loss: 6.185286045074463]\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "75/100 [D loss: 0.0012966239822128165 | D accuracy: 100.0] [G loss: 6.215956211090088]\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "76/100 [D loss: 0.0013086000710957164 | D accuracy: 100.0] [G loss: 6.220928192138672]\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "77/100 [D loss: 0.0012328918183742146 | D accuracy: 100.0] [G loss: 6.078987121582031]\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "78/100 [D loss: 0.0013730782145033038 | D accuracy: 100.0] [G loss: 6.227741718292236]\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "79/100 [D loss: 0.001411703775427325 | D accuracy: 100.0] [G loss: 6.269256591796875]\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "80/100 [D loss: 0.0017026437025115045 | D accuracy: 100.0] [G loss: 6.072441101074219]\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "81/100 [D loss: 0.0017220701778909216 | D accuracy: 100.0] [G loss: 5.9015069007873535]\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "82/100 [D loss: 0.0017504501159109889 | D accuracy: 100.0] [G loss: 5.974978446960449]\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "83/100 [D loss: 0.001684050576083694 | D accuracy: 100.0] [G loss: 6.106900691986084]\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "84/100 [D loss: 0.001968804757826119 | D accuracy: 100.0] [G loss: 5.934139251708984]\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "85/100 [D loss: 0.002034050881144705 | D accuracy: 100.0] [G loss: 5.834445953369141]\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "86/100 [D loss: 0.001921890659505663 | D accuracy: 100.0] [G loss: 5.875441074371338]\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "87/100 [D loss: 0.0021433797676037878 | D accuracy: 100.0] [G loss: 5.827308654785156]\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "88/100 [D loss: 0.0020991133088579996 | D accuracy: 100.0] [G loss: 5.851866722106934]\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "89/100 [D loss: 0.002137154341478265 | D accuracy: 100.0] [G loss: 5.66064453125]\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "90/100 [D loss: 0.0022368363632196708 | D accuracy: 100.0] [G loss: 5.730804443359375]\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "91/100 [D loss: 0.0023065671211099925 | D accuracy: 100.0] [G loss: 5.741121768951416]\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "92/100 [D loss: 0.0022282037130082356 | D accuracy: 100.0] [G loss: 5.585329055786133]\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "93/100 [D loss: 0.0022097700276782926 | D accuracy: 100.0] [G loss: 5.548122882843018]\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "94/100 [D loss: 0.0026116850854835095 | D accuracy: 100.0] [G loss: 5.576689720153809]\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "95/100 [D loss: 0.002783604669135712 | D accuracy: 100.0] [G loss: 5.5459394454956055]\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "96/100 [D loss: 0.002874583280996164 | D accuracy: 100.0] [G loss: 5.549485683441162]\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "97/100 [D loss: 0.0027672373704000536 | D accuracy: 100.0] [G loss: 5.435427665710449]\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "98/100 [D loss: 0.0029653993021395756 | D accuracy: 100.0] [G loss: 5.332969665527344]\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "99/100 [D loss: 0.003217977937624003 | D accuracy: 100.0] [G loss: 5.3859734535217285]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, LeakyReLU, Flatten, Reshape\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Define the generator model\n",
    "def build_generator():\n",
    "    model = Sequential([\n",
    "        Dense(128, input_dim=100),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "        Dense(784, activation='sigmoid'),\n",
    "        Reshape((28, 28, 1))\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Define the discriminator model\n",
    "def build_discriminator():\n",
    "    model = Sequential([\n",
    "        Flatten(input_shape=(28, 28, 1)),\n",
    "        Dense(128),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Define the GAN model\n",
    "def build_gan(generator, discriminator):\n",
    "    model = Sequential([\n",
    "        generator,\n",
    "        discriminator\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Loss functions and optimizers\n",
    "def compile_models(generator, discriminator, gan):\n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    discriminator.trainable = False\n",
    "    gan.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "# Training function\n",
    "def train_gan(generator, discriminator, gan, epochs, batch_size, noise_dim):\n",
    "    (x_train, _), (_, _) = tf.keras.datasets.mnist.load_data()\n",
    "    x_train = (x_train / 127.5) - 1.0  # Normalize to [-1, 1]\n",
    "    x_train = np.expand_dims(x_train, axis=-1)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Train discriminator\n",
    "        idx = np.random.randint(0, x_train.shape[0], batch_size)\n",
    "        real_imgs = x_train[idx]\n",
    "        noise = np.random.normal(0, 1, (batch_size, noise_dim))\n",
    "        fake_imgs = generator.predict(noise)\n",
    "        \n",
    "        d_loss_real = discriminator.train_on_batch(real_imgs, np.ones((batch_size, 1)))\n",
    "        d_loss_fake = discriminator.train_on_batch(fake_imgs, np.zeros((batch_size, 1)))\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "        \n",
    "        # Train generator\n",
    "        noise = np.random.normal(0, 1, (batch_size, noise_dim))\n",
    "        g_loss = gan.train_on_batch(noise, np.ones((batch_size, 1)))\n",
    "\n",
    "        print(f\"{epoch}/{epochs} [D loss: {d_loss[0]} | D accuracy: {100*d_loss[1]}] [G loss: {g_loss}]\")\n",
    "\n",
    "# Parameters\n",
    "noise_dim = 100\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "\n",
    "# Initialize models\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "gan = build_gan(generator, discriminator)\n",
    "\n",
    "# Compile models\n",
    "compile_models(generator, discriminator, gan)\n",
    "\n",
    "# Train the GAN\n",
    "train_gan(generator, discriminator, gan, epochs, batch_size, noise_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50c8a3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/10000 | Discriminator Loss: 1.3673646854152155 | Generator Loss: 0.6932898828521699\n",
      "Epoch 100/10000 | Discriminator Loss: 1.3863426289182454 | Generator Loss: 0.6955398039142306\n",
      "Epoch 200/10000 | Discriminator Loss: 1.3783276357677972 | Generator Loss: 0.7009244783068874\n",
      "Epoch 300/10000 | Discriminator Loss: 1.379064041800909 | Generator Loss: 0.6987808019030556\n",
      "Epoch 400/10000 | Discriminator Loss: 1.382616607067742 | Generator Loss: 0.6939111065566943\n",
      "Epoch 500/10000 | Discriminator Loss: 1.3834139887791221 | Generator Loss: 0.6932189390109719\n",
      "Epoch 600/10000 | Discriminator Loss: 1.381148982407292 | Generator Loss: 0.6958987669706982\n",
      "Epoch 700/10000 | Discriminator Loss: 1.3788100738107207 | Generator Loss: 0.696440820104006\n",
      "Epoch 800/10000 | Discriminator Loss: 1.3801858197509445 | Generator Loss: 0.6961199354629051\n",
      "Epoch 900/10000 | Discriminator Loss: 1.3839849690700843 | Generator Loss: 0.6943145966406559\n",
      "Epoch 1000/10000 | Discriminator Loss: 1.3870631929239496 | Generator Loss: 0.6922038491579333\n",
      "Epoch 1100/10000 | Discriminator Loss: 1.385214260618497 | Generator Loss: 0.6938314488859375\n",
      "Epoch 1200/10000 | Discriminator Loss: 1.3865597753022891 | Generator Loss: 0.6934520367693839\n",
      "Epoch 1300/10000 | Discriminator Loss: 1.385051289439681 | Generator Loss: 0.694707336632571\n",
      "Epoch 1400/10000 | Discriminator Loss: 1.3880431891898541 | Generator Loss: 0.6925475504077134\n",
      "Epoch 1500/10000 | Discriminator Loss: 1.3864802062804722 | Generator Loss: 0.6934165498611745\n",
      "Epoch 1600/10000 | Discriminator Loss: 1.385107986766727 | Generator Loss: 0.6935693195915985\n",
      "Epoch 1700/10000 | Discriminator Loss: 1.3865724437710112 | Generator Loss: 0.6928673970085064\n",
      "Epoch 1800/10000 | Discriminator Loss: 1.3873119621480932 | Generator Loss: 0.6926295855605857\n",
      "Epoch 1900/10000 | Discriminator Loss: 1.3862889685901978 | Generator Loss: 0.6932712012771792\n",
      "Epoch 2000/10000 | Discriminator Loss: 1.3871773111344217 | Generator Loss: 0.6927804828147304\n",
      "Epoch 2100/10000 | Discriminator Loss: 1.3854283924703514 | Generator Loss: 0.6937820548270326\n",
      "Epoch 2200/10000 | Discriminator Loss: 1.3854736513762722 | Generator Loss: 0.6936967619492133\n",
      "Epoch 2300/10000 | Discriminator Loss: 1.3846187926755773 | Generator Loss: 0.6939508533178637\n",
      "Epoch 2400/10000 | Discriminator Loss: 1.386031672981476 | Generator Loss: 0.6931850287889264\n",
      "Epoch 2500/10000 | Discriminator Loss: 1.3860167613197854 | Generator Loss: 0.6931979998476621\n",
      "Epoch 2600/10000 | Discriminator Loss: 1.385392075691672 | Generator Loss: 0.6936694448884013\n",
      "Epoch 2700/10000 | Discriminator Loss: 1.3851798523135903 | Generator Loss: 0.6938157253050868\n",
      "Epoch 2800/10000 | Discriminator Loss: 1.3858852094628364 | Generator Loss: 0.6934576134903181\n",
      "Epoch 2900/10000 | Discriminator Loss: 1.3857676921053015 | Generator Loss: 0.6932700076975671\n",
      "Epoch 3000/10000 | Discriminator Loss: 1.385798389031068 | Generator Loss: 0.6932868431364173\n",
      "Epoch 3100/10000 | Discriminator Loss: 1.3858561611997655 | Generator Loss: 0.6933058393091615\n",
      "Epoch 3200/10000 | Discriminator Loss: 1.3857862869760513 | Generator Loss: 0.6935068764448583\n",
      "Epoch 3300/10000 | Discriminator Loss: 1.3847960379963216 | Generator Loss: 0.6939526216062455\n",
      "Epoch 3400/10000 | Discriminator Loss: 1.3847792280469995 | Generator Loss: 0.6940286031590092\n",
      "Epoch 3500/10000 | Discriminator Loss: 1.3845606417476723 | Generator Loss: 0.6939467293985767\n",
      "Epoch 3600/10000 | Discriminator Loss: 1.3852012452277669 | Generator Loss: 0.6937204311539185\n",
      "Epoch 3700/10000 | Discriminator Loss: 1.3857316302745177 | Generator Loss: 0.6933819453464772\n",
      "Epoch 3800/10000 | Discriminator Loss: 1.3855015311845302 | Generator Loss: 0.6935015642457129\n",
      "Epoch 3900/10000 | Discriminator Loss: 1.38542345236394 | Generator Loss: 0.6935753258230636\n",
      "Epoch 4000/10000 | Discriminator Loss: 1.3853781571965014 | Generator Loss: 0.6936699879401584\n",
      "Epoch 4100/10000 | Discriminator Loss: 1.38563506174504 | Generator Loss: 0.6934777923205945\n",
      "Epoch 4200/10000 | Discriminator Loss: 1.3856782302033228 | Generator Loss: 0.6934307321121426\n",
      "Epoch 4300/10000 | Discriminator Loss: 1.3857316643901139 | Generator Loss: 0.6933893742203083\n",
      "Epoch 4400/10000 | Discriminator Loss: 1.3857686180891413 | Generator Loss: 0.6934710596904107\n",
      "Epoch 4500/10000 | Discriminator Loss: 1.3854548512818625 | Generator Loss: 0.6935478445396879\n",
      "Epoch 4600/10000 | Discriminator Loss: 1.3858437682736997 | Generator Loss: 0.693341584034682\n",
      "Epoch 4700/10000 | Discriminator Loss: 1.38552597155486 | Generator Loss: 0.6934602413083275\n",
      "Epoch 4800/10000 | Discriminator Loss: 1.3856538080512164 | Generator Loss: 0.69341964303867\n",
      "Epoch 4900/10000 | Discriminator Loss: 1.385668013121794 | Generator Loss: 0.69348165854736\n",
      "Epoch 5000/10000 | Discriminator Loss: 1.385446485089905 | Generator Loss: 0.6934706725273234\n",
      "Epoch 5100/10000 | Discriminator Loss: 1.3852440299112971 | Generator Loss: 0.693620922959167\n",
      "Epoch 5200/10000 | Discriminator Loss: 1.3854312618660387 | Generator Loss: 0.6935409643558544\n",
      "Epoch 5300/10000 | Discriminator Loss: 1.385574035280702 | Generator Loss: 0.6935221793938859\n",
      "Epoch 5400/10000 | Discriminator Loss: 1.3853269597801732 | Generator Loss: 0.6936382904019335\n",
      "Epoch 5500/10000 | Discriminator Loss: 1.385596742367185 | Generator Loss: 0.693473589737744\n",
      "Epoch 5600/10000 | Discriminator Loss: 1.3854579745139521 | Generator Loss: 0.6935537285136196\n",
      "Epoch 5700/10000 | Discriminator Loss: 1.3852105066137639 | Generator Loss: 0.6937197541466524\n",
      "Epoch 5800/10000 | Discriminator Loss: 1.3851084893067973 | Generator Loss: 0.6937500139203586\n",
      "Epoch 5900/10000 | Discriminator Loss: 1.385480325930095 | Generator Loss: 0.6934712394466047\n",
      "Epoch 6000/10000 | Discriminator Loss: 1.38551950724126 | Generator Loss: 0.693573125433474\n",
      "Epoch 6100/10000 | Discriminator Loss: 1.3854921921929242 | Generator Loss: 0.6935579184998205\n",
      "Epoch 6200/10000 | Discriminator Loss: 1.3854974277744345 | Generator Loss: 0.6936302820883202\n",
      "Epoch 6300/10000 | Discriminator Loss: 1.38558380005601 | Generator Loss: 0.6934806083930666\n",
      "Epoch 6400/10000 | Discriminator Loss: 1.3855591924386057 | Generator Loss: 0.6934727534116761\n",
      "Epoch 6500/10000 | Discriminator Loss: 1.385624345180259 | Generator Loss: 0.6935288192119275\n",
      "Epoch 6600/10000 | Discriminator Loss: 1.3855878148915906 | Generator Loss: 0.6935005504262114\n",
      "Epoch 6700/10000 | Discriminator Loss: 1.3856467734254083 | Generator Loss: 0.6934518604786043\n",
      "Epoch 6800/10000 | Discriminator Loss: 1.385732606562141 | Generator Loss: 0.6934562462275786\n",
      "Epoch 6900/10000 | Discriminator Loss: 1.385611437014448 | Generator Loss: 0.6934627777747827\n",
      "Epoch 7000/10000 | Discriminator Loss: 1.3858565688093745 | Generator Loss: 0.6933786099610603\n",
      "Epoch 7100/10000 | Discriminator Loss: 1.3858435567467595 | Generator Loss: 0.6933674211364318\n",
      "Epoch 7200/10000 | Discriminator Loss: 1.385747884514713 | Generator Loss: 0.6933874097492114\n",
      "Epoch 7300/10000 | Discriminator Loss: 1.3858196005521344 | Generator Loss: 0.6933857920824791\n",
      "Epoch 7400/10000 | Discriminator Loss: 1.385862916962588 | Generator Loss: 0.6933490274074173\n",
      "Epoch 7500/10000 | Discriminator Loss: 1.385947113477898 | Generator Loss: 0.6933130319279767\n",
      "Epoch 7600/10000 | Discriminator Loss: 1.3857377369452795 | Generator Loss: 0.6934770647206683\n",
      "Epoch 7700/10000 | Discriminator Loss: 1.3857520707623436 | Generator Loss: 0.6934002649410267\n",
      "Epoch 7800/10000 | Discriminator Loss: 1.3859312763169396 | Generator Loss: 0.6933066058699148\n",
      "Epoch 7900/10000 | Discriminator Loss: 1.3859300537108563 | Generator Loss: 0.693294689250335\n",
      "Epoch 8000/10000 | Discriminator Loss: 1.3858782824755789 | Generator Loss: 0.6933808260954574\n",
      "Epoch 8100/10000 | Discriminator Loss: 1.3858486039104223 | Generator Loss: 0.6933585972085736\n",
      "Epoch 8200/10000 | Discriminator Loss: 1.385801226597621 | Generator Loss: 0.6934579206880179\n",
      "Epoch 8300/10000 | Discriminator Loss: 1.3859325818052968 | Generator Loss: 0.6933092078667115\n",
      "Epoch 8400/10000 | Discriminator Loss: 1.3858317224982264 | Generator Loss: 0.6933973477301406\n",
      "Epoch 8500/10000 | Discriminator Loss: 1.3859745671841206 | Generator Loss: 0.6933007767542988\n",
      "Epoch 8600/10000 | Discriminator Loss: 1.385914079915616 | Generator Loss: 0.6933285578090319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8700/10000 | Discriminator Loss: 1.385821536507457 | Generator Loss: 0.6933942298497333\n",
      "Epoch 8800/10000 | Discriminator Loss: 1.3858537043669987 | Generator Loss: 0.6933780314761407\n",
      "Epoch 8900/10000 | Discriminator Loss: 1.3858062939261888 | Generator Loss: 0.6933862499799917\n",
      "Epoch 9000/10000 | Discriminator Loss: 1.3858925350862403 | Generator Loss: 0.693347712824054\n",
      "Epoch 9100/10000 | Discriminator Loss: 1.385866436734107 | Generator Loss: 0.6933646168820407\n",
      "Epoch 9200/10000 | Discriminator Loss: 1.3859031626077098 | Generator Loss: 0.6933515764988605\n",
      "Epoch 9300/10000 | Discriminator Loss: 1.3859430504160697 | Generator Loss: 0.6933352199500638\n",
      "Epoch 9400/10000 | Discriminator Loss: 1.3859948600944048 | Generator Loss: 0.6932986160931486\n",
      "Epoch 9500/10000 | Discriminator Loss: 1.386018808446156 | Generator Loss: 0.6933077812304953\n",
      "Epoch 9600/10000 | Discriminator Loss: 1.386058755332531 | Generator Loss: 0.6932564145421748\n",
      "Epoch 9700/10000 | Discriminator Loss: 1.3859396745044674 | Generator Loss: 0.6933234902019435\n",
      "Epoch 9800/10000 | Discriminator Loss: 1.385843917119078 | Generator Loss: 0.6934021725987445\n",
      "Epoch 9900/10000 | Discriminator Loss: 1.3859567033091125 | Generator Loss: 0.6933018179568589\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Helper functions\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1.0 - np.tanh(x)**2\n",
    "\n",
    "# Define the generator model\n",
    "class Generator:\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = np.random.randn(input_dim, output_dim) * 0.01\n",
    "        self.biases = np.zeros((1, output_dim))\n",
    "\n",
    "    def forward(self, z):\n",
    "        return tanh(np.dot(z, self.weights) + self.biases)\n",
    "\n",
    "# Define the discriminator model\n",
    "class Discriminator:\n",
    "    def __init__(self, input_dim):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = np.random.randn(input_dim, 1) * 0.01\n",
    "        self.biases = np.zeros((1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return sigmoid(np.dot(x, self.weights) + self.biases)\n",
    "\n",
    "# Training function\n",
    "def train_gan(generator, discriminator, data, epochs, batch_size, noise_dim, learning_rate):\n",
    "    for epoch in range(epochs):\n",
    "        # Generate fake data\n",
    "        z = np.random.randn(batch_size, noise_dim)\n",
    "        fake_data = generator.forward(z)\n",
    "\n",
    "        # Train discriminator\n",
    "        real_data = data[np.random.randint(0, data.shape[0], batch_size)]\n",
    "        real_labels = np.ones((batch_size, 1))\n",
    "        fake_labels = np.zeros((batch_size, 1))\n",
    "\n",
    "        real_output = discriminator.forward(real_data)\n",
    "        fake_output = discriminator.forward(fake_data)\n",
    "\n",
    "        # Discriminator loss and gradients\n",
    "        d_loss_real = -np.mean(np.log(real_output))\n",
    "        d_loss_fake = -np.mean(np.log(1 - fake_output))\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "        # Compute gradients for discriminator\n",
    "        d_loss_grad = (real_output - 1) + fake_output\n",
    "\n",
    "        # Update discriminator weights and biases\n",
    "        discriminator.weights -= learning_rate * np.dot(real_data.T, d_loss_grad)\n",
    "        discriminator.biases -= learning_rate * np.sum(d_loss_grad, axis=0, keepdims=True)\n",
    "\n",
    "        # Train generator\n",
    "        z = np.random.randn(batch_size, noise_dim)\n",
    "        fake_data = generator.forward(z)\n",
    "        fake_output = discriminator.forward(fake_data)\n",
    "\n",
    "        # Generator loss and gradients\n",
    "        g_loss = -np.mean(np.log(fake_output))\n",
    "\n",
    "        # Compute gradients for generator\n",
    "        g_loss_grad = fake_output - 1\n",
    "\n",
    "        # Update generator weights and biases\n",
    "        generator.weights -= learning_rate * np.dot(z.T, g_loss_grad)\n",
    "        generator.biases -= learning_rate * np.sum(g_loss_grad, axis=0, keepdims=True)\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(f'Epoch {epoch}/{epochs} | Discriminator Loss: {d_loss} | Generator Loss: {g_loss}')\n",
    "\n",
    "# Parameters\n",
    "noise_dim = 100\n",
    "data_dim = 784  # Example for flattened 28x28 images\n",
    "epochs = 10000\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Dummy data (e.g., flattened MNIST images)\n",
    "data = np.random.randn(1000, data_dim)\n",
    "\n",
    "# Initialize models\n",
    "generator = Generator(noise_dim, data_dim)\n",
    "discriminator = Discriminator(data_dim)\n",
    "\n",
    "# Train the GAN\n",
    "train_gan(generator, discriminator, data, epochs, batch_size, noise_dim, learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c231d3",
   "metadata": {},
   "source": [
    "## Bidirectional Generative Adversarial Networks (BiGANs)\n",
    "\n",
    "The Generative Adversarial Network (GAN) framework traditionally focuses on learning the mapping from a latent space to a data space. However, many applications benefit from learning both the forward mapping (latent to data) and the inverse mapping (data to latent). Bidirectional Generative Adversarial Networks (BiGANs) address this by introducing an encoder to complement the traditional generator and discriminator.\n",
    "\n",
    "### Overview\n",
    "\n",
    "In a standard GAN, the setup consists of:\n",
    "\n",
    "- **Generator \\(G\\)**: Maps latent samples \\(z\\) to generated data \\(G(z)\\).\n",
    "- **Discriminator \\(D\\)**: Distinguishes between real data and generated data.\n",
    "  \n",
    "BiGAN extends this by adding an **Encoder \\(E\\)** that maps data \\(x\\) back to the latent space \\(E(x)\\). The BiGAN framework includes:\n",
    "\n",
    "1. **Generator \\(G\\)**: Maps latent samples \\(z\\) to generated data \\(G(z)\\).\n",
    "2. **Encoder \\(E\\)**: Maps data \\(x\\) to latent space \\(E(x)\\).\n",
    "3. **Discriminator \\(D\\)**: Discriminates between:\n",
    "   - The tuple \\((G(z), z)\\), which is the generated data and its corresponding latent code.\n",
    "   - The tuple \\((x, E(x))\\), which is the real data and its encoded latent representation.\n",
    "\n",
    "The discriminator thus learns to distinguish between these tuples in both data and latent spaces.\n",
    "\n",
    "### BiGAN Objective\n",
    "\n",
    "The BiGAN framework trains the generator \\(G\\) and encoder \\(E\\) in a minimax game against the discriminator \\(D\\). The training objective is formulated as:\n",
    "\n",
    "$$\n",
    "\\min_{G, E} \\max_{D} V(G, E, D) = \\mathbb{E}_{x \\sim P_x} \\left[ \\mathbb{E}_{z \\sim P_E(\\cdot|x)} \\left[ \\log D(x, E(x)) \\right] \\right]\n",
    "+ \\mathbb{E}_{z \\sim P_z} \\left[ \\mathbb{E}_{x \\sim P_G(\\cdot|z)} \\left[ \\log \\left(1 - D(G(z), z) \\right) \\right] \\right]\n",
    "$$\n",
    "\n",
    "where:\n",
    "- \\(P_x\\) is the data distribution.\n",
    "- \\(P_z\\) is the prior distribution over latent variables.\n",
    "- \\(P_E(\\cdot|x)\\) is the conditional distribution of \\(z\\) given \\(x\\).\n",
    "- \\(P_G(\\cdot|z)\\) is the distribution of \\(x\\) given \\(z\\).\n",
    "\n",
    "### Generalized BiGAN Objective\n",
    "\n",
    "In some scenarios, it is beneficial to work with different spaces for the generator and encoder. We can generalize the BiGAN objective with functions \\(g(x)\\) and \\(g(z)\\) that map the data and latent spaces to alternative spaces \\(x^\\prime\\) and \\(z^\\prime\\). The generalized objective becomes:\n",
    "\n",
    "$$\n",
    "V(G, E, D) = \\mathbb{E}_{x \\sim P_x} \\left[ \\mathbb{E}_{z^\\prime \\sim P_E(\\cdot|x)} \\left[ \\log D(g(x), z^\\prime) \\right] \\right]\n",
    "+ \\mathbb{E}_{z \\sim P_z} \\left[ \\mathbb{E}_{x^\\prime \\sim P_G(\\cdot|z)} \\left[ \\log \\left(1 - D(x^\\prime, g(z)) \\right) \\right] \\right]\n",
    "$$\n",
    "\n",
    "where:\n",
    "- \\(g(x): x \\rightarrow x^\\prime\\) and \\(g(z): z \\rightarrow z^\\prime\\) are mappings to alternative spaces.\n",
    "- \\(E: x \\rightarrow z^\\prime\\) and \\(G: z \\rightarrow x^\\prime\\) are the generalized encoder and generator, respectively.\n",
    "- \\(D: x^\\prime \\times z^\\prime \\rightarrow [0, 1]\\) is the generalized discriminator.\n",
    "\n",
    "This formulation simplifies to the original BiGAN objective when \\(g(x) = x\\) and \\(g(z) = z\\).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e98a331f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Discriminator Loss: 0.8389650232113997, Generator Loss: 0.023077398256270327\n",
      "Iteration 1: Discriminator Loss: 1.5543132924389131, Generator Loss: 0.4412395210379177\n",
      "Iteration 2: Discriminator Loss: 0.650138179203201, Generator Loss: 0.5877939679816339\n",
      "Iteration 3: Discriminator Loss: 2.1062078063828507, Generator Loss: 0.5696365524398422\n",
      "Iteration 4: Discriminator Loss: 1.6207304982612272, Generator Loss: 0.10754715659936356\n",
      "Iteration 5: Discriminator Loss: 1.3590042771778212, Generator Loss: 0.17667900768794056\n",
      "Iteration 6: Discriminator Loss: 0.055383663731513835, Generator Loss: 0.5283620105397226\n",
      "Iteration 7: Discriminator Loss: 2.6844636285646373, Generator Loss: 0.0037931487281276585\n",
      "Iteration 8: Discriminator Loss: 1.867410195066205, Generator Loss: 1.3530708375012612\n",
      "Iteration 9: Discriminator Loss: 1.5430787958828422, Generator Loss: 0.46132076300415\n",
      "Iteration 10: Discriminator Loss: 1.6514824882250672, Generator Loss: 0.8878177417811494\n",
      "Iteration 11: Discriminator Loss: 1.7053584153579573, Generator Loss: 0.2656374422509944\n",
      "Iteration 12: Discriminator Loss: 1.372824770896793, Generator Loss: 0.956803069684793\n",
      "Iteration 13: Discriminator Loss: 1.7235473512124462, Generator Loss: 1.274719732730945\n",
      "Iteration 14: Discriminator Loss: 0.4401174374002934, Generator Loss: 0.05827057462456073\n",
      "Iteration 15: Discriminator Loss: 2.275373719371164, Generator Loss: 2.670260160760955\n",
      "Iteration 16: Discriminator Loss: 1.002714362062482, Generator Loss: 0.11592099489909936\n",
      "Iteration 17: Discriminator Loss: 3.344697474954005, Generator Loss: 1.3923086824804853\n",
      "Iteration 18: Discriminator Loss: 1.959741269401154, Generator Loss: 0.2711698729486635\n",
      "Iteration 19: Discriminator Loss: 1.4804730185030903, Generator Loss: 0.9139135486374009\n",
      "Iteration 20: Discriminator Loss: 0.8407641162993587, Generator Loss: 3.9865678644802296\n",
      "Iteration 21: Discriminator Loss: 2.710424825442067, Generator Loss: 0.8438669144349797\n",
      "Iteration 22: Discriminator Loss: 3.1510834743359513, Generator Loss: 0.9890050908433019\n",
      "Iteration 23: Discriminator Loss: 1.477513042878032, Generator Loss: 2.714184701609206\n",
      "Iteration 24: Discriminator Loss: 4.004488282240787, Generator Loss: 0.29945102410939706\n",
      "Iteration 25: Discriminator Loss: 1.6425361691980878, Generator Loss: 0.2758319509255529\n",
      "Iteration 26: Discriminator Loss: 1.4282165916498981, Generator Loss: 0.8552329047028513\n",
      "Iteration 27: Discriminator Loss: 1.3458892434270053, Generator Loss: 0.3933843848575159\n",
      "Iteration 28: Discriminator Loss: 2.7703788740028887, Generator Loss: 0.6382685564567903\n",
      "Iteration 29: Discriminator Loss: 0.6953208933177952, Generator Loss: 2.6268586991109624\n",
      "Iteration 30: Discriminator Loss: 2.4165072321912118, Generator Loss: 0.28764416089871064\n",
      "Iteration 31: Discriminator Loss: 1.9465933655092513, Generator Loss: 0.24039441784335885\n",
      "Iteration 32: Discriminator Loss: 1.007301393285772, Generator Loss: 0.2248515217197432\n",
      "Iteration 33: Discriminator Loss: 2.12683636814139, Generator Loss: 0.529111041041569\n",
      "Iteration 34: Discriminator Loss: 2.2369345173485353, Generator Loss: 1.0791113827593724\n",
      "Iteration 35: Discriminator Loss: 1.041075138084526, Generator Loss: 1.2767650291407264\n",
      "Iteration 36: Discriminator Loss: 0.12165037020662155, Generator Loss: 0.07062214988115896\n",
      "Iteration 37: Discriminator Loss: 3.5018383712250296, Generator Loss: 1.8132960575552943\n",
      "Iteration 38: Discriminator Loss: 1.7398481998211812, Generator Loss: 2.107032742017251\n",
      "Iteration 39: Discriminator Loss: 1.898589743683366, Generator Loss: 1.4784054615928672\n",
      "Iteration 40: Discriminator Loss: 3.6988378549877448, Generator Loss: 0.16589325111444353\n",
      "Iteration 41: Discriminator Loss: 4.334375614694452, Generator Loss: 1.5831750347786953\n",
      "Iteration 42: Discriminator Loss: 2.76784659591116, Generator Loss: 0.15800623352345874\n",
      "Iteration 43: Discriminator Loss: 0.5469412296160676, Generator Loss: 0.8584004752951604\n",
      "Iteration 44: Discriminator Loss: 1.863124036447112, Generator Loss: 0.4971322787086261\n",
      "Iteration 45: Discriminator Loss: 1.8596688854250898, Generator Loss: 1.2442984549466911\n",
      "Iteration 46: Discriminator Loss: 1.974166123211726, Generator Loss: 0.5684886501183201\n",
      "Iteration 47: Discriminator Loss: 1.4306021839272256, Generator Loss: 0.4581072473155187\n",
      "Iteration 48: Discriminator Loss: 2.9315034950756784, Generator Loss: 1.7282555660893115\n",
      "Iteration 49: Discriminator Loss: 1.9337008798928652, Generator Loss: 0.626927412622787\n",
      "Iteration 50: Discriminator Loss: 2.477881498453921, Generator Loss: 3.009753232348032\n",
      "Iteration 51: Discriminator Loss: 1.922399765030868, Generator Loss: 0.9016950902246325\n",
      "Iteration 52: Discriminator Loss: 2.9542212758369706, Generator Loss: 0.049659740493824725\n",
      "Iteration 53: Discriminator Loss: 2.831238163929608, Generator Loss: 1.8284780965028529\n",
      "Iteration 54: Discriminator Loss: 0.1362806326061747, Generator Loss: 0.3279837575224862\n",
      "Iteration 55: Discriminator Loss: 0.5442985966940095, Generator Loss: 1.5356276297472342\n",
      "Iteration 56: Discriminator Loss: 0.909712636415623, Generator Loss: 0.09192449105500486\n",
      "Iteration 57: Discriminator Loss: 2.6716762792041777, Generator Loss: 6.590669167087759\n",
      "Iteration 58: Discriminator Loss: 1.5513134801281832, Generator Loss: 0.3543352202818461\n",
      "Iteration 59: Discriminator Loss: 2.6205257149814236, Generator Loss: 0.7433891852560107\n",
      "Iteration 60: Discriminator Loss: 0.5573409597046434, Generator Loss: 2.3883685221105715\n",
      "Iteration 61: Discriminator Loss: 0.3494568604935029, Generator Loss: 0.5196010715001638\n",
      "Iteration 62: Discriminator Loss: 1.9809686832857767, Generator Loss: 0.048241332314679274\n",
      "Iteration 63: Discriminator Loss: 3.4796494912079705, Generator Loss: 0.34545119931484874\n",
      "Iteration 64: Discriminator Loss: 1.803153705667799, Generator Loss: 0.7544609042829117\n",
      "Iteration 65: Discriminator Loss: 0.4350555663168682, Generator Loss: 1.3367073924535398\n",
      "Iteration 66: Discriminator Loss: 2.2881129010617536, Generator Loss: 0.08628462454919508\n",
      "Iteration 67: Discriminator Loss: 2.0006481285925726, Generator Loss: 7.7070288281518025\n",
      "Iteration 68: Discriminator Loss: 1.052354537012623, Generator Loss: 0.6119544408158339\n",
      "Iteration 69: Discriminator Loss: 4.3255223031153704, Generator Loss: 0.32565906859241744\n",
      "Iteration 70: Discriminator Loss: 3.677567063559146, Generator Loss: 0.5692691353214392\n",
      "Iteration 71: Discriminator Loss: 2.8147775610085994, Generator Loss: 1.6226715591654863\n",
      "Iteration 72: Discriminator Loss: 0.4398015446506305, Generator Loss: 0.022019951051075726\n",
      "Iteration 73: Discriminator Loss: 3.08573721193585, Generator Loss: 0.905929664355292\n",
      "Iteration 74: Discriminator Loss: 1.5481218060873791, Generator Loss: 0.13951935146620337\n",
      "Iteration 75: Discriminator Loss: 2.3704734701584935, Generator Loss: 0.6787450426032501\n",
      "Iteration 76: Discriminator Loss: 3.9348758791768352, Generator Loss: 0.09907644801009771\n",
      "Iteration 77: Discriminator Loss: 0.4075318505614933, Generator Loss: 1.3117759763809322\n",
      "Iteration 78: Discriminator Loss: 0.5910568834132688, Generator Loss: 1.5442088503933664\n",
      "Iteration 79: Discriminator Loss: 2.158585546000403, Generator Loss: 0.5925912060262047\n",
      "Iteration 80: Discriminator Loss: 1.041896752245215, Generator Loss: 0.2262572730145507\n",
      "Iteration 81: Discriminator Loss: 1.3485674182766658, Generator Loss: 1.0867113665933825\n",
      "Iteration 82: Discriminator Loss: 0.5726000585316712, Generator Loss: 0.4035170009740327\n",
      "Iteration 83: Discriminator Loss: 0.7190882887627654, Generator Loss: 0.31257417658049214\n",
      "Iteration 84: Discriminator Loss: 0.9913084759669557, Generator Loss: 1.7335148710148227\n",
      "Iteration 85: Discriminator Loss: 2.0192763527248383, Generator Loss: 1.631197522474138\n",
      "Iteration 86: Discriminator Loss: 0.24930448353982898, Generator Loss: 0.1381250440586643\n",
      "Iteration 87: Discriminator Loss: 1.421244285291384, Generator Loss: 0.6634258154563826\n",
      "Iteration 88: Discriminator Loss: 1.6361754047670491, Generator Loss: 1.48772917262022\n",
      "Iteration 89: Discriminator Loss: 4.952787941523579, Generator Loss: 1.8805522691269783\n",
      "Iteration 90: Discriminator Loss: 0.3036016112018375, Generator Loss: 5.151279388856053\n",
      "Iteration 91: Discriminator Loss: 1.4408224730511368, Generator Loss: 0.26811548045675704\n",
      "Iteration 92: Discriminator Loss: 0.8044216985145085, Generator Loss: 1.5533760992291474\n",
      "Iteration 93: Discriminator Loss: 0.7123192956309017, Generator Loss: 1.0304048779286066\n",
      "Iteration 94: Discriminator Loss: 2.156916041059255, Generator Loss: 0.6659728788552248\n",
      "Iteration 95: Discriminator Loss: 1.8145103402928882, Generator Loss: 0.6715304942621649\n",
      "Iteration 96: Discriminator Loss: 2.846148982745825, Generator Loss: 1.0101571153160487\n",
      "Iteration 97: Discriminator Loss: 1.4203751505704207, Generator Loss: 0.5515026408940401\n",
      "Iteration 98: Discriminator Loss: 6.37117394661364, Generator Loss: 0.31255300421040744\n",
      "Iteration 99: Discriminator Loss: 0.8275573938535229, Generator Loss: 1.7497776206836784\n",
      "Iteration 100: Discriminator Loss: 2.204026562998784, Generator Loss: 0.6699673353877713\n",
      "Iteration 101: Discriminator Loss: 2.2094591262661147, Generator Loss: 0.3055622906537443\n",
      "Iteration 102: Discriminator Loss: 3.5231374444953336, Generator Loss: 0.9171060195151887\n",
      "Iteration 103: Discriminator Loss: 1.4069123376261428, Generator Loss: 1.7950975702682577\n",
      "Iteration 104: Discriminator Loss: 2.907247151042626, Generator Loss: 1.1799196521360957\n",
      "Iteration 105: Discriminator Loss: 0.9486542910942376, Generator Loss: 0.6580777627404468\n",
      "Iteration 106: Discriminator Loss: 0.6279511006094772, Generator Loss: 1.1388142708342692\n",
      "Iteration 107: Discriminator Loss: 0.966159861407613, Generator Loss: 0.061368147019480616\n",
      "Iteration 108: Discriminator Loss: 3.723371231550143, Generator Loss: 0.2144299067656433\n",
      "Iteration 109: Discriminator Loss: 1.2448901419873124, Generator Loss: 1.0897405054128912\n",
      "Iteration 110: Discriminator Loss: 5.299070725241175, Generator Loss: 0.7259895792061425\n",
      "Iteration 111: Discriminator Loss: 5.157468578937923, Generator Loss: 0.8983090398919488\n",
      "Iteration 112: Discriminator Loss: 1.8167528898654548, Generator Loss: 0.037701627495290276\n",
      "Iteration 113: Discriminator Loss: 1.5430876169883172, Generator Loss: 1.774918294386601\n",
      "Iteration 114: Discriminator Loss: 1.878830367057673, Generator Loss: 0.660695192083402\n",
      "Iteration 115: Discriminator Loss: 3.074102357258675, Generator Loss: 0.3857131175676851\n",
      "Iteration 116: Discriminator Loss: 4.031333337696767, Generator Loss: 0.4396723328393546\n",
      "Iteration 117: Discriminator Loss: 1.1468870422925277, Generator Loss: 0.14565387425477633\n",
      "Iteration 118: Discriminator Loss: 2.765346720477914, Generator Loss: 0.15444021113337922\n",
      "Iteration 119: Discriminator Loss: 3.617097681538333, Generator Loss: 0.09670651881313828\n",
      "Iteration 120: Discriminator Loss: 0.8162924004343965, Generator Loss: 0.007014379632085969\n",
      "Iteration 121: Discriminator Loss: 0.8876965447262968, Generator Loss: 0.09252965555245558\n",
      "Iteration 122: Discriminator Loss: 0.35695019073882817, Generator Loss: 2.3651984335938074\n",
      "Iteration 123: Discriminator Loss: 2.1523121120479014, Generator Loss: 1.0211688580633254\n",
      "Iteration 124: Discriminator Loss: 5.721640970754763, Generator Loss: 0.3863843984757091\n",
      "Iteration 125: Discriminator Loss: 4.112930858652503, Generator Loss: 0.21993345219701077\n",
      "Iteration 126: Discriminator Loss: 2.6627234589592987, Generator Loss: 0.7279533508631874\n",
      "Iteration 127: Discriminator Loss: 3.4461194058192506, Generator Loss: 3.588352708951633\n",
      "Iteration 128: Discriminator Loss: 1.996475637998659, Generator Loss: 2.444220090578861\n",
      "Iteration 129: Discriminator Loss: 3.527778835198629, Generator Loss: 1.3031373111251314\n",
      "Iteration 130: Discriminator Loss: 0.31694920784060343, Generator Loss: 1.1510870762310776\n",
      "Iteration 131: Discriminator Loss: 0.8132988533543495, Generator Loss: 0.04309126230381994\n",
      "Iteration 132: Discriminator Loss: 0.44338698078727273, Generator Loss: 0.5403559323599707\n",
      "Iteration 133: Discriminator Loss: 6.275083209393385, Generator Loss: 1.822610524947803\n",
      "Iteration 134: Discriminator Loss: 2.3302250002766702, Generator Loss: 0.038664073280784646\n",
      "Iteration 135: Discriminator Loss: 0.5753285248391027, Generator Loss: 2.6347496473791296\n",
      "Iteration 136: Discriminator Loss: 0.7103726974054925, Generator Loss: 1.200497574478115\n",
      "Iteration 137: Discriminator Loss: 0.8050828865768715, Generator Loss: 0.11060332173880785\n",
      "Iteration 138: Discriminator Loss: 2.431006142168391, Generator Loss: 0.4389052711820767\n",
      "Iteration 139: Discriminator Loss: 0.9669413107927757, Generator Loss: 2.3570386484504633\n",
      "Iteration 140: Discriminator Loss: 2.6055545789661503, Generator Loss: 0.05679193567827725\n",
      "Iteration 141: Discriminator Loss: 3.227602401661157, Generator Loss: 0.7829973199515451\n",
      "Iteration 142: Discriminator Loss: 3.3269357639985553, Generator Loss: 0.27350427080445955\n",
      "Iteration 143: Discriminator Loss: 0.6032686866125087, Generator Loss: 0.5665916982775142\n",
      "Iteration 144: Discriminator Loss: 0.5628432289359089, Generator Loss: 0.36633207621678077\n",
      "Iteration 145: Discriminator Loss: 2.529773524805679, Generator Loss: 0.3809119483609517\n",
      "Iteration 146: Discriminator Loss: 0.9422288149430762, Generator Loss: 0.465109375961387\n",
      "Iteration 147: Discriminator Loss: 2.4901763959327172, Generator Loss: 0.05219192965480484\n",
      "Iteration 148: Discriminator Loss: 1.6075140552497282, Generator Loss: 0.03919147015198747\n",
      "Iteration 149: Discriminator Loss: 3.515102022806071, Generator Loss: 0.8812888539022018\n",
      "Iteration 150: Discriminator Loss: 1.2409025741029431, Generator Loss: 0.9486211893253857\n",
      "Iteration 151: Discriminator Loss: 2.1623178548319792, Generator Loss: 0.6519547803043546\n",
      "Iteration 152: Discriminator Loss: 2.3332577270039723, Generator Loss: 1.4446625113675506\n",
      "Iteration 153: Discriminator Loss: 3.145047583260812, Generator Loss: 0.8845486074049347\n",
      "Iteration 154: Discriminator Loss: 4.149445967227441, Generator Loss: 1.326115094113638\n",
      "Iteration 155: Discriminator Loss: 1.468827672895665, Generator Loss: 0.9474806623181212\n",
      "Iteration 156: Discriminator Loss: 2.288835103156819, Generator Loss: 0.6305707425544425\n",
      "Iteration 157: Discriminator Loss: 2.2809582094095484, Generator Loss: 3.222757718784113\n",
      "Iteration 158: Discriminator Loss: 0.23215446318637098, Generator Loss: 2.610934835101727\n",
      "Iteration 159: Discriminator Loss: 4.2918250393754285, Generator Loss: 0.36490386636000804\n",
      "Iteration 160: Discriminator Loss: 1.444572333567343, Generator Loss: 2.8610065988757722\n",
      "Iteration 161: Discriminator Loss: 2.7063675836151555, Generator Loss: 1.6901462440593986\n",
      "Iteration 162: Discriminator Loss: 0.9790604040662065, Generator Loss: 0.077901676020862\n",
      "Iteration 163: Discriminator Loss: 0.14221996737164044, Generator Loss: 0.2837844955874085\n",
      "Iteration 164: Discriminator Loss: 6.059837747713958, Generator Loss: 0.44932277687304917\n",
      "Iteration 165: Discriminator Loss: 4.51440166806423, Generator Loss: 0.7561897615388367\n",
      "Iteration 166: Discriminator Loss: 1.3954237921638675, Generator Loss: 2.02246962825632\n",
      "Iteration 167: Discriminator Loss: 4.4172316238710465, Generator Loss: 2.9121126454741746\n",
      "Iteration 168: Discriminator Loss: 3.2382431019925244, Generator Loss: 0.4290482514571365\n",
      "Iteration 169: Discriminator Loss: 0.5921624256149163, Generator Loss: 0.9248623498628911\n",
      "Iteration 170: Discriminator Loss: 2.996043055363559, Generator Loss: 2.709478933892634\n",
      "Iteration 171: Discriminator Loss: 0.652651990767117, Generator Loss: 0.18453961630558202\n",
      "Iteration 172: Discriminator Loss: 4.788788753187061, Generator Loss: 0.7229625146799057\n",
      "Iteration 173: Discriminator Loss: 1.5169900057788799, Generator Loss: 0.6085029680317651\n",
      "Iteration 174: Discriminator Loss: 3.7033638154140416, Generator Loss: 2.387435699817361\n",
      "Iteration 175: Discriminator Loss: 1.832266871905813, Generator Loss: 2.1262037511439793\n",
      "Iteration 176: Discriminator Loss: 0.7776419193691682, Generator Loss: 0.37819776826671303\n",
      "Iteration 177: Discriminator Loss: 0.49981628329116246, Generator Loss: 1.6385529456520764\n",
      "Iteration 178: Discriminator Loss: 3.688867202155057, Generator Loss: 0.21256061473648502\n",
      "Iteration 179: Discriminator Loss: 3.052280956980817, Generator Loss: 0.2605770288217425\n",
      "Iteration 180: Discriminator Loss: 5.328299412961631, Generator Loss: 0.6861209036454162\n",
      "Iteration 181: Discriminator Loss: 0.5478972410642506, Generator Loss: 0.7929332153185654\n",
      "Iteration 182: Discriminator Loss: 3.6930800293863406, Generator Loss: 0.5130847876007668\n",
      "Iteration 183: Discriminator Loss: 1.300171374876145, Generator Loss: 0.8842704893740958\n",
      "Iteration 184: Discriminator Loss: 1.7548758739169419, Generator Loss: 0.06651704393468091\n",
      "Iteration 185: Discriminator Loss: 0.6291409903407296, Generator Loss: 1.4046921161554238\n",
      "Iteration 186: Discriminator Loss: 0.9336220903163112, Generator Loss: 3.122110640781284\n",
      "Iteration 187: Discriminator Loss: 0.664744754773168, Generator Loss: 1.1403495688795342\n",
      "Iteration 188: Discriminator Loss: 2.7844834240546743, Generator Loss: 0.21243696698988423\n",
      "Iteration 189: Discriminator Loss: 2.8672236316412136, Generator Loss: 0.3362986088093485\n",
      "Iteration 190: Discriminator Loss: 0.5165729276213328, Generator Loss: 0.5988534038078138\n",
      "Iteration 191: Discriminator Loss: 0.509467107673595, Generator Loss: 0.21291245748027438\n",
      "Iteration 192: Discriminator Loss: 4.694116526473456, Generator Loss: 3.366854358177693\n",
      "Iteration 193: Discriminator Loss: 2.4792887224628775, Generator Loss: 0.20016934606875114\n",
      "Iteration 194: Discriminator Loss: 1.0218749732802666, Generator Loss: 0.025865715655445036\n",
      "Iteration 195: Discriminator Loss: 2.9384977402322825, Generator Loss: 0.06777649888751612\n",
      "Iteration 196: Discriminator Loss: 3.478684363664368, Generator Loss: 0.17243182637304763\n",
      "Iteration 197: Discriminator Loss: 3.3061983965899597, Generator Loss: 8.75386455390385\n",
      "Iteration 198: Discriminator Loss: 1.3021713066410086, Generator Loss: 1.8709313245613381\n",
      "Iteration 199: Discriminator Loss: 0.9499107363283492, Generator Loss: 0.6507547786856765\n",
      "Iteration 200: Discriminator Loss: 2.5319241078579147, Generator Loss: 0.12374153429083087\n",
      "Iteration 201: Discriminator Loss: 1.2428163015496598, Generator Loss: 0.673359819431003\n",
      "Iteration 202: Discriminator Loss: 2.497771165409218, Generator Loss: 1.9210516425255637\n",
      "Iteration 203: Discriminator Loss: 0.747763685916354, Generator Loss: 0.858591939369935\n",
      "Iteration 204: Discriminator Loss: 1.371328981439638, Generator Loss: 0.21551819042000678\n",
      "Iteration 205: Discriminator Loss: 3.660448990260763, Generator Loss: 0.2593801229627696\n",
      "Iteration 206: Discriminator Loss: 4.442674073456899, Generator Loss: 2.7455864028376067\n",
      "Iteration 207: Discriminator Loss: 0.9526116317429194, Generator Loss: 1.249312126974424\n",
      "Iteration 208: Discriminator Loss: 0.388127706432589, Generator Loss: 0.29497219173546735\n",
      "Iteration 209: Discriminator Loss: 0.5967744901786101, Generator Loss: 0.059562231092829036\n",
      "Iteration 210: Discriminator Loss: 1.2268531751471126, Generator Loss: 1.525666628764844\n",
      "Iteration 211: Discriminator Loss: 3.3430936773299473, Generator Loss: 1.475395815396499\n",
      "Iteration 212: Discriminator Loss: 1.6213890178851393, Generator Loss: 2.470115609467734\n",
      "Iteration 213: Discriminator Loss: 2.046242094349944, Generator Loss: 1.4521266677003404\n",
      "Iteration 214: Discriminator Loss: 0.6706675658847603, Generator Loss: 0.2004697221870786\n",
      "Iteration 215: Discriminator Loss: 3.947659869807704, Generator Loss: 1.7615153871714222\n",
      "Iteration 216: Discriminator Loss: 1.0963820293683118, Generator Loss: 0.5109223002540674\n",
      "Iteration 217: Discriminator Loss: 1.0162360844199398, Generator Loss: 1.257060859117656\n",
      "Iteration 218: Discriminator Loss: 1.137461549589674, Generator Loss: 1.4519298921429147\n",
      "Iteration 219: Discriminator Loss: 0.8500357185000444, Generator Loss: 0.055468316367902844\n",
      "Iteration 220: Discriminator Loss: 4.207233816390457, Generator Loss: 1.047398198217578\n",
      "Iteration 221: Discriminator Loss: 8.667050212733336, Generator Loss: 2.0495902321285793\n",
      "Iteration 222: Discriminator Loss: 1.69137869792381, Generator Loss: 0.11314153950910177\n",
      "Iteration 223: Discriminator Loss: 1.6814143393267864, Generator Loss: 2.3866318260973443\n",
      "Iteration 224: Discriminator Loss: 2.3789996941552074, Generator Loss: 0.05721412975270686\n",
      "Iteration 225: Discriminator Loss: 0.7864461743800537, Generator Loss: 2.0488253286967715\n",
      "Iteration 226: Discriminator Loss: 0.7487520749912638, Generator Loss: 0.018442910424081277\n",
      "Iteration 227: Discriminator Loss: 1.3950652730373623, Generator Loss: 0.27553691700203065\n",
      "Iteration 228: Discriminator Loss: 6.647424087071551, Generator Loss: 3.280943880036879\n",
      "Iteration 229: Discriminator Loss: 1.819895913167434, Generator Loss: 0.17268313143144373\n",
      "Iteration 230: Discriminator Loss: 2.0840133153034914, Generator Loss: 2.4859614648068526\n",
      "Iteration 231: Discriminator Loss: 1.5820004682589777, Generator Loss: 0.26135371263162116\n",
      "Iteration 232: Discriminator Loss: 1.7483837703981748, Generator Loss: 0.6323124587404313\n",
      "Iteration 233: Discriminator Loss: 2.2189437294159235, Generator Loss: 0.5665270975994318\n",
      "Iteration 234: Discriminator Loss: 2.8754347939927865, Generator Loss: 0.17773706868639197\n",
      "Iteration 235: Discriminator Loss: 2.122412980877435, Generator Loss: 1.605299580723897\n",
      "Iteration 236: Discriminator Loss: 4.907060404935313, Generator Loss: 0.5577346482700622\n",
      "Iteration 237: Discriminator Loss: 3.042326705084699, Generator Loss: 1.4467060967623726\n",
      "Iteration 238: Discriminator Loss: 1.3170413505295964, Generator Loss: 0.4607240638380898\n",
      "Iteration 239: Discriminator Loss: 2.7546628746597106, Generator Loss: 2.3240262403716456\n",
      "Iteration 240: Discriminator Loss: 2.78318150778351, Generator Loss: 1.226325725407727\n",
      "Iteration 241: Discriminator Loss: 0.34371515856421125, Generator Loss: 1.6401507731232763\n",
      "Iteration 242: Discriminator Loss: 1.924133758447871, Generator Loss: 1.4416595121176288\n",
      "Iteration 243: Discriminator Loss: 5.178810852936251, Generator Loss: 0.3452315394959435\n",
      "Iteration 244: Discriminator Loss: 8.583400515739477, Generator Loss: 1.0092767509632945\n",
      "Iteration 245: Discriminator Loss: 4.798045337298049, Generator Loss: 0.1991547158421308\n",
      "Iteration 246: Discriminator Loss: 0.26322572153960705, Generator Loss: 0.41222731831828274\n",
      "Iteration 247: Discriminator Loss: 1.6868652026183737, Generator Loss: 1.184205616090505\n",
      "Iteration 248: Discriminator Loss: 1.6917772647117222, Generator Loss: 0.48237971912241073\n",
      "Iteration 249: Discriminator Loss: 2.950654744210979, Generator Loss: 1.9929829581992087\n",
      "Iteration 250: Discriminator Loss: 1.508111637900215, Generator Loss: 0.3027411517315014\n",
      "Iteration 251: Discriminator Loss: 1.0085540981602235, Generator Loss: 1.001951486048093\n",
      "Iteration 252: Discriminator Loss: 4.486214842572235, Generator Loss: 0.9940151124238175\n",
      "Iteration 253: Discriminator Loss: 0.592188445757729, Generator Loss: 1.3938575135134659\n",
      "Iteration 254: Discriminator Loss: 2.6102326732161347, Generator Loss: 1.2019243517750313\n",
      "Iteration 255: Discriminator Loss: 3.1778362167286116, Generator Loss: 0.982436926707424\n",
      "Iteration 256: Discriminator Loss: 1.2087982436524596, Generator Loss: 0.5814674087375166\n",
      "Iteration 257: Discriminator Loss: 0.7817256237000929, Generator Loss: 0.01142232088164307\n",
      "Iteration 258: Discriminator Loss: 3.4967857504264632, Generator Loss: 0.17490865625229216\n",
      "Iteration 259: Discriminator Loss: 1.2945519214180652, Generator Loss: 0.9244211334479843\n",
      "Iteration 260: Discriminator Loss: 2.029336886423127, Generator Loss: 1.5346654674498053\n",
      "Iteration 261: Discriminator Loss: 0.8856887315937334, Generator Loss: 0.49589135201048806\n",
      "Iteration 262: Discriminator Loss: 0.7424627843426124, Generator Loss: 0.198915981224549\n",
      "Iteration 263: Discriminator Loss: 2.3831547157718598, Generator Loss: 2.645443416295054\n",
      "Iteration 264: Discriminator Loss: 1.880267783683001, Generator Loss: 0.8491856147055442\n",
      "Iteration 265: Discriminator Loss: 3.2704308350951536, Generator Loss: 0.25974978681313626\n",
      "Iteration 266: Discriminator Loss: 0.9129735797717147, Generator Loss: 0.05828416670653724\n",
      "Iteration 267: Discriminator Loss: 1.3007383618527593, Generator Loss: 0.3878364591871076\n",
      "Iteration 268: Discriminator Loss: 4.602894018918985, Generator Loss: 1.9012985993947362\n",
      "Iteration 269: Discriminator Loss: 0.7280227687144668, Generator Loss: 4.086233207615803\n",
      "Iteration 270: Discriminator Loss: 0.9768024456916458, Generator Loss: 0.6554046865450109\n",
      "Iteration 271: Discriminator Loss: 5.50412022732605, Generator Loss: 1.4205084013706735\n",
      "Iteration 272: Discriminator Loss: 2.8985749971142334, Generator Loss: 0.07469207508514576\n",
      "Iteration 273: Discriminator Loss: 9.029139529186606, Generator Loss: 1.4113229986364768\n",
      "Iteration 274: Discriminator Loss: 3.7610078905279742, Generator Loss: 0.5368132464611086\n",
      "Iteration 275: Discriminator Loss: 2.86472971868169, Generator Loss: 2.883666956935672\n",
      "Iteration 276: Discriminator Loss: 2.370263014752487, Generator Loss: 2.318731403761541\n",
      "Iteration 277: Discriminator Loss: 1.2799264158213868, Generator Loss: 0.031162058249176176\n",
      "Iteration 278: Discriminator Loss: 0.9671233219413943, Generator Loss: 2.1306270886242795\n",
      "Iteration 279: Discriminator Loss: 1.9092944825304183, Generator Loss: 1.9634175202173096\n",
      "Iteration 280: Discriminator Loss: 5.710450009881859, Generator Loss: 1.5433393875409378\n",
      "Iteration 281: Discriminator Loss: 0.8205942388217653, Generator Loss: 1.8508180765300397\n",
      "Iteration 282: Discriminator Loss: 0.11965928196347991, Generator Loss: 0.2411518469531121\n",
      "Iteration 283: Discriminator Loss: 2.7212651152443925, Generator Loss: 0.4038403790783662\n",
      "Iteration 284: Discriminator Loss: 4.14201905993964, Generator Loss: 2.5219276807577873\n",
      "Iteration 285: Discriminator Loss: 1.4627984773985383, Generator Loss: 0.7698458741241098\n",
      "Iteration 286: Discriminator Loss: 1.8317903975698144, Generator Loss: 0.5793792881707445\n",
      "Iteration 287: Discriminator Loss: 6.38867930077361, Generator Loss: 0.37358041923078306\n",
      "Iteration 288: Discriminator Loss: 0.7710229571868126, Generator Loss: 0.41956239236065945\n",
      "Iteration 289: Discriminator Loss: 2.99005707299201, Generator Loss: 0.8084462954041481\n",
      "Iteration 290: Discriminator Loss: 5.7103258171495614, Generator Loss: 0.05401434800736876\n",
      "Iteration 291: Discriminator Loss: 3.4337579714247783, Generator Loss: 0.11822137084154147\n",
      "Iteration 292: Discriminator Loss: 2.3310207569277512, Generator Loss: 0.36395228946244307\n",
      "Iteration 293: Discriminator Loss: 2.3700588406636083, Generator Loss: 0.17383173232227372\n",
      "Iteration 294: Discriminator Loss: 1.4898462344028813, Generator Loss: 0.835981344585059\n",
      "Iteration 295: Discriminator Loss: 3.496524784590126, Generator Loss: 1.792615776581687\n",
      "Iteration 296: Discriminator Loss: 1.115924929298201, Generator Loss: 0.045880044658468816\n",
      "Iteration 297: Discriminator Loss: 1.4133284051779835, Generator Loss: 1.0032525623724522\n",
      "Iteration 298: Discriminator Loss: 4.328673504671496, Generator Loss: 0.18963213866581205\n",
      "Iteration 299: Discriminator Loss: 1.5547329431818784, Generator Loss: 2.107068066525748\n",
      "Iteration 300: Discriminator Loss: 2.0240307366743515, Generator Loss: 0.917312210811201\n",
      "Iteration 301: Discriminator Loss: 5.323022525777787, Generator Loss: 0.5206269505561746\n",
      "Iteration 302: Discriminator Loss: 2.595473947158162, Generator Loss: 0.8637449892380389\n",
      "Iteration 303: Discriminator Loss: 4.385125192136798, Generator Loss: 0.6031257726427388\n",
      "Iteration 304: Discriminator Loss: 0.9303050113710976, Generator Loss: 1.9356934105268522\n",
      "Iteration 305: Discriminator Loss: 2.169831768722947, Generator Loss: 0.9209474197577411\n",
      "Iteration 306: Discriminator Loss: 0.9848717741263798, Generator Loss: 1.7831404963666735\n",
      "Iteration 307: Discriminator Loss: 1.5845722531377506, Generator Loss: 0.19627400341272405\n",
      "Iteration 308: Discriminator Loss: 0.5196577171490782, Generator Loss: 0.24693334590404578\n",
      "Iteration 309: Discriminator Loss: 1.7052144219389047, Generator Loss: 0.06013935100553698\n",
      "Iteration 310: Discriminator Loss: 1.308113314860203, Generator Loss: 0.17852749845399266\n",
      "Iteration 311: Discriminator Loss: 0.2774523808865378, Generator Loss: 1.25852390723778\n",
      "Iteration 312: Discriminator Loss: 0.40645465075959714, Generator Loss: 0.11853149839038832\n",
      "Iteration 313: Discriminator Loss: 2.740866678364595, Generator Loss: 0.5278570958953451\n",
      "Iteration 314: Discriminator Loss: 1.1707892290870534, Generator Loss: 1.5001857060684316\n",
      "Iteration 315: Discriminator Loss: 4.105513370479136, Generator Loss: 0.23054750266420504\n",
      "Iteration 316: Discriminator Loss: 2.5127320241529647, Generator Loss: 0.40072655231385645\n",
      "Iteration 317: Discriminator Loss: 3.4372844226705874, Generator Loss: 0.8594789580705061\n",
      "Iteration 318: Discriminator Loss: 0.7333662049048454, Generator Loss: 0.5285548845831752\n",
      "Iteration 319: Discriminator Loss: 3.3510512435539965, Generator Loss: 4.644249037280999\n",
      "Iteration 320: Discriminator Loss: 1.3438527866054752, Generator Loss: 0.07717678408236833\n",
      "Iteration 321: Discriminator Loss: 2.104318756373965, Generator Loss: 4.673909515534053\n",
      "Iteration 322: Discriminator Loss: 2.9574949426379273, Generator Loss: 1.0256836843036068\n",
      "Iteration 323: Discriminator Loss: 2.7148443644262414, Generator Loss: 0.41715222837400584\n",
      "Iteration 324: Discriminator Loss: 3.631720712917189, Generator Loss: 0.9738064174503986\n",
      "Iteration 325: Discriminator Loss: 2.2763251775705373, Generator Loss: 0.733109188193803\n",
      "Iteration 326: Discriminator Loss: 1.5261899809410644, Generator Loss: 1.5627408910486726\n",
      "Iteration 327: Discriminator Loss: 1.9367637480276334, Generator Loss: 0.40237912010715116\n",
      "Iteration 328: Discriminator Loss: 1.6322824394473678, Generator Loss: 0.14514873865598088\n",
      "Iteration 329: Discriminator Loss: 2.444277528514303, Generator Loss: 0.09918148119875252\n",
      "Iteration 330: Discriminator Loss: 1.5266582031886857, Generator Loss: 3.896098208594165\n",
      "Iteration 331: Discriminator Loss: 0.8474306555993358, Generator Loss: 1.1001614338263035\n",
      "Iteration 332: Discriminator Loss: 3.2314144391480992, Generator Loss: 0.21591838008939168\n",
      "Iteration 333: Discriminator Loss: 0.2703755032394781, Generator Loss: 0.0154127900752388\n",
      "Iteration 334: Discriminator Loss: 1.500907162013887, Generator Loss: 1.8789052568127085\n",
      "Iteration 335: Discriminator Loss: 2.855064910586139, Generator Loss: 2.182250969311381\n",
      "Iteration 336: Discriminator Loss: 0.42586947183837437, Generator Loss: 1.2816368911236415\n",
      "Iteration 337: Discriminator Loss: 1.9059310684510524, Generator Loss: 1.378136082031128\n",
      "Iteration 338: Discriminator Loss: 2.209878749832295, Generator Loss: 0.4105155338824809\n",
      "Iteration 339: Discriminator Loss: 1.4903216409722475, Generator Loss: 0.1497161415998979\n",
      "Iteration 340: Discriminator Loss: 2.4483097401947815, Generator Loss: 0.3338047859714329\n",
      "Iteration 341: Discriminator Loss: 1.0282327254804091, Generator Loss: 0.9389438880672544\n",
      "Iteration 342: Discriminator Loss: 5.468558608763436, Generator Loss: 1.8845244805392356\n",
      "Iteration 343: Discriminator Loss: 2.8135059331574412, Generator Loss: 1.0148855252196138\n",
      "Iteration 344: Discriminator Loss: 3.9438734782488947, Generator Loss: 1.469412752366028\n",
      "Iteration 345: Discriminator Loss: 0.6522301751822261, Generator Loss: 0.43586765718931436\n",
      "Iteration 346: Discriminator Loss: 2.0005890884537907, Generator Loss: 1.5142807633253965\n",
      "Iteration 347: Discriminator Loss: 2.054983772822968, Generator Loss: 1.478763635590416\n",
      "Iteration 348: Discriminator Loss: 1.0179032535733483, Generator Loss: 0.35885180665291977\n",
      "Iteration 349: Discriminator Loss: 2.0859940625496813, Generator Loss: 0.4026747177346594\n",
      "Iteration 350: Discriminator Loss: 1.5014989746812388, Generator Loss: 1.1855329902256224\n",
      "Iteration 351: Discriminator Loss: 1.2306215479505012, Generator Loss: 0.6652902131294679\n",
      "Iteration 352: Discriminator Loss: 0.8259605557667308, Generator Loss: 0.07115315918440195\n",
      "Iteration 353: Discriminator Loss: 5.300364178256972, Generator Loss: 0.10058991711325131\n",
      "Iteration 354: Discriminator Loss: 0.609589154757942, Generator Loss: 1.8058189066412738\n",
      "Iteration 355: Discriminator Loss: 1.0746635812420284, Generator Loss: 0.9786546960033836\n",
      "Iteration 356: Discriminator Loss: 1.3068300477030514, Generator Loss: 0.8547579278581489\n",
      "Iteration 357: Discriminator Loss: 0.5683839980697645, Generator Loss: 0.46582896456463724\n",
      "Iteration 358: Discriminator Loss: 4.540816960163413, Generator Loss: 0.2016642948086051\n",
      "Iteration 359: Discriminator Loss: 2.5514752694061418, Generator Loss: 0.9170537612616289\n",
      "Iteration 360: Discriminator Loss: 0.6971958830573458, Generator Loss: 0.10383892627537575\n",
      "Iteration 361: Discriminator Loss: 1.0709275973824406, Generator Loss: 0.6040493680633215\n",
      "Iteration 362: Discriminator Loss: 0.15696478337368153, Generator Loss: 3.642742929648541\n",
      "Iteration 363: Discriminator Loss: 0.6049880252135138, Generator Loss: 0.7071365633467555\n",
      "Iteration 364: Discriminator Loss: 2.0178654800510554, Generator Loss: 0.5975050770175471\n",
      "Iteration 365: Discriminator Loss: 1.4872992921028019, Generator Loss: 0.85441814938462\n",
      "Iteration 366: Discriminator Loss: 1.7383050197011896, Generator Loss: 1.6422519633712067\n",
      "Iteration 367: Discriminator Loss: 0.9035055179952316, Generator Loss: 1.0028717508631861\n",
      "Iteration 368: Discriminator Loss: 1.9991290763142762, Generator Loss: 0.7771898929174559\n",
      "Iteration 369: Discriminator Loss: 2.3796202869433167, Generator Loss: 1.7338809060694385\n",
      "Iteration 370: Discriminator Loss: 1.4138085445161088, Generator Loss: 0.558183013919426\n",
      "Iteration 371: Discriminator Loss: 3.270897192185886, Generator Loss: 1.599840902562194\n",
      "Iteration 372: Discriminator Loss: 1.0120869366590919, Generator Loss: 0.2868401620589174\n",
      "Iteration 373: Discriminator Loss: 1.0280564482677912, Generator Loss: 0.5862290494994193\n",
      "Iteration 374: Discriminator Loss: 2.1918818359463983, Generator Loss: 1.047187209646155\n",
      "Iteration 375: Discriminator Loss: 0.36730159732638956, Generator Loss: 0.659476485522166\n",
      "Iteration 376: Discriminator Loss: 2.470619659605243, Generator Loss: 0.19100927835198606\n",
      "Iteration 377: Discriminator Loss: 0.9371761985727105, Generator Loss: 0.07810256415599777\n",
      "Iteration 378: Discriminator Loss: 3.5138711747521274, Generator Loss: 0.02627876809798296\n",
      "Iteration 379: Discriminator Loss: 1.8392263057314913, Generator Loss: 0.5238347153411377\n",
      "Iteration 380: Discriminator Loss: 2.767915538607622, Generator Loss: 1.5909775687473269\n",
      "Iteration 381: Discriminator Loss: 2.260453896767495, Generator Loss: 0.8688638323960999\n",
      "Iteration 382: Discriminator Loss: 2.136116473378125, Generator Loss: 0.9787016648815798\n",
      "Iteration 383: Discriminator Loss: 1.4091064486795561, Generator Loss: 1.26510236660744\n",
      "Iteration 384: Discriminator Loss: 2.0845968770217738, Generator Loss: 0.9919197444926358\n",
      "Iteration 385: Discriminator Loss: 1.61516605026778, Generator Loss: 1.0732068762807367\n",
      "Iteration 386: Discriminator Loss: 2.31043857394214, Generator Loss: 0.4079263965833923\n",
      "Iteration 387: Discriminator Loss: 2.3121763098973833, Generator Loss: 1.0924648855525338\n",
      "Iteration 388: Discriminator Loss: 0.5726288064470403, Generator Loss: 0.08126269696856057\n",
      "Iteration 389: Discriminator Loss: 1.1693077065350783, Generator Loss: 2.901283994843136\n",
      "Iteration 390: Discriminator Loss: 1.5946720706757043, Generator Loss: 1.8018745812765302\n",
      "Iteration 391: Discriminator Loss: 0.7903103225339405, Generator Loss: 0.7516773200727438\n",
      "Iteration 392: Discriminator Loss: 0.6158459720445828, Generator Loss: 0.4338229075429255\n",
      "Iteration 393: Discriminator Loss: 1.278212130182021, Generator Loss: 1.1495237647181833\n",
      "Iteration 394: Discriminator Loss: 2.503407855334402, Generator Loss: 0.060003157442666065\n",
      "Iteration 395: Discriminator Loss: 1.555219147664459, Generator Loss: 0.14199074826048724\n",
      "Iteration 396: Discriminator Loss: 1.6187407955001856, Generator Loss: 0.27380024812441783\n",
      "Iteration 397: Discriminator Loss: 1.3328016988202496, Generator Loss: 0.5787843824221205\n",
      "Iteration 398: Discriminator Loss: 3.1634593177071277, Generator Loss: 0.9071593103654263\n",
      "Iteration 399: Discriminator Loss: 3.0969117745019883, Generator Loss: 0.40453522306206324\n",
      "Iteration 400: Discriminator Loss: 1.9013809953038037, Generator Loss: 0.24807659920306957\n",
      "Iteration 401: Discriminator Loss: 1.170872476580254, Generator Loss: 1.3692359744072486\n",
      "Iteration 402: Discriminator Loss: 0.9663252167568503, Generator Loss: 0.9540403132119526\n",
      "Iteration 403: Discriminator Loss: 0.3770274748713184, Generator Loss: 0.2859389363822614\n",
      "Iteration 404: Discriminator Loss: 1.7497853667902876, Generator Loss: 0.028916527971155098\n",
      "Iteration 405: Discriminator Loss: 1.4124916004776256, Generator Loss: 1.3934021377682049\n",
      "Iteration 406: Discriminator Loss: 1.0811301022944138, Generator Loss: 1.3653823509998528\n",
      "Iteration 407: Discriminator Loss: 2.1099977484885546, Generator Loss: 0.5510730159787948\n",
      "Iteration 408: Discriminator Loss: 1.8143398940963433, Generator Loss: 0.1904331989413297\n",
      "Iteration 409: Discriminator Loss: 0.9048463412217772, Generator Loss: 0.6404881652085712\n",
      "Iteration 410: Discriminator Loss: 0.5621376667625916, Generator Loss: 1.7407871469373548\n",
      "Iteration 411: Discriminator Loss: 1.8192409237600202, Generator Loss: 0.012233254052006957\n",
      "Iteration 412: Discriminator Loss: 2.243161623824787, Generator Loss: 0.338142003985934\n",
      "Iteration 413: Discriminator Loss: 1.387990892852347, Generator Loss: 2.5332390443762236\n",
      "Iteration 414: Discriminator Loss: 0.7249034168335966, Generator Loss: 2.5659818051613854\n",
      "Iteration 415: Discriminator Loss: 1.0475440810146985, Generator Loss: 0.033612331223877474\n",
      "Iteration 416: Discriminator Loss: 1.787129870234665, Generator Loss: 0.3231067073687295\n",
      "Iteration 417: Discriminator Loss: 1.3254909544840725, Generator Loss: 0.0012191408836099214\n",
      "Iteration 418: Discriminator Loss: 0.5465722943988707, Generator Loss: 0.7292088655402831\n",
      "Iteration 419: Discriminator Loss: 4.256114927928136, Generator Loss: 0.6518050517083777\n",
      "Iteration 420: Discriminator Loss: 2.6298802523116946, Generator Loss: 0.6191667848302249\n",
      "Iteration 421: Discriminator Loss: 1.954816459456802, Generator Loss: 1.261613050048658\n",
      "Iteration 422: Discriminator Loss: 0.6841209970637812, Generator Loss: 0.4531602330229128\n",
      "Iteration 423: Discriminator Loss: 0.5265077313810922, Generator Loss: 2.823330767649732\n",
      "Iteration 424: Discriminator Loss: 0.5806364675099234, Generator Loss: 2.0807416290881866\n",
      "Iteration 425: Discriminator Loss: 1.0918206993942026, Generator Loss: 1.1138728275956422\n",
      "Iteration 426: Discriminator Loss: 3.6071906418596873, Generator Loss: 0.09463706624850114\n",
      "Iteration 427: Discriminator Loss: 2.1798236498598156, Generator Loss: 2.9837279556504\n",
      "Iteration 428: Discriminator Loss: 1.4351855828151718, Generator Loss: 1.147009051241177\n",
      "Iteration 429: Discriminator Loss: 0.2639504379302665, Generator Loss: 0.837464051296467\n",
      "Iteration 430: Discriminator Loss: 5.07711222311355, Generator Loss: 0.26619876775337986\n",
      "Iteration 431: Discriminator Loss: 0.8509609937920029, Generator Loss: 0.597400894082775\n",
      "Iteration 432: Discriminator Loss: 1.231250705232388, Generator Loss: 0.6434432710890476\n",
      "Iteration 433: Discriminator Loss: 1.8104335835713075, Generator Loss: 1.884468438723746\n",
      "Iteration 434: Discriminator Loss: 2.4527515270682887, Generator Loss: 0.1701498282203767\n",
      "Iteration 435: Discriminator Loss: 0.42144201403778386, Generator Loss: 1.3023374839007271\n",
      "Iteration 436: Discriminator Loss: 0.38981636939712516, Generator Loss: 3.007968829022016\n",
      "Iteration 437: Discriminator Loss: 1.6332090651644513, Generator Loss: 4.049232398512702\n",
      "Iteration 438: Discriminator Loss: 2.9992398148676456, Generator Loss: 1.8954733698720514\n",
      "Iteration 439: Discriminator Loss: 2.4538237841274393, Generator Loss: 4.306337641652954\n",
      "Iteration 440: Discriminator Loss: 1.907789472493561, Generator Loss: 0.052627231034091004\n",
      "Iteration 441: Discriminator Loss: 1.1860903056182308, Generator Loss: 0.21569186526848552\n",
      "Iteration 442: Discriminator Loss: 0.8998460341802672, Generator Loss: 1.030522323046864\n",
      "Iteration 443: Discriminator Loss: 0.5873739872887254, Generator Loss: 1.5626934170826021\n",
      "Iteration 444: Discriminator Loss: 2.635747045362447, Generator Loss: 0.24124742839946645\n",
      "Iteration 445: Discriminator Loss: 4.802028220646842, Generator Loss: 1.5782797268709228\n",
      "Iteration 446: Discriminator Loss: 1.0342658135289018, Generator Loss: 1.01051749124124\n",
      "Iteration 447: Discriminator Loss: 1.8533251076296489, Generator Loss: 2.5554285947136\n",
      "Iteration 448: Discriminator Loss: 4.158685581375121, Generator Loss: 0.30876422568066053\n",
      "Iteration 449: Discriminator Loss: 3.0189231015541917, Generator Loss: 1.913963735598145\n",
      "Iteration 450: Discriminator Loss: 1.1290286774318103, Generator Loss: 0.36853768337919157\n",
      "Iteration 451: Discriminator Loss: 2.379186058849185, Generator Loss: 0.5619902493114305\n",
      "Iteration 452: Discriminator Loss: 0.6634797180685846, Generator Loss: 0.0018047743546158682\n",
      "Iteration 453: Discriminator Loss: 5.560101591336843, Generator Loss: 0.600654211384851\n",
      "Iteration 454: Discriminator Loss: 0.8858371462425518, Generator Loss: 2.2016219394347\n",
      "Iteration 455: Discriminator Loss: 0.5027481870291203, Generator Loss: 2.040170075669955\n",
      "Iteration 456: Discriminator Loss: 1.6115190698286472, Generator Loss: 0.9918723429409039\n",
      "Iteration 457: Discriminator Loss: 3.843941331101807, Generator Loss: 1.1201090889060024\n",
      "Iteration 458: Discriminator Loss: 2.300583940004671, Generator Loss: 2.869622432988682\n",
      "Iteration 459: Discriminator Loss: 1.6803117351028367, Generator Loss: 0.05840706907079092\n",
      "Iteration 460: Discriminator Loss: 1.1227926683878193, Generator Loss: 0.11384957918738632\n",
      "Iteration 461: Discriminator Loss: 5.076693301539923, Generator Loss: 0.4672967336515343\n",
      "Iteration 462: Discriminator Loss: 2.316274775094918, Generator Loss: 0.19496057210105341\n",
      "Iteration 463: Discriminator Loss: 7.183883335326112, Generator Loss: 0.7713223164132537\n",
      "Iteration 464: Discriminator Loss: 2.108742283254429, Generator Loss: 0.15360461901259587\n",
      "Iteration 465: Discriminator Loss: 1.904625135965071, Generator Loss: 3.4744013663150155\n",
      "Iteration 466: Discriminator Loss: 0.23967481122047174, Generator Loss: 1.0038830303006816\n",
      "Iteration 467: Discriminator Loss: 8.848257807040403, Generator Loss: 2.467675782240718\n",
      "Iteration 468: Discriminator Loss: 2.928687665444085, Generator Loss: 0.39375905067824485\n",
      "Iteration 469: Discriminator Loss: 0.6436778531918241, Generator Loss: 2.828556152594095\n",
      "Iteration 470: Discriminator Loss: 2.173055599560526, Generator Loss: 0.3670188627742257\n",
      "Iteration 471: Discriminator Loss: 0.10163794932515091, Generator Loss: 0.3707114743664254\n",
      "Iteration 472: Discriminator Loss: 0.40376722910795515, Generator Loss: 3.36852057969493\n",
      "Iteration 473: Discriminator Loss: 0.9016983013680875, Generator Loss: 1.0622945257738228\n",
      "Iteration 474: Discriminator Loss: 2.3143551531591022, Generator Loss: 0.7308657167831643\n",
      "Iteration 475: Discriminator Loss: 5.174927496494277, Generator Loss: 1.260530019225642\n",
      "Iteration 476: Discriminator Loss: 0.7043820121126644, Generator Loss: 0.05343300621824234\n",
      "Iteration 477: Discriminator Loss: 0.9693528092593239, Generator Loss: 0.4558559916514451\n",
      "Iteration 478: Discriminator Loss: 0.9484078823460843, Generator Loss: 0.22301957401698544\n",
      "Iteration 479: Discriminator Loss: 1.7387119646623996, Generator Loss: 0.3330078033772103\n",
      "Iteration 480: Discriminator Loss: 0.967993261032047, Generator Loss: 2.0046794872037808\n",
      "Iteration 481: Discriminator Loss: 2.3888025755600677, Generator Loss: 0.05203401626339424\n",
      "Iteration 482: Discriminator Loss: 1.0618774446552357, Generator Loss: 0.4330928548540441\n",
      "Iteration 483: Discriminator Loss: 0.7063770938182808, Generator Loss: 0.13077411075801312\n",
      "Iteration 484: Discriminator Loss: 1.4038936698831825, Generator Loss: 0.33684059677795325\n",
      "Iteration 485: Discriminator Loss: 3.458435719946439, Generator Loss: 1.136928923113746\n",
      "Iteration 486: Discriminator Loss: 1.6066768014886454, Generator Loss: 1.5640794463915206\n",
      "Iteration 487: Discriminator Loss: 1.0640831471139853, Generator Loss: 0.4813034416980356\n",
      "Iteration 488: Discriminator Loss: 2.7751019421228156, Generator Loss: 2.098120447597858\n",
      "Iteration 489: Discriminator Loss: 3.7104446734698477, Generator Loss: 3.162975783172269\n",
      "Iteration 490: Discriminator Loss: 1.280161284472978, Generator Loss: 0.07828719272786612\n",
      "Iteration 491: Discriminator Loss: 1.453272648405283, Generator Loss: 1.1053757313855996\n",
      "Iteration 492: Discriminator Loss: 2.8084476618195002, Generator Loss: 1.137642356236809\n",
      "Iteration 493: Discriminator Loss: 2.925336443560266, Generator Loss: 0.3536474211097493\n",
      "Iteration 494: Discriminator Loss: 0.15970382404709724, Generator Loss: 0.0050907487204271094\n",
      "Iteration 495: Discriminator Loss: 6.544798543105676, Generator Loss: 0.682893935253759\n",
      "Iteration 496: Discriminator Loss: 1.2825748127224088, Generator Loss: 1.1057404069137704\n",
      "Iteration 497: Discriminator Loss: 0.787795826259826, Generator Loss: 2.5796110300742336\n",
      "Iteration 498: Discriminator Loss: 1.119628610327125, Generator Loss: 1.4245398944453498\n",
      "Iteration 499: Discriminator Loss: 2.084298227958614, Generator Loss: 1.172352783663676\n",
      "Iteration 500: Discriminator Loss: 7.179445477061843, Generator Loss: 0.43813389071216857\n",
      "Iteration 501: Discriminator Loss: 3.032574663576148, Generator Loss: 0.6569733442700626\n",
      "Iteration 502: Discriminator Loss: 2.3132937721633473, Generator Loss: 0.3545231260254035\n",
      "Iteration 503: Discriminator Loss: 1.1514466241195749, Generator Loss: 2.0007173563609983\n",
      "Iteration 504: Discriminator Loss: 3.979520239401606, Generator Loss: 1.6840742229098773\n",
      "Iteration 505: Discriminator Loss: 0.6201813794330575, Generator Loss: 0.5591302718890193\n",
      "Iteration 506: Discriminator Loss: 0.9564615200742473, Generator Loss: 0.47573756955941204\n",
      "Iteration 507: Discriminator Loss: 0.9582366621048066, Generator Loss: 0.8305737755551151\n",
      "Iteration 508: Discriminator Loss: 0.2437387201298074, Generator Loss: 0.6998417398986205\n",
      "Iteration 509: Discriminator Loss: 0.7979049685538919, Generator Loss: 0.6515867275994108\n",
      "Iteration 510: Discriminator Loss: 4.835874322989314, Generator Loss: 2.313938374983501\n",
      "Iteration 511: Discriminator Loss: 3.8597387414048723, Generator Loss: 0.29028676321298225\n",
      "Iteration 512: Discriminator Loss: 2.6545169257119507, Generator Loss: 1.5640511626532132\n",
      "Iteration 513: Discriminator Loss: 2.428069634878981, Generator Loss: 0.9189405062411174\n",
      "Iteration 514: Discriminator Loss: 3.476932832824551, Generator Loss: 0.3373607754190628\n",
      "Iteration 515: Discriminator Loss: 1.5729147162328525, Generator Loss: 1.0679640963133752\n",
      "Iteration 516: Discriminator Loss: 0.8338272773304602, Generator Loss: 0.23640989060898082\n",
      "Iteration 517: Discriminator Loss: 2.5031642796637628, Generator Loss: 0.7156156331397596\n",
      "Iteration 518: Discriminator Loss: 4.526405839513448, Generator Loss: 0.32696668417600194\n",
      "Iteration 519: Discriminator Loss: 0.8896869286102334, Generator Loss: 0.1797848506335545\n",
      "Iteration 520: Discriminator Loss: 0.7409215424079127, Generator Loss: 0.5349505882015589\n",
      "Iteration 521: Discriminator Loss: 0.8137006183796203, Generator Loss: 1.0184119705699592\n",
      "Iteration 522: Discriminator Loss: 1.682204679363366, Generator Loss: 0.6346008770551628\n",
      "Iteration 523: Discriminator Loss: 0.6053368714347238, Generator Loss: 0.3896270250081437\n",
      "Iteration 524: Discriminator Loss: 0.7455815591806938, Generator Loss: 0.48222248599721096\n",
      "Iteration 525: Discriminator Loss: 0.20500358145922748, Generator Loss: 1.7003588781840766\n",
      "Iteration 526: Discriminator Loss: 2.618780792960747, Generator Loss: 0.24896848453086248\n",
      "Iteration 527: Discriminator Loss: 0.6082503761597655, Generator Loss: 3.2158817716136\n",
      "Iteration 528: Discriminator Loss: 1.9343035670928796, Generator Loss: 1.4991705812498037\n",
      "Iteration 529: Discriminator Loss: 1.2211069125272112, Generator Loss: 0.1492942689825522\n",
      "Iteration 530: Discriminator Loss: 5.02090894688821, Generator Loss: 1.149899496525459\n",
      "Iteration 531: Discriminator Loss: 1.4655753843273613, Generator Loss: 1.1955300297867775\n",
      "Iteration 532: Discriminator Loss: 3.4359214495338346, Generator Loss: 0.24921447218667117\n",
      "Iteration 533: Discriminator Loss: 1.3391083946182418, Generator Loss: 1.6943596779405175\n",
      "Iteration 534: Discriminator Loss: 1.3448599132081087, Generator Loss: 1.0268849238921525\n",
      "Iteration 535: Discriminator Loss: 1.3970301943748153, Generator Loss: 0.38769974597306583\n",
      "Iteration 536: Discriminator Loss: 0.2768373508393124, Generator Loss: 0.21638142027305066\n",
      "Iteration 537: Discriminator Loss: 3.8491237149582167, Generator Loss: 0.9835572657750803\n",
      "Iteration 538: Discriminator Loss: 0.500589782116167, Generator Loss: 0.5051968528914792\n",
      "Iteration 539: Discriminator Loss: 0.5916246439127874, Generator Loss: 2.5579290827233856\n",
      "Iteration 540: Discriminator Loss: 3.063183923058797, Generator Loss: 2.0836873133287885\n",
      "Iteration 541: Discriminator Loss: 3.015659809734866, Generator Loss: 0.7128867407751931\n",
      "Iteration 542: Discriminator Loss: 4.8262316079467436, Generator Loss: 1.020381248322836\n",
      "Iteration 543: Discriminator Loss: 2.096824340682148, Generator Loss: 0.03331813544506215\n",
      "Iteration 544: Discriminator Loss: 1.4440367351329422, Generator Loss: 0.27690217681596135\n",
      "Iteration 545: Discriminator Loss: 3.7894580641602893, Generator Loss: 0.49467639065782915\n",
      "Iteration 546: Discriminator Loss: 0.9387569237064548, Generator Loss: 2.578592230773235\n",
      "Iteration 547: Discriminator Loss: 0.611511390479679, Generator Loss: 3.7586836968994533\n",
      "Iteration 548: Discriminator Loss: 3.1780192980781115, Generator Loss: 1.7216824509605402\n",
      "Iteration 549: Discriminator Loss: 2.587201542870913, Generator Loss: 2.1457370447854878\n",
      "Iteration 550: Discriminator Loss: 2.7341771794974044, Generator Loss: 1.583479906171965\n",
      "Iteration 551: Discriminator Loss: 2.980092339803363, Generator Loss: 0.05668924507221407\n",
      "Iteration 552: Discriminator Loss: 3.4763219106164414, Generator Loss: 0.5652511501734079\n",
      "Iteration 553: Discriminator Loss: 2.5238519924081646, Generator Loss: 0.38935513647451425\n",
      "Iteration 554: Discriminator Loss: 3.191397502128133, Generator Loss: 2.627574273761153\n",
      "Iteration 555: Discriminator Loss: 1.8200686605828915, Generator Loss: 1.4231699008323146\n",
      "Iteration 556: Discriminator Loss: 3.569008250980509, Generator Loss: 0.3875171986835822\n",
      "Iteration 557: Discriminator Loss: 1.302967553447357, Generator Loss: 1.518875847988224\n",
      "Iteration 558: Discriminator Loss: 2.5639796681945812, Generator Loss: 0.2085259346703339\n",
      "Iteration 559: Discriminator Loss: 6.499846027580757, Generator Loss: 0.6592424342455379\n",
      "Iteration 560: Discriminator Loss: 0.4071310637302673, Generator Loss: 0.0853505699585522\n",
      "Iteration 561: Discriminator Loss: 1.02507154439559, Generator Loss: 0.26011825360949065\n",
      "Iteration 562: Discriminator Loss: 0.8248496032373714, Generator Loss: 0.9266853100672477\n",
      "Iteration 563: Discriminator Loss: 0.9051485725458056, Generator Loss: 1.1885379015107727\n",
      "Iteration 564: Discriminator Loss: 3.950785020638053, Generator Loss: 1.2926741178330132\n",
      "Iteration 565: Discriminator Loss: 2.11843015807952, Generator Loss: 0.7529639124702117\n",
      "Iteration 566: Discriminator Loss: 1.1933118037750474, Generator Loss: 2.5276161671785604\n",
      "Iteration 567: Discriminator Loss: 0.8031106874432932, Generator Loss: 2.5302275532680008\n",
      "Iteration 568: Discriminator Loss: 1.5883705955936955, Generator Loss: 0.25898527623004597\n",
      "Iteration 569: Discriminator Loss: 4.468147197546328, Generator Loss: 1.4516184038952158\n",
      "Iteration 570: Discriminator Loss: 0.5611385278720478, Generator Loss: 0.7311503975061399\n",
      "Iteration 571: Discriminator Loss: 1.335411476052323, Generator Loss: 2.319328968025002\n",
      "Iteration 572: Discriminator Loss: 0.28609389788518, Generator Loss: 2.245782769088918\n",
      "Iteration 573: Discriminator Loss: 1.7507261392161695, Generator Loss: 1.1888794893002483\n",
      "Iteration 574: Discriminator Loss: 2.5824783993374827, Generator Loss: 0.8995754126126296\n",
      "Iteration 575: Discriminator Loss: 3.8975043391016833, Generator Loss: 0.18355826980058\n",
      "Iteration 576: Discriminator Loss: 2.249796075999816, Generator Loss: 1.3509576381539419\n",
      "Iteration 577: Discriminator Loss: 0.09917024969518246, Generator Loss: 0.35308544755802806\n",
      "Iteration 578: Discriminator Loss: 1.8682953478560218, Generator Loss: 1.3499399057197645\n",
      "Iteration 579: Discriminator Loss: 1.217944961642315, Generator Loss: 0.06087682424326472\n",
      "Iteration 580: Discriminator Loss: 2.218506617158953, Generator Loss: 1.456444548528256\n",
      "Iteration 581: Discriminator Loss: 5.865370520709439, Generator Loss: 1.3718008110845146\n",
      "Iteration 582: Discriminator Loss: 1.37134430581162, Generator Loss: 0.5155427118852375\n",
      "Iteration 583: Discriminator Loss: 1.9605724934929072, Generator Loss: 0.6071435486644493\n",
      "Iteration 584: Discriminator Loss: 2.883660488606353, Generator Loss: 0.37902805014678603\n",
      "Iteration 585: Discriminator Loss: 3.136888421625303, Generator Loss: 2.2683183506031774\n",
      "Iteration 586: Discriminator Loss: 1.7249929526119154, Generator Loss: 0.9591154421745479\n",
      "Iteration 587: Discriminator Loss: 2.9108579202598746, Generator Loss: 0.07646430449517053\n",
      "Iteration 588: Discriminator Loss: 2.537391337715319, Generator Loss: 0.3664045299571207\n",
      "Iteration 589: Discriminator Loss: 1.264943495991908, Generator Loss: 3.046722253351622\n",
      "Iteration 590: Discriminator Loss: 0.8971667660297672, Generator Loss: 0.4756925940901611\n",
      "Iteration 591: Discriminator Loss: 2.1022011150681896, Generator Loss: 0.7777180729758191\n",
      "Iteration 592: Discriminator Loss: 2.9763138888848872, Generator Loss: 2.0131331667256522\n",
      "Iteration 593: Discriminator Loss: 1.7543531432848192, Generator Loss: 0.2155622193057533\n",
      "Iteration 594: Discriminator Loss: 4.8429161607702484, Generator Loss: 1.2922007207217374\n",
      "Iteration 595: Discriminator Loss: 3.7335246657675056, Generator Loss: 1.932730699366029\n",
      "Iteration 596: Discriminator Loss: 0.568576143156289, Generator Loss: 0.7415987347641732\n",
      "Iteration 597: Discriminator Loss: 1.8301043951081468, Generator Loss: 0.16026436108132255\n",
      "Iteration 598: Discriminator Loss: 1.4284702973462462, Generator Loss: 0.3939061547345959\n",
      "Iteration 599: Discriminator Loss: 2.6530864020957505, Generator Loss: 0.3856926496404654\n",
      "Iteration 600: Discriminator Loss: 1.9404292324152808, Generator Loss: 0.03726643040824858\n",
      "Iteration 601: Discriminator Loss: 1.854056190280307, Generator Loss: 1.6001109649055365\n",
      "Iteration 602: Discriminator Loss: 2.082075317897227, Generator Loss: 1.014713234898989\n",
      "Iteration 603: Discriminator Loss: 0.9416045276315117, Generator Loss: 0.7607749924748757\n",
      "Iteration 604: Discriminator Loss: 0.9863398286460414, Generator Loss: 2.616804940427219\n",
      "Iteration 605: Discriminator Loss: 1.5897228973162798, Generator Loss: 0.42711085981349234\n",
      "Iteration 606: Discriminator Loss: 0.109053517453238, Generator Loss: 2.5678681731319624\n",
      "Iteration 607: Discriminator Loss: 0.19849096207214592, Generator Loss: 0.33498735747824493\n",
      "Iteration 608: Discriminator Loss: 2.259345490480839, Generator Loss: 0.6017361834760889\n",
      "Iteration 609: Discriminator Loss: 1.7923884252093683, Generator Loss: 0.6527519951762162\n",
      "Iteration 610: Discriminator Loss: 1.0073864430282675, Generator Loss: 0.7497265604858872\n",
      "Iteration 611: Discriminator Loss: 0.7467489318809616, Generator Loss: 0.36230991959930525\n",
      "Iteration 612: Discriminator Loss: 0.469737395280255, Generator Loss: 0.051852201148866094\n",
      "Iteration 613: Discriminator Loss: 4.285216931756553, Generator Loss: 2.307917996756303\n",
      "Iteration 614: Discriminator Loss: 3.684957659601039, Generator Loss: 1.1366352039289729\n",
      "Iteration 615: Discriminator Loss: 0.09586918752310519, Generator Loss: 0.23200048852486238\n",
      "Iteration 616: Discriminator Loss: 1.0060068047451967, Generator Loss: 1.545872898036089\n",
      "Iteration 617: Discriminator Loss: 0.32358014741336366, Generator Loss: 0.07980965644376702\n",
      "Iteration 618: Discriminator Loss: 4.975732847275378, Generator Loss: 0.9678316730556255\n",
      "Iteration 619: Discriminator Loss: 3.741895600207215, Generator Loss: 0.7101152590210773\n",
      "Iteration 620: Discriminator Loss: 1.5868604431683335, Generator Loss: 2.066062502066708\n",
      "Iteration 621: Discriminator Loss: 1.5179530754630906, Generator Loss: 1.4843373929408372\n",
      "Iteration 622: Discriminator Loss: 2.1975939243377147, Generator Loss: 0.24590904358713372\n",
      "Iteration 623: Discriminator Loss: 1.2476910440049243, Generator Loss: 1.1103737165292054\n",
      "Iteration 624: Discriminator Loss: 1.7374264523949983, Generator Loss: 2.2945988473741594\n",
      "Iteration 625: Discriminator Loss: 1.216414056921111, Generator Loss: 2.9447319722060117\n",
      "Iteration 626: Discriminator Loss: 1.387459440416528, Generator Loss: 0.4083633478961343\n",
      "Iteration 627: Discriminator Loss: 1.4502969737084015, Generator Loss: 0.6510975109141108\n",
      "Iteration 628: Discriminator Loss: 2.369196235033834, Generator Loss: 0.9140159884353477\n",
      "Iteration 629: Discriminator Loss: 0.9143059417264567, Generator Loss: 0.274007763123118\n",
      "Iteration 630: Discriminator Loss: 0.40999133495247986, Generator Loss: 0.92713679926382\n",
      "Iteration 631: Discriminator Loss: 1.2381800354462094, Generator Loss: 0.8386261112207395\n",
      "Iteration 632: Discriminator Loss: 2.813323870547359, Generator Loss: 0.6819347279987318\n",
      "Iteration 633: Discriminator Loss: 1.5180907214088037, Generator Loss: 0.5002350872524071\n",
      "Iteration 634: Discriminator Loss: 4.1352525729304626, Generator Loss: 0.10792105897963937\n",
      "Iteration 635: Discriminator Loss: 1.129869786927548, Generator Loss: 0.2850079090762039\n",
      "Iteration 636: Discriminator Loss: 0.4030955511526426, Generator Loss: 0.40916204541266815\n",
      "Iteration 637: Discriminator Loss: 0.6580329903335917, Generator Loss: 0.032888144645641444\n",
      "Iteration 638: Discriminator Loss: 3.3383050447296063, Generator Loss: 0.17827540416221102\n",
      "Iteration 639: Discriminator Loss: 4.766498365276178, Generator Loss: 1.4705416343947015\n",
      "Iteration 640: Discriminator Loss: 3.910572796299664, Generator Loss: 0.4623155228965457\n",
      "Iteration 641: Discriminator Loss: 0.5764626747893437, Generator Loss: 0.42049137579251517\n",
      "Iteration 642: Discriminator Loss: 4.614758988776958, Generator Loss: 1.968370932708553\n",
      "Iteration 643: Discriminator Loss: 2.067282052839124, Generator Loss: 0.7887407588349464\n",
      "Iteration 644: Discriminator Loss: 0.6857975998890548, Generator Loss: 1.8933633984707188\n",
      "Iteration 645: Discriminator Loss: 3.572741611428331, Generator Loss: 0.3571089846776234\n",
      "Iteration 646: Discriminator Loss: 1.51042007494378, Generator Loss: 1.3662348074133932\n",
      "Iteration 647: Discriminator Loss: 0.8477194411117491, Generator Loss: 4.16395387152181\n",
      "Iteration 648: Discriminator Loss: 1.5778861609233095, Generator Loss: 1.9696560788314594\n",
      "Iteration 649: Discriminator Loss: 0.5080330543471595, Generator Loss: 0.537673258447908\n",
      "Iteration 650: Discriminator Loss: 1.1434152413488945, Generator Loss: 0.4894489103740541\n",
      "Iteration 651: Discriminator Loss: 4.221437905525864, Generator Loss: 0.8094372239486826\n",
      "Iteration 652: Discriminator Loss: 2.685040510027292, Generator Loss: 1.7422408996885037\n",
      "Iteration 653: Discriminator Loss: 0.8670697376721702, Generator Loss: 0.8452636991162912\n",
      "Iteration 654: Discriminator Loss: 0.9391653948178809, Generator Loss: 0.5101812267704168\n",
      "Iteration 655: Discriminator Loss: 1.0836257296542946, Generator Loss: 0.45567509028321\n",
      "Iteration 656: Discriminator Loss: 1.4773911994476006, Generator Loss: 0.12443478872653502\n",
      "Iteration 657: Discriminator Loss: 3.8223807609873566, Generator Loss: 6.405750094241799\n",
      "Iteration 658: Discriminator Loss: 1.126503754403703, Generator Loss: 0.15827579987429105\n",
      "Iteration 659: Discriminator Loss: 1.5325594062724976, Generator Loss: 0.6866222703161062\n",
      "Iteration 660: Discriminator Loss: 2.897021987838802, Generator Loss: 0.8951850885875812\n",
      "Iteration 661: Discriminator Loss: 2.4351883425073644, Generator Loss: 0.6835829878724404\n",
      "Iteration 662: Discriminator Loss: 2.000927410870526, Generator Loss: 2.03155548108428\n",
      "Iteration 663: Discriminator Loss: 2.124131906430802, Generator Loss: 0.9055132385017394\n",
      "Iteration 664: Discriminator Loss: 2.3228226937968355, Generator Loss: 0.3384012683574066\n",
      "Iteration 665: Discriminator Loss: 0.7659100373191842, Generator Loss: 3.6576322279003994\n",
      "Iteration 666: Discriminator Loss: 3.6572804270330477, Generator Loss: 0.6991603303741307\n",
      "Iteration 667: Discriminator Loss: 2.0559083987621993, Generator Loss: 0.24164738297242022\n",
      "Iteration 668: Discriminator Loss: 4.997246472908197, Generator Loss: 0.3027469243162706\n",
      "Iteration 669: Discriminator Loss: 1.945536746804529, Generator Loss: 0.8032050842245199\n",
      "Iteration 670: Discriminator Loss: 1.4831691029887848, Generator Loss: 0.8692153554758903\n",
      "Iteration 671: Discriminator Loss: 0.7647865443257765, Generator Loss: 0.29040838171430206\n",
      "Iteration 672: Discriminator Loss: 1.0179993382911292, Generator Loss: 0.21521340802340355\n",
      "Iteration 673: Discriminator Loss: 1.8164494229002985, Generator Loss: 0.774016423463062\n",
      "Iteration 674: Discriminator Loss: 0.8041306169433118, Generator Loss: 0.7217014883512759\n",
      "Iteration 675: Discriminator Loss: 1.7413095655507442, Generator Loss: 2.542259728577083\n",
      "Iteration 676: Discriminator Loss: 4.727835683985096, Generator Loss: 0.10362881770948548\n",
      "Iteration 677: Discriminator Loss: 0.5228975111044678, Generator Loss: 3.652448676707021\n",
      "Iteration 678: Discriminator Loss: 3.8801555515862347, Generator Loss: 0.2217229842035113\n",
      "Iteration 679: Discriminator Loss: 3.775109269876213, Generator Loss: 0.9511482237733845\n",
      "Iteration 680: Discriminator Loss: 4.6363454196570135, Generator Loss: 0.41717230834704117\n",
      "Iteration 681: Discriminator Loss: 1.956863609940521, Generator Loss: 2.117363014770818\n",
      "Iteration 682: Discriminator Loss: 0.5008877903532737, Generator Loss: 0.3992865922431025\n",
      "Iteration 683: Discriminator Loss: 3.199227968789499, Generator Loss: 1.4718493061980913\n",
      "Iteration 684: Discriminator Loss: 0.22304330952182244, Generator Loss: 2.9942508132479326\n",
      "Iteration 685: Discriminator Loss: 0.8095881729806107, Generator Loss: 2.9371798103492663\n",
      "Iteration 686: Discriminator Loss: 0.639640291734131, Generator Loss: 1.6684421097351552\n",
      "Iteration 687: Discriminator Loss: 0.4559843116459893, Generator Loss: 0.2988337818718647\n",
      "Iteration 688: Discriminator Loss: 1.0548864661344985, Generator Loss: 1.5571985283429843\n",
      "Iteration 689: Discriminator Loss: 1.9216528623663052, Generator Loss: 0.02636509781391963\n",
      "Iteration 690: Discriminator Loss: 1.6197889863251742, Generator Loss: 0.016758740095767406\n",
      "Iteration 691: Discriminator Loss: 0.5080268855420738, Generator Loss: 2.6248058536852366\n",
      "Iteration 692: Discriminator Loss: 1.1308672572171785, Generator Loss: 2.0545534026120773\n",
      "Iteration 693: Discriminator Loss: 0.06558445520992312, Generator Loss: 0.6230709942450309\n",
      "Iteration 694: Discriminator Loss: 1.505914884190615, Generator Loss: 0.7827086933210544\n",
      "Iteration 695: Discriminator Loss: 1.7300029927967973, Generator Loss: 0.3096046554073002\n",
      "Iteration 696: Discriminator Loss: 5.508565572372786, Generator Loss: 3.3569192678313797\n",
      "Iteration 697: Discriminator Loss: 3.491592195732189, Generator Loss: 4.503457228294593\n",
      "Iteration 698: Discriminator Loss: 3.233780847234907, Generator Loss: 0.14007065063002852\n",
      "Iteration 699: Discriminator Loss: 3.0028190096716934, Generator Loss: 0.07865627936741591\n",
      "Iteration 700: Discriminator Loss: 4.527946801536984, Generator Loss: 0.8063408803807998\n",
      "Iteration 701: Discriminator Loss: 1.2028167012946755, Generator Loss: 0.7623564882875389\n",
      "Iteration 702: Discriminator Loss: 0.6903594473474732, Generator Loss: 1.9454186689602042\n",
      "Iteration 703: Discriminator Loss: 1.2884306522304378, Generator Loss: 0.11072207609823564\n",
      "Iteration 704: Discriminator Loss: 3.686038373921739, Generator Loss: 0.9050200349498991\n",
      "Iteration 705: Discriminator Loss: 2.0672340191627296, Generator Loss: 3.8471246830310184\n",
      "Iteration 706: Discriminator Loss: 2.6058634097876094, Generator Loss: 2.3105526514286137\n",
      "Iteration 707: Discriminator Loss: 0.9590043382212842, Generator Loss: 0.7352558665416933\n",
      "Iteration 708: Discriminator Loss: 1.277899332674252, Generator Loss: 0.1953653554054253\n",
      "Iteration 709: Discriminator Loss: 1.2501957883314103, Generator Loss: 0.28906471020638\n",
      "Iteration 710: Discriminator Loss: 0.9739838801195616, Generator Loss: 0.5507225141327546\n",
      "Iteration 711: Discriminator Loss: 4.231485640097952, Generator Loss: 0.9321705564023081\n",
      "Iteration 712: Discriminator Loss: 2.225021718426752, Generator Loss: 0.35253674566027526\n",
      "Iteration 713: Discriminator Loss: 2.2343567804782296, Generator Loss: 0.6648353984699288\n",
      "Iteration 714: Discriminator Loss: 1.6694815240129335, Generator Loss: 0.8288797750720198\n",
      "Iteration 715: Discriminator Loss: 1.3257364848204107, Generator Loss: 2.8325946988991264\n",
      "Iteration 716: Discriminator Loss: 1.1650797660091032, Generator Loss: 0.3178592484562919\n",
      "Iteration 717: Discriminator Loss: 0.49182771261144004, Generator Loss: 0.2415097638352728\n",
      "Iteration 718: Discriminator Loss: 2.541125499709284, Generator Loss: 0.8695779905900289\n",
      "Iteration 719: Discriminator Loss: 1.0364478889919646, Generator Loss: 0.19777328722959847\n",
      "Iteration 720: Discriminator Loss: 1.1061578719719112, Generator Loss: 0.7295897113759824\n",
      "Iteration 721: Discriminator Loss: 2.3946292178638307, Generator Loss: 0.14760685483445532\n",
      "Iteration 722: Discriminator Loss: 2.2432818687769043, Generator Loss: 0.573356196751934\n",
      "Iteration 723: Discriminator Loss: 3.360857911326282, Generator Loss: 0.41862450157285896\n",
      "Iteration 724: Discriminator Loss: 0.5302961666462469, Generator Loss: 0.25102591997442664\n",
      "Iteration 725: Discriminator Loss: 0.7360278317036154, Generator Loss: 0.3100455452066937\n",
      "Iteration 726: Discriminator Loss: 1.2518120131349029, Generator Loss: 0.1180310075979197\n",
      "Iteration 727: Discriminator Loss: 1.0569269724678219, Generator Loss: 1.6728981248684598\n",
      "Iteration 728: Discriminator Loss: 1.5439503035624877, Generator Loss: 2.9857778079451593\n",
      "Iteration 729: Discriminator Loss: 1.3562880339591465, Generator Loss: 1.7491602955593546\n",
      "Iteration 730: Discriminator Loss: 1.0470718875774174, Generator Loss: 2.455521408597512\n",
      "Iteration 731: Discriminator Loss: 2.280177714542252, Generator Loss: 0.04851287935902918\n",
      "Iteration 732: Discriminator Loss: 1.2826550286516578, Generator Loss: 0.30694600135180805\n",
      "Iteration 733: Discriminator Loss: 0.8694837361076979, Generator Loss: 3.1539189786420088\n",
      "Iteration 734: Discriminator Loss: 1.5430132701170676, Generator Loss: 0.7102159583315603\n",
      "Iteration 735: Discriminator Loss: 2.212036742859778, Generator Loss: 7.3330003038738365\n",
      "Iteration 736: Discriminator Loss: 0.13922347779643002, Generator Loss: 3.496582218431844\n",
      "Iteration 737: Discriminator Loss: 1.7841388234258688, Generator Loss: 0.5110118382519949\n",
      "Iteration 738: Discriminator Loss: 2.7844504737356877, Generator Loss: 1.728798330024668\n",
      "Iteration 739: Discriminator Loss: 2.892849670943673, Generator Loss: 0.4937387058830115\n",
      "Iteration 740: Discriminator Loss: 1.213721969147183, Generator Loss: 1.5329737099046703\n",
      "Iteration 741: Discriminator Loss: 2.1352239026790603, Generator Loss: 0.7360154687593471\n",
      "Iteration 742: Discriminator Loss: 0.5986702306683864, Generator Loss: 0.3514277221002528\n",
      "Iteration 743: Discriminator Loss: 0.7281628365995674, Generator Loss: 0.22563351067281165\n",
      "Iteration 744: Discriminator Loss: 1.2774005842685416, Generator Loss: 0.40588949659095186\n",
      "Iteration 745: Discriminator Loss: 2.229480915953982, Generator Loss: 1.6485710320042937\n",
      "Iteration 746: Discriminator Loss: 4.137727800455942, Generator Loss: 1.007056777733166\n",
      "Iteration 747: Discriminator Loss: 3.075109424602831, Generator Loss: 0.24854039118880797\n",
      "Iteration 748: Discriminator Loss: 1.476144964290755, Generator Loss: 1.11716722001595\n",
      "Iteration 749: Discriminator Loss: 4.2717099769036, Generator Loss: 1.6716576438991864\n",
      "Iteration 750: Discriminator Loss: 0.8960856056900712, Generator Loss: 0.5582019437406366\n",
      "Iteration 751: Discriminator Loss: 2.206348045178414, Generator Loss: 2.6115084044574894\n",
      "Iteration 752: Discriminator Loss: 1.2041215056504018, Generator Loss: 1.818158770202988\n",
      "Iteration 753: Discriminator Loss: 0.6199597599223535, Generator Loss: 0.111008273352456\n",
      "Iteration 754: Discriminator Loss: 1.5352987516119425, Generator Loss: 0.5573977036535397\n",
      "Iteration 755: Discriminator Loss: 0.6896437886760427, Generator Loss: 0.4860539966394831\n",
      "Iteration 756: Discriminator Loss: 1.251253565150777, Generator Loss: 3.601785828697373\n",
      "Iteration 757: Discriminator Loss: 2.5881554635525297, Generator Loss: 0.5546867753003383\n",
      "Iteration 758: Discriminator Loss: 3.463634830230636, Generator Loss: 2.8295179708882094\n",
      "Iteration 759: Discriminator Loss: 2.932687098225647, Generator Loss: 0.5457055797914756\n",
      "Iteration 760: Discriminator Loss: 2.2932237016917316, Generator Loss: 1.0428481077442557\n",
      "Iteration 761: Discriminator Loss: 0.5155531780558082, Generator Loss: 0.1952170085026643\n",
      "Iteration 762: Discriminator Loss: 0.9649698656490052, Generator Loss: 2.8080598086844546\n",
      "Iteration 763: Discriminator Loss: 6.370857895823926, Generator Loss: 1.4139802407754904\n",
      "Iteration 764: Discriminator Loss: 0.4434539844776463, Generator Loss: 0.07285181762101064\n",
      "Iteration 765: Discriminator Loss: 3.0049078647968983, Generator Loss: 0.004672671829861656\n",
      "Iteration 766: Discriminator Loss: 2.312256182682993, Generator Loss: 0.3262295667485263\n",
      "Iteration 767: Discriminator Loss: 5.01717385746968, Generator Loss: 1.665016822938042\n",
      "Iteration 768: Discriminator Loss: 2.6969824476721573, Generator Loss: 2.0841471489796723\n",
      "Iteration 769: Discriminator Loss: 4.707724063292294, Generator Loss: 1.547183102312523\n",
      "Iteration 770: Discriminator Loss: 1.8338952695490696, Generator Loss: 0.49180276961563524\n",
      "Iteration 771: Discriminator Loss: 0.9980053184266772, Generator Loss: 0.3299031024206939\n",
      "Iteration 772: Discriminator Loss: 0.9266102264858156, Generator Loss: 0.1096036934103682\n",
      "Iteration 773: Discriminator Loss: 3.9351507903924916, Generator Loss: 2.5701325681174496\n",
      "Iteration 774: Discriminator Loss: 2.088973549988943, Generator Loss: 0.2503934800826161\n",
      "Iteration 775: Discriminator Loss: 2.009515763640261, Generator Loss: 0.07442191522058413\n",
      "Iteration 776: Discriminator Loss: 0.7873748985438473, Generator Loss: 0.875496529321873\n",
      "Iteration 777: Discriminator Loss: 1.2690893196602655, Generator Loss: 0.21083215337159553\n",
      "Iteration 778: Discriminator Loss: 1.6281644975943805, Generator Loss: 0.7134249847684625\n",
      "Iteration 779: Discriminator Loss: 1.1487029090351677, Generator Loss: 1.368030509577708\n",
      "Iteration 780: Discriminator Loss: 0.9845267716506587, Generator Loss: 1.667416409022796\n",
      "Iteration 781: Discriminator Loss: 3.470198557463098, Generator Loss: 1.0964774195987237\n",
      "Iteration 782: Discriminator Loss: 2.4147260576930587, Generator Loss: 0.676321992018928\n",
      "Iteration 783: Discriminator Loss: 5.571377996808313, Generator Loss: 0.1739985643670185\n",
      "Iteration 784: Discriminator Loss: 0.8175681386586491, Generator Loss: 0.19617221059182371\n",
      "Iteration 785: Discriminator Loss: 0.11016881483050601, Generator Loss: 0.5101113722693732\n",
      "Iteration 786: Discriminator Loss: 2.80431621066648, Generator Loss: 1.2088914449448007\n",
      "Iteration 787: Discriminator Loss: 1.6087962739731472, Generator Loss: 0.2430234421226543\n",
      "Iteration 788: Discriminator Loss: 3.571454425820789, Generator Loss: 1.4431193196314578\n",
      "Iteration 789: Discriminator Loss: 2.0655989145781626, Generator Loss: 0.2161771417181293\n",
      "Iteration 790: Discriminator Loss: 1.9818579898222544, Generator Loss: 1.5320669409941998\n",
      "Iteration 791: Discriminator Loss: 1.4657037578467615, Generator Loss: 0.7985104348233875\n",
      "Iteration 792: Discriminator Loss: 1.988880804508875, Generator Loss: 1.7288319491333828\n",
      "Iteration 793: Discriminator Loss: 2.409760706535249, Generator Loss: 1.2259484977237094\n",
      "Iteration 794: Discriminator Loss: 1.0406154587002665, Generator Loss: 0.9116307432883589\n",
      "Iteration 795: Discriminator Loss: 1.3040208598660352, Generator Loss: 1.2153095914915026\n",
      "Iteration 796: Discriminator Loss: 1.1701067592777807, Generator Loss: 0.6506824473432904\n",
      "Iteration 797: Discriminator Loss: 0.5794728093209383, Generator Loss: 0.6866820580869566\n",
      "Iteration 798: Discriminator Loss: 1.8437374616553337, Generator Loss: 0.13746119202387863\n",
      "Iteration 799: Discriminator Loss: 1.7369017797040232, Generator Loss: 0.47579599988134375\n",
      "Iteration 800: Discriminator Loss: 2.7041992955334164, Generator Loss: 0.27421423846173737\n",
      "Iteration 801: Discriminator Loss: 6.0001779064129686, Generator Loss: 0.42063427204796977\n",
      "Iteration 802: Discriminator Loss: 3.5021369492585324, Generator Loss: 1.1135957374964576\n",
      "Iteration 803: Discriminator Loss: 1.5961660019714206, Generator Loss: 2.0716350730568243\n",
      "Iteration 804: Discriminator Loss: 0.5602798276891316, Generator Loss: 0.3401453641319512\n",
      "Iteration 805: Discriminator Loss: 3.828407653677472, Generator Loss: 0.16705595920243121\n",
      "Iteration 806: Discriminator Loss: 2.1187971125094167, Generator Loss: 0.07317316965818389\n",
      "Iteration 807: Discriminator Loss: 1.3749246395137065, Generator Loss: 1.3343691149364967\n",
      "Iteration 808: Discriminator Loss: 0.6344249128056433, Generator Loss: 1.7625350118474823\n",
      "Iteration 809: Discriminator Loss: 5.91059582021488, Generator Loss: 0.23877145168639058\n",
      "Iteration 810: Discriminator Loss: 0.26033253001545825, Generator Loss: 0.9738878771660735\n",
      "Iteration 811: Discriminator Loss: 1.0242296113469067, Generator Loss: 0.059043276716505645\n",
      "Iteration 812: Discriminator Loss: 1.2868431176223418, Generator Loss: 0.8379907627461288\n",
      "Iteration 813: Discriminator Loss: 0.5291155556972083, Generator Loss: 1.8739335494618456\n",
      "Iteration 814: Discriminator Loss: 0.4040471384365443, Generator Loss: 0.10365154464286298\n",
      "Iteration 815: Discriminator Loss: 0.10809799550263913, Generator Loss: 1.8063058483671708\n",
      "Iteration 816: Discriminator Loss: 2.9880138797966485, Generator Loss: 0.8452868172345404\n",
      "Iteration 817: Discriminator Loss: 1.730218337560317, Generator Loss: 0.4715661096541402\n",
      "Iteration 818: Discriminator Loss: 0.4485651660818075, Generator Loss: 1.1224610457683108\n",
      "Iteration 819: Discriminator Loss: 0.8728405257705639, Generator Loss: 0.7060123970307406\n",
      "Iteration 820: Discriminator Loss: 1.6986296023040244, Generator Loss: 0.5049169125035486\n",
      "Iteration 821: Discriminator Loss: 2.885270019123117, Generator Loss: 0.040320180742913166\n",
      "Iteration 822: Discriminator Loss: 1.6595299205023293, Generator Loss: 1.3831983021572396\n",
      "Iteration 823: Discriminator Loss: 5.319618529405416, Generator Loss: 1.2398792535998944\n",
      "Iteration 824: Discriminator Loss: 1.810318246972927, Generator Loss: 1.6502026000978234\n",
      "Iteration 825: Discriminator Loss: 9.157744432879575, Generator Loss: 1.8225861123266642\n",
      "Iteration 826: Discriminator Loss: 1.5679447942424172, Generator Loss: 0.64590857775199\n",
      "Iteration 827: Discriminator Loss: 0.09231586822037878, Generator Loss: 0.10434392332687865\n",
      "Iteration 828: Discriminator Loss: 3.4086512963641784, Generator Loss: 1.6341319774773435\n",
      "Iteration 829: Discriminator Loss: 4.982190789452391, Generator Loss: 0.5888016274647694\n",
      "Iteration 830: Discriminator Loss: 2.054677828562286, Generator Loss: 0.8367195741445771\n",
      "Iteration 831: Discriminator Loss: 4.950165494314831, Generator Loss: 0.6694514092867297\n",
      "Iteration 832: Discriminator Loss: 0.4137743878535508, Generator Loss: 0.2069322571932151\n",
      "Iteration 833: Discriminator Loss: 1.0022855344588872, Generator Loss: 0.16856865192747505\n",
      "Iteration 834: Discriminator Loss: 1.2100671903676539, Generator Loss: 0.18947062738027629\n",
      "Iteration 835: Discriminator Loss: 4.681532521460649, Generator Loss: 2.2873212314486326\n",
      "Iteration 836: Discriminator Loss: 1.7838136109993843, Generator Loss: 0.19373539033343073\n",
      "Iteration 837: Discriminator Loss: 0.563616130663948, Generator Loss: 0.35277923431201297\n",
      "Iteration 838: Discriminator Loss: 1.7913472086840647, Generator Loss: 0.3390727855312198\n",
      "Iteration 839: Discriminator Loss: 0.14486756913527282, Generator Loss: 0.02759966191366735\n",
      "Iteration 840: Discriminator Loss: 0.9337209699680129, Generator Loss: 0.44205976543574693\n",
      "Iteration 841: Discriminator Loss: 0.9134822967880811, Generator Loss: 1.6419606312389927\n",
      "Iteration 842: Discriminator Loss: 0.8392310346848839, Generator Loss: 0.3934151774996049\n",
      "Iteration 843: Discriminator Loss: 0.5570731090026562, Generator Loss: 1.8650900703377509\n",
      "Iteration 844: Discriminator Loss: 1.2771835999540906, Generator Loss: 0.27202375075609736\n",
      "Iteration 845: Discriminator Loss: 0.29420062134248526, Generator Loss: 1.9630134701015116\n",
      "Iteration 846: Discriminator Loss: 1.3877752293931864, Generator Loss: 1.4789948301803753\n",
      "Iteration 847: Discriminator Loss: 1.5784909004234857, Generator Loss: 0.15792319297649884\n",
      "Iteration 848: Discriminator Loss: 0.9292646932048848, Generator Loss: 0.7272508840151581\n",
      "Iteration 849: Discriminator Loss: 0.15018469616893582, Generator Loss: 0.684380438528182\n",
      "Iteration 850: Discriminator Loss: 1.689894019278727, Generator Loss: 0.8406698386425066\n",
      "Iteration 851: Discriminator Loss: 1.7547907226290984, Generator Loss: 0.4030027716657918\n",
      "Iteration 852: Discriminator Loss: 2.7542666690206534, Generator Loss: 0.40886664728605376\n",
      "Iteration 853: Discriminator Loss: 1.9926145759234235, Generator Loss: 2.031354237288148\n",
      "Iteration 854: Discriminator Loss: 3.2133205803941696, Generator Loss: 0.11072424943387316\n",
      "Iteration 855: Discriminator Loss: 1.8121131055652082, Generator Loss: 0.13360123619603345\n",
      "Iteration 856: Discriminator Loss: 0.47687800635354427, Generator Loss: 2.500554321243669\n",
      "Iteration 857: Discriminator Loss: 2.1606894276490145, Generator Loss: 1.9667537236240762\n",
      "Iteration 858: Discriminator Loss: 0.55689103178454, Generator Loss: 0.119584552033196\n",
      "Iteration 859: Discriminator Loss: 2.3035859246756454, Generator Loss: 0.8038745675133466\n",
      "Iteration 860: Discriminator Loss: 3.1995846225058755, Generator Loss: 0.09276085504411853\n",
      "Iteration 861: Discriminator Loss: 0.8506052650848849, Generator Loss: 0.0171088308117698\n",
      "Iteration 862: Discriminator Loss: 1.171786487475456, Generator Loss: 0.9136450769266216\n",
      "Iteration 863: Discriminator Loss: 0.6813500532657804, Generator Loss: 0.004445701813390417\n",
      "Iteration 864: Discriminator Loss: 4.353674747351631, Generator Loss: 0.30477980264533283\n",
      "Iteration 865: Discriminator Loss: 0.6774345890272974, Generator Loss: 0.9434345183749223\n",
      "Iteration 866: Discriminator Loss: 0.6458132034028858, Generator Loss: 0.15016383603311095\n",
      "Iteration 867: Discriminator Loss: 2.990691764052643, Generator Loss: 2.229373157192849\n",
      "Iteration 868: Discriminator Loss: 1.4020673474048615, Generator Loss: 0.3513928246281172\n",
      "Iteration 869: Discriminator Loss: 0.9934955646017672, Generator Loss: 0.21603401872307249\n",
      "Iteration 870: Discriminator Loss: 2.6698729997657207, Generator Loss: 0.06359875569578176\n",
      "Iteration 871: Discriminator Loss: 3.004075702514969, Generator Loss: 0.565807326044461\n",
      "Iteration 872: Discriminator Loss: 8.581747353210815, Generator Loss: 0.2573581583652031\n",
      "Iteration 873: Discriminator Loss: 0.07073348975406793, Generator Loss: 0.014027969873188066\n",
      "Iteration 874: Discriminator Loss: 1.6149238084364597, Generator Loss: 0.08459592167048942\n",
      "Iteration 875: Discriminator Loss: 2.8580815808809263, Generator Loss: 0.23293016081215454\n",
      "Iteration 876: Discriminator Loss: 1.7760123853355412, Generator Loss: 0.16050782556076434\n",
      "Iteration 877: Discriminator Loss: 0.8885414316251476, Generator Loss: 0.20659870086651708\n",
      "Iteration 878: Discriminator Loss: 3.078531181020031, Generator Loss: 0.33531869155880945\n",
      "Iteration 879: Discriminator Loss: 0.2915828103097013, Generator Loss: 1.9614959635197\n",
      "Iteration 880: Discriminator Loss: 0.8267803104850477, Generator Loss: 2.089532222013509\n",
      "Iteration 881: Discriminator Loss: 0.6036691774156044, Generator Loss: 0.11886263308877879\n",
      "Iteration 882: Discriminator Loss: 3.2527304564368356, Generator Loss: 0.7491756237128125\n",
      "Iteration 883: Discriminator Loss: 2.438549252623453, Generator Loss: 1.2926267994278229\n",
      "Iteration 884: Discriminator Loss: 0.5078816994313115, Generator Loss: 2.4383250565816117\n",
      "Iteration 885: Discriminator Loss: 1.770235212158597, Generator Loss: 0.08833098889555024\n",
      "Iteration 886: Discriminator Loss: 1.7947242120640257, Generator Loss: 1.8230640350022103\n",
      "Iteration 887: Discriminator Loss: 0.8400519087172453, Generator Loss: 0.8317266288747994\n",
      "Iteration 888: Discriminator Loss: 0.3131149185278671, Generator Loss: 0.05028739186837793\n",
      "Iteration 889: Discriminator Loss: 4.158660191149305, Generator Loss: 0.7673636833938182\n",
      "Iteration 890: Discriminator Loss: 0.1673521327712156, Generator Loss: 1.0308446036499392\n",
      "Iteration 891: Discriminator Loss: 4.82187226333003, Generator Loss: 3.103971505450819\n",
      "Iteration 892: Discriminator Loss: 0.9465039775406161, Generator Loss: 0.591320019335555\n",
      "Iteration 893: Discriminator Loss: 1.2241224028317539, Generator Loss: 1.2299042509514977\n",
      "Iteration 894: Discriminator Loss: 1.2006207013321237, Generator Loss: 0.1491152123315206\n",
      "Iteration 895: Discriminator Loss: 1.0017098337825006, Generator Loss: 0.24431473521409716\n",
      "Iteration 896: Discriminator Loss: 0.330440040504359, Generator Loss: 1.0428799717223958\n",
      "Iteration 897: Discriminator Loss: 0.6831172017314984, Generator Loss: 0.898057995217256\n",
      "Iteration 898: Discriminator Loss: 1.727622959299162, Generator Loss: 0.6726201616041297\n",
      "Iteration 899: Discriminator Loss: 1.8901546905320108, Generator Loss: 0.0025908552741827867\n",
      "Iteration 900: Discriminator Loss: 2.128676497658655, Generator Loss: 0.7228265363677512\n",
      "Iteration 901: Discriminator Loss: 3.4584508161720495, Generator Loss: 1.7283946801874444\n",
      "Iteration 902: Discriminator Loss: 1.4759454679662904, Generator Loss: 0.9891300324372174\n",
      "Iteration 903: Discriminator Loss: 0.8325359903093686, Generator Loss: 0.3826036945679292\n",
      "Iteration 904: Discriminator Loss: 0.4349135567759467, Generator Loss: 0.24971161184432475\n",
      "Iteration 905: Discriminator Loss: 1.2974041876339366, Generator Loss: 1.1115365565197461\n",
      "Iteration 906: Discriminator Loss: 0.4661869954968354, Generator Loss: 0.1444838877745335\n",
      "Iteration 907: Discriminator Loss: 0.044059425351935196, Generator Loss: 0.9209180775568179\n",
      "Iteration 908: Discriminator Loss: 4.334282019552097, Generator Loss: 0.8241202485361249\n",
      "Iteration 909: Discriminator Loss: 1.3826299650276055, Generator Loss: 0.7575277192818747\n",
      "Iteration 910: Discriminator Loss: 4.461491629836397, Generator Loss: 0.7867958551791304\n",
      "Iteration 911: Discriminator Loss: 4.076061691353489, Generator Loss: 4.610862455555357\n",
      "Iteration 912: Discriminator Loss: 0.8708220127799233, Generator Loss: 0.4671789647210598\n",
      "Iteration 913: Discriminator Loss: 2.058464259084532, Generator Loss: 0.44196986089914536\n",
      "Iteration 914: Discriminator Loss: 2.3435904098441216, Generator Loss: 2.797614021833119\n",
      "Iteration 915: Discriminator Loss: 2.7069448200431143, Generator Loss: 0.3556061976574054\n",
      "Iteration 916: Discriminator Loss: 1.2434019915241783, Generator Loss: 0.1804151899720864\n",
      "Iteration 917: Discriminator Loss: 2.2613730702143164, Generator Loss: 0.2958843480392426\n",
      "Iteration 918: Discriminator Loss: 0.552541622913408, Generator Loss: 1.1115343148782435\n",
      "Iteration 919: Discriminator Loss: 3.6951457674805988, Generator Loss: 4.401517252663775\n",
      "Iteration 920: Discriminator Loss: 3.613696573556165, Generator Loss: 0.2425259673336965\n",
      "Iteration 921: Discriminator Loss: 2.01346462440075, Generator Loss: 0.3216127435601769\n",
      "Iteration 922: Discriminator Loss: 6.5056302944089595, Generator Loss: 0.36785958077753267\n",
      "Iteration 923: Discriminator Loss: 0.6163346402117489, Generator Loss: 1.4291547321634126\n",
      "Iteration 924: Discriminator Loss: 1.9948906316228623, Generator Loss: 0.2277110374295567\n",
      "Iteration 925: Discriminator Loss: 0.44876602240045466, Generator Loss: 0.4261799672742208\n",
      "Iteration 926: Discriminator Loss: 0.8241958818382329, Generator Loss: 0.07977619309702087\n",
      "Iteration 927: Discriminator Loss: 0.39627243023002867, Generator Loss: 0.7495121418956746\n",
      "Iteration 928: Discriminator Loss: 1.8122290385279696, Generator Loss: 0.08040373124318259\n",
      "Iteration 929: Discriminator Loss: 6.732461049690771, Generator Loss: 0.5196531507827182\n",
      "Iteration 930: Discriminator Loss: 0.34979698606040044, Generator Loss: 1.0329564637369502\n",
      "Iteration 931: Discriminator Loss: 2.4322106037413174, Generator Loss: 1.6540948896750358\n",
      "Iteration 932: Discriminator Loss: 1.1830281895805543, Generator Loss: 1.0941528362623183\n",
      "Iteration 933: Discriminator Loss: 0.803514457436387, Generator Loss: 0.3933258456878238\n",
      "Iteration 934: Discriminator Loss: 3.5492838989710354, Generator Loss: 0.6812818857547258\n",
      "Iteration 935: Discriminator Loss: 0.2339481064364354, Generator Loss: 0.0038776852022984172\n",
      "Iteration 936: Discriminator Loss: 1.3096298181185306, Generator Loss: 3.6561507109101954\n",
      "Iteration 937: Discriminator Loss: 2.029461840909087, Generator Loss: 0.4449245363106704\n",
      "Iteration 938: Discriminator Loss: 1.658349804857524, Generator Loss: 2.437155855826764\n",
      "Iteration 939: Discriminator Loss: 2.4433394377851183, Generator Loss: 0.05845952149508077\n",
      "Iteration 940: Discriminator Loss: 1.8781931318676262, Generator Loss: 0.810615183203798\n",
      "Iteration 941: Discriminator Loss: 0.22757293676416462, Generator Loss: 0.750138137978559\n",
      "Iteration 942: Discriminator Loss: 1.1214343558884712, Generator Loss: 2.555005866597863\n",
      "Iteration 943: Discriminator Loss: 4.772125858355586, Generator Loss: 1.0095282158169343\n",
      "Iteration 944: Discriminator Loss: 1.3074040321200964, Generator Loss: 0.9060526561988502\n",
      "Iteration 945: Discriminator Loss: 4.581188854878457, Generator Loss: 0.40197159112741365\n",
      "Iteration 946: Discriminator Loss: 0.6468057867004292, Generator Loss: 0.16388447747977167\n",
      "Iteration 947: Discriminator Loss: 6.645905644366218, Generator Loss: 0.21056537224367397\n",
      "Iteration 948: Discriminator Loss: 0.25903034882941145, Generator Loss: 0.4940063658978523\n",
      "Iteration 949: Discriminator Loss: 0.8884700385605949, Generator Loss: 1.2092814541083279\n",
      "Iteration 950: Discriminator Loss: 0.49973638329795356, Generator Loss: 0.6050841028516348\n",
      "Iteration 951: Discriminator Loss: 1.0533552040417424, Generator Loss: 0.36476521799384226\n",
      "Iteration 952: Discriminator Loss: 0.6672543183937002, Generator Loss: 1.1453414312560264\n",
      "Iteration 953: Discriminator Loss: 1.7334714494660286, Generator Loss: 0.21171871987817578\n",
      "Iteration 954: Discriminator Loss: 0.5855359037670602, Generator Loss: 0.060727233605791256\n",
      "Iteration 955: Discriminator Loss: 0.5000720574594119, Generator Loss: 0.4784248318114773\n",
      "Iteration 956: Discriminator Loss: 0.488448995766279, Generator Loss: 0.14608398790821114\n",
      "Iteration 957: Discriminator Loss: 2.2322084755795633, Generator Loss: 0.4389917779027139\n",
      "Iteration 958: Discriminator Loss: 4.241278732865102, Generator Loss: 1.866790270544004\n",
      "Iteration 959: Discriminator Loss: 1.39672666030439, Generator Loss: 0.3566735098636844\n",
      "Iteration 960: Discriminator Loss: 0.1734180719958627, Generator Loss: 0.8512800353966348\n",
      "Iteration 961: Discriminator Loss: 0.5879282119007275, Generator Loss: 0.5021305699884274\n",
      "Iteration 962: Discriminator Loss: 0.5043136548093554, Generator Loss: 0.44184427769693835\n",
      "Iteration 963: Discriminator Loss: 1.3522996012133541, Generator Loss: 0.7636673229307824\n",
      "Iteration 964: Discriminator Loss: 1.8942732819500232, Generator Loss: 0.19458621427042835\n",
      "Iteration 965: Discriminator Loss: 1.4064122601527753, Generator Loss: 1.1540981229612501\n",
      "Iteration 966: Discriminator Loss: 2.302800691745923, Generator Loss: 0.6109970018844029\n",
      "Iteration 967: Discriminator Loss: 1.0142651972841565, Generator Loss: 0.29377759991975655\n",
      "Iteration 968: Discriminator Loss: 0.7278109847096927, Generator Loss: 0.9067050721990134\n",
      "Iteration 969: Discriminator Loss: 1.241134426860341, Generator Loss: 0.02290399310584594\n",
      "Iteration 970: Discriminator Loss: 1.2942796595180606, Generator Loss: 0.6014232587776185\n",
      "Iteration 971: Discriminator Loss: 2.441105747085931, Generator Loss: 1.0895726758498496\n",
      "Iteration 972: Discriminator Loss: 1.9514029438642178, Generator Loss: 2.9551915749020337\n",
      "Iteration 973: Discriminator Loss: 1.7437042172641313, Generator Loss: 0.2581689835501393\n",
      "Iteration 974: Discriminator Loss: 0.809104990019066, Generator Loss: 6.09072035703831\n",
      "Iteration 975: Discriminator Loss: 1.5696610254146366, Generator Loss: 0.0036652329656624424\n",
      "Iteration 976: Discriminator Loss: 0.19780794737788154, Generator Loss: 1.2320464544659646\n",
      "Iteration 977: Discriminator Loss: 3.905707024450261, Generator Loss: 0.7819008283853207\n",
      "Iteration 978: Discriminator Loss: 2.5527305297379703, Generator Loss: 0.6842248257906735\n",
      "Iteration 979: Discriminator Loss: 1.135603402227092, Generator Loss: 0.5996740257530501\n",
      "Iteration 980: Discriminator Loss: 1.7039612706758853, Generator Loss: 1.5077677174862885\n",
      "Iteration 981: Discriminator Loss: 4.7932100116502765, Generator Loss: 3.0856452979395166\n",
      "Iteration 982: Discriminator Loss: 1.0541035399366996, Generator Loss: 0.5250662660420238\n",
      "Iteration 983: Discriminator Loss: 1.9524015139335826, Generator Loss: 0.11731334074721969\n",
      "Iteration 984: Discriminator Loss: 0.8034939108109787, Generator Loss: 3.3928392165125363\n",
      "Iteration 985: Discriminator Loss: 0.40227490599524063, Generator Loss: 1.3934738953741685\n",
      "Iteration 986: Discriminator Loss: 2.6821296281813147, Generator Loss: 0.26271108276746796\n",
      "Iteration 987: Discriminator Loss: 0.8107205139240771, Generator Loss: 0.6649171217827198\n",
      "Iteration 988: Discriminator Loss: 0.976809477841443, Generator Loss: 0.6331947010235861\n",
      "Iteration 989: Discriminator Loss: 1.2380532125272714, Generator Loss: 1.3063485035603044\n",
      "Iteration 990: Discriminator Loss: 2.422552338212409, Generator Loss: 2.326466583906831\n",
      "Iteration 991: Discriminator Loss: 1.960496974245314, Generator Loss: 0.4366163359549393\n",
      "Iteration 992: Discriminator Loss: 3.005720229977914, Generator Loss: 1.2207640439089138\n",
      "Iteration 993: Discriminator Loss: 1.6697839261564849, Generator Loss: 1.5254140972194303\n",
      "Iteration 994: Discriminator Loss: 2.1670260187954717, Generator Loss: 2.0227908789230375\n",
      "Iteration 995: Discriminator Loss: 0.7024978813209286, Generator Loss: 0.741051942056592\n",
      "Iteration 996: Discriminator Loss: 1.8868437040190045, Generator Loss: 0.90360622632911\n",
      "Iteration 997: Discriminator Loss: 3.647269797825428, Generator Loss: 1.47264429438387\n",
      "Iteration 998: Discriminator Loss: 5.977642439530125, Generator Loss: 1.0569731525097699\n",
      "Iteration 999: Discriminator Loss: 1.383893149661601, Generator Loss: 0.605808645899451\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the generator\n",
    "def generator(z, theta_G):\n",
    "    # Dummy generator: identity function for simplicity\n",
    "    return z\n",
    "\n",
    "# Define the encoder\n",
    "def encoder(x, theta_E):\n",
    "    # Dummy encoder: identity function for simplicity\n",
    "    return x\n",
    "\n",
    "# Define the discriminator\n",
    "def discriminator(x, z, theta_D):\n",
    "    # Dummy discriminator: simple heuristic for example\n",
    "    return np.random.rand()  # Random value for simplicity\n",
    "\n",
    "# Define the loss functions for GAN\n",
    "def discriminator_loss(x, z, theta_D):\n",
    "    # Loss for the discriminator when distinguishing real data\n",
    "    D_real = discriminator(x, encoder(x, None), theta_D)\n",
    "    D_fake = discriminator(generator(z, None), z, theta_D)\n",
    "    loss_D = -np.log(D_real) - np.log(1 - D_fake)\n",
    "    return loss_D\n",
    "\n",
    "def generator_loss(z, theta_G, theta_D):\n",
    "    # Loss for the generator to fool the discriminator\n",
    "    D_fake = discriminator(generator(z, None), z, theta_D)\n",
    "    loss_G = -np.log(D_fake)\n",
    "    return loss_G\n",
    "\n",
    "# Training loop for BiGAN\n",
    "def train_bigan(num_iterations, learning_rate):\n",
    "    # Initialize parameters\n",
    "    theta_G = np.zeros((1,))  # Dummy initialization\n",
    "    theta_E = np.zeros((1,))  # Dummy initialization\n",
    "    theta_D = np.zeros((1,))  # Dummy initialization\n",
    "\n",
    "    for iteration in range(num_iterations):\n",
    "        # Sample noise and real data\n",
    "        z = np.random.randn(1)\n",
    "        x = np.random.randn(1)\n",
    "        \n",
    "        # Compute losses\n",
    "        loss_D = discriminator_loss(x, z, theta_D)\n",
    "        loss_G = generator_loss(z, theta_G, theta_D)\n",
    "        \n",
    "        # Update parameters (Dummy update step for illustration)\n",
    "        theta_D -= learning_rate * loss_D\n",
    "        theta_G -= learning_rate * loss_G\n",
    "\n",
    "        # Print losses\n",
    "        print(f\"Iteration {iteration}: Discriminator Loss: {loss_D}, Generator Loss: {loss_G}\")\n",
    "\n",
    "# Hyperparameters\n",
    "num_iterations = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Train the BiGAN\n",
    "train_bigan(num_iterations, learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13e46ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-24 23:47:47.407359: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 188160000 exceeds 10% of free system memory.\n",
      "2024-08-24 23:47:47.922758: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 188160000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/tmp/ipykernel_12968/1325297434.py\", line 69, in train_step  *\n        real_output, fake_output, encoded_data = biGAN((generated_images, images), training=True)\n    File \"/home/radha/anaconda3/envs/cv37/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_filerc58268p.py\", line 11, in tf__call\n        Gz = ag__.converted_call(ag__.ld(self).generator, (ag__.ld(z),), None, fscope)\n\n    ValueError: Exception encountered when calling layer \"bi_gan\" \"                 f\"(type BiGAN).\n    \n    in user code:\n    \n        File \"/tmp/ipykernel_12968/1325297434.py\", line 43, in call  *\n            Gz = self.generator(z)\n        File \"/home/radha/anaconda3/envs/cv37/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/home/radha/anaconda3/envs/cv37/lib/python3.7/site-packages/keras/engine/input_spec.py\", line 296, in assert_input_compatibility\n            f'Input {input_index} of layer \"{layer_name}\" is '\n    \n        ValueError: Input 0 of layer \"sequential_6\" is incompatible with the layer: expected shape=(None, 100), found shape=(64, 28, 28, 1)\n    \n    \n    Call arguments received by layer \"bi_gan\" \"                 f\"(type BiGAN):\n      â€¢ inputs=('tf.Tensor(shape=(64, 28, 28, 1), dtype=float32)', 'tf.Tensor(shape=(64, 28, 28, 1), dtype=float32)')\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12968/1325297434.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;31m# Start training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_12968/1325297434.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataset, epochs)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mimage_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbiGAN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch + 1} completed\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cv37/lib/python3.7/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/__autograph_generated_fileoi18hb8r.py\u001b[0m in \u001b[0;36mtf__train_step\u001b[0;34m(images, biGAN, batch_size)\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgen_tape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0menc_tape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdisc_tape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                     \u001b[0mgenerated_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbiGAN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                     \u001b[0;34m(\u001b[0m\u001b[0mreal_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_data\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbiGAN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m                     \u001b[0mdisc_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscriminator_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                     \u001b[0mgen_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cv37/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/__autograph_generated_filerc58268p.py\u001b[0m in \u001b[0;36mtf__call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUndefinedReturnValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                 \u001b[0mGz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m                 \u001b[0mEz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0mDx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/tmp/ipykernel_12968/1325297434.py\", line 69, in train_step  *\n        real_output, fake_output, encoded_data = biGAN((generated_images, images), training=True)\n    File \"/home/radha/anaconda3/envs/cv37/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_filerc58268p.py\", line 11, in tf__call\n        Gz = ag__.converted_call(ag__.ld(self).generator, (ag__.ld(z),), None, fscope)\n\n    ValueError: Exception encountered when calling layer \"bi_gan\" \"                 f\"(type BiGAN).\n    \n    in user code:\n    \n        File \"/tmp/ipykernel_12968/1325297434.py\", line 43, in call  *\n            Gz = self.generator(z)\n        File \"/home/radha/anaconda3/envs/cv37/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/home/radha/anaconda3/envs/cv37/lib/python3.7/site-packages/keras/engine/input_spec.py\", line 296, in assert_input_compatibility\n            f'Input {input_index} of layer \"{layer_name}\" is '\n    \n        ValueError: Input 0 of layer \"sequential_6\" is incompatible with the layer: expected shape=(None, 100), found shape=(64, 28, 28, 1)\n    \n    \n    Call arguments received by layer \"bi_gan\" \"                 f\"(type BiGAN):\n      â€¢ inputs=('tf.Tensor(shape=(64, 28, 28, 1), dtype=float32)', 'tf.Tensor(shape=(64, 28, 28, 1), dtype=float32)')\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Flatten, Reshape\n",
    "from tensorflow.keras.models import Sequential\n",
    "import numpy as np\n",
    "\n",
    "# Define the generator\n",
    "def build_generator():\n",
    "    model = Sequential([\n",
    "        Dense(256, activation='relu', input_shape=(100,)),\n",
    "        Dense(784, activation='sigmoid'),\n",
    "        Reshape((28, 28, 1))\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Define the encoder\n",
    "def build_encoder():\n",
    "    model = Sequential([\n",
    "        Flatten(input_shape=(28, 28, 1)),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dense(100)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Define the discriminator\n",
    "def build_discriminator():\n",
    "    model = Sequential([\n",
    "        Flatten(input_shape=(28, 28, 1)),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Define the BiGAN class\n",
    "class BiGAN(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(BiGAN, self).__init__()\n",
    "        self.generator = build_generator()\n",
    "        self.encoder = build_encoder()\n",
    "        self.discriminator = build_discriminator()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z, x = inputs\n",
    "        Gz = self.generator(z)\n",
    "        Ez = self.encoder(x)\n",
    "        Dx = self.discriminator(x)\n",
    "        D_Gz = self.discriminator(Gz)\n",
    "        return Dx, D_Gz, Ez\n",
    "\n",
    "# Loss functions\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    return tf.keras.losses.binary_crossentropy(tf.ones_like(real_output), real_output) + \\\n",
    "           tf.keras.losses.binary_crossentropy(tf.zeros_like(fake_output), fake_output)\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return tf.keras.losses.binary_crossentropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "# Optimizers\n",
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "encoder_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "\n",
    "# Training step\n",
    "@tf.function\n",
    "def train_step(images, biGAN, batch_size):\n",
    "    noise = tf.random.normal([batch_size, 100])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as enc_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = biGAN.generator(noise, training=True)\n",
    "        real_output, fake_output, encoded_data = biGAN((generated_images, images), training=True)\n",
    "\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        # Encoder loss could be added here depending on the specific formulation\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, biGAN.generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, biGAN.discriminator.trainable_variables)\n",
    "    gradients_of_encoder = enc_tape.gradient(enc_loss, biGAN.encoder.trainable_variables)  # Define enc_loss\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, biGAN.generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, biGAN.discriminator.trainable_variables))\n",
    "    encoder_optimizer.apply_gradients(zip(gradients_of_encoder, biGAN.encoder.trainable_variables))  # Apply encoder gradients\n",
    "\n",
    "# Training loop\n",
    "def train(dataset, epochs):\n",
    "    biGAN = BiGAN()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for image_batch in dataset:\n",
    "            train_step(image_batch, biGAN, batch_size=64)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1} completed\")\n",
    "\n",
    "# Load dataset (example with MNIST)\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, _), (_, _) = mnist.load_data()\n",
    "x_train = np.expand_dims(x_train, axis=-1).astype('float32') / 255.0\n",
    "dataset = tf.data.Dataset.from_tensor_slices(x_train).shuffle(60000).batch(64)\n",
    "\n",
    "# Start training\n",
    "train(dataset, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0e18aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, 28, 28, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " input_6 (InputLayer)           [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " sequential_9 (Sequential)      (None, 100)          347076      ['input_5[0][0]']                \n",
      "                                                                                                  \n",
      " sequential_10 (Sequential)     (None, 28, 28, 1)    798977      ['input_6[0][0]']                \n",
      "                                                                                                  \n",
      " model (Functional)             (None, 1)            447841      ['input_5[0][0]',                \n",
      "                                                                  'sequential_9[0][0]',           \n",
      "                                                                  'sequential_10[0][0]',          \n",
      "                                                                  'input_6[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,593,894\n",
      "Trainable params: 1,593,894\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Define input shapes\n",
    "image_shape = (28, 28, 1)\n",
    "latent_dim = 100\n",
    "\n",
    "# Encoder: Maps images to the latent space\n",
    "def build_encoder():\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.InputLayer(input_shape=image_shape),\n",
    "        layers.Conv2D(32, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(latent_dim)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Generator: Maps latent vectors to images\n",
    "def build_generator():\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.InputLayer(input_shape=(latent_dim,)),\n",
    "        layers.Dense(7*7*128),\n",
    "        layers.Reshape((7, 7, 128)),\n",
    "        layers.Conv2DTranspose(64, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.ReLU(),\n",
    "        layers.Conv2DTranspose(32, kernel_size=4, strides=2, padding=\"same\"),\n",
    "        layers.ReLU(),\n",
    "        layers.Conv2DTranspose(1, kernel_size=7, activation='sigmoid', padding=\"same\")\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Discriminator: Takes both image and latent vector tuples (G(z), z) and (x, E(x))\n",
    "def build_discriminator():\n",
    "    image_input = layers.Input(shape=image_shape)\n",
    "    latent_input = layers.Input(shape=(latent_dim,))\n",
    "\n",
    "    x = layers.Conv2D(32, kernel_size=4, strides=2, padding=\"same\")(image_input)\n",
    "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "    x = layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\")(x)\n",
    "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "\n",
    "    combined = layers.Concatenate()([x, latent_input])\n",
    "    combined = layers.Dense(128)(combined)\n",
    "    combined = layers.LeakyReLU(alpha=0.2)(combined)\n",
    "    combined = layers.Dense(1, activation='sigmoid')(combined)\n",
    "\n",
    "    model = tf.keras.Model([image_input, latent_input], combined)\n",
    "    return model\n",
    "\n",
    "# Instantiate the models\n",
    "encoder = build_encoder()\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "\n",
    "# Inputs\n",
    "real_images = layers.Input(shape=image_shape)\n",
    "latent_vectors = layers.Input(shape=(latent_dim,))\n",
    "\n",
    "# Generate fake images from latent vectors\n",
    "fake_images = generator(latent_vectors)\n",
    "\n",
    "# Encode real images into latent space\n",
    "encoded_images = encoder(real_images)\n",
    "\n",
    "# Discriminator outputs\n",
    "real_disc = discriminator([real_images, encoded_images])\n",
    "fake_disc = discriminator([fake_images, latent_vectors])\n",
    "\n",
    "# Define the full BiGAN model\n",
    "bigan = tf.keras.Model([real_images, latent_vectors], [real_disc, fake_disc])\n",
    "\n",
    "# Compile the BiGAN model\n",
    "bigan.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "# Summary of the model\n",
    "bigan.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05596c35",
   "metadata": {},
   "source": [
    "# Variational Autoencoders (VAEs)\n",
    "\n",
    "In the previous sections, we explored two methods for generating data points \\( x \\) from a noise vector \\( z \\): **standard GANs** and **BiGANs**. Now, we shift our focus to an alternative method: **Variational Autoencoders (VAEs)**. \n",
    "\n",
    "VAEs are called \"autoencoders\" because the final training objective derived from this setup includes both an encoder and a decoder, resembling a traditional autoencoder [22]. However, unlike traditional autoencoders, VAEs introduce a probabilistic approach to modeling the latent space.\n",
    "\n",
    "## VAE Architecture\n",
    "\n",
    "The construction of a VAE is depicted in **Figure 7.19**. \n",
    "\n",
    "$$\n",
    "\\text{Figure 7.19: A Variational Autoencoder (VAE) connected with a standard Generative Adversarial Network (GAN). Unlike the BiGAN, in which the encoder's output \\( E(x) \\) is one of the bidirectional inputs, the autoencoder \\( E \\) here executes variation \\( z \\sim E(x) \\) that is used as the random noise in the GAN.}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "G \\\\\n",
    "& z \\sim E(x) \\\\\n",
    "& \\text{E} \\\\\n",
    "& \\text{VAE} \\\\\n",
    "& \\text{GAN} \\\\\n",
    "& G\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### Encoder\n",
    "\n",
    "The encoder \\( E(x) \\) maps the input data \\( x \\) to a distribution over the latent space, typically assumed to be Gaussian:\n",
    "\n",
    "$$\n",
    "q(z|x) = \\mathcal{N}(z; \\mu(x), \\sigma^2(x))\n",
    "$$\n",
    "### Reparameterization Trick\n",
    "\n",
    "To allow backpropagation through the sampling process, the **reparameterization trick** is used. The latent variable \\( z \\) is expressed as:\n",
    "\n",
    "$$\n",
    "z = \\mu + \\sigma \\cdot \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, 1)\n",
    "$$\n",
    "\n",
    "### Decoder\n",
    "\n",
    "The decoder \\( G(z) \\) takes a sample \\( z \\) from the latent space and reconstructs the input data \\( x \\), generating \\( x' \\) from \\( p(x|z) \\).\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "The VAE loss consists of two components:\n",
    "\n",
    "- **Reconstruction Loss**: Measures how well the decoder reconstructs the input data.\n",
    "  \n",
    "$$\n",
    "\\text{Reconstruction Loss} = -\\mathbb{E}_{q(z|x)} [\\log p(x|z)]\n",
    "$$\n",
    "\n",
    "- **KL Divergence**: Measures the divergence between the learned distribution \\( q(z|x) \\) and the prior distribution \\( p(z) \\), typically a standard normal distribution.\n",
    "\n",
    "$$\n",
    "\\text{KL Divergence} = \\text{KL}(q(z|x) \\| p(z))\n",
    "$$\n",
    "\n",
    "The overall loss function is:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = -\\mathbb{E}_{q(z|x)} [\\log p(x|z)] + \\text{KL}(q(z|x) \\| p(z))\n",
    "$$\n",
    "\n",
    "### Connection with GANs\n",
    "\n",
    "In advanced models, VAEs can be combined with GANs to leverage the strengths of both models. The VAE ensures a structured latent space, while the GAN improves the quality of generated data by using a discriminator.\n",
    "\n",
    "This combination results in a more powerful generative model that benefits from the stability of the VAE and the sharpness of GAN-generated samples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0df328",
   "metadata": {},
   "source": [
    "# Generative Modeling\n",
    "\n",
    "**Generative modeling** is a broad area of machine learning that focuses on creating models of distributions \\( p(x) \\), where \\( x \\in X \\) and \\( X \\) is potentially a high-dimensional space. For example, in image data, each \"datapoint\" (image) can have thousands or millions of dimensions (pixels). The generative model needs to capture the dependencies between pixels, such as nearby pixels having similar colors and being organized into objects [22].\n",
    "\n",
    "## Random Functions in Generative Models\n",
    "\n",
    "Let \\( f_\\theta(z) \\) represent a family of random functions in the generator \\( G \\), parameterized by a fixed generative model parameter vector \\( \\theta \\) in some space \\( \\Theta \\). To generate a datapoint \\( x \\), we use:\n",
    "\n",
    "$$\n",
    "x = f_\\theta(z) \\in X\n",
    "$$\n",
    "\n",
    "where \\( z \\) is a random noise vector in the space \\( Z \\). The goal is to generate a datapoint according to:\n",
    "\n",
    "$$\n",
    "x = \\arg \\max_x P_\\theta(x) = \\int_z f_\\theta(z) P_\\theta(z) \\, dz.\n",
    "$$\n",
    "\n",
    "However, since the random function \\( f_\\theta(z) \\) is difficult to determine, we can use a distribution \\( P_\\theta(x|z) \\) instead, which makes the dependence of \\( x \\) on \\( z \\) explicit by using the law of total probability. Thus, the maximization becomes:\n",
    "\n",
    "$$\n",
    "x = \\arg \\max_x P_\\theta(x) = \\int_z P_\\theta(x|z) P_\\theta(z) \\, dz.\n",
    "$$\n",
    "\n",
    "## Sampling from the Latent Space\n",
    "\n",
    "To solve the above equation, we can draw samples of \\( z \\) from a standard Gaussian distribution \\( \\mathcal{N}(0, I) \\).\n",
    "\n",
    "Let the prior over the latent variables be the centered isotropic multivariate Gaussian:\n",
    "\n",
    "$$\n",
    "P_\\theta(z) = \\mathcal{N}(z; 0, I)\n",
    "$$\n",
    "\n",
    "and \\( P_\\theta(x|z) \\) be a multivariate Gaussian (in the case of real-valued data) or Bernoulli (in the case of binary data) whose distribution parameters are computed from \\( z \\) using a multilayer perceptron (MLP). The MLP is a fully connected neural network with a single hidden layer, defined as follows [79]:\n",
    "\n",
    "### Bernoulli MLP as Decoder\n",
    "\n",
    "A multivariate Bernoulli probability \\( P_\\theta(x|z) \\) is computed from \\( z \\) with a fully connected neural network with a single hidden layer:\n",
    "\n",
    "$$\n",
    "\\log P_\\theta(x|z) = \\sum_{i=1}^D \\left[ x_i \\log y_i + (1 - x_i) \\log(1 - y_i) \\right],\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "y = f_\\sigma(W_2 \\, \\text{tanh}(W_1 z + b_1) + b_2)\n",
    "$$\n",
    "\n",
    "Here, \\( f_\\sigma \\) is the element-wise sigmoid activation function, and \\( \\theta = \\{W_1, W_2, b_1, b_2\\} \\) are the weights and biases of the MLP.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ea03c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-25 00:09:06.227520: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 188160000 exceeds 10% of free system memory.\n",
      "2024-08-25 00:09:06.377985: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 188160000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"/home/radha/anaconda3/envs/cv37/lib/python3.7/site-packages/keras/engine/training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n\n    TypeError: tf__vae_loss() missing 2 required positional arguments: 'z_mean' and 'z_log_var'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12968/2366415401.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;31m# Train the VAE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;31m# Generate new samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cv37/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cv37/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    File \"/home/radha/anaconda3/envs/cv37/lib/python3.7/site-packages/keras/engine/training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n\n    TypeError: tf__vae_loss() missing 2 required positional arguments: 'z_mean' and 'z_log_var'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "# Encoder model\n",
    "class Encoder(Model):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.dense1 = layers.Dense(512, activation='relu')\n",
    "        self.dense2 = layers.Dense(256, activation='relu')\n",
    "        self.mean = layers.Dense(latent_dim)\n",
    "        self.log_var = layers.Dense(latent_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        z_mean = self.mean(x)\n",
    "        z_log_var = self.log_var(x)\n",
    "        return z_mean, z_log_var\n",
    "\n",
    "# Sampling function for the reparameterization trick\n",
    "def sampling(z_mean, z_log_var):\n",
    "    batch = tf.shape(z_mean)[0]\n",
    "    dim = tf.shape(z_mean)[1]\n",
    "    epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "# Decoder model\n",
    "class Decoder(Model):\n",
    "    def __init__(self, original_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dense1 = layers.Dense(256, activation='relu')\n",
    "        self.dense2 = layers.Dense(512, activation='relu')\n",
    "        self.dense3 = layers.Dense(original_dim, activation='sigmoid')\n",
    "\n",
    "    def call(self, z):\n",
    "        z = self.dense1(z)\n",
    "        z = self.dense2(z)\n",
    "        reconstructed = self.dense3(z)\n",
    "        return reconstructed\n",
    "\n",
    "# VAE model\n",
    "class VAE(Model):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def call(self, x):\n",
    "        z_mean, z_log_var = self.encoder(x)\n",
    "        z = sampling(z_mean, z_log_var)\n",
    "        reconstructed = self.decoder(z)\n",
    "        return reconstructed\n",
    "\n",
    "# Loss function for VAE\n",
    "def vae_loss(x, reconstructed_x, z_mean, z_log_var):\n",
    "    reconstruction_loss = tf.reduce_mean(\n",
    "        tf.keras.losses.binary_crossentropy(x, reconstructed_x))\n",
    "    reconstruction_loss *= x.shape[1] * x.shape[2]  # Flattened size\n",
    "    kl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
    "    kl_loss = tf.reduce_mean(kl_loss) * -0.5\n",
    "    return reconstruction_loss + kl_loss\n",
    "\n",
    "# Set dimensions\n",
    "original_dim = 28 * 28  # Assuming input is a 28x28 image\n",
    "latent_dim = 2\n",
    "\n",
    "# Create the VAE\n",
    "encoder = Encoder(latent_dim)\n",
    "decoder = Decoder(original_dim)\n",
    "vae = VAE(encoder, decoder)\n",
    "\n",
    "# Compile the model\n",
    "vae.compile(optimizer='adam', loss=vae_loss)\n",
    "\n",
    "# Load dataset (MNIST for example)\n",
    "(x_train, _), (x_test, _) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_train = x_train.reshape(-1, 28, 28, 1)  # Reshape to 28x28x1\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "x_test = x_test.reshape(-1, 28, 28, 1)  # Reshape to 28x28x1\n",
    "'''\n",
    "# Train the VAE\n",
    "vae.fit(x_train, x_train, epochs=50, batch_size=64, validation_data=(x_test, x_test))\n",
    "\n",
    "# Generate new samples\n",
    "z_sample = tf.random.normal(shape=(64, latent_dim))\n",
    "generated_samples = decoder(z_sample)\n",
    "\n",
    "Encoder: The encoder compresses the input xx into a mean z_{\\text{mean}}z \n",
    "mean\n",
    "â€‹\n",
    "  and a logarithm of the variance z_{\\text{log\\_var}}z \n",
    "log_var\n",
    "â€‹\n",
    "  of the latent space distribution.\n",
    "\n",
    "Sampling: The reparameterization trick is used to sample zz from the latent space. This allows the model to be trained using backpropagation.\n",
    "\n",
    "Decoder: The decoder reconstructs the input xx from the sampled zz.\n",
    "\n",
    "VAE Model: The VAE model ties together the encoder and decoder.\n",
    "\n",
    "Loss Function: The loss function for the VAE combines the reconstruction loss (measuring how well the decoder reconstructs the input) and the Kullback-Leibler (KL) divergence (ensuring the learned latent space distribution is close to a standard normal distribution).\n",
    "\n",
    "Training: The model is trained on the MNIST dataset as an example. '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb960ff3",
   "metadata": {},
   "source": [
    "# Variational Autoencoders (VAEs)\n",
    "\n",
    "## Introduction\n",
    "Variational Autoencoders (VAEs) are a type of generative model that aim to encode data points \\( x \\) into a latent space \\( z \\) and then decode them back to reconstruct \\( x \\). The VAE framework is based on the concept of maximizing the likelihood of the data and minimizing the error between the data distribution and the latent space distribution.\n",
    "\n",
    "## The Variational Lower Bound\n",
    "\n",
    "The goal of the VAE is to maximize the log-likelihood of the data, \\( \\log P(x) \\), which can be derived as follows:\n",
    "\n",
    "$$\n",
    "\\log P(x) - \\text{KL}[Q(z) \\parallel P(z|x)] = \\mathbb{E}_{z \\sim Q} \\log P(x|z) - \\text{KL}[Q(z) \\parallel P(z)] \\quad \\text{(7.16.20)}\n",
    "$$\n",
    "\n",
    "Here, \\( Q(z) \\) is an approximation of the true posterior distribution \\( P(z|x) \\), and KL is the Kullback-Leibler divergence, which measures the difference between two probability distributions.\n",
    "\n",
    "## Inference and Optimization\n",
    "\n",
    "Since we are interested in inferring \\( P(x) \\), it makes sense to construct a distribution \\( Q \\) that depends on \\( x \\) and minimizes the KL divergence between \\( Q(z|x) \\) and \\( P(z|x) \\):\n",
    "\n",
    "$$\n",
    "\\log P(x) - \\text{KL}[Q(z|x) \\parallel P(z|x)] = \\mathbb{E}_{z \\sim Q} \\log P(x|z) - \\text{KL}[Q(z|x) \\parallel P(z)] \\quad \\text{(7.16.21)}\n",
    "$$\n",
    "\n",
    "This equation is the core of the VAE. The left-hand side contains the quantity that should be maximized: \\( \\log P(x) \\) (plus the KL divergence term that ensures \\( Q \\) produces latent variables \\( z \\) that can reconstruct \\( x \\)).\n",
    "\n",
    "The right-hand side is the objective function that we optimize using stochastic gradient descent (SGD), given an appropriate choice of \\( Q \\).\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In conclusion, the distribution \\( Q \\) is responsible for \"encoding\" \\( x \\) into the latent space \\( z \\), while \\( P \\) is responsible for \"decoding\" \\( z \\) to reconstruct \\( x \\).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93157ed5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (60000,28,28,1) and (784,256) not aligned: 1 (dim 3) != 784 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12968/2298949142.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;31m# Example training call (assuming x_train is the training data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbiases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_12968/2298949142.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(x_train, weights, biases, epochs, learning_rate)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;31m# Forward pass through the encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mz_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_log_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbiases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m# Sample z from the latent space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_12968/2298949142.py\u001b[0m in \u001b[0;36mencoder\u001b[0;34m(x, weights, biases)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# Define the encoder network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbiases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mz_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'w1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbiases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0mz_log_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'w2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbiases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mz_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_log_var\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (60000,28,28,1) and (784,256) not aligned: 1 (dim 3) != 784 (dim 0)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Initialize the weights and biases for a fully connected layer\n",
    "def init_weights(input_dim, output_dim):\n",
    "    weights = np.random.randn(input_dim, output_dim) * 0.01\n",
    "    biases = np.zeros((1, output_dim))\n",
    "    return weights, biases\n",
    "\n",
    "# Define the sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Define the encoder network\n",
    "def encoder(x, weights, biases):\n",
    "    z_mean = np.dot(x, weights['w1']) + biases['b1']\n",
    "    z_log_var = np.dot(x, weights['w2']) + biases['b2']\n",
    "    return z_mean, z_log_var\n",
    "\n",
    "# Define the decoder network\n",
    "def decoder(z, weights, biases):\n",
    "    x_recon = sigmoid(np.dot(z, weights['w3']) + biases['b3'])\n",
    "    return x_recon\n",
    "# Initialize the weights and biases for a fully connected layer\n",
    "def init_weights(input_dim, output_dim):\n",
    "    weights = np.random.randn(input_dim, output_dim) * 0.01\n",
    "    biases = np.zeros((1, output_dim))\n",
    "    return weights, biases\n",
    "\n",
    "# Define the sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Define the encoder network\n",
    "def encoder(x, weights, biases):\n",
    "    z_mean = np.dot(x, weights['w1']) + biases['b1']\n",
    "    z_log_var = np.dot(x, weights['w2']) + biases['b2']\n",
    "    return z_mean, z_log_var\n",
    "\n",
    "# Define the decoder network\n",
    "def decoder(z, weights, biases):\n",
    "    x_recon = sigmoid(np.dot(z, weights['w3']) + biases['b3'])\n",
    "    return x_recon\n",
    "input_dim = 784  # For MNIST images (28x28 pixels)\n",
    "latent_dim = 2   # Latent space dimension\n",
    "hidden_dim = 256 # Size of hidden layer\n",
    "\n",
    "# Initialize weights and biases for the encoder and decoder\n",
    "weights = {\n",
    "    'w1': init_weights(input_dim, hidden_dim)[0],\n",
    "    'w2': init_weights(input_dim, hidden_dim)[0],\n",
    "    'w3': init_weights(latent_dim, input_dim)[0]\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': init_weights(input_dim, hidden_dim)[1],\n",
    "    'b2': init_weights(input_dim, hidden_dim)[1],\n",
    "    'b3': init_weights(latent_dim, input_dim)[1]\n",
    "}\n",
    "def compute_loss(x, x_recon, z_mean, z_log_var):\n",
    "    # Reconstruction loss (binary cross-entropy)\n",
    "    recon_loss = -np.sum(x * np.log(x_recon + 1e-9) + (1 - x) * np.log(1 - x_recon + 1e-9))\n",
    "\n",
    "    # KL divergence\n",
    "    kl_loss = -0.5 * np.sum(1 + z_log_var - np.square(z_mean) - np.exp(z_log_var))\n",
    "\n",
    "    return recon_loss + kl_loss\n",
    "def train(x_train, weights, biases, epochs=100, learning_rate=0.001):\n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass through the encoder\n",
    "        z_mean, z_log_var = encoder(x_train, weights, biases)\n",
    "\n",
    "        # Sample z from the latent space\n",
    "        eps = np.random.randn(*z_mean.shape)\n",
    "        z = z_mean + np.exp(z_log_var / 2) * eps\n",
    "\n",
    "        # Forward pass through the decoder\n",
    "        x_recon = decoder(z, weights, biases)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = compute_loss(x_train, x_recon, z_mean, z_log_var)\n",
    "\n",
    "        # Backpropagation (gradient descent)\n",
    "        grad_x_recon = x_recon - x_train\n",
    "        grad_w3 = np.dot(z.T, grad_x_recon)\n",
    "        grad_b3 = np.sum(grad_x_recon, axis=0)\n",
    "\n",
    "        grad_z = np.dot(grad_x_recon, weights['w3'].T)\n",
    "        grad_z_mean = grad_z + (z_mean / np.exp(z_log_var)) * 0.5\n",
    "        grad_z_log_var = grad_z * eps / (2 * np.exp(z_log_var / 2))\n",
    "\n",
    "        grad_w1 = np.dot(x_train.T, grad_z_mean)\n",
    "        grad_b1 = np.sum(grad_z_mean, axis=0)\n",
    "        grad_w2 = np.dot(x_train.T, grad_z_log_var)\n",
    "        grad_b2 = np.sum(grad_z_log_var, axis=0)\n",
    "\n",
    "        # Update weights and biases\n",
    "        weights['w3'] -= learning_rate * grad_w3\n",
    "        biases['b3'] -= learning_rate * grad_b3\n",
    "        weights['w1'] -= learning_rate * grad_w1\n",
    "        biases['b1'] -= learning_rate * grad_b1\n",
    "        weights['w2'] -= learning_rate * grad_w2\n",
    "        biases['b2'] -= learning_rate * grad_b2\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch}, Loss: {loss}')\n",
    "\n",
    "# Example training call (assuming x_train is the training data)\n",
    "train(x_train, weights, biases)\n",
    "##Rectify it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8b71400",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-25 16:53:04.960375: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-25 16:53:29.540227: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-08-25 16:53:29.540267: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-08-25 16:53:31.509304: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-25 16:54:24.232851: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-08-25 16:54:24.260235: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-08-25 16:54:24.260306: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2024-08-25 16:55:42.334720: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2024-08-25 16:55:42.361678: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2024-08-25 16:55:42.361744: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (radha-Vostro-15-3568): /proc/driver/nvidia/version does not exist\n",
      "2024-08-25 16:55:42.417105: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-25 16:55:45.078141: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 188160000 exceeds 10% of free system memory.\n",
      "2024-08-25 16:55:45.253654: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 188160000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.1987"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-25 16:56:04.957333: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 31360000 exceeds 10% of free system memory.\n",
      "2024-08-25 16:56:04.980770: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 31360000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 12s 29ms/step - loss: 0.1986 - val_loss: 0.1215\n",
      "Epoch 2/50\n",
      "235/235 [==============================] - 6s 24ms/step - loss: 0.1096 - val_loss: 0.0997\n",
      "Epoch 3/50\n",
      "235/235 [==============================] - 6s 25ms/step - loss: 0.0960 - val_loss: 0.0911\n",
      "Epoch 4/50\n",
      "235/235 [==============================] - 6s 25ms/step - loss: 0.0900 - val_loss: 0.0885\n",
      "Epoch 5/50\n",
      "235/235 [==============================] - 6s 25ms/step - loss: 0.0863 - val_loss: 0.0837\n",
      "Epoch 6/50\n",
      "235/235 [==============================] - 6s 24ms/step - loss: 0.0836 - val_loss: 0.0819\n",
      "Epoch 7/50\n",
      "235/235 [==============================] - 6s 25ms/step - loss: 0.0818 - val_loss: 0.0809\n",
      "Epoch 8/50\n",
      "235/235 [==============================] - 6s 24ms/step - loss: 0.0804 - val_loss: 0.0790\n",
      "Epoch 9/50\n",
      "235/235 [==============================] - 6s 24ms/step - loss: 0.0794 - val_loss: 0.0784\n",
      "Epoch 10/50\n",
      "235/235 [==============================] - 6s 24ms/step - loss: 0.0785 - val_loss: 0.0775\n",
      "Epoch 11/50\n",
      "235/235 [==============================] - 6s 25ms/step - loss: 0.0778 - val_loss: 0.0777\n",
      "Epoch 12/50\n",
      "235/235 [==============================] - 6s 25ms/step - loss: 0.0773 - val_loss: 0.0764\n",
      "Epoch 13/50\n",
      "235/235 [==============================] - 6s 25ms/step - loss: 0.0767 - val_loss: 0.0762\n",
      "Epoch 14/50\n",
      "235/235 [==============================] - 6s 25ms/step - loss: 0.0763 - val_loss: 0.0754\n",
      "Epoch 15/50\n",
      "235/235 [==============================] - 6s 25ms/step - loss: 0.0758 - val_loss: 0.0751\n",
      "Epoch 16/50\n",
      "235/235 [==============================] - 6s 26ms/step - loss: 0.0755 - val_loss: 0.0746\n",
      "Epoch 17/50\n",
      "235/235 [==============================] - 6s 25ms/step - loss: 0.0751 - val_loss: 0.0748\n",
      "Epoch 18/50\n",
      "235/235 [==============================] - 6s 25ms/step - loss: 0.0748 - val_loss: 0.0743\n",
      "Epoch 19/50\n",
      "235/235 [==============================] - 6s 25ms/step - loss: 0.0745 - val_loss: 0.0742\n",
      "Epoch 20/50\n",
      "235/235 [==============================] - 6s 25ms/step - loss: 0.0743 - val_loss: 0.0739\n",
      "Epoch 21/50\n",
      "235/235 [==============================] - 6s 24ms/step - loss: 0.0741 - val_loss: 0.0735\n",
      "Epoch 22/50\n",
      "235/235 [==============================] - 6s 24ms/step - loss: 0.0739 - val_loss: 0.0735\n",
      "Epoch 23/50\n",
      "235/235 [==============================] - 6s 25ms/step - loss: 0.0737 - val_loss: 0.0733\n",
      "Epoch 24/50\n",
      "235/235 [==============================] - 6s 26ms/step - loss: 0.0735 - val_loss: 0.0731\n",
      "Epoch 25/50\n",
      "235/235 [==============================] - 6s 25ms/step - loss: 0.0733 - val_loss: 0.0730\n",
      "Epoch 26/50\n",
      "235/235 [==============================] - 6s 25ms/step - loss: 0.0732 - val_loss: 0.0727\n",
      "Epoch 27/50\n",
      "235/235 [==============================] - 6s 26ms/step - loss: 0.0731 - val_loss: 0.0727\n",
      "Epoch 28/50\n",
      "235/235 [==============================] - 7s 29ms/step - loss: 0.0729 - val_loss: 0.0725\n",
      "Epoch 29/50\n",
      "235/235 [==============================] - 6s 27ms/step - loss: 0.0728 - val_loss: 0.0725\n",
      "Epoch 30/50\n",
      "235/235 [==============================] - 7s 29ms/step - loss: 0.0727 - val_loss: 0.0724\n",
      "Epoch 31/50\n",
      "235/235 [==============================] - 6s 27ms/step - loss: 0.0726 - val_loss: 0.0723\n",
      "Epoch 32/50\n",
      "235/235 [==============================] - 6s 27ms/step - loss: 0.0724 - val_loss: 0.0722\n",
      "Epoch 33/50\n",
      "235/235 [==============================] - 6s 24ms/step - loss: 0.0723 - val_loss: 0.0721\n",
      "Epoch 34/50\n",
      "235/235 [==============================] - 6s 25ms/step - loss: 0.0723 - val_loss: 0.0722\n",
      "Epoch 35/50\n",
      "235/235 [==============================] - 6s 26ms/step - loss: 0.0722 - val_loss: 0.0720\n",
      "Epoch 36/50\n",
      "235/235 [==============================] - 7s 28ms/step - loss: 0.0720 - val_loss: 0.0718\n",
      "Epoch 37/50\n",
      "235/235 [==============================] - 6s 26ms/step - loss: 0.0720 - val_loss: 0.0720\n",
      "Epoch 38/50\n",
      "235/235 [==============================] - 6s 26ms/step - loss: 0.0719 - val_loss: 0.0717\n",
      "Epoch 39/50\n",
      "235/235 [==============================] - 6s 26ms/step - loss: 0.0718 - val_loss: 0.0716\n",
      "Epoch 40/50\n",
      "235/235 [==============================] - 6s 26ms/step - loss: 0.0718 - val_loss: 0.0715\n",
      "Epoch 41/50\n",
      "235/235 [==============================] - 6s 28ms/step - loss: 0.0717 - val_loss: 0.0716\n",
      "Epoch 42/50\n",
      "235/235 [==============================] - 6s 27ms/step - loss: 0.0716 - val_loss: 0.0716\n",
      "Epoch 43/50\n",
      "235/235 [==============================] - 6s 27ms/step - loss: 0.0716 - val_loss: 0.0713\n",
      "Epoch 44/50\n",
      "235/235 [==============================] - 6s 24ms/step - loss: 0.0715 - val_loss: 0.0713\n",
      "Epoch 45/50\n",
      "235/235 [==============================] - 6s 25ms/step - loss: 0.0715 - val_loss: 0.0713\n",
      "Epoch 46/50\n",
      "235/235 [==============================] - 6s 25ms/step - loss: 0.0714 - val_loss: 0.0713\n",
      "Epoch 47/50\n",
      "235/235 [==============================] - 5s 22ms/step - loss: 0.0714 - val_loss: 0.0713\n",
      "Epoch 48/50\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 0.0713 - val_loss: 0.0712\n",
      "Epoch 49/50\n",
      "235/235 [==============================] - 6s 23ms/step - loss: 0.0713 - val_loss: 0.0711\n",
      "Epoch 50/50\n",
      "235/235 [==============================] - 6s 26ms/step - loss: 0.0713 - val_loss: 0.0712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-25 17:01:18.377175: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 31360000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 3ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2000x400 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "(x_train, _), (x_test, _) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = x_train.reshape((x_train.shape[0], 28 * 28))\n",
    "x_test = x_test.reshape((x_test.shape[0], 28 * 28))\n",
    "\n",
    "# Define the encoder\n",
    "input_img = tf.keras.Input(shape=(784,))\n",
    "encoded = layers.Dense(256, activation='relu')(input_img)\n",
    "z = layers.Dense(64, activation='relu')(encoded)\n",
    "\n",
    "# Define the decoder\n",
    "decoded = layers.Dense(256, activation='relu')(z)\n",
    "output_img = layers.Dense(784, activation='sigmoid')(decoded)\n",
    "\n",
    "# Build the autoencoder model\n",
    "autoencoder = models.Model(input_img, output_img)\n",
    "\n",
    "# Compile the model\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "# Train the model\n",
    "autoencoder.fit(x_train, x_train, epochs=50, batch_size=256, shuffle=True, validation_data=(x_test, x_test))\n",
    "\n",
    "# Test the model\n",
    "decoded_imgs = autoencoder.predict(x_test)\n",
    "\n",
    "# Optionally: visualize the results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 10  # Number of digits to display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # Display original images\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Display reconstructed images\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3e825a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
