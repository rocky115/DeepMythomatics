{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9176cc99",
   "metadata": {},
   "source": [
    "'''\n",
    " * Copyright (c) 2018 Radhamadhab Dalai\n",
    " *\n",
    " * Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    " * of this software and associated documentation files (the \"Software\"), to deal\n",
    " * in the Software without restriction, including without limitation the rights\n",
    " * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    " * copies of the Software, and to permit persons to whom the Software is\n",
    " * furnished to do so, subject to the following conditions:\n",
    " *\n",
    " * The above copyright notice and this permission notice shall be included in\n",
    " * all copies or substantial portions of the Software.\n",
    " *\n",
    " * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    " * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    " * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    " * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    " * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    " * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
    " * THE SOFTWARE.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c91449f",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks (CNNs)\n",
    "\n",
    "Convolutional Neural Networks (CNNs) have been extremely successful in image, speech, audio, and video recognition tasks due to their ability to exploit translational equivariance/invariance with respect to grid structures (in 1, 2, and 3 dimensions) [13].\n",
    "\n",
    "## Gabor Filter Banks\n",
    "\n",
    "Gabor filter banks are a powerful method for feature extraction. The method involves the following steps:\n",
    "1. **Creating Gabor Filters**: A bank of \\( N \\) Gabor filters is created.\n",
    "2. **Convolution**: Each filter is convolved with the input image to produce \\( N \\) different images.\n",
    "3. **Pooling**: Pixels from each image are pooled to extract relevant information.\n",
    "\n",
    "The process mainly consists of two steps: convolution and pooling.\n",
    "\n",
    "## Historical Background\n",
    "\n",
    "- **Neocognitron**: Proposed by Fukushima [32], the Neocognitron is generally seen as a model that inspired CNNs computationally.\n",
    "- **LeNet**: The first convolutional neural network, LeNet, was invented by Le Cun et al. [88] for handwritten digit recognition and further popularized by LeCun et al. [89].\n",
    "\n",
    "## Benefits of CNNs\n",
    "\n",
    "Compared to traditional fully connected neural networks, CNNs offer the advantage of a reduced number of parameters to be learned. The typical layers in a CNN include:\n",
    "\n",
    "1. **Input Layer**: Feeds data into the network. Inputs can be raw data (e.g., image pixels) or their transformations to highlight specific aspects of the data.\n",
    "\n",
    "2. **Convolutional Layers**: Contain a series of filters with fixed sizes used to perform convolutions on the data to generate feature maps.\n",
    "\n",
    "3. **Pooling Layers**: Focus on the most important patterns by reducing the dimensionality of the feature maps used by subsequent layers. Also known as downsampling layers.\n",
    "\n",
    "4. **Rectified Linear Unit (ReLU)**: Applies a nonlinear function to the output \\( x \\) of the previous layer, such as \\( f(x) = \\max(0, x) \\). ReLU layers contribute to faster convergence in training CNNs [84].\n",
    "\n",
    "5. **Fully Connected Layers**: Used for understanding patterns generated by the previous layers. Neurons in these layers are fully connected to all activations in the previous layer and are also called inner product layers. After training, features from these layers can be used in transfer learning to train another classifier.\n",
    "\n",
    "6. **Loss Layers**: Specify how the network training penalizes the deviation between the predicted and true labels. Various loss functions can be used, including Softmax, Sigmoid, Cross-entropy, and Euclidean loss.\n",
    "\n",
    "## Key Component: Convolutional Layers\n",
    "\n",
    "Convolutional layers are the most essential components of CNNs and are discussed first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b13a6138",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-26 19:25:00.758951: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-26 19:25:06.816579: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-07-26 19:25:06.816631: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-07-26 19:25:07.302505: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-26 19:25:18.765854: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-07-26 19:25:18.766182: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-07-26 19:25:18.766217: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2024-07-26 19:25:36.418101: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2024-07-26 19:25:36.508835: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2024-07-26 19:25:36.509029: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (radha-Vostro-15-3568): /proc/driver/nvidia/version does not exist\n",
      "2024-07-26 19:25:36.541432: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 62, 62, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 31, 31, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 29, 29, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 14, 14, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 12, 12, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 6, 6, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4608)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               589952    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 684,490\n",
      "Trainable params: 684,490\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "4/4 [==============================] - 4s 248ms/step - loss: 2.3566 - accuracy: 0.0600\n",
      "Epoch 2/5\n",
      "4/4 [==============================] - 2s 458ms/step - loss: 2.3155 - accuracy: 0.0800\n",
      "Epoch 3/5\n",
      "4/4 [==============================] - 3s 653ms/step - loss: 2.3048 - accuracy: 0.1000\n",
      "Epoch 4/5\n",
      "4/4 [==============================] - 3s 788ms/step - loss: 2.2941 - accuracy: 0.1000\n",
      "Epoch 5/5\n",
      "4/4 [==============================] - 3s 624ms/step - loss: 2.2860 - accuracy: 0.1400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff2d5a4cf10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, ReLU\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Create a simple CNN model\n",
    "model = Sequential([\n",
    "    # Input layer (you can specify input shape for the first layer)\n",
    "    Conv2D(filters=32, kernel_size=(3, 3), input_shape=(64, 64, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    Conv2D(filters=128, kernel_size=(3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10, activation='softmax')  # Assume 10 classes for classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\n",
    "# Dummy data for demonstration\n",
    "import numpy as np\n",
    "X_train = np.random.rand(100, 64, 64, 3)  # 100 samples of 64x64 RGB images\n",
    "y_train = np.random.randint(0, 10, size=(100,))  # 100 target labels\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f0506f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 2.3837\n",
      "Epoch [2/5], Loss: 2.2323\n",
      "Epoch [3/5], Loss: 2.4041\n",
      "Epoch [4/5], Loss: 2.5652\n",
      "Epoch [5/5], Loss: 2.1216\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(128 * 8 * 8, 128)  # Assuming input images are 64x64\n",
    "        self.fc2 = nn.Linear(128, 10)  # Assuming 10 classes for classification\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 128 * 8 * 8)  # Flatten\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "model = SimpleCNN()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Dummy data for demonstration\n",
    "X_train = torch.rand(100, 3, 64, 64)  # 100 samples of 64x64 RGB images\n",
    "y_train = torch.randint(0, 10, (100,))  # 100 target labels\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b9d110",
   "metadata": {},
   "source": [
    "# Hankel Matrix and Convolution\n",
    "### Convolution Operations\n",
    "\n",
    "1. **One-dimensional Convolution (General Forms)**\n",
    "\n",
    "   $$ y_i = (x \\ast f)_i = \\sum_{j=1}^{n} x(j) f(i - j + 1), \\quad i = 1, \\ldots, 2d - 1 $$\n",
    "\n",
    "   $$ y_i = (x \\ast f)_i = \\sum_{j=1}^{d} x(i - j + 1) f(j), \\quad i = 1, \\ldots, 2d - 1 $$\n",
    "\n",
    "   $$ y_i = (x \\ast f)_i = \\sum_{j=1}^{d} x(i + j - 1) f(j), \\quad i = 1, \\ldots, n - d + 1 $$\n",
    "\n",
    "2. **Matrix-Vector Form of Convolution**\n",
    "\n",
    "   $$ y = (x \\ast f) = H(x) f $$\n",
    "\n",
    "   where\n",
    "\n",
    "   $$ H(x) = \\begin{bmatrix}\n",
    "   x(1) & x(2) & \\cdots & x(d) \\\\\n",
    "   x(2) & x(3) & \\cdots & x(d + 1) \\\\\n",
    "   \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "   x(n - d + 1) & x(n - d + 2) & \\cdots & x(n)\n",
    "   \\end{bmatrix} \\in \\mathbb{R}^{(n-d+1) \\times d} $$\n",
    "\n",
    "3. **Wrap-Around Hankel Matrix**\n",
    "\n",
    "   $$ H_d(x) = \\begin{bmatrix}\n",
    "   x(1) & x(2) & \\cdots & x(d) \\\\\n",
    "   x(2) & x(3) & \\cdots & x(d + 1) \\\\\n",
    "   \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "   x(n - d + 1) & x(n - d + 2) & \\cdots & x(n) \\\\\n",
    "   x(n - d + 2) & x(n - d + 3) & \\cdots & x(1) \\\\\n",
    "   \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "   x(n) & x(1) & \\cdots & x(d - 1)\n",
    "   \\end{bmatrix} $$\n",
    "\n",
    "4. **Theorem 7.1 (Rank of Hankel Matrix)**\n",
    "\n",
    "   If \\( r + 1 \\) denotes the minimum length of the annihilating filters that annihilate the signal \\( x \\), then for a given Hankel structured matrix \\( H_d(x) \\in H(n, d) \\) with \\( d > r \\), its rank is given by:\n",
    "\n",
    "   $$ \\text{rank}(H_d(x)) = r $$\n",
    "\n",
    "### Two-Dimensional Convolution\n",
    "\n",
    "1. **Two-Dimensional Convolution (General Forms)**\n",
    "\n",
    "   $$ s(t) = I(a, b) \\ast K(a, b) = \\sum_{a} \\sum_{b} I(a, b) K(m - a, n - b) $$\n",
    "\n",
    "   $$ s(t) = I(a, b) \\ast K(a, b) = \\sum_{a} \\sum_{b} I(m - a, n - b) K(a, b) $$\n",
    "\n",
    "   $$ s(t) = I(a, b) \\ast K(a, b) = \\sum_{a} \\sum_{b} I(m + a, n + b) K(a, b) $$\n",
    "\n",
    "2. **2-D Convolution with Input Image and Filter**\n",
    "\n",
    "   Given a 2-D image \\( X = [x_1, \\ldots, x_p] \\in \\mathbb{R}^{n \\times p} \\) and a 2-D filter \\( \\Phi = [\\phi_1, \\ldots, \\phi_q] \\in \\mathbb{R}^{d \\times q} \\):\n",
    "\n",
    "   $$ (X \\ast \\Phi)_{m,k} = \\sum_{i=1}^{d} \\sum_{j=1}^{q} x_{m+i-1, k+j-1} \\phi_{i,j} $$\n",
    "\n",
    "   Matrix-vector form:\n",
    "\n",
    "   $$ Y = (X \\ast \\Phi) = H_{d,q}(X) \\Phi $$\n",
    "\n",
    "   where\n",
    "\n",
    "   $$ H_{d,q}(X) = \\begin{bmatrix}\n",
    "   H_{d}(x_1) \\phi_1 & \\cdots & H_{d}(x_1) \\phi_q \\\\\n",
    "   \\vdots & \\ddots & \\vdots \\\\\n",
    "   H_{d}(x_p) \\phi_1 & \\cdots & H_{d}(x_p) \\phi_q\n",
    "   \\end{bmatrix} $$\n",
    "\n",
    "### Convolution Types\n",
    "\n",
    "1. **Single-Input Single-Output (SISO) Convolution**\n",
    "\n",
    "   $$ y = x \\ast \\phi = H_d(x) \\phi $$\n",
    "\n",
    "2. **Single-Input Multi-Output (SIMO) Convolution**\n",
    "\n",
    "   $$ Y = x \\ast \\Phi = H_d(x) \\Phi $$\n",
    "\n",
    "   where \\( \\Phi = [\\phi_1, \\ldots, \\phi_q] \\in \\mathbb{R}^{d \\times q} \\).\n",
    "\n",
    "3. **Multi-Input Multi-Output (MIMO) Convolution**\n",
    "\n",
    "   $$ y_i = \\sum_{j=1}^{p} z_j \\ast \\phi_{i,j} $$\n",
    "\n",
    "   Matrix-vector form:\n",
    "\n",
    "   $$ Y = \\sum_{j=1}^{p} H_d(z_j) \\phi_j = H_{d|p}(Z) \\Phi $$\n",
    "\n",
    "4. **Multi-Input Single-Output (MISO) Convolution**\n",
    "\n",
    "   $$ y = H_{d|p}(Z) \\phi $$\n",
    "\n",
    "   where \\( \\phi = [\\phi_1, \\ldots, \\phi_p]^T \\).\n",
    "\n",
    "### Block Hankel Matrix for 2-D Convolution\n",
    "\n",
    "1. **Block Hankel Matrix**\n",
    "\n",
    "   $$ H_{d_1, d_2}(X) = \\begin{bmatrix}\n",
    "   H_{d_1}(x_1) & H_{d_1}(x_2) & \\cdots & H_{d_1}(x_{d_2}) \\\\\n",
    "   \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "   H_{d_1}(x_{n_2}) & H_{d_1}(x_1) & \\cdots & H_{d_1}(x_{d_2 - 1})\n",
    "   \\end{bmatrix} $$\n",
    "\n",
    "2. **2-D Convolution Matrix-Vector Form**\n",
    "\n",
    "   $$ \\text{vec}(Y) = H_{d_1, d_2}(X) \\text{vec}(K) $$\n",
    "\n",
    "3. **Extended Block Hankel Matrix for Multi-Channel 2-D Convolution**\n",
    "\n",
    "   $$ H_{d_1, d_2|p}(X^{(1)}, \\ldots, X^{(p)}) = \\begin{bmatrix}\n",
    "   H_{d_1, d_2}(X^{(1)}) & \\cdots & H_{d_1, d_2}(X^{(p)})\n",
    "   \\end{bmatrix} $$\n",
    "\n",
    "   2-D MIMO Convolution:\n",
    "\n",
    "   $$ Y = H_{d_1, d_2|p}(X^{(1)}, \\ldots, X^{(p)}) K $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e273d2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1D Convolution result: [1.8 2.7 3.6]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import convolve\n",
    "\n",
    "# Function to compute 1D convolution using a Hankel matrix\n",
    "def hankel_matrix_1d(x, d):\n",
    "    n = len(x)\n",
    "    H = np.zeros((n - d + 1, d))\n",
    "    for i in range(n - d + 1):\n",
    "        H[i, :] = x[i:i + d]\n",
    "    return H\n",
    "\n",
    "def convolve_1d(x, f):\n",
    "    d = len(f)\n",
    "    H = hankel_matrix_1d(x, d)\n",
    "    y = H @ f\n",
    "    return y\n",
    "\n",
    "# Example usage\n",
    "x = np.array([1, 2, 3, 4, 5])\n",
    "f = np.array([0.2, 0.5, 0.2])\n",
    "y = convolve_1d(x, f)\n",
    "print(\"1D Convolution result:\", y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "460c55f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2D Convolution result:\n",
      " [[4 4]\n",
      " [4 4]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import convolve2d\n",
    "\n",
    "# Function to compute 2D convolution\n",
    "def convolve_2d(image, kernel):\n",
    "    return convolve2d(image, kernel, mode='valid')\n",
    "\n",
    "# Example usage\n",
    "image = np.array([[1, 2, 3],\n",
    "                  [4, 5, 6],\n",
    "                  [7, 8, 9]])\n",
    "kernel = np.array([[1, 0],\n",
    "                   [0, -1]])\n",
    "result = convolve_2d(image, kernel)\n",
    "print(\"2D Convolution result:\\n\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69b8f66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2D Hankel Matrix:\n",
      " [[ 1.  2.  5.  6.]\n",
      " [ 2.  3.  6.  7.]\n",
      " [ 3.  4.  7.  8.]\n",
      " [ 5.  6.  9. 10.]\n",
      " [ 6.  7. 10. 11.]\n",
      " [ 7.  8. 11. 12.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def hankel_matrix_2d(X, d1, d2):\n",
    "    n1, n2 = X.shape\n",
    "    H = np.zeros(((n1 - d1 + 1) * (n2 - d2 + 1), d1 * d2))\n",
    "    for i in range(n1 - d1 + 1):\n",
    "        for j in range(n2 - d2 + 1):\n",
    "            H[i * (n2 - d2 + 1) + j, :] = X[i:i + d1, j:j + d2].flatten()\n",
    "    return H\n",
    "\n",
    "# Example usage\n",
    "X = np.array([[1, 2, 3, 4],\n",
    "              [5, 6, 7, 8],\n",
    "              [9, 10, 11, 12]])\n",
    "d1, d2 = 2, 2\n",
    "H = hankel_matrix_2d(X, d1, d2)\n",
    "print(\"2D Hankel Matrix:\\n\", H)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53711e80",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (3,1) (8,3) (3,1) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7570/2080131625.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHankelConvolutionalModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_7570/2080131625.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, y, epochs, learning_rate)\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m                 \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(X)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_7570/2080131625.py\u001b[0m in \u001b[0;36mupdate_weights\u001b[0;34m(self, grad, learning_rate)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (3,1) (8,3) (3,1) "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate synthetic data\n",
    "def generate_synthetic_data(n_samples, n_features, n_targets):\n",
    "    X = np.random.randn(n_samples, n_features)\n",
    "    y = np.random.randn(n_samples, n_targets)\n",
    "    return X, y\n",
    "\n",
    "# Example usage\n",
    "n_samples = 100\n",
    "n_features = 10\n",
    "n_targets = 1\n",
    "X_train, y_train = generate_synthetic_data(n_samples, n_features, n_targets)\n",
    "import numpy as np\n",
    "\n",
    "class HankelConvolutionalModel:\n",
    "    def __init__(self, d):\n",
    "        self.d = d\n",
    "        self.W = np.random.randn(d, 1)  # Random initial weights\n",
    "\n",
    "    def hankel_matrix(self, x):\n",
    "        n = len(x)\n",
    "        H = np.zeros((n - self.d + 1, self.d))\n",
    "        for i in range(n - self.d + 1):\n",
    "            H[i, :] = x[i:i + self.d]\n",
    "        return H\n",
    "\n",
    "    def forward(self, x):\n",
    "        H = self.hankel_matrix(x)\n",
    "        return H @ self.W\n",
    "\n",
    "    def compute_loss(self, y_pred, y_true):\n",
    "        return np.mean((y_pred - y_true) ** 2)\n",
    "\n",
    "    def gradient(self, x, y_true):\n",
    "        y_pred = self.forward(x)\n",
    "        loss = self.compute_loss(y_pred, y_true)\n",
    "        grad = 2 * np.mean((y_pred - y_true)[:, np.newaxis] * self.hankel_matrix(x), axis=0)\n",
    "        return grad, loss\n",
    "\n",
    "    def update_weights(self, grad, learning_rate):\n",
    "        self.W -= learning_rate * grad\n",
    "\n",
    "    def train(self, X, y, epochs, learning_rate):\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for x, y_true in zip(X, y):\n",
    "                grad, loss = self.gradient(x, y_true)\n",
    "                self.update_weights(grad, learning_rate)\n",
    "                total_loss += loss\n",
    "            print(f'Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(X)}')\n",
    "\n",
    "# Example usage\n",
    "d = 3\n",
    "model = HankelConvolutionalModel(d)\n",
    "model.train(X_train, y_train, epochs=10, learning_rate=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2e5c299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.183805478706877\n",
      "Epoch 2/10, Loss: 1.0770005872878148\n",
      "Epoch 3/10, Loss: 1.0770005876594309\n",
      "Epoch 4/10, Loss: 1.0770005876594309\n",
      "Epoch 5/10, Loss: 1.0770005876594309\n",
      "Epoch 6/10, Loss: 1.0770005876594309\n",
      "Epoch 7/10, Loss: 1.0770005876594309\n",
      "Epoch 8/10, Loss: 1.0770005876594309\n",
      "Epoch 9/10, Loss: 1.0770005876594309\n",
      "Epoch 10/10, Loss: 1.0770005876594309\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class HankelConvolutionalModel:\n",
    "    def __init__(self, d):\n",
    "        self.d = d\n",
    "        self.W = np.random.randn(d, 1)  # Initialize weights with shape (d, 1)\n",
    "\n",
    "    def hankel_matrix(self, x):\n",
    "        n = len(x)\n",
    "        H = np.zeros((n - self.d + 1, self.d))\n",
    "        for i in range(n - self.d + 1):\n",
    "            H[i, :] = x[i:i + self.d]\n",
    "        return H\n",
    "\n",
    "    def forward(self, x):\n",
    "        H = self.hankel_matrix(x)\n",
    "        return H @ self.W  # @ is matrix multiplication\n",
    "\n",
    "    def compute_loss(self, y_pred, y_true):\n",
    "        return np.mean((y_pred - y_true) ** 2)\n",
    "\n",
    "    def gradient(self, x, y_true):\n",
    "        y_pred = self.forward(x)\n",
    "        loss = self.compute_loss(y_pred, y_true)\n",
    "        H = self.hankel_matrix(x)\n",
    "        grad = 2 * H.T @ (H @ self.W - y_true[:, np.newaxis]) / len(y_true)\n",
    "        return grad, loss\n",
    "\n",
    "    def update_weights(self, grad, learning_rate):\n",
    "        self.W -= learning_rate * grad\n",
    "\n",
    "    def train(self, X, y, epochs, learning_rate):\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for x, y_true in zip(X, y):\n",
    "                grad, loss = self.gradient(x, y_true)\n",
    "                self.update_weights(grad, learning_rate)\n",
    "                total_loss += loss\n",
    "            print(f'Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(X)}')\n",
    "\n",
    "# Example usage\n",
    "d = 3\n",
    "model = HankelConvolutionalModel(d)\n",
    "\n",
    "# Generate synthetic data\n",
    "def generate_synthetic_data(n_samples, n_features, n_targets):\n",
    "    X = np.random.randn(n_samples, n_features)\n",
    "    y = np.random.randn(n_samples, n_targets)\n",
    "    return X, y\n",
    "\n",
    "n_samples = 100\n",
    "n_features = 10\n",
    "n_targets = 1\n",
    "X_train, y_train = generate_synthetic_data(n_samples, n_features, n_targets)\n",
    "\n",
    "# Train the model\n",
    "model.train(X_train, y_train, epochs=10, learning_rate=0.01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1eef03",
   "metadata": {},
   "source": [
    "Pooling is one of the most important operations in CNN. A pooling operator\n",
    "operates on an individual feature channel, aggregating data of a local region (e.g.,\n",
    "a rectangle) and transforming them into one single value. Common choices include\n",
    "max pooling (using the maximum operator) and average pooling (using the average\n",
    "operator), both of which are hand-crafted. Traditional convolution with sliding strides larger than one pixel can also be regarded as a pooling operation. However,\n",
    "this kind of pooling operation does not deal with each input feature channel\n",
    "independently. Instead, it uses all the input feature channels to generate each output\n",
    "feature channel. Therefore, the convolutional pooling operation consumes many\n",
    "extra parameters.\n",
    "On the pooling operation, the following points are to be noted:\n",
    "\n",
    "1. Pooling operates over a portion of the input and applies a function \\( f \\) over this input to produce the output.\n",
    "2. The function \\( f \\) is commonly the max operation (leading to max pooling), but other variants such as average or \\(\\ell_2\\) norm can be used as an alternative.\n",
    "3. For a two-dimensional input, this is a rectangular portion.\n",
    "4. The output produced as a result of pooling is much smaller in dimensionality as compared to the input.\n",
    "\n",
    "Let \\( w^l_k \\) and \\( b^k_l \\) be the weight vector and bias term of the \\(k\\)-th filter of the \\(l\\)-th layer, respectively, and \\( x^l_{i,j} \\) be the input patch centered at location \\((i, j)\\) of the \\(l\\)-th layer. Then, the feature value at location \\((i, j)\\) in the \\(k\\)-th feature map of \\(l\\)-th layer, \\( z^l_{i,j,k} \\), is calculated by\n",
    "\n",
    "$$\n",
    "z^l_{i,j,k} = \\left( w^l_k \\right)^T x^l_{i,j} + b^k_l.\n",
    "$$\n",
    "\n",
    "The activation value \\( a^l_{i,j,k} \\) of convolutional feature \\( z^l_{i,j,k} \\) can be computed as\n",
    "\n",
    "$$\n",
    "a^l_{i,j,k} = a\\left( z^l_{i,j,k} \\right),\n",
    "$$\n",
    "\n",
    "where \\( a(\\cdot) \\) denotes an activation function.\n",
    "\n",
    "The pooling layer aims to achieve shift-invariance by reducing the resolution of the feature maps. It is usually placed between two convolutional layers. Each feature map of a pooling layer is connected to its corresponding feature map of the preceding convolutional layer. Denoting the pooling function as \\(\\text{pool}(\\cdot)\\) for each feature map \\( a^l_{i,j,k} \\), then\n",
    "\n",
    "$$\n",
    "y^l_{i,j,k} = \\text{pool}\\left( a^l_{m,n,k} \\right), \\quad \\forall m, n \\in R_{ij},\n",
    "$$\n",
    "\n",
    "where \\( R_{ij} \\) is a local neighborhood around location \\((i, j)\\).\n",
    "\n",
    "The following are some pooling methods used in CNNs:\n",
    "\n",
    "1. \\(\\ell_p\\) pooling: This pooling is a biologically inspired pooling process modeled on complex cells. The summary statistic in \\(\\ell_p\\) pooling is the \\(\\ell_p\\) norm of the inputs into the pool. That is, if nodes \\((i, j)\\) are in a pool \\(k\\), then the output of the pool is\n",
    "\n",
    "$$\n",
    "y^k_{i,j} = \\left( \\sum_{(m,n) \\in R_{ij}} \\left| a^k_{m,n} \\right|^p \\right)^{1/p},\n",
    "$$\n",
    "\n",
    "where \\( a^k_{m,n} \\) is the feature value at location \\((m, n)\\) within the pooling region \\( R_{ij} \\) in the \\(k\\)-th feature map. The \\(\\ell_p\\) pooling contains the following two conventional choices:\n",
    "    - When \\( p = 1 \\), the \\(\\ell_1\\) pooling gives the sum pooling:\n",
    "    $$\n",
    "    y^k_{i,j} = \\sum_{(m,n) \\in R_{ij}} a^k_{m,n}.\n",
    "    $$\n",
    "    In sum pooling, all elements in a pooling region are considered. When combined with linear rectification nonlinearities, strong activations may be down-weighted since many zero elements are included in the sum. Even worse, with \\(\\tanh(\\cdot)\\) nonlinearities, strong positive and negative activations can cancel each other out, leading to small pooled responses.\n",
    "    - When \\( p \\to \\infty \\), \\(\\ell_p\\) pooling corresponds to the max pooling:\n",
    "    $$\n",
    "    y^k_{i,j} = \\max_{(m,n) \\in R_{ij}} \\left| a^k_{m,n} \\right| \\cdot \\text{sign} \\left( \\arg \\max_{(m,n) \\in R_{ij}} \\left| a^k_{m,n} \\right| \\right).\n",
    "    $$\n",
    "\n",
    "A common form of downsampling is max pooling where the stride size of the pooling layer equals the pooling size. Max pooling serves to reduce the sizes of the feature vectors and the parameters of CNNs, which decreases training time and memory requirements, but this pooling easily overfits the training set in practice, making it hard to generalize well to test examples.\n",
    "\n",
    "2. Mixed pooling: This pooling method is the combination of a max pooling and an average pooling:\n",
    "    $$\n",
    "    y^k_{i,j} = \\lambda \\max_{(m,n) \\in R_{ij}} a^k_{m,n} + (1 - \\lambda) \\frac{1}{|R_{ij}|} \\sum_{(m,n) \\in R_{ij}} a^k_{m,n},\n",
    "    $$\n",
    "    where \\(\\lambda\\) is a random value being either 0 or 1 which indicates the choice of either using average pooling or max pooling. During forward propagation process, \\(\\lambda\\) is recorded and will be used for the backpropagation operation.\n",
    "\n",
    "3. Stochastic pooling: It first computes the probabilities \\( p \\) for each region \\((i, j)\\) by normalizing the activations within the region:\n",
    "    $$\n",
    "    p_j = \\frac{a_i}{\\sum_{i \\in R_j} a_i},\n",
    "    $$\n",
    "    then samples from the multinomial distribution based on \\( p \\) to pick a location \\( j \\) within the region. The pooled activation is then simply\n",
    "    $$\n",
    "    y_j = a_l \\quad \\text{where} \\quad l \\sim P(p_1, \\ldots, p_{|R_j|}).\n",
    "    $$\n",
    "\n",
    "Max pooling only captures the strongest activation of the filter template with the input for each region. However, there may be additional activations in the same pooling region that should be taken into account when passing information up the network and stochastic pooling ensures that these non-maximal activations will also be utilized. Unlike the average pooling and the max pooling representing only one-modal distribution of activations within a region, the stochastic pooling, via selecting different \\( l \\), can represent multi-modal distributions of activations within a region. Compared with max pooling, stochastic pooling can avoid overfitting due to the stochastic component.\n",
    "\n",
    "4. Spectral pooling: The idea behind spectral pooling, presented by Rippel et al. in 2015, stems from the observation that the frequency domain provides an ideal basis for inputs with spatial structure. Suppose we are given an input \\( x \\in \\mathbb{R}^{M \\times N} \\), and some desired output map dimensionality \\( H \\times W \\). First, compute the discrete Fourier transform (DFT) of the input into the frequency domain as \\( y = F(x) \\in \\mathbb{C}^{M \\times N} \\), and assume that the direct current component has been shifted to the center of the domain as is standard practice. Then, truncate \\( y \\) to \\( \\hat{y} \\in \\mathbb{C}^{H \\times W} \\). Finally, return its inverse DFT as \\( \\hat{x} = F^{-1}(\\hat{y}) \\in \\mathbb{R}^{H \\times W} \\). The main advantages of spectral pooling are fast convolution computations, high parameter information retention (compression), flexibility in pooling output dimensions, and further computation savings by implementing spectral parameterization.\n",
    "\n",
    "5. Spatial pyramid pooling (SPP): The last pooling layer (e.g., after the last convolutional layer) is replaced with a spatial pyramid pooling layer. In each spatial bin with the number \\( M \\) of bins, the SPP using some pooling method (e.g., max pooling) generates a fixed-length representation regardless of the input sizes, resulting in the \\( kM \\)-dimensional output vector of the spatial pyramid pooling (\\( k \\) is the number of filters in the last convolutional layer). Then, fixed-dimensional vectors are the input to the fully connected layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81327104",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-29 16:21:50.990303: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-29 16:21:55.553553: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-07-29 16:21:55.553592: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-07-29 16:21:56.226270: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-29 16:22:07.126649: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-07-29 16:22:07.126846: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-07-29 16:22:07.126863: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2024-07-29 16:22:27.999927: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2024-07-29 16:22:28.012644: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2024-07-29 16:22:28.012755: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (radha-Vostro-15-3568): /proc/driver/nvidia/version does not exist\n",
      "2024-07-29 16:22:28.023770: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-29 16:22:28.935859: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 188160000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.1425 - accuracy: 0.9557"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-29 16:23:38.509133: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 31360000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 73s 37ms/step - loss: 0.1425 - accuracy: 0.9557 - val_loss: 0.0432 - val_accuracy: 0.9858\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 80s 43ms/step - loss: 0.0463 - accuracy: 0.9855 - val_loss: 0.0458 - val_accuracy: 0.9857\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 68s 36ms/step - loss: 0.0339 - accuracy: 0.9892 - val_loss: 0.0325 - val_accuracy: 0.9892\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 64s 34ms/step - loss: 0.0247 - accuracy: 0.9920 - val_loss: 0.0320 - val_accuracy: 0.9900\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 75s 40ms/step - loss: 0.0208 - accuracy: 0.9933 - val_loss: 0.0384 - val_accuracy: 0.9872\n",
      "  1/313 [..............................] - ETA: 24s - loss: 0.0013 - accuracy: 1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-29 16:28:29.641883: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 31360000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 5s 14ms/step - loss: 0.0384 - accuracy: 0.9872\n",
      "Test accuracy: 0.9872000217437744\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "train_images = train_images.reshape((60000, 28, 28, 1)).astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255\n",
    "\n",
    "# Define the CNN model\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_images, train_labels, epochs=5, \n",
    "                    validation_data=(test_images, test_labels))\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(f'Test accuracy: {test_acc}')\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efd05c4",
   "metadata": {},
   "source": [
    "## Activation Functions in CNNs\n",
    "\n",
    "### 1. Rectified Linear Unit (ReLU)\n",
    "The standard way to model a neuron’s output \\( f \\) as a function of its input \\( x \\) is to use \\( f(x) = \\tanh(x) \\) or \\( f(x) = (1 + e^{-x})^{-1} \\). However, these saturating nonlinear activation functions are much slower than the non-saturating nonlinear activation function \\( f(x) = \\max(0, x) \\) in gradient descent algorithm. Following Nair and Hinton [106], the neurons with the nonlinearity \\( f(x) = \\max(0, x) \\) are known as rectified linear units (ReLUs). The ReLU is one of the most notable non-saturated activation functions and is defined as:\n",
    "\n",
    "$$\n",
    "a_{i,j,k} = \\max(z_{i,j,k}, 0)\n",
    "$$\n",
    "\n",
    "where \\( z_{i,j,k} \\) is the input of the activation function at location \\((i, j)\\) on the \\(k\\)-th channel. A potential disadvantage of the ReLU unit is that it has zero gradient whenever the unit is not active, which may cause units that are not active initially to never become active since the gradient-based optimization will not adjust their weights. Moreover, it may slow down the training process due to the constant zero gradients.\n",
    "\n",
    "### 2. Noisy Rectified Linear Unit (NReLU) [106]\n",
    "\n",
    "$$\n",
    "a_{i,j,k} = \\max(z_{i,j,k} + N(0, \\sigma^2), 0)\n",
    "$$\n",
    "\n",
    "where \\( N(0, \\sigma^2) \\) is a Gaussian noise with zero mean and variance \\( \\sigma^2 \\). It was shown [106] that NReLUs work better than binary hidden units for recognizing objects and comparing faces, and can deal with large intensity variations much more naturally than binary units.\n",
    "\n",
    "### 3. Leaky Rectified Linear Unit (Leaky ReLU) [102]\n",
    "\n",
    "$$\n",
    "a_{i,j,k} = \\max(z_{i,j,k}, 0) + \\lambda \\min(z_{i,j,k}, 0)\n",
    "$$\n",
    "\n",
    "where \\( \\lambda \\) is a predefined parameter in range (0, 1). As compared with ReLU, Leaky ReLU compresses the negative part to a small, nonzero gradient when the unit is not active.\n",
    "\n",
    "### 4. Parametric Rectified Linear Unit (PReLU) [55]\n",
    "\n",
    "$$\n",
    "a_{i,j,k} = \\max(z_{i,j,k}, 0) + \\lambda \\min(z_{i,j,k}, 0)\n",
    "$$\n",
    "\n",
    "### 5. Randomized Rectified Linear Unit (RReLU) [164]\n",
    "\n",
    "$$\n",
    "a_{i,j,k}^{(n)} = \\max(z_{i,j,k}^{(n)}, 0) + \\lambda_k^{(n)} \\min(z_{i,j,k}^{(n)}, 0)\n",
    "$$\n",
    "\n",
    "where \\( z_{i,j,k}^{(n)} \\) denotes the input of activation function at location \\((i, j)\\) on the \\(k\\)-th channel of the \\(n\\)-th example, \\( \\lambda_k^{(n)} \\) denotes its corresponding sampled parameter, and \\( a_{i,j,k} \\) denotes its corresponding output.\n",
    "\n",
    "### 6. Exponential Linear Unit (ELU) [16]\n",
    "\n",
    "$$\n",
    "a_{i,j,k} = \\max(z_{i,j,k}, 0) + \\min(\\lambda (e^{z_{i,j,k}} - 1), 0)\n",
    "$$\n",
    "\n",
    "where \\( \\lambda \\) is a predefined parameter for controlling the value to which an ELU saturates for negative inputs.\n",
    "\n",
    "### 7. Maxout [37]\n",
    "\n",
    "$$\n",
    "a_{i,j,k} = \\max_{k \\in [1, K]} z_{i,j,k}\n",
    "$$\n",
    "\n",
    "where \\( z_{i,j,k} \\) is the \\(k\\)-th channel of the feature map. Maxout enjoys all the benefits of ReLU since ReLU is actually a special case of maxout, e.g., \\( \\max\\{w_1^T x + b_1, w_2^T x + b_2\\} \\), where \\( w_1 \\) is a zero vector and \\( b_1 \\) is zero. Besides, maxout is particularly well suited for training with Dropout.\n",
    "\n",
    "### 8. Probout [141]\n",
    "Probout is a probabilistic variant of maxout, and first defines a probability for each of \\( k \\) linear units as:\n",
    "\n",
    "$$\n",
    "\\hat{p}_0 = 0.5\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{p}_i = \\frac{e^{\\lambda z_i}}{\\sum_{j=1}^{k} e^{\\lambda z_j}}\n",
    "$$\n",
    "\n",
    "The activation function is then sampled as:\n",
    "\n",
    "$$\n",
    "a_i = \n",
    "\\begin{cases}\n",
    "0, & \\text{if } i = 0; \\\\\n",
    "z_i, & \\text{otherwise}.\n",
    "\\end{cases}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01eccfb6",
   "metadata": {},
   "source": [
    "## Commonly Used Activation Functions in CNNs\n",
    "\n",
    "1. **Rectified Linear Unit (ReLU)**: The ReLU function is defined as:\n",
    "   $$\n",
    "   f(x) = \\max(0, x)\n",
    "   $$\n",
    "   It is preferred due to its non-saturating property which speeds up the convergence of gradient descent.\n",
    "\n",
    "2. **Noisy Rectified Linear Unit (NReLU)**: The NReLU function introduces Gaussian noise:\n",
    "   $$ \n",
    "   a_{i,j,k} = \\max(z_{i,j,k} + N(0, \\sigma^2), 0)\n",
    "   $$\n",
    "   where \\( N(0, \\sigma^2) \\) is Gaussian noise with zero mean and variance \\( \\sigma^2 \\).\n",
    "\n",
    "3. **Leaky Rectified Linear Unit (Leaky ReLU)**: The Leaky ReLU allows a small, non-zero gradient when the unit is not active:\n",
    "   $$\n",
    "   a_{i,j,k} = \\max(z_{i,j,k}, 0) + \\lambda \\min(z_{i,j,k}, 0)\n",
    "   $$\n",
    "   where \\( \\lambda \\) is a small constant.\n",
    "\n",
    "4. **Parametric Rectified Linear Unit (PReLU)**: Similar to Leaky ReLU, but with a learnable parameter:\n",
    "   $$\n",
    "   a_{i,j,k} = \\max(z_{i,j,k}, 0) + \\alpha \\min(z_{i,j,k}, 0)\n",
    "   $$\n",
    "   where \\( \\alpha \\) is a learnable parameter.\n",
    "\n",
    "5. **Randomized Rectified Linear Unit (RReLU)**: Adds a randomized parameter:\n",
    "   $$\n",
    "   a_{i,j,k} = \\max(z_{i,j,k}, 0) + \\lambda_k \\min(z_{i,j,k}, 0)\n",
    "   $$\n",
    "   where \\( \\lambda_k \\) is randomly sampled.\n",
    "\n",
    "6. **Exponential Linear Unit (ELU)**: The ELU function adds an exponential component for negative inputs:\n",
    "   $$\n",
    "   a_{i,j,k} = \\max(z_{i,j,k}, 0) + \\min(\\lambda (e^{z_{i,j,k}} - 1), 0)\n",
    "   $$\n",
    "   where \\( \\lambda \\) controls the value to which ELU saturates for negative inputs.\n",
    "\n",
    "7. **Maxout**: The Maxout function takes the maximum of multiple linear functions:\n",
    "   $$\n",
    "   a_{i,j,k} = \\max_{k \\in [1, K]} (z_{i,j,k})\n",
    "   $$\n",
    "   where \\( z_{i,j,k} \\) is the \\( k \\)-th channel of the feature map.\n",
    "\n",
    "8. **Probout**: A probabilistic variant of Maxout:\n",
    "   $$\n",
    "   p_j = \\frac{e^{\\lambda z_j}}{\\sum_{k=1}^K e^{\\lambda z_k}}\n",
    "   $$\n",
    "   The activation function is then sampled as:\n",
    "   $$\n",
    "   a = \\begin{cases} \n",
    "   0, & \\text{if } j = 0; \\\\\n",
    "   z_j, & \\text{otherwise}.\n",
    "   \\end{cases}\n",
    "   $$\n",
    "\n",
    "## Loss Functions in CNNs\n",
    "\n",
    "1. **Hinge Loss**: Used for large-margin classifiers:\n",
    "   $$\n",
    "   L_{\\text{hinge}} = \\frac{1}{N} \\sum_{i=1}^N \\sum_{j=1}^K \\max(0, 1 - \\delta(y^{(i)}, j) w^T x^{(i)})\n",
    "   $$\n",
    "   where \\( \\delta(y^{(i)}, j) = 1 \\) if \\( y^{(i)} = j \\), and \\(-1\\) otherwise.\n",
    "\n",
    "2. **Softmax Loss**: Converts logits to probabilities and computes cross-entropy:\n",
    "   $$\n",
    "   p_j^{(i)} = \\frac{e^{z_j^{(i)}}}{\\sum_{l=1}^K e^{z_l^{(i)}}}\n",
    "   $$\n",
    "   The softmax loss is:\n",
    "   $$\n",
    "   L_{\\text{softmax}} = -\\frac{1}{N} \\sum_{i=1}^N \\sum_{j=1}^K y_j^{(i)} \\log(p_j^{(i)})\n",
    "   $$\n",
    "\n",
    "3. **Contrastive Loss**: Measures the similarity between pairs:\n",
    "   $$\n",
    "   L_{\\text{contrastive}} = \\frac{1}{2N} \\sum_{i=1}^N \\left((1 - y) d^2 + y \\max(m - d, 0)^2\\right)\n",
    "   $$\n",
    "   where \\( d = \\| z_1 - z_2 \\|_2 \\) and \\( m \\) is the margin.\n",
    "\n",
    "4. **Triplet Loss**: Used to separate positive and negative pairs:\n",
    "   $$\n",
    "   L_{\\text{triplet}} = \\frac{1}{N} \\sum_{i=1}^N \\max(d(a,p) - d(a,n) + m, 0)\n",
    "   $$\n",
    "   where \\( d(a,p) = \\| z_a - z_p \\|_2^2 \\) and \\( d(a,n) = \\| z_a - z_n \\|_2^2 \\).\n",
    "\n",
    "5. **Large-Margin Softmax Loss**: Adds an angular margin:\n",
    "   $$\n",
    "   p_j^{(i)} = \\frac{e^{\\| w_j \\| \\| a^{(i)} \\| \\psi(\\theta_j)}}{\\sum_{l \\neq j} e^{\\| w_l \\| \\| a^{(i)} \\| \\cos(\\theta_l)}}\n",
    "   $$\n",
    "   where:\n",
    "   $$\n",
    "   \\psi(\\theta_j) = (-1)^k \\cos(m\\theta_j) - 2k, \\quad \\theta_j \\in \\left[\\frac{k\\pi}{m}, \\frac{(k+1)\\pi}{m}\\right]\n",
    "   $$\n",
    "\n",
    "6. **Kullback–Leibler (KL) Divergence**: Measures the difference between two probability distributions:\n",
    "   $$\n",
    "   \\text{KL}(p \\| q) = \\sum_{x} p(x) \\log \\left( \\frac{p(x)}{q(x)} \\right)\n",
    "   $$\n",
    "\n",
    "7. **Jensen-Shannon (JS) Divergence**: A symmetrical form of KL divergence:\n",
    "   $$\n",
    "   D_{\\text{JS}}(p \\| q) = \\frac{1}{2} \\left( \\text{KL}\\left(p \\| \\frac{p+q}{2}\\right) + \\text{KL}\\left(q \\| \\frac{p+q}{2}\\right) \\right)\n",
    "   $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12e1b698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hinge Loss: tf.Tensor(1.0024267, shape=(), dtype=float32)\n",
      "Softmax Loss: tf.Tensor(12.892591, shape=(), dtype=float32)\n",
      "Large Margin Softmax Loss: tf.Tensor(11.497324, shape=(), dtype=float32)\n",
      "KL Divergence: tf.Tensor(54.65155, shape=(), dtype=float32)\n",
      "JS Divergence: tf.Tensor(17.935743, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Hinge loss\n",
    "class HingeLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, p=1):\n",
    "        super(HingeLoss, self).__init__()\n",
    "        self.p = p\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        # Ensure y_true is of type float32\n",
    "        y_true = tf.cast(y_true, dtype=tf.float32)\n",
    "        delta = 1 - y_true * y_pred\n",
    "        hinge_loss = tf.maximum(0.0, delta) ** self.p\n",
    "        return tf.reduce_mean(hinge_loss)\n",
    "\n",
    "# Softmax loss\n",
    "class SoftmaxLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self):\n",
    "        super(SoftmaxLoss, self).__init__()\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        # Ensure y_true is of type float32 for compatibility with tf.nn.softmax_cross_entropy_with_logits\n",
    "        y_true = tf.cast(y_true, dtype=tf.float32)\n",
    "        softmax_loss = tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=y_pred)\n",
    "        return tf.reduce_mean(softmax_loss)\n",
    "\n",
    "# Contrastive loss\n",
    "# class ContrastiveLoss(tf.keras.losses.Loss):\n",
    "#     def __init__(self, margin=1.0):\n",
    "#         super(ContrastiveLoss, self).__init__()\n",
    "#         self.margin = margin\n",
    "\n",
    "#     def call(self, y_true, y_pred):\n",
    "#         # Ensure y_true is of the same dtype as y_pred\n",
    "#         y_true = tf.cast(y_true, dtype=y_pred.dtype)\n",
    "#         d = tf.reduce_sum(tf.square(y_pred[:, 0] - y_pred[:, 1]), axis=1)\n",
    "#         contrastive_loss = y_true * d + (1 - y_true) * tf.maximum(self.margin - d, 0)\n",
    "#         return tf.reduce_mean(contrastive_loss)\n",
    "class ContrastiveLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        # Ensure y_true is of type float32\n",
    "        y_true = tf.cast(y_true, dtype=tf.float32)\n",
    "        \n",
    "        # Split y_pred into two parts\n",
    "        y_pred_1, y_pred_2 = tf.split(y_pred, num_or_size_splits=2, axis=1)\n",
    "        \n",
    "        # Compute the squared Euclidean distance\n",
    "        d = tf.reduce_sum(tf.square(y_pred_1 - y_pred_2), axis=1)\n",
    "        \n",
    "        # Contrastive loss calculation\n",
    "        contrastive_loss = y_true * d + (1 - y_true) * tf.maximum(self.margin - d, 0)\n",
    "        return tf.reduce_mean(contrastive_loss)\n",
    "# Triplet loss\n",
    "class TripletLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        # y_pred should be a 3D tensor with shape [batch_size, 3, feature_dim]\n",
    "        anchor, positive, negative = tf.split(y_pred, num_or_size_splits=3, axis=1)\n",
    "        anchor = tf.squeeze(anchor, axis=1)\n",
    "        positive = tf.squeeze(positive, axis=1)\n",
    "        negative = tf.squeeze(negative, axis=1)\n",
    "        pos_dist = tf.reduce_sum(tf.square(anchor - positive), axis=1)\n",
    "        neg_dist = tf.reduce_sum(tf.square(anchor - negative), axis=1)\n",
    "        triplet_loss = tf.maximum(pos_dist - neg_dist + self.margin, 0.0)\n",
    "        return tf.reduce_mean(triplet_loss)\n",
    "\n",
    "# Large-margin softmax loss\n",
    "class LargeMarginSoftmaxLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(LargeMarginSoftmaxLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        # Apply margin to the logits\n",
    "        y_true = tf.cast(y_true, dtype=tf.float32)\n",
    "        cos_theta = tf.nn.softmax(y_pred, axis=-1)\n",
    "        theta = tf.acos(tf.clip_by_value(cos_theta, -1.0, 1.0))\n",
    "        cos_m_theta = tf.cos(theta * self.margin)\n",
    "        large_margin_loss = tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=cos_m_theta)\n",
    "        return tf.reduce_mean(large_margin_loss)\n",
    "\n",
    "# Kullback-Leibler (KL) divergence\n",
    "class KLDivergence(tf.keras.losses.Loss):\n",
    "    def __init__(self):\n",
    "        super(KLDivergence, self).__init__()\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        # Ensure y_true and y_pred are of type float32\n",
    "        y_true = tf.cast(y_true, dtype=tf.float32)\n",
    "        y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
    "        kl_divergence = tf.reduce_sum(y_true * tf.math.log(tf.maximum(y_true, 1e-10) / tf.maximum(y_pred, 1e-10)), axis=-1)\n",
    "        return tf.reduce_mean(kl_divergence)\n",
    "\n",
    "# Jensen-Shannon (JS) divergence\n",
    "class JSDivergence(tf.keras.losses.Loss):\n",
    "    def __init__(self):\n",
    "        super(JSDivergence, self).__init__()\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        # Ensure y_true and y_pred are of type float32\n",
    "        y_true = tf.cast(y_true, dtype=tf.float32)\n",
    "        y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
    "        m = 0.5 * (y_true + y_pred)\n",
    "        js_divergence = 0.5 * (tf.reduce_sum(y_true * tf.math.log(tf.maximum(y_true, 1e-10) / tf.maximum(m, 1e-10)), axis=-1) +\n",
    "                               tf.reduce_sum(y_pred * tf.math.log(tf.maximum(y_pred, 1e-10) / tf.maximum(m, 1e-10)), axis=-1))\n",
    "        return tf.reduce_mean(js_divergence)\n",
    "\n",
    "# Example usage\n",
    "input_data = tf.random.normal([32, 10])\n",
    "labels = tf.random.uniform([32, 10], maxval=2, dtype=tf.int32)\n",
    "\n",
    "hinge_loss = HingeLoss()\n",
    "softmax_loss = SoftmaxLoss()\n",
    "contrastive_loss = ContrastiveLoss()\n",
    "triplet_loss = TripletLoss()\n",
    "large_margin_softmax_loss = LargeMarginSoftmaxLoss()\n",
    "kl_divergence = KLDivergence()\n",
    "js_divergence = JSDivergence()\n",
    "\n",
    "print(\"Hinge Loss:\", hinge_loss(labels, input_data))\n",
    "print(\"Softmax Loss:\", softmax_loss(labels, input_data))\n",
    "# print(\"Contrastive Loss:\", contrastive_loss(labels, input_data))\n",
    "# print(\"Triplet Loss:\", triplet_loss(labels, input_data))\n",
    "print(\"Large Margin Softmax Loss:\", large_margin_softmax_loss(labels, input_data))\n",
    "print(\"KL Divergence:\", kl_divergence(labels, input_data))\n",
    "print(\"JS Divergence:\", js_divergence(labels, input_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1833986d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contrastive Loss: tf.Tensor(10.063724, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Contrastive loss\n",
    "class ContrastiveLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        # Ensure y_true is of type float32\n",
    "        y_true = tf.cast(y_true, dtype=tf.float32)\n",
    "        \n",
    "        # Split y_pred into two parts\n",
    "        y_pred_1, y_pred_2 = tf.split(y_pred, num_or_size_splits=2, axis=1)\n",
    "        \n",
    "        # Compute the squared Euclidean distance\n",
    "        d = tf.reduce_sum(tf.square(y_pred_1 - y_pred_2), axis=1)\n",
    "        \n",
    "        # Contrastive loss calculation\n",
    "        contrastive_loss = y_true * d + (1 - y_true) * tf.maximum(self.margin - d, 0)\n",
    "        return tf.reduce_mean(contrastive_loss)\n",
    "\n",
    "# Example usage\n",
    "input_data = tf.random.normal([32, 20])  # Adjusted to have 20 features (10+10 for pairs)\n",
    "labels = tf.random.uniform([32, 1], maxval=2, dtype=tf.int32)  # Ensure labels are in shape [batch_size, 1]\n",
    "\n",
    "contrastive_loss = ContrastiveLoss()\n",
    "\n",
    "print(\"Contrastive Loss:\", contrastive_loss(labels, input_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d6a4a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triplet Loss: tf.Tensor(2.6696954, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Triplet loss\n",
    "class TripletLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        # Ensure y_pred has exactly 3 parts\n",
    "        batch_size = tf.shape(y_pred)[0]\n",
    "        feature_dim = tf.shape(y_pred)[1]\n",
    "        \n",
    "        if feature_dim % 3 != 0:\n",
    "            raise ValueError(\"y_pred must have dimensions divisible by 3 to be split into anchor, positive, and negative parts.\")\n",
    "        \n",
    "        # Split y_pred into anchor, positive, and negative\n",
    "        anchor, positive, negative = tf.split(y_pred, num_or_size_splits=3, axis=1)\n",
    "        \n",
    "        # Compute the squared Euclidean distances\n",
    "        pos_dist = tf.reduce_sum(tf.square(anchor - positive), axis=1)\n",
    "        neg_dist = tf.reduce_sum(tf.square(anchor - negative), axis=1)\n",
    "        \n",
    "        # Compute triplet loss\n",
    "        triplet_loss = tf.maximum(pos_dist - neg_dist + self.margin, 0.0)\n",
    "        return tf.reduce_mean(triplet_loss)\n",
    "\n",
    "# Example usage\n",
    "# Ensure input_data has a feature dimension divisible by 3 (e.g., 3 * feature_dim)\n",
    "input_data = tf.random.normal([32, 9])  # Example with feature_dim = 3\n",
    "labels = tf.zeros([32, 1])  # Labels are not used in triplet loss, so just a placeholder\n",
    "\n",
    "triplet_loss = TripletLoss()\n",
    "\n",
    "print(\"Triplet Loss:\", triplet_loss(labels, input_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791941a4",
   "metadata": {},
   "source": [
    "Combining the predictions of many different models is a very successful way to reduce test errors, but it appears to be too expensive for big neural networks. Dropout is a very efficient version of model combination through setting to zero the output of each hidden neuron with probability \\( q \\). Moreover, overfitting is a serious problem in deep neural networks with a large number of parameters to be learned. This overfitting can be greatly reduced by using dropout.\n",
    "\n",
    "Dropout, introduced by Hinton et al. in 2012, is a form of regularization for fully connected neural network layers. Each element of a layer’s output is kept with probability \\( p \\), otherwise is randomly dropped with probability \\( (1 - p) \\) during training. This dropout procedure not only has good performance but also its implementation is simple.\n",
    "\n",
    "### Dropout Procedure\n",
    "\n",
    "The dropout procedure can be explained from two different perspectives:\n",
    "\n",
    "- **Randomly Omitting:** Each hidden unit is randomly omitted from the network with probability \\( q = 1 - p \\) (say \\( q = 0.5 \\)). This can prevent complex co-adaptations on the training data.\n",
    "\n",
    "- **Model Averaging:** It is computationally expensive to train all neurons or nodes in many separate layers and then to apply each of these networks to the test data in the standard way. However, dropout randomly performs model averaging with neural networks, making it possible for all of these networks to share the same weights for the hidden units that are present.\n",
    "\n",
    "Dropout is effective for preventing the co-adaptation of feature detectors, and hence can prevent overfitting associated with their co-adaptation.\n",
    "\n",
    "### Questions About Dropout\n",
    "\n",
    "Dropout is an intriguing algorithm for shallow and deep learning, which seems to be more effective than fully connected neural networks, but comes with little formal understanding and raises several interesting questions:\n",
    "\n",
    "1. What kind of model averaging is dropout implementing, exactly or in approximation, when applied to multiple layers?\n",
    "2. How crucial are its parameters? For instance, is \\( q = 0.5 \\) necessary and what happens when other values are used? What happens when other transfer functions are used?\n",
    "3. What are the effects of different values of \\( q \\) for different layers? What happens if dropout is applied to connections rather than units?\n",
    "4. What are precisely the regularization and averaging properties of dropout?\n",
    "5. What are the convergence properties of dropout?\n",
    "\n",
    "### Dropout in Neural Networks\n",
    "\n",
    "Consider a neural network with \\( L \\) hidden layers. Let \\( l \\in \\{1, \\ldots, L\\} \\) be the index of the \\( l \\)-th hidden layer of the network, and \\( z^{(l)} \\) denote the input vector of layer \\( l \\), \\( y^{(l)} \\) denote the output vector of layer \\( l \\) (where \\( y^{(0)} = x \\) is the input). The feedforward operation of a standard fully connected neural network can be described as:\n",
    "\n",
    "$$\n",
    "z^{(l+1)} = W^{(l)} y^{(l)} + b^{(l)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "y^{(l+1)} = f(z^{(l+1)})\n",
    "$$\n",
    "\n",
    "where \\( W^{(l)} \\) and \\( b^{(l)} \\) are, respectively, the weight matrix and bias vector at layer \\( l \\), while \\( f(\\cdot) \\) is any elementwise activation function, such as \\( f(z) = \\frac{1}{1 + \\exp(-z)} \\) or \\( f(z) = \\tanh(z) \\).\n",
    "\n",
    "If dropout is applied to the outputs of a fully connected layer, then the above two equations can be rewritten as:\n",
    "\n",
    "$$\n",
    "z^{(l+1)} = M^{(l)} \\odot (W^{(l)} y^{(l)} + b^{(l)})\n",
    "$$\n",
    "\n",
    "where \\( M^{(l)} \\) denotes the elementwise product of two matrices \\( M \\) and \\( W \\), and \\( M^{(l)} \\in \\mathbb{R}^{d \\times n} \\) is a binary matrix encoding the connection information with elements \\( M_{ij}^{(l)} \\sim \\text{Bernoulli}(p) \\).\n",
    "\n",
    "Here, Bernoulli(p) denotes a Bernoulli distribution of binary random variable \\( v \\):\n",
    "\n",
    "$$\n",
    "P(v = 1) = p \\quad \\text{and} \\quad P(v = 0) = 1 - p\n",
    "$$\n",
    "\n",
    "### Dropout in CNNs\n",
    "\n",
    "Consider a standard CNN composed of alternating convolutional and pooling layers, with fully connected layers on top. On each presentation of a training example, if layer \\( l \\) is followed by a pooling layer, the forward propagation without dropout can be described as:\n",
    "\n",
    "$$\n",
    "a_j^{(l+1)} = \\text{pool}(a_1^{(l)}, \\ldots, a_n^{(l)})\n",
    "$$\n",
    "\n",
    "where \\( a_i^{(l)} \\) with \\( i \\in R_j^{(l)} \\) is pooling region \\( j \\) at layer \\( l \\) and \\( a_i^{(l)} \\) is the activity of each neuron within it, while \\( n = |R_j| \\) is the number of units in \\( R_j \\).\n",
    "\n",
    "With dropout, the forward propagation of max-pooling dropout at training time becomes:\n",
    "\n",
    "$$\n",
    "\\hat{a}^{(l)} = M^{(l)} \\odot a^{(l)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "a_j^{(l+1)} = \\text{pool}(\\hat{a}_1^{(l)}, \\ldots, \\hat{a}_n^{(l)})\n",
    "$$\n",
    "\n",
    "where the activations \\( \\hat{a}_1, \\ldots, \\hat{a}_n \\) in each pooling region \\( j \\) are reordered in nondecreasing order, i.e., \\( 0 \\leq \\hat{a}_1^{(l)} \\leq \\ldots \\leq \\hat{a}_n^{(l)} \\).\n",
    "\n",
    "If layer \\( l \\) is followed by a convolutional layer, the forward propagation with dropout is formulated as:\n",
    "\n",
    "$$\n",
    "m_k^{(l)}(i) \\sim \\text{Bernoulli}(p)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{a}_k^{(l)} = a_k^{(l)} \\odot m_k^{(l)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "z_j^{(l+1)} = \\sum_{k=1}^{n^{(l)}} \\text{conv}(W_j^{(l+1)}, \\hat{a}_k^{(l)})\n",
    "$$\n",
    "\n",
    "$$\n",
    "a_j^{(l+1)} = f(z_j^{(l+1)})\n",
    "$$\n",
    "\n",
    "where \\( a_k^{(l)} \\) denotes the activations of feature map \\( k \\) (for \\( k = 1, \\ldots, n^{(l)} \\)) at layer \\( l \\). The mask vector \\( m_k \\) consists of independent Bernoulli variables \\( m_k(i) \\). This mask is sampled and multiplied with activations in the \\( k \\)-th feature map at layer \\( l \\) to produce dropout-modified activations \\( \\hat{a}_k^{(l)} \\). These modified activations are convolved with filter \\( W_j^{(l+1)} \\) to produce convolved features \\( z_j^{(l+1)} \\). The function \\( f(\\cdot) \\) is an elementwise function applied to the convolved features to get the activations \\( a_j^{(l+1)} \\) of convolutional layers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57c09766",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DropoutFullyConnectedNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_prob):\n",
    "        super(DropoutFullyConnectedNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)  # Dropout layer\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)  # Apply dropout\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "input_size = 784  # Example input size\n",
    "hidden_size = 500\n",
    "output_size = 10  # Example output size (e.g., number of classes)\n",
    "dropout_prob = 0.5\n",
    "\n",
    "model = DropoutFullyConnectedNN(input_size, hidden_size, output_size, dropout_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "550191b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropoutCNN(nn.Module):\n",
    "    def __init__(self, dropout_prob):\n",
    "        super(DropoutCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5)\n",
    "        self.dropout = nn.Dropout2d(p=dropout_prob)  # Dropout layer for 2D convolutional layers\n",
    "        self.fc1 = nn.Linear(1024, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout(x)  # Apply dropout\n",
    "        x = x.view(-1, 1024)  # Flatten the tensor\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "dropout_prob = 0.5\n",
    "\n",
    "model = DropoutCNN(dropout_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4ac0871",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'PILLOW_VERSION' from 'PIL' (/home/radha/anaconda3/envs/cv37/lib/python3.7/site-packages/PIL/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5884/2033157530.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Define the Fully Connected Neural Network with Dropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cv37/lib/python3.7/site-packages/torchvision/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cv37/lib/python3.7/site-packages/torchvision/datasets/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msvhn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSVHN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mphototour\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPhotoTour\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfakedata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFakeData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msemeion\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSEMEION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0momniglot\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOmniglot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cv37/lib/python3.7/site-packages/torchvision/datasets/fakedata.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVisionDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cv37/lib/python3.7/site-packages/torchvision/transforms/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/cv37/lib/python3.7/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cv37/lib/python3.7/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageOps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageEnhance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPILLOW_VERSION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0maccimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'PILLOW_VERSION' from 'PIL' (/home/radha/anaconda3/envs/cv37/lib/python3.7/site-packages/PIL/__init__.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define the Fully Connected Neural Network with Dropout\n",
    "class DropoutFullyConnectedNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_prob):\n",
    "        super(DropoutFullyConnectedNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)  # Dropout layer\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)  # Apply dropout\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Define the Convolutional Neural Network with Dropout\n",
    "class DropoutCNN(nn.Module):\n",
    "    def __init__(self, dropout_prob):\n",
    "        super(DropoutCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5)\n",
    "        self.dropout = nn.Dropout2d(p=dropout_prob)  # Dropout layer for 2D convolutional layers\n",
    "        self.fc1 = nn.Linear(1024, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout(x)  # Apply dropout\n",
    "        x = x.view(-1, 1024)  # Flatten the tensor\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Training and Evaluation\n",
    "def train_and_evaluate(model, train_loader, test_loader, epochs=5, lr=0.01):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch {epoch + 1}, Loss: {running_loss / len(train_loader)}')\n",
    "\n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Accuracy: {100 * correct / total}%')\n",
    "\n",
    "# Main function to run the training and evaluation\n",
    "def main():\n",
    "    # Hyperparameters\n",
    "    input_size = 784  # Example input size for fully connected network\n",
    "    hidden_size = 500\n",
    "    output_size = 10  # Number of classes\n",
    "    dropout_prob = 0.5\n",
    "    batch_size = 64\n",
    "    epochs = 5\n",
    "    learning_rate = 0.01\n",
    "\n",
    "    # Data loading and preprocessing\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    \n",
    "    train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Initialize models\n",
    "    fc_model = DropoutFullyConnectedNN(input_size, hidden_size, output_size, dropout_prob)\n",
    "    cnn_model = DropoutCNN(dropout_prob)\n",
    "\n",
    "    # Train and evaluate Fully Connected Network\n",
    "    print(\"Training Fully Connected Network...\")\n",
    "    train_and_evaluate(fc_model, train_loader, test_loader, epochs, learning_rate)\n",
    "\n",
    "    # Train and evaluate Convolutional Neural Network\n",
    "    print(\"\\nTraining Convolutional Neural Network...\")\n",
    "    train_and_evaluate(cnn_model, train_loader, test_loader, epochs, learning_rate)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e26db95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
