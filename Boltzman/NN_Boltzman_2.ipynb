{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09d0bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " * Copyright (c) 2018 Radhamadhab Dalai\n",
    " *\n",
    " * Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    " * of this software and associated documentation files (the \"Software\"), to deal\n",
    " * in the Software without restriction, including without limitation the rights\n",
    " * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    " * copies of the Software, and to permit persons to whom the Software is\n",
    " * furnished to do so, subject to the following conditions:\n",
    " *\n",
    " * The above copyright notice and this permission notice shall be included in\n",
    " * all copies or substantial portions of the Software.\n",
    " *\n",
    " * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    " * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    " * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    " * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    " * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    " * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
    " * THE SOFTWARE.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66690985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contrastive Divergence Learning\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class RBM:\n",
    "    def __init__(self, num_visible, num_hidden):\n",
    "        self.num_visible = num_visible\n",
    "        self.num_hidden = num_hidden\n",
    "        self.W = np.random.normal(0, 0.1, size=(num_visible, num_hidden))  # Weight matrix\n",
    "        self.b = np.zeros(num_visible)  # Visible biases\n",
    "        self.c = np.zeros(num_hidden)   # Hidden biases\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1.0 / (1 + np.exp(-x))\n",
    "\n",
    "    def sample_hidden(self, visible):\n",
    "        hidden_activations = self.sigmoid(np.dot(visible, self.W) + self.c)\n",
    "        hidden_states = np.random.binomial(1, hidden_activations)\n",
    "        return hidden_activations, hidden_states\n",
    "\n",
    "    def sample_visible(self, hidden):\n",
    "        visible_activations = self.sigmoid(np.dot(hidden, self.W.T) + self.b)\n",
    "        visible_states = np.random.binomial(1, visible_activations)\n",
    "        return visible_activations, visible_states\n",
    "\n",
    "    def contrastive_divergence(self, data, learning_rate=0.1, epochs=100, batch_size=10, k=1):\n",
    "        num_samples = data.shape[0]\n",
    "        for epoch in range(epochs):\n",
    "            np.random.shuffle(data)\n",
    "            for i in range(0, num_samples, batch_size):\n",
    "                batch = data[i:i+batch_size]\n",
    "                self.update_weights(batch, learning_rate, k)\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1} complete. Free energy: {self.free_energy(data)}\")\n",
    "\n",
    "    def update_weights(self, batch, learning_rate, k):\n",
    "        v0 = batch\n",
    "        h0_prob, h0_sample = self.sample_hidden(v0)\n",
    "\n",
    "        vk = v0\n",
    "        hk = h0_sample\n",
    "        for _ in range(k):\n",
    "            vk_prob, vk_sample = self.sample_visible(hk)\n",
    "            hk_prob, hk = self.sample_hidden(vk_sample)\n",
    "\n",
    "        positive_grad = np.dot(v0.T, h0_prob)\n",
    "        negative_grad = np.dot(vk_sample.T, hk_prob)\n",
    "\n",
    "        self.W += learning_rate * (positive_grad - negative_grad) / batch.shape[0]\n",
    "        self.b += learning_rate * np.mean(v0 - vk_sample, axis=0)\n",
    "        self.c += learning_rate * np.mean(h0_prob - hk_prob, axis=0)\n",
    "\n",
    "    def free_energy(self, v):\n",
    "        vbias_term = np.dot(v, self.b)\n",
    "        wx_b = np.dot(v, self.W) + self.c\n",
    "        hidden_term = np.sum(np.log(1 + np.exp(wx_b)), axis=1)\n",
    "        return -hidden_term - vbias_term\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac5ef37e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 complete. Free energy: [-1.74022503 -1.92157537 -1.75184045 -1.803132   -2.1743541  -2.03300667\n",
      " -1.83326608 -1.77516069 -1.84183904 -1.92731382 -1.86881865 -2.1382904\n",
      " -2.00528705 -2.07877459 -1.91484734 -1.92266732 -1.8222435  -1.91102561\n",
      " -1.85732583 -2.03300667 -1.93297515 -1.74022503 -2.0028521  -1.78899529\n",
      " -1.89677059 -2.08686085 -2.00749652 -1.93751805 -2.03300667 -2.1743541\n",
      " -1.96309149 -2.04022157 -1.86881865 -2.17751814 -2.17751814 -1.96812387\n",
      " -2.0849842  -2.00528705 -1.71477736 -1.94760785 -2.03300667 -1.92266732\n",
      " -2.00528705 -1.85732583 -1.77516069 -2.08686085 -2.09555477 -1.69760517\n",
      " -1.75485715 -1.80893841 -1.92731382 -1.66408322 -1.87746942 -1.87746942\n",
      " -2.00528705 -1.9746386  -1.84183904 -1.69760517 -2.04637275 -2.1743541\n",
      " -2.03300667 -1.92157537 -2.08686085 -1.803132   -1.8222435  -1.75485715\n",
      " -2.0491449  -1.66408322 -1.79566977 -1.8311864  -1.76889929 -2.1382904\n",
      " -1.92157537 -1.8311864  -2.0849842  -1.93297515 -1.95645014 -1.9746386\n",
      " -2.0491449  -2.04637275 -2.17751814 -1.96812387 -2.04022157 -2.1382904\n",
      " -1.92266732 -2.0028521  -1.8729152  -1.87746942 -2.17751814 -1.76889929\n",
      " -1.803132   -1.8222435  -1.96812387 -2.1743541  -2.04005022 -1.68115685\n",
      " -1.91484734 -2.04022157 -1.91484734 -1.94760785]\n",
      "Epoch 11 complete. Free energy: [-1.62376876 -1.79983295 -2.14755986 -1.52026609 -1.46340362 -1.90433711\n",
      " -2.08101416 -1.53910388 -2.19797482 -1.39926559 -1.81180259 -1.8658427\n",
      " -1.7050664  -2.02820183 -1.60130043 -1.77980783 -2.03741618 -2.19797482\n",
      " -2.02820183 -1.77980783 -1.70894041 -1.93920146 -1.63113541 -1.56299333\n",
      " -1.56299333 -1.99451321 -1.64677215 -1.93327163 -2.11174555 -2.11174555\n",
      " -1.71709473 -2.03741618 -2.14755986 -1.7050664  -1.92533307 -1.46022264\n",
      " -1.77123516 -1.67233816 -1.86307367 -1.83704815 -1.79983295 -1.88980718\n",
      " -1.70894041 -1.48060778 -1.86307367 -2.12146967 -1.8658427  -1.67233816\n",
      " -2.02820183 -2.19797482 -2.14755986 -2.06685349 -1.53910388 -1.88980718\n",
      " -1.83126555 -2.06685349 -2.05883201 -1.39926559 -1.90433711 -1.75585131\n",
      " -2.03741618 -2.02820183 -1.69543353 -1.93327163 -2.01706223 -1.71709473\n",
      " -1.88007727 -2.01706223 -1.80569938 -2.03741618 -2.00965522 -2.06685349\n",
      " -1.79983295 -1.77123516 -1.88007727 -1.75585131 -1.82027723 -1.83704815\n",
      " -1.99451321 -1.81180259 -1.54691979 -1.63113541 -2.02820183 -1.86307367\n",
      " -1.77123516 -1.88007727 -2.14755986 -1.50451484 -1.48060778 -1.69543353\n",
      " -1.6417905  -1.46340362 -1.60130043 -1.69543353 -1.75585131 -1.77980783\n",
      " -1.56316759 -2.01706223 -1.7050664  -2.19797482]\n",
      "Epoch 21 complete. Free energy: [-2.31613806 -1.89398894 -1.92372515 -2.05282731 -2.47177692 -1.89370391\n",
      " -2.47177692 -2.27080917 -2.55136684 -2.47177692 -2.31613806 -1.83092578\n",
      " -1.48873106 -2.07348842 -2.03838418 -1.82465819 -1.78179281 -2.35957964\n",
      " -1.7330154  -2.31822661 -2.20171154 -2.04844768 -2.39643062 -1.9166483\n",
      " -2.2036483  -1.95682664 -1.85035647 -2.39643062 -1.78179281 -1.48873106\n",
      " -2.32969131 -2.39643062 -1.7957487  -2.07348842 -1.96643152 -2.05282731\n",
      " -2.20171154 -1.94010779 -2.13788409 -2.17242837 -2.12284307 -2.31822661\n",
      " -2.19004066 -2.35957964 -2.04844768 -2.19712282 -2.27080917 -1.82465819\n",
      " -2.20171154 -2.27080917 -2.31613806 -1.69679492 -1.57643349 -1.98442372\n",
      " -2.19004066 -2.24320729 -2.11222111 -2.31822661 -2.03532451 -1.82465819\n",
      " -2.24401906 -2.55136684 -1.64198412 -2.55457891 -2.17242837 -1.92372515\n",
      " -2.55136684 -2.39643062 -2.05006307 -1.96643152 -2.55136684 -1.69679492\n",
      " -1.81341786 -2.27080917 -2.31613806 -2.13141772 -1.57643349 -1.63983068\n",
      " -2.11222111 -1.81341786 -2.63004581 -1.85035647 -1.92372515 -2.32969131\n",
      " -2.17242837 -2.2036483  -1.98442372 -2.24320729 -1.81341786 -2.47175185\n",
      " -1.9166483  -2.03838418 -2.28995536 -2.31613806 -2.28995536 -2.2036483\n",
      " -2.28995536 -2.05006307 -2.05006307 -2.24401906]\n",
      "Epoch 31 complete. Free energy: [-2.23185734 -1.69451782 -1.91435792 -1.7132669  -2.41817933 -1.85114853\n",
      " -2.00424374 -2.18257054 -2.02446491 -1.85114853 -1.6282382  -2.41817933\n",
      " -1.62144045 -1.53229765 -2.25992442 -1.99171746 -2.0712581  -2.16822882\n",
      " -1.89638345 -1.78965449 -1.92745197 -1.78378558 -2.11708267 -1.66891488\n",
      " -2.16707962 -2.00424374 -1.52391513 -1.6250369  -2.04716105 -2.26993597\n",
      " -1.97385861 -2.11481565 -1.6282382  -1.88248028 -1.79724157 -1.62144045\n",
      " -1.53229765 -2.41817933 -1.55229216 -1.89638345 -1.44033908 -2.0712581\n",
      " -2.36732543 -2.18257054 -2.23185734 -1.54543685 -1.66891488 -2.31760197\n",
      " -2.28416075 -2.28416075 -1.85195632 -1.66891488 -1.85195632 -2.28416075\n",
      " -2.14194872 -2.16822882 -2.02837781 -1.79724157 -2.18257054 -1.66786288\n",
      " -2.00424374 -1.65110849 -2.41817933 -2.11481565 -1.52391513 -1.6282382\n",
      " -1.85195632 -1.6250369  -2.11708267 -1.78965449 -2.40916729 -2.28416075\n",
      " -2.36732543 -2.0712581  -1.90577401 -2.00424374 -1.40158142 -2.16822882\n",
      " -1.97385861 -1.92745197 -2.36732543 -1.48203881 -2.18257054 -1.55229216\n",
      " -2.02500934 -1.91435792 -2.04716105 -2.41817933 -1.78965449 -1.99171746\n",
      " -1.69451782 -2.16707962 -2.14123716 -1.62144045 -2.02500934 -2.02446491\n",
      " -1.88248028 -2.01015243 -2.11481565 -2.11708267]\n",
      "Epoch 41 complete. Free energy: [-1.59756014 -2.06042178 -2.1628587  -1.82725178 -1.55064925 -1.84442738\n",
      " -1.75188521 -1.90459782 -2.1628587  -1.88021013 -1.53830077 -1.84442738\n",
      " -1.9048447  -1.51156738 -1.838742   -1.67183299 -1.47142959 -1.32447183\n",
      " -1.77612964 -2.1628587  -2.20340097 -2.22296893 -1.838742   -1.83671743\n",
      " -1.88021013 -2.22788067 -2.13555269 -2.15738953 -1.77612964 -1.59189483\n",
      " -1.75188521 -2.13555269 -1.98198202 -1.72500301 -1.39662629 -1.66727457\n",
      " -2.05622567 -1.51156738 -1.67183299 -1.81153676 -1.61542874 -1.9048447\n",
      " -2.15738953 -1.99223583 -2.03978934 -2.06042178 -1.66727457 -1.32447183\n",
      " -1.29035304 -1.53830077 -1.72500301 -2.03978934 -1.3112893  -2.1628587\n",
      " -1.81153676 -1.48153618 -1.64847745 -1.38125372 -1.6821669  -1.82725178\n",
      " -2.15738953 -1.98198202 -1.72500301 -1.9048447  -1.90459782 -2.21675307\n",
      " -1.77612964 -1.84285055 -2.15738953 -1.55064925 -1.90459782 -2.22296893\n",
      " -1.59756014 -1.82163747 -1.59756014 -2.13555269 -2.05622567 -1.74419474\n",
      " -1.66381807 -1.3112893  -1.9874837  -1.67294114 -2.03978934 -1.64847745\n",
      " -1.51635324 -2.22296893 -2.05242868 -1.66381807 -1.61542874 -1.81153676\n",
      " -2.22296893 -1.61542874 -1.36302087 -1.66381807 -1.82725178 -1.48153618\n",
      " -2.1628587  -1.84285055 -2.03978934 -1.67294114]\n",
      "Epoch 51 complete. Free energy: [-1.56630433 -2.19438897 -1.50070747 -1.44035045 -1.88557342 -1.92569201\n",
      " -2.11862737 -1.38414092 -1.82539538 -2.04904609 -1.45954481 -2.01544453\n",
      " -1.53992411 -1.85669379 -1.84913584 -1.81201985 -1.76080651 -1.63557524\n",
      " -1.58062915 -1.72343897 -2.09513683 -1.68353085 -1.61297707 -2.01877284\n",
      " -1.82539538 -1.67736453 -1.61297707 -2.19438897 -1.96548977 -1.81201985\n",
      " -1.61297707 -2.09513683 -1.81262571 -1.99549094 -2.19438897 -1.84913584\n",
      " -1.41553875 -2.01877284 -1.78920366 -1.98523269 -1.56630433 -1.68353085\n",
      " -1.68758494 -1.63124708 -2.01544453 -2.11181129 -1.91121508 -2.01544453\n",
      " -2.01544453 -1.6683641  -1.31743638 -1.87618827 -2.01544453 -2.09513683\n",
      " -1.90715637 -1.63124708 -1.72343897 -1.58062915 -1.67736453 -1.90715637\n",
      " -1.31743638 -1.28528178 -1.68758494 -2.06803654 -1.85669379 -1.6683641\n",
      " -2.09513683 -1.98523269 -1.81262571 -1.51035664 -1.45954481 -1.5860967\n",
      " -1.84913584 -1.99549094 -1.81262571 -2.06803654 -1.40499009 -2.19438897\n",
      " -1.73470692 -1.53524294 -1.58062915 -1.28528178 -1.78920366 -1.99549094\n",
      " -1.32213117 -1.56630433 -1.50070747 -1.99549094 -2.01877284 -1.91121508\n",
      " -1.5860967  -1.40499009 -1.60238289 -1.85669379 -1.42353148 -1.63557524\n",
      " -1.53524294 -1.90715637 -1.78920366 -1.45954481]\n",
      "Epoch 61 complete. Free energy: [-1.8747636  -1.94483151 -2.2093167  -1.96774555 -1.64233677 -1.8073143\n",
      " -1.66081244 -1.98226984 -1.54571341 -1.88471511 -1.87313127 -1.8073143\n",
      " -1.94791962 -1.95809344 -1.62342409 -1.35368876 -2.27311037 -1.61794383\n",
      " -1.54571341 -1.93497678 -1.52584372 -1.66081244 -1.66748218 -1.34706308\n",
      " -2.24103208 -1.99423663 -1.92133241 -1.65107764 -2.27311037 -1.8808353\n",
      " -1.99325744 -1.71440992 -1.99005956 -1.60956789 -1.55168392 -1.59469089\n",
      " -1.95809344 -2.24103208 -1.94791962 -1.96774555 -1.99005956 -1.93497678\n",
      " -1.63667745 -1.99325744 -1.29159351 -1.70474137 -1.58682126 -2.2093167\n",
      " -1.58682126 -1.37171683 -2.27311037 -1.66081244 -1.93497678 -1.81890293\n",
      " -1.8808353  -1.93497678 -1.65107764 -1.8808353  -2.24103208 -2.27311037\n",
      " -1.33118928 -1.81890293 -1.98226984 -2.26460084 -1.65107764 -2.24930007\n",
      " -1.92133241 -1.63667745 -1.98142092 -1.99423663 -1.66748218 -2.24930007\n",
      " -1.70474137 -1.59469089 -2.21974347 -2.27311037 -1.98226984 -1.61794383\n",
      " -1.27205195 -2.24930007 -1.88471511 -1.8073143  -1.62342409 -1.70474137\n",
      " -2.24103208 -2.2796187  -1.55168392 -1.94791962 -1.52584372 -1.81917662\n",
      " -1.33118928 -1.35368876 -2.24930007 -1.97855101 -1.88471511 -1.60694162\n",
      " -1.87313127 -2.2093167  -1.55783809 -1.54571341]\n",
      "Epoch 71 complete. Free energy: [-1.40880347 -2.58739076 -1.84634484 -2.3227206  -2.21617373 -1.61032317\n",
      " -2.34121596 -1.53743427 -2.21617373 -1.61032317 -2.25905902 -1.5823211\n",
      " -1.63888193 -2.58739076 -1.84023161 -1.96336348 -1.74973253 -1.40880347\n",
      " -2.08379167 -2.64814059 -1.87064403 -2.29602644 -2.01431706 -1.96336348\n",
      " -1.7726356  -1.87064403 -2.00301904 -1.5823211  -1.63888193 -1.90327514\n",
      " -2.3227206  -1.5823211  -2.19503505 -1.49984904 -1.90947077 -2.14456513\n",
      " -1.65061536 -2.21617373 -2.14456513 -1.12649271 -1.96336348 -2.39207559\n",
      " -2.04966343 -2.08379167 -1.78780427 -2.29602644 -1.36942884 -1.38801033\n",
      " -2.25905902 -2.58739076 -1.41932304 -2.34033263 -1.85879398 -1.53743427\n",
      " -2.34033263 -1.87064403 -2.39207559 -1.96336348 -2.3227206  -1.74973253\n",
      " -2.03667963 -1.99759943 -1.63888193 -2.39207559 -1.49984904 -1.49596733\n",
      " -1.41932304 -1.84023161 -1.25975365 -2.39207559 -2.10195567 -1.85879398\n",
      " -2.01431706 -2.09277348 -2.34033263 -1.25975365 -2.09277348 -1.84634484\n",
      " -1.85879398 -1.92325427 -1.71305932 -1.93042478 -2.3227206  -2.39207559\n",
      " -2.08379167 -1.49984904 -1.71305932 -1.80048905 -1.722282   -2.42438301\n",
      " -1.77888212 -1.80048905 -1.90947077 -2.19503505 -2.58739076 -2.03667963\n",
      " -2.14456513 -1.78780427 -2.10195567 -1.78780427]\n",
      "Epoch 81 complete. Free energy: [-2.01390031 -2.04937048 -1.81698744 -1.69744978 -1.45760551 -1.73230115\n",
      " -2.04937048 -1.95187431 -1.75377405 -2.04937048 -1.75377405 -1.11084367\n",
      " -1.36504446 -1.62527181 -1.4251504  -1.36504446 -1.57112585 -1.90709714\n",
      " -1.61950526 -2.32667805 -1.38784572 -1.56907672 -2.21165253 -1.85355774\n",
      " -1.29248058 -1.47693636 -1.36157317 -1.78671158 -1.54840367 -1.89325856\n",
      " -1.4251504  -2.30643979 -1.75377405 -2.01390031 -1.89791634 -1.298951\n",
      " -1.95187431 -1.56062302 -1.54840367 -1.73347733 -1.45760551 -1.4251504\n",
      " -1.69744978 -1.73347733 -2.01390031 -2.19178784 -1.87914942 -1.67083894\n",
      " -1.56907672 -2.1657337  -1.348301   -1.57065751 -1.41503297 -1.11084367\n",
      " -2.32667805 -2.19178784 -1.75091065 -2.32667805 -1.52430008 -2.19178784\n",
      " -2.19178784 -1.69744978 -1.62527181 -1.47693636 -2.07936216 -1.89325856\n",
      " -1.69207784 -1.69207784 -1.47693636 -1.62527181 -1.78949761 -1.41503297\n",
      " -1.95187431 -1.56062302 -2.05604008 -1.57065751 -1.97909777 -1.52430008\n",
      " -1.28236617 -1.56062302 -1.81698744 -1.78949761 -1.85355774 -2.04937048\n",
      " -2.21165253 -2.21165253 -2.07936216 -2.07936216 -2.21165253 -2.02473417\n",
      " -1.73230115 -1.298951   -1.78949761 -2.01390031 -1.78671158 -1.89791634\n",
      " -2.04937048 -1.09382116 -1.94096142 -2.05604008]\n",
      "Epoch 91 complete. Free energy: [-2.13041724 -1.50414834 -1.87246327 -2.00390055 -1.81704453 -1.90513638\n",
      " -2.04489561 -2.00390055 -1.58515075 -2.00390055 -1.23940911 -1.41077953\n",
      " -1.90513638 -2.16285983 -1.83631977 -2.15464344 -1.74244673 -1.86873805\n",
      " -2.16285983 -1.41077953 -1.80300538 -1.88390109 -2.3201765  -1.82816251\n",
      " -1.74893204 -1.84218264 -1.74893204 -2.57985661 -2.25766076 -2.3201765\n",
      " -1.91647887 -2.57985661 -2.24059449 -1.26875178 -2.10742694 -1.88390109\n",
      " -1.23940911 -1.44352305 -1.50536094 -2.00390055 -1.74244673 -1.72509686\n",
      " -2.03595057 -2.16285983 -2.37134363 -1.80410591 -1.80410591 -2.08425022\n",
      " -1.48851842 -2.08425022 -1.58515075 -1.86873805 -1.4047704  -2.04489561\n",
      " -1.77781084 -1.65022761 -2.10742694 -1.80410591 -1.50536094 -1.87246327\n",
      " -1.6057076  -2.24059449 -1.17927197 -2.57985661 -2.41115485 -2.03595057\n",
      " -1.50536094 -1.8942578  -1.6057076  -2.15464344 -2.19707977 -1.97423926\n",
      " -1.88390109 -1.44352305 -1.45215066 -2.25766076 -2.25766076 -2.25766076\n",
      " -1.81704453 -1.48485402 -2.3201765  -2.57985661 -2.24059449 -1.58515075\n",
      " -1.26875178 -1.72509686 -1.50414834 -2.25766076 -1.66160431 -1.74893204\n",
      " -1.83631977 -1.72509686 -2.49890665 -2.3201765  -1.86288865 -2.19707977\n",
      " -2.04489561 -2.13041724 -1.86288865 -2.13041724]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "num_visible = 6\n",
    "num_hidden = 3\n",
    "rbm = RBM(num_visible, num_hidden)\n",
    "\n",
    "# Assuming data is your training dataset, shape (num_samples, num_visible)\n",
    "data = np.random.binomial(1, 0.5, size=(100, num_visible))  # Dummy data\n",
    "\n",
    "rbm.contrastive_divergence(data, learning_rate=0.1, epochs=100, batch_size=10, k=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d2f80d",
   "metadata": {},
   "source": [
    "## Algorithm 7.3: CD1 Fast RBM Learning Algorithm\n",
    "\n",
    "1. **Input**: A training sample $ x_0 $, the number of hidden layer units m, learning rate $ \\eta $, maximum training period T .\n",
    "\n",
    "2. **Initialization**: \n",
    "   - Set the initial state of the visible layer unit $ x_1 = x_0 $.\n",
    "   - Randomly initialize W , b, and c.\n",
    "\n",
    "3. **For t = 1 to T do**\n",
    "   1. **For \\( j = 1 \\) to \\( m \\) do** (for all hidden units)\n",
    "      1. Calculate \\( P(h_1^j = 1 | x_1) = \\sigma \\left(c_j + \\sum_i W_{ij} x_1^i \\right) \\)\n",
    "   2. **End for**\n",
    "   \n",
    "   3. Construct \\( P(h_1 = 1 | x_1) = \\left[ P(h_1^1 = 1 | x_1), \\ldots, P(h_1^m = 1 | x_1) \\right]^T \\)\n",
    "\n",
    "   4. **For \\( i = 1 \\) to \\( n \\) do** (for all visible units)\n",
    "      1. Calculate \\( P(x_2^i = 1 | h_1) = \\sigma \\left( b_i + \\sum_j W_{ij} h_1^j \\right) \\)\n",
    "   5. **End for**\n",
    "\n",
    "   6. **For \\( j = 1 \\) to \\( m \\) do** (for all hidden units)\n",
    "      1. Calculate \\( P(h_2^j = 1 | x_2) = \\sigma \\left( c_j + \\sum_i W_{ij} x_2^i \\right) \\)\n",
    "   7. **End for**\n",
    "\n",
    "   8. Construct \\( P(h_2 = 1 | x_2) = \\left[ P(h_2^1 = 1 | x_2), \\ldots, P(h_2^m = 1 | x_2) \\right]^T \\)\n",
    "\n",
    "   9. Update weights and biases:\n",
    "      - \\( W \\leftarrow W + \\eta \\left( P(h_1 = 1 | x_1) x_1^T - P(h_2 = 1 | x_2) x_2^T \\right) \\)\n",
    "      - \\( b \\leftarrow b + \\eta (x_1 - x_2) \\)\n",
    "      - \\( c \\leftarrow c + \\eta \\left( P(h_1 = 1 | x_1) - P(h_2 = 1 | x_2) \\right) \\)\n",
    "\n",
    "4. **End for**\n",
    "\n",
    "5. **Output**: \\( W \\), \\( b \\), \\( c \\)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "112cf7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Reconstruction error: 0.25020727745176286\n",
      "Epoch 100, Reconstruction error: 0.19526593913238244\n",
      "Epoch 200, Reconstruction error: 0.19113734728849557\n",
      "Epoch 300, Reconstruction error: 0.1792198589607437\n",
      "Epoch 400, Reconstruction error: 0.18320924647445982\n",
      "Epoch 500, Reconstruction error: 0.18139787772999202\n",
      "Epoch 600, Reconstruction error: 0.17679489427426476\n",
      "Epoch 700, Reconstruction error: 0.1784778090162915\n",
      "Epoch 800, Reconstruction error: 0.18079410722245873\n",
      "Epoch 900, Reconstruction error: 0.17693185260783437\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class RBM:\n",
    "    def __init__(self, n_visible, n_hidden, learning_rate=0.1):\n",
    "        self.n_visible = n_visible\n",
    "        self.n_hidden = n_hidden\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.W = np.random.randn(n_visible, n_hidden) * 0.01\n",
    "        self.b = np.zeros(n_visible)\n",
    "        self.c = np.zeros(n_hidden)\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def train(self, data, max_epochs=1000):\n",
    "        for epoch in range(max_epochs):\n",
    "            for x0 in data:\n",
    "                # Positive phase\n",
    "                h1_prob = self.sigmoid(self.c + np.dot(x0, self.W))\n",
    "                h1 = (np.random.rand(self.n_hidden) < h1_prob).astype(np.float32)\n",
    "                \n",
    "                # Negative phase\n",
    "                x2_prob = self.sigmoid(self.b + np.dot(h1, self.W.T))\n",
    "                x2 = (np.random.rand(self.n_visible) < x2_prob).astype(np.float32)\n",
    "                \n",
    "                h2_prob = self.sigmoid(self.c + np.dot(x2, self.W))\n",
    "                \n",
    "                # Update weights and biases\n",
    "                self.W += self.learning_rate * (np.outer(x0, h1_prob) - np.outer(x2, h2_prob))\n",
    "                self.b += self.learning_rate * (x0 - x2)\n",
    "                self.c += self.learning_rate * (h1_prob - h2_prob)\n",
    "            \n",
    "            if epoch % 100 == 0:\n",
    "                error = np.mean(np.square(data - self.reconstruct(data)))\n",
    "                print(f'Epoch {epoch}, Reconstruction error: {error}')\n",
    "    \n",
    "    def reconstruct(self, data):\n",
    "        h = self.sigmoid(self.c + np.dot(data, self.W))\n",
    "        reconstructed_data = self.sigmoid(self.b + np.dot(h, self.W.T))\n",
    "        return reconstructed_data\n",
    "\n",
    "# Example usage:\n",
    "# Create some binary data for training\n",
    "data = np.random.randint(2, size=(100, 6))\n",
    "\n",
    "# Initialize RBM with 6 visible units and 2 hidden units\n",
    "rbm = RBM(n_visible=6, n_hidden=2)\n",
    "\n",
    "# Train RBM\n",
    "rbm.train(data, max_epochs=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24cfadff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RBM layer 1/2 with 6 visible units and 4 hidden units\n",
      "Epoch 0, Reconstruction error: 0.25256792754457763\n",
      "Epoch 100, Reconstruction error: 0.22537702293600176\n",
      "Epoch 200, Reconstruction error: 0.2250872664629361\n",
      "Epoch 300, Reconstruction error: 0.18005507358416376\n",
      "Epoch 400, Reconstruction error: 0.16843223817423028\n",
      "Epoch 500, Reconstruction error: 0.1582156608374342\n",
      "Epoch 600, Reconstruction error: 0.16152565468936741\n",
      "Epoch 700, Reconstruction error: 0.1619548377002168\n",
      "Epoch 800, Reconstruction error: 0.16409116193986678\n",
      "Epoch 900, Reconstruction error: 0.1594841465762499\n",
      "Training RBM layer 2/2 with 4 visible units and 3 hidden units\n",
      "Epoch 0, Reconstruction error: 0.05755355447779868\n",
      "Epoch 100, Reconstruction error: 0.061761361568826105\n",
      "Epoch 200, Reconstruction error: 0.056338898501938746\n",
      "Epoch 300, Reconstruction error: 0.05641870923556357\n",
      "Epoch 400, Reconstruction error: 0.05725537266683869\n",
      "Epoch 500, Reconstruction error: 0.05796535462960344\n",
      "Epoch 600, Reconstruction error: 0.05659994987116585\n",
      "Epoch 700, Reconstruction error: 0.056807987806166096\n",
      "Epoch 800, Reconstruction error: 0.05817633814741249\n",
      "Epoch 900, Reconstruction error: 0.05621684111481788\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (100,3) and (4,3) not aligned: 3 (dim 1) != 4 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4430/755604521.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;31m# Reconstruct data using the trained DBN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m \u001b[0mreconstructed_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdbn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_4430/755604521.py\u001b[0m in \u001b[0;36mreconstruct\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mreconstructed_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformed_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mrbm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrbms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mreconstructed_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrbm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreconstructed_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreconstructed_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4430/755604521.py\u001b[0m in \u001b[0;36mreconstruct\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mreconstructed_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreconstructed_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (100,3) and (4,3) not aligned: 3 (dim 1) != 4 (dim 0)"
     ]
    }
   ],
   "source": [
    "# Multiple Restricted Boltzmann Machines\n",
    "import numpy as np\n",
    "\n",
    "class RBM:\n",
    "    def __init__(self, n_visible, n_hidden, learning_rate=0.1):\n",
    "        self.n_visible = n_visible\n",
    "        self.n_hidden = n_hidden\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.W = np.random.randn(n_visible, n_hidden) * 0.01\n",
    "        self.b = np.zeros(n_visible)\n",
    "        self.c = np.zeros(n_hidden)\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def train(self, data, max_epochs=1000):\n",
    "        for epoch in range(max_epochs):\n",
    "            for x0 in data:\n",
    "                # Positive phase\n",
    "                h1_prob = self.sigmoid(self.c + np.dot(x0, self.W))\n",
    "                h1 = (np.random.rand(self.n_hidden) < h1_prob).astype(np.float32)\n",
    "                \n",
    "                # Negative phase\n",
    "                x2_prob = self.sigmoid(self.b + np.dot(h1, self.W.T))\n",
    "                x2 = (np.random.rand(self.n_visible) < x2_prob).astype(np.float32)\n",
    "                \n",
    "                h2_prob = self.sigmoid(self.c + np.dot(x2, self.W))\n",
    "                \n",
    "                # Update weights and biases\n",
    "                self.W += self.learning_rate * (np.outer(x0, h1_prob) - np.outer(x2, h2_prob))\n",
    "                self.b += self.learning_rate * (x0 - x2)\n",
    "                self.c += self.learning_rate * (h1_prob - h2_prob)\n",
    "            \n",
    "            if epoch % 100 == 0:\n",
    "                error = np.mean(np.square(data - self.reconstruct(data)))\n",
    "                print(f'Epoch {epoch}, Reconstruction error: {error}')\n",
    "    \n",
    "    def reconstruct(self, data):\n",
    "        h = self.sigmoid(self.c + np.dot(data, self.W))\n",
    "        reconstructed_data = self.sigmoid(self.b + np.dot(h, self.W.T))\n",
    "        return reconstructed_data\n",
    "\n",
    "    def transform(self, data):\n",
    "        return self.sigmoid(self.c + np.dot(data, self.W))\n",
    "\n",
    "class DBN:\n",
    "    def __init__(self, layer_sizes, learning_rate=0.1, max_epochs=1000):\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_epochs = max_epochs\n",
    "        self.rbms = []\n",
    "        self._build_rbms()\n",
    "    \n",
    "    def _build_rbms(self):\n",
    "        for i in range(len(self.layer_sizes) - 1):\n",
    "            rbm = RBM(self.layer_sizes[i], self.layer_sizes[i+1], self.learning_rate)\n",
    "            self.rbms.append(rbm)\n",
    "    \n",
    "    def pretrain(self, data):\n",
    "        input_data = data\n",
    "        for i, rbm in enumerate(self.rbms):\n",
    "            print(f'Training RBM layer {i+1}/{len(self.rbms)} with {rbm.n_visible} visible units and {rbm.n_hidden} hidden units')\n",
    "            rbm.train(input_data, self.max_epochs)\n",
    "            input_data = rbm.transform(input_data)\n",
    "    \n",
    "    def reconstruct(self, data):\n",
    "        transformed_data = data\n",
    "        for rbm in self.rbms:\n",
    "            transformed_data = rbm.transform(transformed_data)\n",
    "        reconstructed_data = transformed_data\n",
    "        for rbm in reversed(self.rbms):\n",
    "            reconstructed_data = rbm.reconstruct(reconstructed_data)\n",
    "        return reconstructed_data\n",
    "\n",
    "# Example usage:\n",
    "# Create some binary data for training\n",
    "data = np.random.randint(2, size=(100, 6))\n",
    "\n",
    "# Define layer sizes: 6 visible units, 4 hidden units in the first RBM, and 3 hidden units in the second RBM\n",
    "layer_sizes = [6, 4, 3]\n",
    "\n",
    "# Initialize DBN\n",
    "dbn = DBN(layer_sizes, learning_rate=0.1, max_epochs=1000)\n",
    "\n",
    "# Pretrain DBN\n",
    "dbn.pretrain(data)\n",
    "\n",
    "# Reconstruct data using the trained DBN\n",
    "reconstructed_data = dbn.reconstruct(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f807475a",
   "metadata": {},
   "source": [
    "In the above discussion, the observed “visible” unit is denoted by the binary vector\n",
    "$$\\mathbf{x} = [x_1, \\ldots, x_m]^T$$. Now, consider the case where we have a set of \\( m \\) user-rated\n",
    "movies or commodities. Suppose the user rated movie \\( i \\) as \\( k \\), where \\( k \\in \\{1, \\ldots, K\\} \\).\n",
    "Then the observed “visible” binary rating matrix is defined as\n",
    "$$\n",
    "\\mathbf{X} = \\begin{bmatrix}\n",
    "x_{11} & x_{12} & \\cdots & x_{1m} \\\\\n",
    "x_{21} & x_{22} & \\cdots & x_{2m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_{K1} & x_{K2} & \\cdots & x_{Km}\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{K \\times m}\n",
    "$$\n",
    "where the \\((k, i)\\)th entry is given by\n",
    "$$\n",
    "x_{ik} = \\begin{cases}\n",
    "1, & \\text{if the user rated movie } i \\text{ as } k; \\\\\n",
    "0, & \\text{otherwise}.\n",
    "\\end{cases}\n",
    "$$\n",
    "Therefore, we have \\( K \\) restricted Boltzmann machines with binary hidden units\n",
    "and softmax visible units. For each user, the RBM only includes softmax units for\n",
    "the movies that user has rated. All of the \\( K \\) RBMs have the same binary values\n",
    "of hidden (latent) variables \\( \\mathbf{h} \\).\n",
    "If using a conditional multinomial distribution (a “softmax”) for modeling each\n",
    "column of the observed visible binary rating matrix \\( \\mathbf{X} \\) and a conditional Bernoulli\n",
    "distribution for modeling hidden user features \\( \\mathbf{h} \\), then\n",
    "$$\n",
    "p(x_{ik} = 1 | \\mathbf{h}) = \\frac{\\exp(b_{ik} + \\sum_{j=1}^{F} h_j W_{ij}^k)}{\\sum_{l=1}^{K} \\exp(b_{il} + \\sum_{j=1}^{F} h_j W_{ij}^l)},\n",
    "$$\n",
    "where \\( W_{ij}^k \\) is a symmetric interaction parameter between feature and rating \\( k \\) of\n",
    "movie \\( i \\), \\( b_{ik} \\) is the bias of rating \\( k \\) for movie \\( i \\), and \\( c_j \\) is the bias of feature \\( j \\). Note\n",
    "that the \\( b_{ik} \\) can be initialized to the logs of their respective base rates over all users.\n",
    "The marginal distribution over the visible ratings \\( \\mathbf{X} \\) is\n",
    "$$\n",
    "p(\\mathbf{X}) = \\sum_{\\mathbf{h}} \\frac{\\exp(-E(\\mathbf{X}, \\mathbf{h}))}{\\sum_{\\mathbf{X}', \\mathbf{h}'} \\exp(-E(\\mathbf{X}', \\mathbf{h}'))}\n",
    "$$\n",
    "with an “energy” term given by\n",
    "$$\n",
    "E(\\mathbf{X}, \\mathbf{h}) = - \\sum_{i=1}^{m} \\sum_{j=1}^{F} \\sum_{k=1}^{K} W_{ij}^k h_j x_{ik} + \\sum_{i=1}^{m} \\log(Z_i) - \\sum_{i=1}^{m} \\sum_{k=1}^{K} x_{ik} b_{ik} - \\sum_{j=1}^{F} h_j c_j,\n",
    "$$\n",
    "where\n",
    "$$\n",
    "Z_i = \\sum_{l=1}^{K} \\exp(b_{il} + \\sum_{j=1}^{F} h_j W_{ij}^l)\n",
    "$$\n",
    "is the normalization term that ensures that \\( \\sum_{l=1}^{K} p(x_i = 1|h) = 1 \\). The movies with missing ratings do not make any\n",
    "contribution to the energy function.\n",
    "The symmetric interaction matrix \\( \\mathbf{W} \\) is updated in gradient ascent form as \\( \\mathbf{W} \\leftarrow \\mathbf{W} + \\eta \\Delta \\mathbf{W} \\), where\n",
    "$$\n",
    "\\Delta \\mathbf{W} = \\left\\langle x_{ik} h_j \\right\\rangle_{\\text{data}} - \\left\\langle x_{ik} h_j \\right\\rangle_{\\text{model}},\n",
    "$$\n",
    "where the expectation \\( \\left\\langle x_{ik} h_j \\right\\rangle_{\\text{data}} \\) defines the frequency with which movie \\( i \\) with\n",
    "rating \\( k \\) and feature \\( j \\) are on together when the feature detectors are being driven\n",
    "by the observed user-rating data from the training set using Eq. (7.6.42), and\n",
    "\\( \\left\\langle x_{ik} h_j \\right\\rangle_{\\text{model}} \\) is the corresponding frequency when the hidden units are being driven\n",
    "by reconstructed images.\n",
    "To avoid computing \\( \\left\\langle \\cdot \\right\\rangle_{\\text{model}} \\), Salakhutdinov et al. proposed to follow an\n",
    "approximation to the gradient of a different objective function called “contrastive\n",
    "divergence” (CD):\n",
    "$$\n",
    "\\Delta \\mathbf{W} = \\left\\langle x_{ik} h_j \\right\\rangle_{\\text{data}} - \\left\\langle x_{ik} h_j \\right\\rangle_{T}.\n",
    "$$\n",
    "The expectation \\( \\left\\langle \\cdot \\right\\rangle_{T} \\) represents a distribution of samples from running the Gibbs\n",
    "sampler (Eqs. (7.6.41) and (7.6.42)), initialized at the data, for \\( T \\) full steps. \\( T \\)\n",
    "is typically set to one at the beginning of learning and increased as the learning\n",
    "converges.\n",
    "Restricted Boltzmann machines discussed above are assumed to use binary\n",
    "visible and hidden units, but many other types of unit can also be used. The main\n",
    "use of other types of unit is for dealing with data that is not well-modeled by binary\n",
    "(or logistic) visible units.\n",
    "The following are two typical units that can be used in restricted Boltzmann\n",
    "machines:\n",
    "1. **Softmax and multinomial units**: For a binary unit, the probability of turning on is\n",
    "given by the logistic sigmoid function of its total input \\( x \\):\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + \\exp(-x)}.\n",
    "$$\n",
    "The energy contributed by the unit is \\( -x \\) if it is on and 0 if it is off. This logistic\n",
    "sigmoid function of two states can be generalized to \\( K \\) alternative states, i.e.,\n",
    "$$\n",
    "p_j = \\frac{\\exp(x_j)}{\\sum_{i=1}^{K} \\exp(x_i)},\n",
    "$$\n",
    "which is often called a “softmax” unit. A further generalization of the softmax\n",
    "unit is to sample \\( N \\) times (with replacement) from the probability distribution\n",
    "instead of just sampling once. The \\( K \\) different states can then have integer values\n",
    "bigger than 1, but the values must add to \\( N \\). This is called a multinomial unit and\n",
    "the learning rule is again unchanged.\n",
    "2. **Gaussian visible units**: The binary visible units are replaced by linear units with\n",
    "independent Gaussian noise. In this case, the energy function becomes\n",
    "$$\n",
    "E(\\mathbf{x}, \\mathbf{h}) = \\sum_{j \\in \\text{visible}} \\frac{(x_j - b_j)^2}{2\\sigma_j^2} - \\sum_{i \\in \\text{hidden}} c_i h_i - \\sum_{i, j} \\frac{h_i x_j W_{ij}}{\\sigma_j},\n",
    "$$\n",
    "where \\( \\sigma_i \\) is the standard deviation of the Gaussian noise for visible unit \\( i \\)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e0c4062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Error: 0.2516118916840019\n",
      "Epoch: 100, Error: 0.2100911991787205\n",
      "Epoch: 200, Error: 0.17097789024114401\n",
      "Epoch: 300, Error: 0.1743201215459293\n",
      "Epoch: 400, Error: 0.17543284017863472\n",
      "Epoch: 500, Error: 0.16911788950219805\n",
      "Epoch: 600, Error: 0.16847397695502725\n",
      "Epoch: 700, Error: 0.1631449462565153\n",
      "Epoch: 800, Error: 0.17004938994255508\n",
      "Epoch: 900, Error: 0.1696976419625688\n",
      "[[0.54273148 0.52167021 0.75622622 0.47379943 0.6223005  0.36373293]\n",
      " [0.5501198  0.52491673 0.76816804 0.47514922 0.62169175 0.3540986 ]\n",
      " [0.12268186 0.75464447 0.35179849 0.73827165 0.10278767 0.10575589]\n",
      " [0.43347516 0.55603284 0.67441495 0.51355194 0.53572903 0.33493186]\n",
      " [0.54263147 0.52833697 0.76519059 0.47838252 0.6158852  0.35026675]\n",
      " [0.54724742 0.52375173 0.76403208 0.47457224 0.6223705  0.35778298]\n",
      " [0.54016275 0.52280166 0.7545462  0.4750792  0.61944895 0.36223455]\n",
      " [0.54447509 0.52269214 0.75956962 0.47424302 0.62213869 0.36099225]\n",
      " [0.32168784 0.41862602 0.26427066 0.43654352 0.61493032 0.67394453]\n",
      " [0.56088462 0.52133768 0.75525926 0.47827354 0.59759127 0.34842994]\n",
      " [0.54994186 0.52484773 0.76805003 0.47506721 0.62194897 0.35438   ]\n",
      " [0.22362745 0.36054609 0.09700099 0.41341201 0.61545163 0.81250167]\n",
      " [0.47744376 0.49910939 0.6379104  0.46813217 0.61251184 0.43643222]\n",
      " [0.76777552 0.40255838 0.21571439 0.55386298 0.08818172 0.26977064]\n",
      " [0.5501198  0.52491673 0.76816804 0.47514922 0.62169175 0.3540986 ]\n",
      " [0.54631476 0.52151927 0.75557596 0.47474185 0.61701854 0.36072124]\n",
      " [0.54869641 0.52509081 0.76743484 0.47526681 0.62181343 0.35454182]\n",
      " [0.26452448 0.41397163 0.180516   0.44563694 0.57586947 0.70419506]\n",
      " [0.69319061 0.49866935 0.66300526 0.51626807 0.34206877 0.25747379]\n",
      " [0.54862318 0.52446291 0.76650858 0.47485014 0.62236505 0.35583489]\n",
      " [0.5501198  0.52491673 0.76816804 0.47514922 0.62169175 0.3540986 ]\n",
      " [0.56790027 0.52141718 0.75510982 0.48017845 0.58753778 0.34212172]\n",
      " [0.96343509 0.39131851 0.16867036 0.6799117  0.00433316 0.0515822 ]\n",
      " [0.49608133 0.55638246 0.72966241 0.51225531 0.52970922 0.30309328]\n",
      " [0.54800379 0.52408563 0.7652603  0.47470722 0.62232594 0.35680057]\n",
      " [0.23408783 0.38386438 0.10697738 0.43863285 0.54761995 0.76206155]\n",
      " [0.94082739 0.44285766 0.17791268 0.70275355 0.00466041 0.04641848]\n",
      " [0.80548992 0.47307895 0.540271   0.55704543 0.14246272 0.18019563]\n",
      " [0.54994186 0.52484773 0.76805003 0.47506721 0.62194897 0.35438   ]\n",
      " [0.69319061 0.49866935 0.66300526 0.51626807 0.34206877 0.25747379]\n",
      " [0.54994186 0.52484773 0.76805003 0.47506721 0.62194897 0.35438   ]\n",
      " [0.46437591 0.48576451 0.59621507 0.46081113 0.61976364 0.47098312]\n",
      " [0.26452448 0.41397163 0.180516   0.44563694 0.57586947 0.70419506]\n",
      " [0.54862318 0.52446291 0.76650858 0.47485014 0.62236505 0.35583489]\n",
      " [0.48024866 0.56089939 0.73582181 0.51165715 0.54611084 0.30691855]\n",
      " [0.92619916 0.42504137 0.28723007 0.63522744 0.01765634 0.08512389]\n",
      " [0.02461959 0.89463622 0.13128291 0.89453751 0.00837939 0.02013366]\n",
      " [0.15581731 0.74531997 0.52758137 0.70243452 0.18451933 0.11938815]\n",
      " [0.72655943 0.4941898  0.62302495 0.53236737 0.26182241 0.22773567]\n",
      " [0.54869641 0.52509081 0.76743484 0.47526681 0.62181343 0.35454182]\n",
      " [0.32168784 0.41862602 0.26427066 0.43654352 0.61493032 0.67394453]\n",
      " [0.3956828  0.45226661 0.42898288 0.44825555 0.61873664 0.57272876]\n",
      " [0.54800379 0.52408563 0.7652603  0.47470722 0.62232594 0.35680057]\n",
      " [0.49608133 0.55638246 0.72966241 0.51225531 0.52970922 0.30309328]\n",
      " [0.54273148 0.52167021 0.75622622 0.47379943 0.6223005  0.36373293]\n",
      " [0.63827554 0.50948788 0.70762833 0.50016892 0.45082376 0.29350176]\n",
      " [0.54724742 0.52375173 0.76403208 0.47457224 0.6223705  0.35778298]\n",
      " [0.20255836 0.7163592  0.20926881 0.75785825 0.0379603  0.08444195]\n",
      " [0.56790027 0.52141718 0.75510982 0.48017845 0.58753778 0.34212172]\n",
      " [0.22362745 0.36054609 0.09700099 0.41341201 0.61545163 0.81250167]\n",
      " [0.56088462 0.52133768 0.75525926 0.47827354 0.59759127 0.34842994]\n",
      " [0.5501198  0.52491673 0.76816804 0.47514922 0.62169175 0.3540986 ]\n",
      " [0.54447509 0.52269214 0.75956962 0.47424302 0.62213869 0.36099225]\n",
      " [0.5501198  0.52491673 0.76816804 0.47514922 0.62169175 0.3540986 ]\n",
      " [0.24493485 0.37362929 0.12486515 0.41857503 0.61575353 0.78485861]\n",
      " [0.54447509 0.52269214 0.75956962 0.47424302 0.62213869 0.36099225]\n",
      " [0.92619916 0.42504137 0.28723007 0.63522744 0.01765634 0.08512389]\n",
      " [0.94082739 0.44285766 0.17791268 0.70275355 0.00466041 0.04641848]\n",
      " [0.54994186 0.52484773 0.76805003 0.47506721 0.62194897 0.35438   ]\n",
      " [0.24493485 0.37362929 0.12486515 0.41857503 0.61575353 0.78485861]\n",
      " [0.02461959 0.89463622 0.13128291 0.89453751 0.00837939 0.02013366]\n",
      " [0.54840508 0.52489701 0.76683334 0.47518642 0.6218194  0.35503066]\n",
      " [0.42732227 0.46700057 0.50411305 0.45391463 0.61825828 0.52689924]\n",
      " [0.54869641 0.52509081 0.76743484 0.47526681 0.62181343 0.35454182]\n",
      " [0.46977321 0.48746688 0.60697592 0.46121391 0.62051329 0.46502104]\n",
      " [0.54447509 0.52269214 0.75956962 0.47424302 0.62213869 0.36099225]\n",
      " [0.76777552 0.40255838 0.21571439 0.55386298 0.08818172 0.26977064]\n",
      " [0.5376364  0.51928255 0.74712795 0.47291373 0.62220106 0.37055471]\n",
      " [0.55042053 0.52479858 0.76786734 0.47519549 0.62117201 0.35401208]\n",
      " [0.54273148 0.52167021 0.75622622 0.47379943 0.6223005  0.36373293]\n",
      " [0.54002792 0.51699389 0.73832774 0.47382555 0.6127069  0.37077793]\n",
      " [0.22494873 0.35932536 0.09142688 0.41650043 0.59771864 0.81020713]\n",
      " [0.46977321 0.48746688 0.60697592 0.46121391 0.62051329 0.46502104]\n",
      " [0.54994186 0.52484773 0.76805003 0.47506721 0.62194897 0.35438   ]\n",
      " [0.54991284 0.52492064 0.76830141 0.47505353 0.62217174 0.35432779]\n",
      " [0.02461959 0.89463622 0.13128291 0.89453751 0.00837939 0.02013366]\n",
      " [0.94822432 0.40106079 0.21420727 0.65343072 0.00901048 0.07021072]\n",
      " [0.46437591 0.48576451 0.59621507 0.46081113 0.61976364 0.47098312]\n",
      " [0.48957293 0.49651147 0.64994775 0.46467601 0.62025939 0.43710594]\n",
      " [0.5376364  0.51928255 0.74712795 0.47291373 0.62220106 0.37055471]\n",
      " [0.02461959 0.89463622 0.13128291 0.89453751 0.00837939 0.02013366]\n",
      " [0.5376364  0.51928255 0.74712795 0.47291373 0.62220106 0.37055471]\n",
      " [0.54994186 0.52484773 0.76805003 0.47506721 0.62194897 0.35438   ]\n",
      " [0.80548992 0.47307895 0.540271   0.55704543 0.14246272 0.18019563]\n",
      " [0.15581731 0.74531997 0.52758137 0.70243452 0.18451933 0.11938815]\n",
      " [0.96343509 0.39131851 0.16867036 0.6799117  0.00433316 0.0515822 ]\n",
      " [0.13821434 0.5554181  0.15205685 0.57544252 0.32466973 0.48035085]\n",
      " [0.54016275 0.52280166 0.7545462  0.4750792  0.61944895 0.36223455]\n",
      " [0.02461959 0.89463622 0.13128291 0.89453751 0.00837939 0.02013366]\n",
      " [0.53592194 0.5200838  0.74457039 0.47430508 0.61796841 0.36881295]\n",
      " [0.54869641 0.52509081 0.76743484 0.47526681 0.62181343 0.35454182]\n",
      " [0.22362745 0.36054609 0.09700099 0.41341201 0.61545163 0.81250167]\n",
      " [0.22362745 0.36054609 0.09700099 0.41341201 0.61545163 0.81250167]\n",
      " [0.53310885 0.51714773 0.73885025 0.47211622 0.62213282 0.37668496]\n",
      " [0.54907798 0.52460701 0.76716459 0.47489328 0.62238863 0.35535887]\n",
      " [0.5563711  0.52313764 0.76184184 0.47691383 0.60863429 0.35051798]\n",
      " [0.5563711  0.52313764 0.76184184 0.47691383 0.60863429 0.35051798]\n",
      " [0.54862318 0.52446291 0.76650858 0.47485014 0.62236505 0.35583489]\n",
      " [0.96343509 0.39131851 0.16867036 0.6799117  0.00433316 0.0515822 ]\n",
      " [0.23408783 0.38386438 0.10697738 0.43863285 0.54761995 0.76206155]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class RBM:\n",
    "    def __init__(self, n_visible, n_hidden, learning_rate=0.01, n_iterations=1000):\n",
    "        self.n_visible = n_visible\n",
    "        self.n_hidden = n_hidden\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.W = np.random.randn(n_visible, n_hidden) * 0.1\n",
    "        self.b = np.zeros(n_visible)\n",
    "        self.c = np.zeros(n_hidden)\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def train(self, data):\n",
    "        for epoch in range(self.n_iterations):\n",
    "            for x in data:\n",
    "                x = x.reshape((1, self.n_visible))  # Ensure x is a row vector\n",
    "\n",
    "                # Positive phase\n",
    "                h_prob = self.sigmoid(np.dot(x, self.W) + self.c)\n",
    "                h_state = (h_prob > np.random.rand(1, self.n_hidden)).astype(np.float32)\n",
    "\n",
    "                # Negative phase\n",
    "                x_reconstructed_prob = self.sigmoid(np.dot(h_state, self.W.T) + self.b)\n",
    "                x_reconstructed_state = (x_reconstructed_prob > np.random.rand(1, self.n_visible)).astype(np.float32)\n",
    "\n",
    "                h_reconstructed_prob = self.sigmoid(np.dot(x_reconstructed_state, self.W) + self.c)\n",
    "\n",
    "                # Update weights and biases\n",
    "                self.W += self.learning_rate * (np.dot(x.T, h_prob) - np.dot(x_reconstructed_state.T, h_reconstructed_prob))\n",
    "                self.b += self.learning_rate * (x - x_reconstructed_state).flatten()\n",
    "                self.c += self.learning_rate * (h_prob - h_reconstructed_prob).flatten()\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                error = np.mean((data - self.reconstruct(data)) ** 2)\n",
    "                print(f'Epoch: {epoch}, Error: {error}')\n",
    "\n",
    "    def reconstruct(self, data):\n",
    "        h_prob = self.sigmoid(np.dot(data, self.W) + self.c)\n",
    "        x_reconstructed_prob = self.sigmoid(np.dot(h_prob, self.W.T) + self.b)\n",
    "        return x_reconstructed_prob\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate some random binary data\n",
    "    np.random.seed(0)\n",
    "    data = np.random.randint(2, size=(100, 6))  # 100 samples, 6 visible units\n",
    "\n",
    "    rbm = RBM(n_visible=6, n_hidden=3, learning_rate=0.1, n_iterations=1000)\n",
    "    rbm.train(data)\n",
    "    \n",
    "    # Reconstruct the data\n",
    "    reconstructed_data = rbm.reconstruct(data)\n",
    "    print(reconstructed_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118d231e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
