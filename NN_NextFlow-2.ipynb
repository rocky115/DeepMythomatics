{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a667b281",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "'''Copyright (c) 2004- 2024 , Prof. Radhamadhab Dalai Odisha, India\n",
    "Author's email address :  rmdi115@gmail.com'''\n",
    "###################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c475cafc",
   "metadata": {},
   "source": [
    "###Softmax Regression and Softmax Function\n",
    "### Softmax Function\n",
    "The softmax function is used to convert a vector of raw scores (logits) into probabilities. For a vector \\( z \\) of length \\( K \\), the softmax function is defined as:\n",
    "\n",
    "$\n",
    "\\sigma(z)_j = \\frac{e^{z_j}}{\\sum_{k=1}^{K} e^{z_k}}\n",
    "$\n",
    "\n",
    "where:\n",
    "- $ \\sigma(z)_j $ is the probability of the  j-th class.\n",
    "- $ z_j $ is the j-th element of the input vector z.\n",
    "- K is the number of classes.\n",
    "\n",
    "### Cross-Entropy Loss for Multi-Class Classification\n",
    "The cross-entropy loss for multi-class classification is defined as:\n",
    "\n",
    "$\n",
    "H(y, \\hat{y}) = - \\frac{1}{T} \\sum_{t=1}^{T} \\sum_{k=1}^{K} y_{t,k} \\log(\\hat{y}_{t,k})\n",
    "$\n",
    "\n",
    "where:\n",
    "-  T is the total number of samples.\n",
    "-  K is the number of classes.\n",
    "-  $ y_{t,k} $ is a binary indicator (0 or 1) if class label k is the correct classification for sample t.\n",
    "- $ \\hat{y}_{t,k} $ is the predicted probability of class k for sample t.\n",
    "\n",
    "### Gradient of the Cross-Entropy Loss Function\n",
    "The gradient of the cross-entropy loss function with respect to the predicted probabilities $ \\hat{y}_t $ is:\n",
    "\n",
    "$\n",
    "\\frac{\\partial H(y, \\hat{y})}{\\partial \\hat{y}_t} = \\frac{\\hat{y}_t - y_t}{T}\n",
    "$\n",
    "\n",
    "The gradient with respect to the weights w is:\n",
    "\n",
    "$\n",
    "\\nabla_w H(y, \\hat{y}) = \\frac{1}{T} \\sum_{t=1}^{T} ( \\hat{y}_t - y_t ) x_t\n",
    "$\n",
    "\n",
    "### Weight Update Rule\n",
    "The weight update rule using gradient descent for softmax regression is:\n",
    "\n",
    "$\n",
    "w^{k+1} = w^k - \\eta \\nabla_w H(y, \\hat{y})\n",
    "$\n",
    "\n",
    "where:\n",
    "- $ \\eta $ is the learning rate.\n",
    "- $ w^k $ is the weight vector at iteration k.\n",
    "- $ \\nabla_w H(y, \\hat{y}) $ is the gradient of the KL divergence (cross-entropy loss) with respect to the weights.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0346c674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [[0.68065166 0.18928148 0.13006686]\n",
      " [0.49600597 0.26347298 0.24052105]\n",
      " [0.30814999 0.31266358 0.37918643]\n",
      " [0.1649971  0.3197845  0.5152184 ]]\n",
      "Cross-entropy loss: 0.727915949036652\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Softmax function\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # stability improvement\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "# Cross-entropy loss function for softmax regression\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # Avoid log(0)\n",
    "    return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))\n",
    "\n",
    "# Gradient of the cross-entropy loss function\n",
    "def gradient_cross_entropy_loss(X, y_true, y_pred):\n",
    "    return np.dot(X.T, (y_pred - y_true)) / X.shape[0]\n",
    "\n",
    "# Softmax regression training function\n",
    "def softmax_regression_train(X, y, learning_rate, epochs):\n",
    "    n_samples, n_features = X.shape\n",
    "    n_classes = y.shape[1]\n",
    "    weights = np.zeros((n_features, n_classes))\n",
    "    bias = np.zeros(n_classes)\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        # Linear combination of features and weights\n",
    "        linear_model = np.dot(X, weights) + bias\n",
    "\n",
    "        # Predictions using softmax function\n",
    "        y_pred = softmax(linear_model)\n",
    "\n",
    "        # Compute the gradient of the loss function\n",
    "        gradient = gradient_cross_entropy_loss(X, y, y_pred)\n",
    "        bias_gradient = np.mean(y_pred - y, axis=0)\n",
    "\n",
    "        # Update weights and bias using gradient descent\n",
    "        weights -= learning_rate * gradient\n",
    "        bias -= learning_rate * bias_gradient\n",
    "\n",
    "    return weights, bias\n",
    "\n",
    "# Softmax regression prediction function\n",
    "def softmax_regression_predict(X, weights, bias):\n",
    "    linear_model = np.dot(X, weights) + bias\n",
    "    return softmax(linear_model)\n",
    "\n",
    "# Example data (one-hot encoded labels)\n",
    "X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n",
    "y = np.array([[1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 0, 1]])\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.01\n",
    "epochs = 1000\n",
    "weights, bias = softmax_regression_train(X, y, learning_rate, epochs)\n",
    "\n",
    "# Prediction\n",
    "y_pred = softmax_regression_predict(X, weights, bias)\n",
    "print(\"Predictions:\", y_pred)\n",
    "\n",
    "# Loss\n",
    "loss = cross_entropy_loss(y, y_pred)\n",
    "print(\"Cross-entropy loss:\", loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d7a217",
   "metadata": {},
   "source": [
    "### Softmax Regression\n",
    "\n",
    "Given a set of training samples {($x_1$, $y_1$), $\\ldots$, ($x_T$, $y_T$)}, where $x_t \\in \\mathbb{R}^n$ and $y_t \\in \\{1, \\ldots, k\\} $, we design a weight matrix W:\n",
    "\n",
    "$\n",
    "W = [w_1, w_2, \\ldots, w_k]\n",
    "$\n",
    "\n",
    "### Softmax Function\n",
    "\n",
    "The softmax function converts a vector of raw scores (logits) into probabilities. For a vector z of length k, the softmax function is defined as:\n",
    "\n",
    "$\n",
    "\\sigma(z)_j = \\frac{e^{z_j}}{\\sum_{k=1}^{K} e^{z_k}}\n",
    "$\n",
    "\n",
    "where:\n",
    "- $ \\sigma(z)_j $ is the probability of the j-th class.\n",
    "- $ z_j $ is the j-th element of the input vector z.\n",
    "- k is the number of classes.\n",
    "\n",
    "### Cross-Entropy Loss for Multi-Class Classification\n",
    "\n",
    "The cross-entropy loss for multi-class classification is defined as:\n",
    "\n",
    "$\n",
    "H(y, \\hat{y}) = - \\frac{1}{T} \\sum_{t=1}^{T} \\sum_{i=1}^{k} y_{t,i} \\log(\\hat{y}_{t,i})\n",
    "$\n",
    "\n",
    "where:\n",
    "- T  is the total number of samples.\n",
    "- k  is the number of classes.\n",
    "- y_{t,i}  is a binary indicator (0 or 1) if class label i is the correct classification for sample t .\n",
    "- $\\hat{y}_{t,i} $ is the predicted probability of class i for sample t.\n",
    "\n",
    "### Gradient of the Cross-Entropy Loss Function\n",
    "\n",
    "The gradient of the cross-entropy loss function with respect to the predicted probabilities $\\hat{y}_t$ is:\n",
    "\n",
    "$\n",
    "\\frac{\\partial H(y, \\hat{y})}{\\partial \\hat{y}_t} = \\frac{\\hat{y}_t - y_t}{T}\n",
    "$\n",
    "\n",
    "The gradient with respect to the weights W is:\n",
    "\n",
    "$\n",
    "\\nabla_W H(y, \\hat{y}) = \\frac{1}{T} \\sum_{t=1}^{T} X_t^T (\\hat{y}_t - y_t)\n",
    "$\n",
    "\n",
    "### Weight Update Rule\n",
    "\n",
    "The weight update rule using gradient descent for softmax regression is:\n",
    "\n",
    "$\n",
    "W^{k+1} = W^k - \\eta \\nabla_W H(y, \\hat{y})\n",
    "$\n",
    "\n",
    "where:\n",
    "- $ \\eta $ is the learning rate.\n",
    "- $W^k $ is the weight vector at iteration k.\n",
    "- $\\nabla_W H(y, \\hat{y}) $ is the gradient of the KL divergence (cross-entropy loss) with respect to the weights.\n",
    "\n",
    "### RNN Output\n",
    "\n",
    "For an RNN, its output vector $g(Wx_t)$ is directly used as the hypothetical function. Letting $ z = Wx_t $, the hypothetical function is:\n",
    "\n",
    "$\n",
    "h_W(x_t) = g(Wx_t)\n",
    "$\n",
    "\n",
    "### Softmax Function for RNN Output\n",
    "\n",
    "The softmax function for the vector z is:\n",
    "\n",
    "$\n",
    "\\sigma(z)_j = \\frac{e^{z_j}}{\\sum_{i=1}^{k} e^{z_i}}\n",
    "$\n",
    "\n",
    "### Cross-Entropy Loss for RNN\n",
    "\n",
    "The cross-entropy loss for a single training example in the context of RNNs is:\n",
    "\n",
    "$\n",
    "H(y_t, \\hat{y}_t) = - \\sum_{i=1}^{k} y_{t,i} \\log(\\hat{y}_{t,i})\n",
    "$\n",
    "\n",
    "### Weight Update Rule for RNN\n",
    "\n",
    "The weight update rule using gradient descent for updating the weights W in an RNN is:\n",
    "\n",
    "$\n",
    "W^{k+1} = W^k - \\eta \\nabla_W H(y, \\hat{y})\n",
    "$\n",
    "\n",
    "where:\n",
    "- $\\eta$  is the learning rate.\n",
    "- $W^k $ is the weight matrix at iteration $ k $.\n",
    "- $\\nabla_W H(y, \\hat{y}) $ is the gradient of the cross-entropy loss with respect to the weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad244fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/1000, Loss: 1.0435256411205036\n",
      "Epoch 200/1000, Loss: 0.9840876611465466\n",
      "Epoch 300/1000, Loss: 0.869546581208506\n",
      "Epoch 400/1000, Loss: 0.7182150784529611\n",
      "Epoch 500/1000, Loss: 0.6425357646396047\n",
      "Epoch 600/1000, Loss: 0.5854458052724199\n",
      "Epoch 700/1000, Loss: 0.5113694386531344\n",
      "Epoch 800/1000, Loss: 0.4230903768958066\n",
      "Epoch 900/1000, Loss: 0.3280583317952764\n",
      "Epoch 1000/1000, Loss: 0.20328721850364254\n",
      "Predictions: [[[0.7384881  0.15151101 0.11000089]]\n",
      "\n",
      " [[0.925965   0.07198433 0.00205067]]\n",
      "\n",
      " [[0.19359036 0.75806083 0.04834881]]\n",
      "\n",
      " [[0.00288275 0.05147516 0.94564209]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the softmax function\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))  # stability improvement\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "# Define the cross-entropy loss function\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # Avoid log(0)\n",
    "    return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))\n",
    "\n",
    "# Define the RNN cell\n",
    "class SimpleRNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.Wx = np.random.randn(input_size, hidden_size) * 0.01\n",
    "        self.Wh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "        self.Wo = np.random.randn(hidden_size, output_size) * 0.01\n",
    "        self.bh = np.zeros((1, hidden_size))\n",
    "        self.bo = np.zeros((1, output_size))\n",
    "        \n",
    "        # Initialize hidden state\n",
    "        self.h = np.zeros((1, hidden_size))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.h = np.tanh(np.dot(x, self.Wx) + np.dot(self.h, self.Wh) + self.bh)\n",
    "        o = np.dot(self.h, self.Wo) + self.bo\n",
    "        return softmax(o)\n",
    "    \n",
    "    def backward(self, x, y_true, y_pred):\n",
    "        # Compute the gradient of the loss with respect to the output\n",
    "        dL_dO = y_pred - y_true\n",
    "        \n",
    "        # Compute the gradients of Wo and bo\n",
    "        dL_dWo = np.dot(self.h.T, dL_dO)\n",
    "        dL_dbo = dL_dO\n",
    "        \n",
    "        # Backpropagation through time (for simplicity, one time step)\n",
    "        dL_dh = np.dot(dL_dO, self.Wo.T)\n",
    "        dL_dWh = np.dot(self.h.T, dL_dh * (1 - self.h ** 2))\n",
    "        dL_dWx = np.dot(x.T, dL_dh * (1 - self.h ** 2))\n",
    "        dL_dbh = dL_dh * (1 - self.h ** 2)\n",
    "        \n",
    "        # Update the weights and biases\n",
    "        self.Wo -= self.learning_rate * dL_dWo\n",
    "        self.bo -= self.learning_rate * np.sum(dL_dbo, axis=0, keepdims=True)\n",
    "        self.Wh -= self.learning_rate * dL_dWh\n",
    "        self.Wx -= self.learning_rate * dL_dWx\n",
    "        self.bh -= self.learning_rate * np.sum(dL_dbh, axis=0, keepdims=True)\n",
    "\n",
    "# Training the RNN\n",
    "def train_rnn(rnn, X_train, y_train, epochs=1000):\n",
    "    for epoch in range(epochs):\n",
    "        loss = 0\n",
    "        for x, y in zip(X_train, y_train):\n",
    "            x = x.reshape(1, -1)\n",
    "            y = y.reshape(1, -1)\n",
    "            y_pred = rnn.forward(x)\n",
    "            loss += cross_entropy_loss(y, y_pred)\n",
    "            rnn.backward(x, y, y_pred)\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss/len(X_train)}')\n",
    "\n",
    "# Example data\n",
    "X_train = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n",
    "y_train = np.array([[1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 0, 1]])\n",
    "\n",
    "# Initialize RNN\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 5\n",
    "output_size = y_train.shape[1]\n",
    "learning_rate = 0.01\n",
    "\n",
    "rnn = SimpleRNN(input_size, hidden_size, output_size, learning_rate)\n",
    "\n",
    "# Train RNN\n",
    "train_rnn(rnn, X_train, y_train, epochs=1000)\n",
    "\n",
    "# Test the RNN\n",
    "def predict_rnn(rnn, X):\n",
    "    y_pred = []\n",
    "    for x in X:\n",
    "        x = x.reshape(1, -1)\n",
    "        y_pred.append(rnn.forward(x))\n",
    "    return np.array(y_pred)\n",
    "\n",
    "y_pred = predict_rnn(rnn, X_train)\n",
    "print(\"Predictions:\", y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5d54a29",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7567/1370028221.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;31m# Train RNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m \u001b[0mtrain_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;31m# Test the RNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_7567/1370028221.py\u001b[0m in \u001b[0;36mtrain_rnn\u001b[0;34m(rnn, X_train, y_train, epochs)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_7567/1370028221.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmat_vec_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmat_vec_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvec_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmat_vec_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_7567/1370028221.py\u001b[0m in \u001b[0;36mvec_add\u001b[0;34m(vec1, vec2)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Vector addition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvec_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvec1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvec2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Element-wise vector multiplication\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_7567/1370028221.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Vector addition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvec_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvec1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvec2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Element-wise vector multiplication\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Define the softmax function\n",
    "def softmax(x):\n",
    "    exp_x = [math.exp(i) for i in x]\n",
    "    sum_exp_x = sum(exp_x)\n",
    "    return [i / sum_exp_x for i in exp_x]\n",
    "\n",
    "# Define the cross-entropy loss function\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    epsilon = 1e-15\n",
    "    y_pred = [max(min(i, 1 - epsilon), epsilon) for i in y_pred]  # Avoid log(0)\n",
    "    return -sum([y_true[i] * math.log(y_pred[i]) for i in range(len(y_true))])\n",
    "\n",
    "# Matrix-vector multiplication\n",
    "def mat_vec_mul(mat, vec):\n",
    "    return [sum(mat[i][j] * vec[j] for j in range(len(vec))) for i in range(len(mat))]\n",
    "\n",
    "# Vector addition\n",
    "def vec_add(vec1, vec2):\n",
    "    return [vec1[i] + vec2[i] for i in range(len(vec1))]\n",
    "\n",
    "# Element-wise vector multiplication\n",
    "def vec_mul(vec1, vec2):\n",
    "    return [vec1[i] * vec2[i] for i in range(len(vec1))]\n",
    "\n",
    "# Tanh activation function\n",
    "def tanh(x):\n",
    "    return [(math.exp(2 * i) - 1) / (math.exp(2 * i) + 1) for i in x]\n",
    "\n",
    "# Derivative of tanh activation function\n",
    "def tanh_derivative(x):\n",
    "    return [1 - i**2 for i in x]\n",
    "\n",
    "# Simple RNN cell\n",
    "class SimpleRNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.Wx = [[0.01 for _ in range(hidden_size)] for _ in range(input_size)]\n",
    "        self.Wh = [[0.01 for _ in range(hidden_size)] for _ in range(hidden_size)]\n",
    "        self.Wo = [[0.01 for _ in range(output_size)] for _ in range(hidden_size)]\n",
    "        self.bh = [0.0 for _ in range(hidden_size)]\n",
    "        self.bo = [0.0 for _ in range(output_size)]\n",
    "        \n",
    "        # Initialize hidden state\n",
    "        self.h = [0.0 for _ in range(hidden_size)]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.h = tanh(vec_add(mat_vec_mul(self.Wx, x), vec_add(mat_vec_mul(self.Wh, self.h), self.bh)))\n",
    "        o = vec_add(mat_vec_mul(self.Wo, self.h), self.bo)\n",
    "        return softmax(o)\n",
    "    \n",
    "    def backward(self, x, y_true, y_pred):\n",
    "        # Compute the gradient of the loss with respect to the output\n",
    "        dL_dO = [y_pred[i] - y_true[i] for i in range(len(y_true))]\n",
    "        \n",
    "        # Compute the gradients of Wo and bo\n",
    "        dL_dWo = [[self.h[i] * dL_dO[j] for j in range(len(dL_dO))] for i in range(len(self.h))]\n",
    "        dL_dbo = dL_dO\n",
    "        \n",
    "        # Backpropagation through time (for simplicity, one time step)\n",
    "        dL_dh = mat_vec_mul(self.Wo, dL_dO)\n",
    "        dL_dh = vec_mul(dL_dh, tanh_derivative(self.h))\n",
    "        \n",
    "        dL_dWh = [[self.h[i] * dL_dh[j] for j in range(len(dL_dh))] for i in range(len(self.h))]\n",
    "        dL_dWx = [[x[i] * dL_dh[j] for j in range(len(dL_dh))] for i in range(len(x))]\n",
    "        dL_dbh = dL_dh\n",
    "        \n",
    "        # Update the weights and biases\n",
    "        self.Wo = [[self.Wo[i][j] - self.learning_rate * dL_dWo[i][j] for j in range(len(dL_dWo[0]))] for i in range(len(self.Wo))]\n",
    "        self.bo = [self.bo[i] - self.learning_rate * dL_dbo[i] for i in range(len(self.bo))]\n",
    "        self.Wh = [[self.Wh[i][j] - self.learning_rate * dL_dWh[i][j] for j in range(len(dL_dWh[0]))] for i in range(len(self.Wh))]\n",
    "        self.Wx = [[self.Wx[i][j] - self.learning_rate * dL_dWx[i][j] for j in range(len(dL_dWx[0]))] for i in range(len(self.Wx))]\n",
    "        self.bh = [self.bh[i] - self.learning_rate * dL_dbh[i] for i in range(len(self.bh))]\n",
    "\n",
    "# Training the RNN\n",
    "def train_rnn(rnn, X_train, y_train, epochs=1000):\n",
    "    for epoch in range(epochs):\n",
    "        loss = 0\n",
    "        for x, y in zip(X_train, y_train):\n",
    "            y_pred = rnn.forward(x)\n",
    "            loss += cross_entropy_loss(y, y_pred)\n",
    "            rnn.backward(x, y, y_pred)\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss / len(X_train)}')\n",
    "\n",
    "# Example data\n",
    "X_train = [[1, 2], [2, 3], [3, 4], [4, 5]]\n",
    "y_train = [[1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 0, 1]]\n",
    "\n",
    "# Initialize RNN\n",
    "input_size = len(X_train[0])\n",
    "hidden_size = 5\n",
    "output_size = len(y_train[0])\n",
    "learning_rate = 0.01\n",
    "\n",
    "rnn = SimpleRNN(input_size, hidden_size, output_size, learning_rate)\n",
    "\n",
    "# Train RNN\n",
    "train_rnn(rnn, X_train, y_train, epochs=1000)\n",
    "\n",
    "# Test the RNN\n",
    "def predict_rnn(rnn, X):\n",
    "    y_pred = []\n",
    "    for x in X:\n",
    "        y_pred.append(rnn.forward(x))\n",
    "    return y_pred\n",
    "\n",
    "y_pred = predict_rnn(rnn, X_train)\n",
    "print(\"Predictions:\", y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78ed6335",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7567/1370028221.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;31m# Train RNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m \u001b[0mtrain_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;31m# Test the RNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_7567/1370028221.py\u001b[0m in \u001b[0;36mtrain_rnn\u001b[0;34m(rnn, X_train, y_train, epochs)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_7567/1370028221.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmat_vec_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmat_vec_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvec_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmat_vec_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_7567/1370028221.py\u001b[0m in \u001b[0;36mvec_add\u001b[0;34m(vec1, vec2)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Vector addition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvec_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvec1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvec2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Element-wise vector multiplication\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_7567/1370028221.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Vector addition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvec_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvec1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvec2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Element-wise vector multiplication\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Define the softmax function\n",
    "def softmax(x):\n",
    "    exp_x = [math.exp(i) for i in x]\n",
    "    sum_exp_x = sum(exp_x)\n",
    "    return [i / sum_exp_x for i in exp_x]\n",
    "\n",
    "# Define the cross-entropy loss function\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    epsilon = 1e-15\n",
    "    y_pred = [max(min(i, 1 - epsilon), epsilon) for i in y_pred]  # Avoid log(0)\n",
    "    return -sum([y_true[i] * math.log(y_pred[i]) for i in range(len(y_true))])\n",
    "\n",
    "# Matrix-vector multiplication\n",
    "def mat_vec_mul(mat, vec):\n",
    "    return [sum(mat[i][j] * vec[j] for j in range(len(vec))) for i in range(len(mat))]\n",
    "\n",
    "# Vector addition\n",
    "def vec_add(vec1, vec2):\n",
    "    return [vec1[i] + vec2[i] for i in range(len(vec1))]\n",
    "\n",
    "# Element-wise vector multiplication\n",
    "def vec_mul(vec1, vec2):\n",
    "    return [vec1[i] * vec2[i] for i in range(len(vec1))]\n",
    "\n",
    "# Tanh activation function\n",
    "def tanh(x):\n",
    "    return [(math.exp(2 * i) - 1) / (math.exp(2 * i) + 1) for i in x]\n",
    "\n",
    "# Derivative of tanh activation function\n",
    "def tanh_derivative(x):\n",
    "    return [1 - i**2 for i in x]\n",
    "\n",
    "# Simple RNN cell\n",
    "class SimpleRNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.Wx = [[0.01 for _ in range(hidden_size)] for _ in range(input_size)]\n",
    "        self.Wh = [[0.01 for _ in range(hidden_size)] for _ in range(hidden_size)]\n",
    "        self.Wo = [[0.01 for _ in range(output_size)] for _ in range(hidden_size)]\n",
    "        self.bh = [0.0 for _ in range(hidden_size)]\n",
    "        self.bo = [0.0 for _ in range(output_size)]\n",
    "        \n",
    "        # Initialize hidden state\n",
    "        self.h = [0.0 for _ in range(hidden_size)]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.h = tanh(vec_add(mat_vec_mul(self.Wx, x), vec_add(mat_vec_mul(self.Wh, self.h), self.bh)))\n",
    "        o = vec_add(mat_vec_mul(self.Wo, self.h), self.bo)\n",
    "        return softmax(o)\n",
    "    \n",
    "    def backward(self, x, y_true, y_pred):\n",
    "        # Compute the gradient of the loss with respect to the output\n",
    "        dL_dO = [y_pred[i] - y_true[i] for i in range(len(y_true))]\n",
    "        \n",
    "        # Compute the gradients of Wo and bo\n",
    "        dL_dWo = [[self.h[i] * dL_dO[j] for j in range(len(dL_dO))] for i in range(len(self.h))]\n",
    "        dL_dbo = dL_dO\n",
    "        \n",
    "        # Backpropagation through time (for simplicity, one time step)\n",
    "        dL_dh = mat_vec_mul(self.Wo, dL_dO)\n",
    "        dL_dh = vec_mul(dL_dh, tanh_derivative(self.h))\n",
    "        \n",
    "        dL_dWh = [[self.h[i] * dL_dh[j] for j in range(len(dL_dh))] for i in range(len(self.h))]\n",
    "        dL_dWx = [[x[i] * dL_dh[j] for j in range(len(dL_dh))] for i in range(len(x))]\n",
    "        dL_dbh = dL_dh\n",
    "        \n",
    "        # Update the weights and biases\n",
    "        self.Wo = [[self.Wo[i][j] - self.learning_rate * dL_dWo[i][j] for j in range(len(dL_dWo[0]))] for i in range(len(self.Wo))]\n",
    "        self.bo = [self.bo[i] - self.learning_rate * dL_dbo[i] for i in range(len(self.bo))]\n",
    "        self.Wh = [[self.Wh[i][j] - self.learning_rate * dL_dWh[i][j] for j in range(len(dL_dWh[0]))] for i in range(len(self.Wh))]\n",
    "        self.Wx = [[self.Wx[i][j] - self.learning_rate * dL_dWx[i][j] for j in range(len(dL_dWx[0]))] for i in range(len(self.Wx))]\n",
    "        self.bh = [self.bh[i] - self.learning_rate * dL_dbh[i] for i in range(len(self.bh))]\n",
    "\n",
    "# Training the RNN\n",
    "def train_rnn(rnn, X_train, y_train, epochs=1000):\n",
    "    for epoch in range(epochs):\n",
    "        loss = 0\n",
    "        for x, y in zip(X_train, y_train):\n",
    "            y_pred = rnn.forward(x)\n",
    "            loss += cross_entropy_loss(y, y_pred)\n",
    "            rnn.backward(x, y, y_pred)\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss / len(X_train)}')\n",
    "\n",
    "# Example data\n",
    "X_train = [[1, 2], [2, 3], [3, 4], [4, 5]]\n",
    "y_train = [[1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 0, 1]]\n",
    "\n",
    "# Initialize RNN\n",
    "input_size = len(X_train[0])\n",
    "hidden_size = 5\n",
    "output_size = len(y_train[0])\n",
    "learning_rate = 0.01\n",
    "\n",
    "rnn = SimpleRNN(input_size, hidden_size, output_size, learning_rate)\n",
    "\n",
    "# Train RNN\n",
    "train_rnn(rnn, X_train, y_train, epochs=1000)\n",
    "\n",
    "# Test the RNN\n",
    "def predict_rnn(rnn, X):\n",
    "    y_pred = []\n",
    "    for x in X:\n",
    "        y_pred.append(rnn.forward(x))\n",
    "    return y_pred\n",
    "\n",
    "y_pred = predict_rnn(rnn, X_train)\n",
    "print(\"Predictions:\", y_pred)\n",
    "# give a try"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9284f078",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network Equations\n",
    "\n",
    "## Definitions and Variables\n",
    "- $ \\mathbf{x}_t \\in \\mathbb{R}^n $: Input vector to the hidden layer at time t.\n",
    "- $ \\mathbf{h}_t \\in \\mathbb{R}^l $: Output of hidden layer (hidden state) at time t.\n",
    "- $ \\hat{\\mathbf{y}}_t \\in \\mathbb{R}^m $: Output produced by RNN at time t.\n",
    "- $ \\mathbf{y} \\in \\mathbb{R}^m $: Desired output.\n",
    "- $\\mathbf{U} \\in \\mathbb{R}^{l \\times n} $: Input-hidden weight matrix.\n",
    "- $ \\mathbf{W} \\in \\mathbb{R}^{l \\times l} $: Hidden-hidden weight matrix.\n",
    "- $ \\mathbf{V} \\in \\mathbb{R}^{m \\times l} $: Hidden-output weight matrix.\n",
    "- $\\mathbf{b}_t \\in \\mathbb{R}^l $: Bias vectors (intercept terms) of hidden layers.\n",
    "- $\\mathbf{c}_t \\in \\mathbb{R}^m $: Bias vectors (intercept terms) of output layers.\n",
    "- $f(\\mathbf{h}) \\in \\mathbb{R} $: Elementwise activation function of hidden layer.\n",
    "- $ g(\\mathbf{y}) \\in \\mathbb{R} $: Elementwise activation function of output layer.\n",
    "\n",
    "## Feedforward Network (FFN) Dynamics\n",
    "\n",
    "### Hidden States for FFN:\n",
    "$$\n",
    "h_j(t) = f(\\text{net}_j), \\quad \\text{for } j = 1, \\ldots, l,\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{net}_j = \\sum_{i=1}^n U_{ji} x_i(t) + b_j(t), \\quad \\text{for } j = 1, \\ldots, l.\n",
    "$$\n",
    "\n",
    "### Output Nodes for FFN:\n",
    "$$\n",
    "y_k(t) = g(\\text{net}_k), \\quad \\text{for } k = 1, \\ldots, m,\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{net}_k = \\sum_{j=1}^l V_{kj} h_j(t) + c_j(t), \\quad \\text{for } k = 1, \\ldots, m.\n",
    "$$\n",
    "\n",
    "### FFN in Matrix-Vector Form:\n",
    "$$\n",
    "\\mathbf{h}_t = f(\\mathbf{U} \\mathbf{x}_t + \\mathbf{b}_t),\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{y}_t = g(\\mathbf{V} \\mathbf{h}_t + \\mathbf{c}_t).\n",
    "$$\n",
    "\n",
    "## Recurrent Neural Network (RNN) Dynamics\n",
    "\n",
    "### Hidden State Recurrence:\n",
    "$$\n",
    "h_j(t) = f(\\text{net}_j), \\quad \\text{for } j = 1, \\ldots, l,\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{net}_j = \\sum_{i=1}^n U_{ji} x_i(t) + \\sum_{\\tilde{j}=1}^l W_{j\\tilde{j}} h_{\\tilde{j}}(t-1) + b_j, \\quad \\text{for } j = 1, \\ldots, l.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8679c3c8",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network (RNN) Equations\n",
    "\n",
    "## Feedforward Network (FFN) Dynamics\n",
    "\n",
    "The dynamical system for FFNs can be written in matrix-vector form as:\n",
    "\n",
    "$$\n",
    "\\mathbf{h}_t = f(\\mathbf{U}_{hx} \\mathbf{x}_t + \\mathbf{b}_t),\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{y}_t = g(\\mathbf{V}_{yh} \\mathbf{h}_t + \\mathbf{c}_t).\n",
    "$$\n",
    "\n",
    "## Recurrent Neural Network (RNN) Dynamics\n",
    "\n",
    "An RNN is a neural network that simulates a discrete-time dynamical system with:\n",
    "- An input vector $ \\mathbf{x}_t = [x_1(t), \\ldots, x_n(t)]^T $,\n",
    "- An output vector $ \\mathbf{y}_t = [y_1(t), \\ldots, y_m(t)]^T $,\n",
    "- A hidden state vector $ \\mathbf{h}_t = [h_1(t), \\ldots, h_l(t)]^T $,\n",
    "\n",
    "where the subscript t represents the time step.\n",
    "\n",
    "RNNs have two recurrence forms. The general structure of a three-layer RNN with hidden state recurrence can be described by the following dynamical system:\n",
    "\n",
    "### Hidden States for Hidden State Recurrence\n",
    "\n",
    "$$\n",
    "h_j(t) = f(\\text{net}_j), \\quad \\text{for } j = 1, \\ldots, l,\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{net}_j = \\sum_{i=1}^n U_{ji} x_i(t) + \\sum_{\\tilde{j}=1}^l W_{j\\tilde{j}} h_{\\tilde{j}}(t - 1) + b_j, \\quad \\text{for } j = 1, \\ldots, l.\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{U}_{hx} $ is the input-hidden weight matrix,\n",
    "- $ \\mathbf{W}_{hh} $ is the hidden-hidden weight matrix,\n",
    "- $ \\mathbf{V}_{yh} $ is the hidden-output weight matrix,\n",
    "- $ \\mathbf{b}_t $ is the bias vector for the hidden layers,\n",
    "- $ \\mathbf{c}_t $ is the bias vector for the output layers,\n",
    "- f  is the activation function for the hidden layer,\n",
    "- g  is the activation function for the output layer.\n",
    "\n",
    "These equations capture the essence of how inputs are processed through the hidden states and how the recurrent connections influence the hidden states across time steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3bd3e1",
   "metadata": {},
   "source": [
    "# General Structure of a Regular Unidirectional Three-Layer RNN with Output Recurrence\n",
    "\n",
    "## Description\n",
    "The general structure of a regular unidirectional three-layer Recurrent Neural Network (RNN) with output recurrence can be visualized as having a delay line \\( z^{-1} \\). This structure can be depicted in two forms:\n",
    "1. **RNN with a delay line** (left side).\n",
    "2. **RNN unfolded in time** for two time steps (right side).\n",
    "\n",
    "### RNN with Delay Line\n",
    "In this structure, the RNN processes input sequences and has recurrent connections that influence the current hidden state and output based on the previous time step.\n",
    "\n",
    "### RNN Unfolded in Time\n",
    "When unfolded in time, the RNN's operations can be seen across multiple time steps, showing how the hidden states and outputs at each time step depend on the previous states and outputs.\n",
    "\n",
    "## Equations\n",
    "\n",
    "### Hidden States for Hidden State Recurrence\n",
    "The hidden state \\( h_j(t) \\) at time \\( t \\) is calculated as:\n",
    "\n",
    "$$\n",
    "h_j(t) = f(\\text{net}_j), \\quad \\text{for } j = 1, \\ldots, l,\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\text{net}_j = \\sum_{i=1}^n U_{ji} x_i(t) + \\sum_{\\tilde{j}=1}^l W_{j\\tilde{j}} h_{\\tilde{j}}(t - 1) + b_j, \\quad \\text{for } j = 1, \\ldots, l.\n",
    "$$\n",
    "\n",
    "### Output Nodes for Output Recurrence\n",
    "The output \\( y_k(t) \\) at time \\( t \\) is calculated as:\n",
    "\n",
    "$$\n",
    "y_k(t) = g(\\text{net}_k), \\quad \\text{for } k = 1, \\ldots, m,\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\text{net}_k = \\sum_{j=1}^l V_{kj} h_j(t) + \\sum_{\\tilde{k}=1}^m W'_{k\\tilde{k}} y_{\\tilde{k}}(t - 1) + c_k, \\quad \\text{for } k = 1, \\ldots, m.\n",
    "$$\n",
    "\n",
    "## Matrices and Vectors\n",
    "- $ \\mathbf{x}_t \\in \\mathbb{R}^n $: Input vector at time t.\n",
    "- $ \\mathbf{h}_t \\in \\mathbb{R}^l $: Hidden state vector at time  t .\n",
    "- $ \\mathbf{y}_t \\in \\mathbb{R}^m $: Output vector at time t.\n",
    "- $ \\mathbf{U} \\in \\mathbb{R}^{l \\times n} $: Input-hidden weight matrix.\n",
    "- $ \\mathbf{W} \\in \\mathbb{R}^{l \\times l} $: Hidden-hidden weight matrix.\n",
    "- $ \\mathbf{V} \\in \\mathbb{R}^{m \\times l} $: Hidden-output weight matrix.\n",
    "- $ \\mathbf{W'} \\in \\mathbb{R}^{m \\times m} $: Output-output weight matrix (for output recurrence).\n",
    "- $ \\mathbf{b}_t \\in \\mathbb{R}^l $: Bias vector for hidden layers.\n",
    "- $ \\mathbf{c}_t \\in \\mathbb{R}^m $: Bias vector for output layers.\n",
    "-  f : Activation function for the hidden layer.\n",
    "-  g : Activation function for the output layer.\n",
    "\n",
    "### Explanation\n",
    "1. **Hidden State Recurrence**: The hidden state at each time step t depends on the current input, the previous hidden state, and a bias term.\n",
    "2. **Output Recurrence**: The output at each time step t depends on the current hidden state, the previous output, and a bias term.\n",
    "\n",
    "These equations illustrate the recursive nature of RNNs, where the network's current state and output are influenced by previous states and outputs, enabling the modeling of temporal dependencies in sequential data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6d68b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden States: [[0.23549574953849794, 0.47770001216849795], [0.7473861026001217, 0.9301695100217859]]\n",
      "Outputs: [[0.6032653732179408, 0.6596486166222596], [0.7787151218361145, 0.8750413532290525]]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return math.tanh(x)\n",
    "\n",
    "def rnn_forward(X, U, W, V, W_prime, b, c, timesteps):\n",
    "    n = len(X[0])  # Input dimension\n",
    "    l = len(U)     # Number of hidden units\n",
    "    m = len(V)     # Output dimension\n",
    "    \n",
    "    # Initialize hidden states and outputs\n",
    "    h = [[0] * l for _ in range(timesteps)]\n",
    "    y = [[0] * m for _ in range(timesteps)]\n",
    "    \n",
    "    for t in range(timesteps):\n",
    "        for j in range(l):\n",
    "            net_j = 0\n",
    "            for i in range(n):\n",
    "                net_j += U[j][i] * X[t][i]\n",
    "            for j_tilde in range(l):\n",
    "                if t > 0:\n",
    "                    net_j += W[j][j_tilde] * h[t-1][j_tilde]\n",
    "            net_j += b[j]\n",
    "            h[t][j] = tanh(net_j)\n",
    "        \n",
    "        for k in range(m):\n",
    "            net_k = 0\n",
    "            for j in range(l):\n",
    "                net_k += V[k][j] * h[t][j]\n",
    "            for k_tilde in range(m):\n",
    "                if t > 0:\n",
    "                    net_k += W_prime[k][k_tilde] * y[t-1][k_tilde]\n",
    "            net_k += c[k]\n",
    "            y[t][k] = sigmoid(net_k)\n",
    "    \n",
    "    return h, y\n",
    "\n",
    "# Example usage\n",
    "timesteps = 2\n",
    "n = 3  # Input dimension\n",
    "l = 2  # Number of hidden units\n",
    "m = 2  # Output dimension\n",
    "\n",
    "# Example input for 2 timesteps\n",
    "X = [\n",
    "    [0.1, 0.2, 0.3],\n",
    "    [0.4, 0.5, 0.6]\n",
    "]\n",
    "\n",
    "# Initialize weights and biases\n",
    "U = [\n",
    "    [0.1, 0.2, 0.3],\n",
    "    [0.4, 0.5, 0.6]\n",
    "]\n",
    "\n",
    "W = [\n",
    "    [0.7, 0.8],\n",
    "    [0.9, 1.0]\n",
    "]\n",
    "\n",
    "V = [\n",
    "    [0.1, 0.2],\n",
    "    [0.3, 0.4]\n",
    "]\n",
    "\n",
    "W_prime = [\n",
    "    [0.5, 0.6],\n",
    "    [0.7, 0.8]\n",
    "]\n",
    "\n",
    "b = [0.1, 0.2]\n",
    "c = [0.3, 0.4]\n",
    "\n",
    "# Perform forward pass\n",
    "hidden_states, outputs = rnn_forward(X, U, W, V, W_prime, b, c, timesteps)\n",
    "\n",
    "print(\"Hidden States:\", hidden_states)\n",
    "print(\"Outputs:\", outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f908097f",
   "metadata": {},
   "source": [
    "### Backpropagation Through Time (BPTT)\n",
    "\n",
    "The design goal of an RNN is to calculate the gradient of the error with respect to parameters U , V, and W, and then learn good parameters using stochastic gradient descent (SGD). By mimicking the sum of the errors, the gradients at each time step are also summed up for one training example to get:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial W} = \\sum_{t=1}^{T} \\frac{\\partial E_t}{\\partial W}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial V} = \\sum_{t=1}^{T} \\frac{\\partial E_t}{\\partial V}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial U} = \\sum_{t=1}^{T} \\frac{\\partial E_t}{\\partial U}\n",
    "$$\n",
    "\n",
    "The backpropagation through time (BPTT) algorithm [159] plays an important role in designing RNNs. The basic idea of this algorithm is to apply the chain rule of differentiation to calculate the above gradients backwards starting from the error.\n",
    "\n",
    "1. Calculate the gradient $ \\frac{\\partial E_t}{\\partial V_{kj}(t)} $:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E_t}{\\partial V_{kj}(t)} = \\frac{\\partial E_t}{\\partial \\hat{y}_{pk}(t)} \\cdot \\frac{\\partial \\hat{y}_{pk}(t)}{\\partial \\text{net}_{pk}(t)} \\cdot \\frac{\\partial \\text{net}_{pk}(t)}{\\partial V_{kj}(t)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= - (y_{pk}(t) - \\hat{y}_{pk}(t)) \\cdot g'(\\hat{y}_{pk}(t)) \\cdot h_{pj}(t)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= -\\delta_{pk}(t) \\cdot h_{pj}(t)\n",
    "$$\n",
    "\n",
    "where $ \\delta_{pk}(t) $is called the error for output nodes at time step \\( t \\) and is defined as:\n",
    "\n",
    "$$\n",
    "\\delta_{pk}(t) = -\\frac{\\partial E_t}{\\partial \\hat{y}_{pk}(t)} \\cdot \\frac{\\partial \\hat{y}_{pk}(t)}{\\partial \\text{net}_{pk}(t)} = (y_{pk}(t) - \\hat{y}_{pk}(t)) \\cdot g'(\\hat{y}_{pk}(t))\n",
    "$$\n",
    "\n",
    "2. Calculate the gradient $ \\frac{\\partial E_t}{\\partial W_{jk}(t)} $:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E_t}{\\partial W_{jk}(t)} = \\sum_{k=1}^{m} -\\delta_{pj}(t) \\cdot \\hat{y}_{pk}(t - 1)\n",
    "$$\n",
    "\n",
    "where $ \\delta_{pj}(t) $ is known as the error for hidden nodes given by:\n",
    "\n",
    "$$\n",
    "\\delta_{pj}(t) = \\sum_{k=1}^{m} \\delta_{pk} \\cdot V_{kj} \\cdot f'(h_{pj})\n",
    "$$\n",
    "\n",
    "3. Calculate the gradient $ \\frac{\\partial E_t}{\\partial U_{ji}(t)} $:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E_t}{\\partial U_{ji}(t)} = \\sum_{i=1}^{n} -\\delta_{pj}(t) \\cdot x_{pi}(t)\n",
    "$$\n",
    "\n",
    "The above discussion can be summarized into the following formulae:\n",
    "\n",
    "$$\n",
    "\\delta_{pk}(t) = (y_{pk}(t) - \\hat{y}_{pk}(t)) \\cdot g'(\\hat{y}_{pk}(t))\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\delta_{pj}(t) = \\sum_{k=1}^{m} \\delta_{pk}(t) \\cdot V_{jk}(t) \\cdot f'(h_{pj}(t))\n",
    "$$\n",
    "\n",
    "$$\n",
    "V_{kj}(t + 1) = V_{kj}(t) + \\Delta V_{kj}(t) = V_{kj}(t) + \\eta \\sum_{p=1}^{P} \\delta_{pk} \\cdot h_{pj}(t)\n",
    "$$\n",
    "\n",
    "$$\n",
    "W_{jk}(t + 1) = W_{jk}(t) + \\Delta W_{jk}(t) = W_{jk}(t) + \\eta \\sum_{p=1}^{P} \\delta_{pj} \\cdot \\hat{y}_{pk}(t - 1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "U_{ji}(t + 1) = U_{ji}(t) + \\Delta U_{ji}(t) = U_{ji}(t) + \\eta \\sum_{p=1}^{P} \\delta_{pj} \\cdot x_{pi}(t)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da227a18",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (64,1) into shape (64,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5411/505050980.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;31m# Backward pass (assuming y is the target output)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_5411/505050980.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                 \u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (64,1) into shape (64,)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Derivative of activation function\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "class RNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Weight matrices\n",
    "        self.U = np.random.randn(hidden_size, input_size)\n",
    "        self.W = np.random.randn(hidden_size, hidden_size)\n",
    "        self.V = np.random.randn(output_size, hidden_size)\n",
    "        \n",
    "        # Biases\n",
    "        self.b_hidden = np.zeros((hidden_size, 1))\n",
    "        self.b_output = np.zeros((output_size, 1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        T = x.shape[1]\n",
    "        h = np.zeros((self.hidden_size, T))\n",
    "        y = np.zeros((self.output_size, T))\n",
    "        \n",
    "        for t in range(T):\n",
    "            if t == 0:\n",
    "                h[:, t] = sigmoid(np.dot(self.U, x[:, t].reshape(-1, 1)) + self.b_hidden)\n",
    "            else:\n",
    "                h[:, t] = sigmoid(np.dot(self.U, x[:, t].reshape(-1, 1)) + np.dot(self.W, h[:, t-1].reshape(-1, 1)) + self.b_hidden)\n",
    "            \n",
    "            y[:, t] = sigmoid(np.dot(self.V, h[:, t].reshape(-1, 1)) + self.b_output)\n",
    "        \n",
    "        return h, y\n",
    "    \n",
    "    def backward(self, x, y, h, lr):\n",
    "        T = x.shape[1]\n",
    "        dU = np.zeros_like(self.U)\n",
    "        dW = np.zeros_like(self.W)\n",
    "        dV = np.zeros_like(self.V)\n",
    "        db_hidden = np.zeros_like(self.b_hidden)\n",
    "        db_output = np.zeros_like(self.b_output)\n",
    "        \n",
    "        for t in range(T-1, -1, -1):\n",
    "            delta_output = y[:, t] - x[:, t]\n",
    "            dV += np.dot(delta_output, h[:, t].reshape(1, -1))\n",
    "            db_output += delta_output.reshape(-1, 1)\n",
    "            \n",
    "            if t == 0:\n",
    "                delta_hidden = np.dot(self.V.T, delta_output) * sigmoid_derivative(h[:, t])\n",
    "            else:\n",
    "                delta_hidden = np.dot(self.W.T, delta_hidden) * sigmoid_derivative(h[:, t])\n",
    "                \n",
    "            dU += np.dot(delta_hidden, x[:, t].reshape(1, -1))\n",
    "            db_hidden += delta_hidden.reshape(-1, 1)\n",
    "            \n",
    "            if t > 0:\n",
    "                dW += np.dot(delta_hidden, h[:, t-1].reshape(1, -1))\n",
    "                \n",
    "        # Update weights and biases\n",
    "        self.U -= lr * dU\n",
    "        self.W -= lr * dW\n",
    "        self.V -= lr * dV\n",
    "        self.b_hidden -= lr * db_hidden\n",
    "        self.b_output -= lr * db_output\n",
    "\n",
    "# Example usage\n",
    "input_size = 100 # Assuming input matrix size is 100xT\n",
    "hidden_size = 64\n",
    "output_size = 100\n",
    "\n",
    "# Create an RNN instance\n",
    "rnn = RNN(input_size, hidden_size, output_size)\n",
    "\n",
    "# Generate random input matrix (assuming it contains Sanskrit characters)\n",
    "x = np.random.randn(input_size, 10) # Assuming 10 time steps\n",
    "\n",
    "# Forward pass\n",
    "h, y = rnn.forward(x)\n",
    "\n",
    "# Backward pass (assuming y is the target output)\n",
    "lr = 0.01\n",
    "rnn.backward(x, y, h, lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02412939",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (64,128) and (44,1) not aligned: 128 (dim 1) != 44 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5411/1958886656.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;31m# Backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_5411/505050980.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                 \u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (64,128) and (44,1) not aligned: 128 (dim 1) != 44 (dim 0)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the Sanskrit to Odia mapping\n",
    "sanskrit_to_odia = {\n",
    "    '': '',\n",
    "    '': '',\n",
    "    '': '',\n",
    "    '': '',\n",
    "    '': '',\n",
    "    # Add more mappings as needed\n",
    "}\n",
    "\n",
    "# Define the RNN class\n",
    "# class RNN:\n",
    "    # Constructor and methods implementation ...\n",
    "\n",
    "# Initialize the RNN model\n",
    "input_size = 128  # Assuming ASCII representation\n",
    "hidden_size = 64\n",
    "output_size = 128  # Assuming ASCII representation\n",
    "rnn = RNN(input_size, hidden_size, output_size)\n",
    "\n",
    "# Prepare training data\n",
    "def prepare_data(text, mapping):\n",
    "    input_data = []\n",
    "    target_data = []\n",
    "    for char in text:\n",
    "        # Convert Sanskrit character to ASCII code\n",
    "        input_char = ord(char)\n",
    "        input_data.append(input_char)\n",
    "        \n",
    "        # Convert Sanskrit character to Odia character\n",
    "        if char in mapping:\n",
    "            target_char = mapping[char]\n",
    "        else:\n",
    "            target_char = char\n",
    "        # Convert Odia character to ASCII code\n",
    "        target_data.append(ord(target_char))\n",
    "    \n",
    "    return np.array(input_data).reshape(-1, 1), np.array(target_data).reshape(-1, 1)\n",
    "\n",
    "# Sample input text in Sanskrit\n",
    "sanskrit_text = \"     \"\n",
    "\n",
    "# Prepare training data\n",
    "input_data, target_data = prepare_data(sanskrit_text, sanskrit_to_odia)\n",
    "\n",
    "# Train the RNN model (assuming forward and backward methods are implemented)\n",
    "lr = 0.01\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    hidden_states, predictions = rnn.forward(input_data)\n",
    "    \n",
    "    # Backward pass\n",
    "    rnn.backward(input_data, target_data, hidden_states, lr)\n",
    "\n",
    "# Once trained, you can use the model to map Sanskrit characters to Odia characters\n",
    "# by passing the input through the network's forward method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92ce083",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
