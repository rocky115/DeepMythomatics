{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e655e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "'''Copyright (c) 2004- 2024 , Prof. Radhamadhab Dalai Odisha, India\n",
    "Author's email address :  rmdi115@gmail.com'''\n",
    "###################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617f192d",
   "metadata": {},
   "source": [
    "###Softmax Regression and Softmax Function\n",
    "### Softmax Function\n",
    "The softmax function is used to convert a vector of raw scores (logits) into probabilities. For a vector \\( z \\) of length \\( K \\), the softmax function is defined as:\n",
    "\n",
    "$\n",
    "\\sigma(z)_j = \\frac{e^{z_j}}{\\sum_{k=1}^{K} e^{z_k}}\n",
    "$\n",
    "\n",
    "where:\n",
    "- $ \\sigma(z)_j $ is the probability of the  j-th class.\n",
    "- $ z_j $ is the j-th element of the input vector z.\n",
    "- K is the number of classes.\n",
    "\n",
    "### Cross-Entropy Loss for Multi-Class Classification\n",
    "The cross-entropy loss for multi-class classification is defined as:\n",
    "\n",
    "$\n",
    "H(y, \\hat{y}) = - \\frac{1}{T} \\sum_{t=1}^{T} \\sum_{k=1}^{K} y_{t,k} \\log(\\hat{y}_{t,k})\n",
    "$\n",
    "\n",
    "where:\n",
    "-  T is the total number of samples.\n",
    "-  K is the number of classes.\n",
    "-  $ y_{t,k} $ is a binary indicator (0 or 1) if class label k is the correct classification for sample t.\n",
    "- $ \\hat{y}_{t,k} $ is the predicted probability of class k for sample t.\n",
    "\n",
    "### Gradient of the Cross-Entropy Loss Function\n",
    "The gradient of the cross-entropy loss function with respect to the predicted probabilities $ \\hat{y}_t $ is:\n",
    "\n",
    "$\n",
    "\\frac{\\partial H(y, \\hat{y})}{\\partial \\hat{y}_t} = \\frac{\\hat{y}_t - y_t}{T}\n",
    "$\n",
    "\n",
    "The gradient with respect to the weights w is:\n",
    "\n",
    "$\n",
    "\\nabla_w H(y, \\hat{y}) = \\frac{1}{T} \\sum_{t=1}^{T} ( \\hat{y}_t - y_t ) x_t\n",
    "$\n",
    "\n",
    "### Weight Update Rule\n",
    "The weight update rule using gradient descent for softmax regression is:\n",
    "\n",
    "$\n",
    "w^{k+1} = w^k - \\eta \\nabla_w H(y, \\hat{y})\n",
    "$\n",
    "\n",
    "where:\n",
    "- $ \\eta $ is the learning rate.\n",
    "- $ w^k $ is the weight vector at iteration k.\n",
    "- $ \\nabla_w H(y, \\hat{y}) $ is the gradient of the KL divergence (cross-entropy loss) with respect to the weights.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f5a70c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [[0.68065166 0.18928148 0.13006686]\n",
      " [0.49600597 0.26347298 0.24052105]\n",
      " [0.30814999 0.31266358 0.37918643]\n",
      " [0.1649971  0.3197845  0.5152184 ]]\n",
      "Cross-entropy loss: 0.727915949036652\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Softmax function\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # stability improvement\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "# Cross-entropy loss function for softmax regression\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # Avoid log(0)\n",
    "    return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))\n",
    "\n",
    "# Gradient of the cross-entropy loss function\n",
    "def gradient_cross_entropy_loss(X, y_true, y_pred):\n",
    "    return np.dot(X.T, (y_pred - y_true)) / X.shape[0]\n",
    "\n",
    "# Softmax regression training function\n",
    "def softmax_regression_train(X, y, learning_rate, epochs):\n",
    "    n_samples, n_features = X.shape\n",
    "    n_classes = y.shape[1]\n",
    "    weights = np.zeros((n_features, n_classes))\n",
    "    bias = np.zeros(n_classes)\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        # Linear combination of features and weights\n",
    "        linear_model = np.dot(X, weights) + bias\n",
    "\n",
    "        # Predictions using softmax function\n",
    "        y_pred = softmax(linear_model)\n",
    "\n",
    "        # Compute the gradient of the loss function\n",
    "        gradient = gradient_cross_entropy_loss(X, y, y_pred)\n",
    "        bias_gradient = np.mean(y_pred - y, axis=0)\n",
    "\n",
    "        # Update weights and bias using gradient descent\n",
    "        weights -= learning_rate * gradient\n",
    "        bias -= learning_rate * bias_gradient\n",
    "\n",
    "    return weights, bias\n",
    "\n",
    "# Softmax regression prediction function\n",
    "def softmax_regression_predict(X, weights, bias):\n",
    "    linear_model = np.dot(X, weights) + bias\n",
    "    return softmax(linear_model)\n",
    "\n",
    "# Example data (one-hot encoded labels)\n",
    "X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n",
    "y = np.array([[1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 0, 1]])\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.01\n",
    "epochs = 1000\n",
    "weights, bias = softmax_regression_train(X, y, learning_rate, epochs)\n",
    "\n",
    "# Prediction\n",
    "y_pred = softmax_regression_predict(X, weights, bias)\n",
    "print(\"Predictions:\", y_pred)\n",
    "\n",
    "# Loss\n",
    "loss = cross_entropy_loss(y, y_pred)\n",
    "print(\"Cross-entropy loss:\", loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b074d10",
   "metadata": {},
   "source": [
    "### Softmax Regression\n",
    "\n",
    "Given a set of training samples {($x_1$, $y_1$), $\\ldots$, ($x_T$, $y_T$)}, where $x_t \\in \\mathbb{R}^n$ and $y_t \\in \\{1, \\ldots, k\\} $, we design a weight matrix W:\n",
    "\n",
    "$\n",
    "W = [w_1, w_2, \\ldots, w_k]\n",
    "$\n",
    "\n",
    "### Softmax Function\n",
    "\n",
    "The softmax function converts a vector of raw scores (logits) into probabilities. For a vector z of length k, the softmax function is defined as:\n",
    "\n",
    "$\n",
    "\\sigma(z)_j = \\frac{e^{z_j}}{\\sum_{k=1}^{K} e^{z_k}}\n",
    "$\n",
    "\n",
    "where:\n",
    "- $ \\sigma(z)_j $ is the probability of the j-th class.\n",
    "- $ z_j $ is the j-th element of the input vector z.\n",
    "- k is the number of classes.\n",
    "\n",
    "### Cross-Entropy Loss for Multi-Class Classification\n",
    "\n",
    "The cross-entropy loss for multi-class classification is defined as:\n",
    "\n",
    "$\n",
    "H(y, \\hat{y}) = - \\frac{1}{T} \\sum_{t=1}^{T} \\sum_{i=1}^{k} y_{t,i} \\log(\\hat{y}_{t,i})\n",
    "$\n",
    "\n",
    "where:\n",
    "- T  is the total number of samples.\n",
    "- k  is the number of classes.\n",
    "- y_{t,i}  is a binary indicator (0 or 1) if class label i is the correct classification for sample t .\n",
    "- $\\hat{y}_{t,i} $ is the predicted probability of class i for sample t.\n",
    "\n",
    "### Gradient of the Cross-Entropy Loss Function\n",
    "\n",
    "The gradient of the cross-entropy loss function with respect to the predicted probabilities $\\hat{y}_t$ is:\n",
    "\n",
    "$\n",
    "\\frac{\\partial H(y, \\hat{y})}{\\partial \\hat{y}_t} = \\frac{\\hat{y}_t - y_t}{T}\n",
    "$\n",
    "\n",
    "The gradient with respect to the weights W is:\n",
    "\n",
    "$\n",
    "\\nabla_W H(y, \\hat{y}) = \\frac{1}{T} \\sum_{t=1}^{T} X_t^T (\\hat{y}_t - y_t)\n",
    "$\n",
    "\n",
    "### Weight Update Rule\n",
    "\n",
    "The weight update rule using gradient descent for softmax regression is:\n",
    "\n",
    "$\n",
    "W^{k+1} = W^k - \\eta \\nabla_W H(y, \\hat{y})\n",
    "$\n",
    "\n",
    "where:\n",
    "- $ \\eta $ is the learning rate.\n",
    "- $W^k $ is the weight vector at iteration k.\n",
    "- $\\nabla_W H(y, \\hat{y}) $ is the gradient of the KL divergence (cross-entropy loss) with respect to the weights.\n",
    "\n",
    "### RNN Output\n",
    "\n",
    "For an RNN, its output vector $g(Wx_t)$ is directly used as the hypothetical function. Letting $ z = Wx_t $, the hypothetical function is:\n",
    "\n",
    "$\n",
    "h_W(x_t) = g(Wx_t)\n",
    "$\n",
    "\n",
    "### Softmax Function for RNN Output\n",
    "\n",
    "The softmax function for the vector z is:\n",
    "\n",
    "$\n",
    "\\sigma(z)_j = \\frac{e^{z_j}}{\\sum_{i=1}^{k} e^{z_i}}\n",
    "$\n",
    "\n",
    "### Cross-Entropy Loss for RNN\n",
    "\n",
    "The cross-entropy loss for a single training example in the context of RNNs is:\n",
    "\n",
    "$\n",
    "H(y_t, \\hat{y}_t) = - \\sum_{i=1}^{k} y_{t,i} \\log(\\hat{y}_{t,i})\n",
    "$\n",
    "\n",
    "### Weight Update Rule for RNN\n",
    "\n",
    "The weight update rule using gradient descent for updating the weights W in an RNN is:\n",
    "\n",
    "$\n",
    "W^{k+1} = W^k - \\eta \\nabla_W H(y, \\hat{y})\n",
    "$\n",
    "\n",
    "where:\n",
    "- $\\eta$  is the learning rate.\n",
    "- $W^k $ is the weight matrix at iteration $ k $.\n",
    "- $\\nabla_W H(y, \\hat{y}) $ is the gradient of the cross-entropy loss with respect to the weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d2d5ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/1000, Loss: 1.0435256411205036\n",
      "Epoch 200/1000, Loss: 0.9840876611465466\n",
      "Epoch 300/1000, Loss: 0.869546581208506\n",
      "Epoch 400/1000, Loss: 0.7182150784529611\n",
      "Epoch 500/1000, Loss: 0.6425357646396047\n",
      "Epoch 600/1000, Loss: 0.5854458052724199\n",
      "Epoch 700/1000, Loss: 0.5113694386531344\n",
      "Epoch 800/1000, Loss: 0.4230903768958066\n",
      "Epoch 900/1000, Loss: 0.3280583317952764\n",
      "Epoch 1000/1000, Loss: 0.20328721850364254\n",
      "Predictions: [[[0.7384881  0.15151101 0.11000089]]\n",
      "\n",
      " [[0.925965   0.07198433 0.00205067]]\n",
      "\n",
      " [[0.19359036 0.75806083 0.04834881]]\n",
      "\n",
      " [[0.00288275 0.05147516 0.94564209]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the softmax function\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))  # stability improvement\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "# Define the cross-entropy loss function\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # Avoid log(0)\n",
    "    return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))\n",
    "\n",
    "# Define the RNN cell\n",
    "class SimpleRNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.Wx = np.random.randn(input_size, hidden_size) * 0.01\n",
    "        self.Wh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "        self.Wo = np.random.randn(hidden_size, output_size) * 0.01\n",
    "        self.bh = np.zeros((1, hidden_size))\n",
    "        self.bo = np.zeros((1, output_size))\n",
    "        \n",
    "        # Initialize hidden state\n",
    "        self.h = np.zeros((1, hidden_size))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.h = np.tanh(np.dot(x, self.Wx) + np.dot(self.h, self.Wh) + self.bh)\n",
    "        o = np.dot(self.h, self.Wo) + self.bo\n",
    "        return softmax(o)\n",
    "    \n",
    "    def backward(self, x, y_true, y_pred):\n",
    "        # Compute the gradient of the loss with respect to the output\n",
    "        dL_dO = y_pred - y_true\n",
    "        \n",
    "        # Compute the gradients of Wo and bo\n",
    "        dL_dWo = np.dot(self.h.T, dL_dO)\n",
    "        dL_dbo = dL_dO\n",
    "        \n",
    "        # Backpropagation through time (for simplicity, one time step)\n",
    "        dL_dh = np.dot(dL_dO, self.Wo.T)\n",
    "        dL_dWh = np.dot(self.h.T, dL_dh * (1 - self.h ** 2))\n",
    "        dL_dWx = np.dot(x.T, dL_dh * (1 - self.h ** 2))\n",
    "        dL_dbh = dL_dh * (1 - self.h ** 2)\n",
    "        \n",
    "        # Update the weights and biases\n",
    "        self.Wo -= self.learning_rate * dL_dWo\n",
    "        self.bo -= self.learning_rate * np.sum(dL_dbo, axis=0, keepdims=True)\n",
    "        self.Wh -= self.learning_rate * dL_dWh\n",
    "        self.Wx -= self.learning_rate * dL_dWx\n",
    "        self.bh -= self.learning_rate * np.sum(dL_dbh, axis=0, keepdims=True)\n",
    "\n",
    "# Training the RNN\n",
    "def train_rnn(rnn, X_train, y_train, epochs=1000):\n",
    "    for epoch in range(epochs):\n",
    "        loss = 0\n",
    "        for x, y in zip(X_train, y_train):\n",
    "            x = x.reshape(1, -1)\n",
    "            y = y.reshape(1, -1)\n",
    "            y_pred = rnn.forward(x)\n",
    "            loss += cross_entropy_loss(y, y_pred)\n",
    "            rnn.backward(x, y, y_pred)\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss/len(X_train)}')\n",
    "\n",
    "# Example data\n",
    "X_train = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n",
    "y_train = np.array([[1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 0, 1]])\n",
    "\n",
    "# Initialize RNN\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 5\n",
    "output_size = y_train.shape[1]\n",
    "learning_rate = 0.01\n",
    "\n",
    "rnn = SimpleRNN(input_size, hidden_size, output_size, learning_rate)\n",
    "\n",
    "# Train RNN\n",
    "train_rnn(rnn, X_train, y_train, epochs=1000)\n",
    "\n",
    "# Test the RNN\n",
    "def predict_rnn(rnn, X):\n",
    "    y_pred = []\n",
    "    for x in X:\n",
    "        x = x.reshape(1, -1)\n",
    "        y_pred.append(rnn.forward(x))\n",
    "    return np.array(y_pred)\n",
    "\n",
    "y_pred = predict_rnn(rnn, X_train)\n",
    "print(\"Predictions:\", y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "304b7351",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7567/1370028221.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;31m# Train RNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m \u001b[0mtrain_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;31m# Test the RNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_7567/1370028221.py\u001b[0m in \u001b[0;36mtrain_rnn\u001b[0;34m(rnn, X_train, y_train, epochs)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_7567/1370028221.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmat_vec_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmat_vec_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvec_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmat_vec_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_7567/1370028221.py\u001b[0m in \u001b[0;36mvec_add\u001b[0;34m(vec1, vec2)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Vector addition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvec_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvec1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvec2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Element-wise vector multiplication\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_7567/1370028221.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Vector addition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvec_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvec1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvec2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Element-wise vector multiplication\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Define the softmax function\n",
    "def softmax(x):\n",
    "    exp_x = [math.exp(i) for i in x]\n",
    "    sum_exp_x = sum(exp_x)\n",
    "    return [i / sum_exp_x for i in exp_x]\n",
    "\n",
    "# Define the cross-entropy loss function\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    epsilon = 1e-15\n",
    "    y_pred = [max(min(i, 1 - epsilon), epsilon) for i in y_pred]  # Avoid log(0)\n",
    "    return -sum([y_true[i] * math.log(y_pred[i]) for i in range(len(y_true))])\n",
    "\n",
    "# Matrix-vector multiplication\n",
    "def mat_vec_mul(mat, vec):\n",
    "    return [sum(mat[i][j] * vec[j] for j in range(len(vec))) for i in range(len(mat))]\n",
    "\n",
    "# Vector addition\n",
    "def vec_add(vec1, vec2):\n",
    "    return [vec1[i] + vec2[i] for i in range(len(vec1))]\n",
    "\n",
    "# Element-wise vector multiplication\n",
    "def vec_mul(vec1, vec2):\n",
    "    return [vec1[i] * vec2[i] for i in range(len(vec1))]\n",
    "\n",
    "# Tanh activation function\n",
    "def tanh(x):\n",
    "    return [(math.exp(2 * i) - 1) / (math.exp(2 * i) + 1) for i in x]\n",
    "\n",
    "# Derivative of tanh activation function\n",
    "def tanh_derivative(x):\n",
    "    return [1 - i**2 for i in x]\n",
    "\n",
    "# Simple RNN cell\n",
    "class SimpleRNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.Wx = [[0.01 for _ in range(hidden_size)] for _ in range(input_size)]\n",
    "        self.Wh = [[0.01 for _ in range(hidden_size)] for _ in range(hidden_size)]\n",
    "        self.Wo = [[0.01 for _ in range(output_size)] for _ in range(hidden_size)]\n",
    "        self.bh = [0.0 for _ in range(hidden_size)]\n",
    "        self.bo = [0.0 for _ in range(output_size)]\n",
    "        \n",
    "        # Initialize hidden state\n",
    "        self.h = [0.0 for _ in range(hidden_size)]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.h = tanh(vec_add(mat_vec_mul(self.Wx, x), vec_add(mat_vec_mul(self.Wh, self.h), self.bh)))\n",
    "        o = vec_add(mat_vec_mul(self.Wo, self.h), self.bo)\n",
    "        return softmax(o)\n",
    "    \n",
    "    def backward(self, x, y_true, y_pred):\n",
    "        # Compute the gradient of the loss with respect to the output\n",
    "        dL_dO = [y_pred[i] - y_true[i] for i in range(len(y_true))]\n",
    "        \n",
    "        # Compute the gradients of Wo and bo\n",
    "        dL_dWo = [[self.h[i] * dL_dO[j] for j in range(len(dL_dO))] for i in range(len(self.h))]\n",
    "        dL_dbo = dL_dO\n",
    "        \n",
    "        # Backpropagation through time (for simplicity, one time step)\n",
    "        dL_dh = mat_vec_mul(self.Wo, dL_dO)\n",
    "        dL_dh = vec_mul(dL_dh, tanh_derivative(self.h))\n",
    "        \n",
    "        dL_dWh = [[self.h[i] * dL_dh[j] for j in range(len(dL_dh))] for i in range(len(self.h))]\n",
    "        dL_dWx = [[x[i] * dL_dh[j] for j in range(len(dL_dh))] for i in range(len(x))]\n",
    "        dL_dbh = dL_dh\n",
    "        \n",
    "        # Update the weights and biases\n",
    "        self.Wo = [[self.Wo[i][j] - self.learning_rate * dL_dWo[i][j] for j in range(len(dL_dWo[0]))] for i in range(len(self.Wo))]\n",
    "        self.bo = [self.bo[i] - self.learning_rate * dL_dbo[i] for i in range(len(self.bo))]\n",
    "        self.Wh = [[self.Wh[i][j] - self.learning_rate * dL_dWh[i][j] for j in range(len(dL_dWh[0]))] for i in range(len(self.Wh))]\n",
    "        self.Wx = [[self.Wx[i][j] - self.learning_rate * dL_dWx[i][j] for j in range(len(dL_dWx[0]))] for i in range(len(self.Wx))]\n",
    "        self.bh = [self.bh[i] - self.learning_rate * dL_dbh[i] for i in range(len(self.bh))]\n",
    "\n",
    "# Training the RNN\n",
    "def train_rnn(rnn, X_train, y_train, epochs=1000):\n",
    "    for epoch in range(epochs):\n",
    "        loss = 0\n",
    "        for x, y in zip(X_train, y_train):\n",
    "            y_pred = rnn.forward(x)\n",
    "            loss += cross_entropy_loss(y, y_pred)\n",
    "            rnn.backward(x, y, y_pred)\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss / len(X_train)}')\n",
    "\n",
    "# Example data\n",
    "X_train = [[1, 2], [2, 3], [3, 4], [4, 5]]\n",
    "y_train = [[1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 0, 1]]\n",
    "\n",
    "# Initialize RNN\n",
    "input_size = len(X_train[0])\n",
    "hidden_size = 5\n",
    "output_size = len(y_train[0])\n",
    "learning_rate = 0.01\n",
    "\n",
    "rnn = SimpleRNN(input_size, hidden_size, output_size, learning_rate)\n",
    "\n",
    "# Train RNN\n",
    "train_rnn(rnn, X_train, y_train, epochs=1000)\n",
    "\n",
    "# Test the RNN\n",
    "def predict_rnn(rnn, X):\n",
    "    y_pred = []\n",
    "    for x in X:\n",
    "        y_pred.append(rnn.forward(x))\n",
    "    return y_pred\n",
    "\n",
    "y_pred = predict_rnn(rnn, X_train)\n",
    "print(\"Predictions:\", y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f2653d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7567/1370028221.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;31m# Train RNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m \u001b[0mtrain_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;31m# Test the RNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_7567/1370028221.py\u001b[0m in \u001b[0;36mtrain_rnn\u001b[0;34m(rnn, X_train, y_train, epochs)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_7567/1370028221.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmat_vec_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmat_vec_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvec_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmat_vec_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_7567/1370028221.py\u001b[0m in \u001b[0;36mvec_add\u001b[0;34m(vec1, vec2)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Vector addition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvec_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvec1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvec2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Element-wise vector multiplication\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_7567/1370028221.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Vector addition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvec_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvec1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvec2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Element-wise vector multiplication\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Define the softmax function\n",
    "def softmax(x):\n",
    "    exp_x = [math.exp(i) for i in x]\n",
    "    sum_exp_x = sum(exp_x)\n",
    "    return [i / sum_exp_x for i in exp_x]\n",
    "\n",
    "# Define the cross-entropy loss function\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    epsilon = 1e-15\n",
    "    y_pred = [max(min(i, 1 - epsilon), epsilon) for i in y_pred]  # Avoid log(0)\n",
    "    return -sum([y_true[i] * math.log(y_pred[i]) for i in range(len(y_true))])\n",
    "\n",
    "# Matrix-vector multiplication\n",
    "def mat_vec_mul(mat, vec):\n",
    "    return [sum(mat[i][j] * vec[j] for j in range(len(vec))) for i in range(len(mat))]\n",
    "\n",
    "# Vector addition\n",
    "def vec_add(vec1, vec2):\n",
    "    return [vec1[i] + vec2[i] for i in range(len(vec1))]\n",
    "\n",
    "# Element-wise vector multiplication\n",
    "def vec_mul(vec1, vec2):\n",
    "    return [vec1[i] * vec2[i] for i in range(len(vec1))]\n",
    "\n",
    "# Tanh activation function\n",
    "def tanh(x):\n",
    "    return [(math.exp(2 * i) - 1) / (math.exp(2 * i) + 1) for i in x]\n",
    "\n",
    "# Derivative of tanh activation function\n",
    "def tanh_derivative(x):\n",
    "    return [1 - i**2 for i in x]\n",
    "\n",
    "# Simple RNN cell\n",
    "class SimpleRNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.Wx = [[0.01 for _ in range(hidden_size)] for _ in range(input_size)]\n",
    "        self.Wh = [[0.01 for _ in range(hidden_size)] for _ in range(hidden_size)]\n",
    "        self.Wo = [[0.01 for _ in range(output_size)] for _ in range(hidden_size)]\n",
    "        self.bh = [0.0 for _ in range(hidden_size)]\n",
    "        self.bo = [0.0 for _ in range(output_size)]\n",
    "        \n",
    "        # Initialize hidden state\n",
    "        self.h = [0.0 for _ in range(hidden_size)]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.h = tanh(vec_add(mat_vec_mul(self.Wx, x), vec_add(mat_vec_mul(self.Wh, self.h), self.bh)))\n",
    "        o = vec_add(mat_vec_mul(self.Wo, self.h), self.bo)\n",
    "        return softmax(o)\n",
    "    \n",
    "    def backward(self, x, y_true, y_pred):\n",
    "        # Compute the gradient of the loss with respect to the output\n",
    "        dL_dO = [y_pred[i] - y_true[i] for i in range(len(y_true))]\n",
    "        \n",
    "        # Compute the gradients of Wo and bo\n",
    "        dL_dWo = [[self.h[i] * dL_dO[j] for j in range(len(dL_dO))] for i in range(len(self.h))]\n",
    "        dL_dbo = dL_dO\n",
    "        \n",
    "        # Backpropagation through time (for simplicity, one time step)\n",
    "        dL_dh = mat_vec_mul(self.Wo, dL_dO)\n",
    "        dL_dh = vec_mul(dL_dh, tanh_derivative(self.h))\n",
    "        \n",
    "        dL_dWh = [[self.h[i] * dL_dh[j] for j in range(len(dL_dh))] for i in range(len(self.h))]\n",
    "        dL_dWx = [[x[i] * dL_dh[j] for j in range(len(dL_dh))] for i in range(len(x))]\n",
    "        dL_dbh = dL_dh\n",
    "        \n",
    "        # Update the weights and biases\n",
    "        self.Wo = [[self.Wo[i][j] - self.learning_rate * dL_dWo[i][j] for j in range(len(dL_dWo[0]))] for i in range(len(self.Wo))]\n",
    "        self.bo = [self.bo[i] - self.learning_rate * dL_dbo[i] for i in range(len(self.bo))]\n",
    "        self.Wh = [[self.Wh[i][j] - self.learning_rate * dL_dWh[i][j] for j in range(len(dL_dWh[0]))] for i in range(len(self.Wh))]\n",
    "        self.Wx = [[self.Wx[i][j] - self.learning_rate * dL_dWx[i][j] for j in range(len(dL_dWx[0]))] for i in range(len(self.Wx))]\n",
    "        self.bh = [self.bh[i] - self.learning_rate * dL_dbh[i] for i in range(len(self.bh))]\n",
    "\n",
    "# Training the RNN\n",
    "def train_rnn(rnn, X_train, y_train, epochs=1000):\n",
    "    for epoch in range(epochs):\n",
    "        loss = 0\n",
    "        for x, y in zip(X_train, y_train):\n",
    "            y_pred = rnn.forward(x)\n",
    "            loss += cross_entropy_loss(y, y_pred)\n",
    "            rnn.backward(x, y, y_pred)\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss / len(X_train)}')\n",
    "\n",
    "# Example data\n",
    "X_train = [[1, 2], [2, 3], [3, 4], [4, 5]]\n",
    "y_train = [[1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 0, 1]]\n",
    "\n",
    "# Initialize RNN\n",
    "input_size = len(X_train[0])\n",
    "hidden_size = 5\n",
    "output_size = len(y_train[0])\n",
    "learning_rate = 0.01\n",
    "\n",
    "rnn = SimpleRNN(input_size, hidden_size, output_size, learning_rate)\n",
    "\n",
    "# Train RNN\n",
    "train_rnn(rnn, X_train, y_train, epochs=1000)\n",
    "\n",
    "# Test the RNN\n",
    "def predict_rnn(rnn, X):\n",
    "    y_pred = []\n",
    "    for x in X:\n",
    "        y_pred.append(rnn.forward(x))\n",
    "    return y_pred\n",
    "\n",
    "y_pred = predict_rnn(rnn, X_train)\n",
    "print(\"Predictions:\", y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c136c9b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
