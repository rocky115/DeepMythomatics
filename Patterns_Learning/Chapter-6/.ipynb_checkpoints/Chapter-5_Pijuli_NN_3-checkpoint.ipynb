{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a22023",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " * Copyright (c) 2008 Radhamadhab Dalai\n",
    " *\n",
    " * Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    " * of this software and associated documentation files (the \"Software\"), to deal\n",
    " * in the Software without restriction, including without limitation the rights\n",
    " * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    " * copies of the Software, and to permit persons to whom the Software is\n",
    " * furnished to do so, subject to the following conditions:\n",
    " *\n",
    " * The above copyright notice and this permission notice shall be included in\n",
    " * all copies or substantial portions of the Software.\n",
    " *\n",
    " * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    " * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    " * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    " * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    " * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    " * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
    " * THE SOFTWARE.\n",
    "'''"
   ]
  },
  {
   "attachments": {
    "image-2.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdQAAABxCAIAAADj48XSAAAftUlEQVR4Ae1dfZAUxdmfshKLWFTda1Sy+cNggr4pDbeWASFgxFC1SQWMVAEe1oYKUAViKsUdSaFSgcte9qzwYZmK5l5NzB9ogkuCRihDnb58hItkXxBI3iJrQRLwdYHjw11PL3gb7465m36r99mde3ZmZ3amu3c+bnvrCp6Z6f7107+n+5n+mm6FyJ9kQDIgGZAMeM6A4nmKMkHJgGRAMiAZINL5ykIgGZAMSAZ8YEA6Xx9Il0lKBiQDkgHpfGUZkAxIBiQDPjAgna8PpMskJQOSAcmAdL6yDEgGJAOSAR8YkM7XB9JlkpIByYBkQDpfWQYkA5IByYAPDEjn6wPpMknJgGRAMiCdrywDkgHJgGTABwak8/WBdJmkZEAyIBmQzleWAcmAZEAy4AMD0vn6QLpMUjIgGZAM1Nf5KvLnOQNCyrTnWssExdREyaP3DDDXODEmt0peUSrwDZdWseR9hwyY+TTfcQhlCGbGMd8xRJGXrhgw8Gm4dAWFA5txzHdweCm7ZcDAp+HSFVqFc3QV00lgHs2c4MswBgZEES4Kx6CevLRiQBThonCs9JT3DQzwEC6dr4HMcF/yFAWcc1E4GFPKNgyIIlwUjo2q8hFmgIdw6Xwxk6GXeYoCzrwoHIwpZRsGRBEuCsdGVfkIM8BDuHS+mMnQyzxFAWdeFA7GlLINA6IIF4Vjo6p8hBngIVw6X8xk6GWeooAzLwoHY0rZhgFRhIvCsVFVPsIM8BAunS9mMvQyT1HAmReFgzGlbMOAKMJF4dioKh9hBngIl84XMxl6maco4MyLwsGYUrZhQBThonBsVJWPMAM8hEvni5kMvcxTFHDmReFgTCnbMCCKcFE4NqrKR5gBHsKl88VMhl7mKQo486JwMKaUbRgQRbgoHBtV5SPMAA/h0vliJkMv8xQFnHlROBhTyjYMiCJcFI6NqvIRZoCHcJHOt6+v79ChQ6I0wzihkEdHR5988smnnnpqZGQEFD5//nwikfjd737nUP/h4eFjx445DFw1GFtRGB4e7u7uxoBsOBghXPLOnTt/9KMf5fN5UHtoaGjr1q1btmwZHR11kpGPPvpo3759hULBSeCqYdgI1zRt//79AwMDOiYbjh49dMKxY8c2btx4/PhxXfPt27dv3LhRN6V+30o4efLkhx9+CE/PnTv3xz/+UVVVq8Dm+zyEC3O+R48enTFjxsqVK7F+PJphHCyrKoE6ks+XBPzUX/mXv/yloij//Oc/CSGjo6M333zzrFmzPvroIydavfnmm/PmzfvSl77kJLBVGAbCc7lcS0vLlClTMCYDDo5uJWez9ImqkkzGKog/9//xj38oivL8889D8j/4wQ8URbl06ZITbfr6+u699949e/Z85jOfuXDhgpMo5jBshHd0dCiK0tvbqwOy4ejRrQQwHCEknbYK4s/9oaGhT37yk/F4HJI/cOCAoii7du3SNM2JQo8++uikSZNeeeUVQsjx48dbWlpeeeWVZcuWOYkLYXgIF+Z8CSEdHR0eON9UikQitBBEIqStzZIlKCUe1/M1a9ZMmzYNWpG/+tWvvvzlL//4xz+2VNH04Pjx4947X0JIT0+PB843++deRSHd296LfWUg0vQxNaFepxEVmQz1zh7X8507d86cOfOxxx4jhGQymdmzZ3/ta19DStmJvb297e3tw8PD1113XbZajuwil58x12EPnK+q0rqWTNI/RSm2eLJZajv4K7cT9cZQPl+yYDlzdfz/vffemzlz5t13300I+fe///3QQw9NmDBhcHDQeZL3338/ON9FixZBJ/WWW245ffq0QwRmwxFCwud8CaE+V1FILGZp43yeBkgmaZhIxDKYQ36dB1u+fPmqVaueeeaZCxcubNy4cerUqX/6059w9M2bNz9U+cNmHp/ON5ullohEiKJ0K/MVhUSUy/mFq+EOtVNbG3XE5TociVCrQT1ndWWYckfymjVrnn322YULF46MjDzyyCPf/e53DW/N1157rdJuD7322ms69OnTp++5557169frd9wKzHXYA+dLstn8L36vKNRW6U8/QP8z/0WjbfediEwahVZRKuWWAMbwO3fufP7556+//npo/P30pz81vDXPnDljMNzmzZtxYrrzbW5u3rNnDyGkubnZMAqHwxtkZsOF0vnm87Tawl95jM5ACL3s7i6VEJswVaJx3Mrlco8++ujmzZvXFH/ZbPZTn/qU4SU8ODjYX/nD/SNO5ws7mTLkoGrLl6dUUR1UlaRSJBqlZojFSDqtDqrwLqR1GHqvhQIdgIB3aSRCw6sqvDhpG7liFJohWy6iLFu27MiRI83NzT//+c//8pe/NDc3G96aqqpW2q1fHxnUNO3q1avvv//+zTff/Ne//tVFquWgzIajFdg07MBrONCqUKAGiMehFiW/8CJ9azZ9nFx5jjZ4y29KKkD7N5VSm++KKfvpy/Tb75dzVvf/W1tb+/v7J0yYsHfv3q6urtbWVsNbU9M0g+EMVVJ3vnfdddfu3bsJIXfcccfrr7/uRHUewwl2volEwoNhh3S61OZNJklXV3WKVLXU5oX2b/VAou++/PLLf/jDH15++WVFUfbs2bNr1y7DS5gQ0tnZeX/l78yZM7oinM4XqqKO5lw4ePCg4GEHaP9Eo9RC5WmofJ664nyeet5YrFI78NSRCIlGk997j9bzYvtXr+OVoQVf5XK5devW5fN5RVESiURfX5/5rbl79+5Ku92/a9cu0OPpp59evHgxtJi2bNnCphybx9Q0zex82RQYi5VOl3xuNEo7IOm0+q9C8e1JbReN6vYciwESbRXdqEauu0J7NnveMj6uz/V3vvMdQsjtt98+ffr0kZGRaDRqeGu+8847BsMlk0msi+58V6xYsW3bNk3TPvvZz549exaHsZHZDAeAwoYdMpnMtGnTpk6devjwYV1Xt5p1dVEDqyo1Ok/FKxQq6jkPlJ4Xe6Gnp+eLX/zi448//ve//x2aUXffffedd9557tw5+4j602PHjn3rW9+68cYbV69erd90K7glnBBy5cqVBx98cOLEiTt27NCTc4uTTpcas11PDdEhBUWhbtct70XDx5T96ZXb8pdU8NS6SnUSLl++fO+9986ZM2doaGju3Lnnz59fsGBBJBLZv3+/wxQzmcySJUs6Ozubm5tzuZzDWIZgbgmH6Js2bVIUZd26dTqaW5x8vtSCoRZ87m+lnkpbG8OUaFdXqdrGvvB/aeWecu9GV02woGnaqlWrPv/5z586derhhx8+ceLED3/4w2uvvfZnP/uZ85Q2bNgwefLkuXPnvv7662+//fb8+fM7Ozs3bNjgHMEt4RhZmPPFoLrsVjMYG4TOqWfDBbq2/gpa8QfLJJg1cUu4VUJucdJp6m/j912kQwpTltGuKPMvkym1e8NTAjRNO3PmzNWrV5kz7ZZwq4Tc4sAgXnzhIO0jKgnqPss9FaskHN1PpWiBKI0uOYrhSyBYSqhpGgiFQgEvHXGiklvCMWawnC/Mq4bBapjDAMk8RQFngwEn3vw2rcCL/4ZxGOVCoTRsxOPEGdP2JxoD4VUVdY2jqqnlexWFRP/jnBi3q6sF/leIK9cxgye4JhxlwVvnC8NJ8TgdVDL8xWJEUWLK/sg1ubb/fCMyoZ/2Xg1hDJepFJ2igT8Y9UcZa0yRpyhgxtzhqGrXtBeo5115TtiLE8aehMHhzAVRdke4dQ7c4RQKma88TKfIFmRhMZk1MNOTWIxW4XH9c0d4JRXeOt9sdsxd6n5TFzKZru+/k//F70kqlfpeWv3NDrvAyeSYay46bpiWLfZ+4zRiwzSasEF5igIjTrGVmv70A+k99DOhdLq4DhRj8chdXaHovfJkEeL6YLhslkQi+TmLuzbRj4BgIpQ/IxUI2Sw1X9C+qKlQkfeCx3DeOl/enNrGh1Uv6TSdQdDdMawhtY03nh7yFAXMg1McGDKMxQT3WLEqMJzs2cJRnLSHslPCa6nkFAdYbWtzPSlaSwHjc5jGcTv1akQJ7rVTwqvlYBw5X0P2CgXaDIPF+pEIy+S7ATAMlzxFAefPEY7ueetdtRrA/zoiHFvIQnaEA3xWrriywOO+DdM4gZ95Y86nI8It0Mev89UzrKp0uTh8lTF+CwFkl6co6IQ5Wi/smecFtcBfjF/zeWc4Lz0v2C6ZpCvYxumPx3AN4HzB6qpKG790PVS87l0t/8oZT1HAWtfAKRQ8/QoCNBvX/rcG4dg2tnINHO89LyF0SGr8TpzWINzeWLZPeR/yaMabdtX40F7zZvl+VQXqfFMU4XY48Pmgzc4a9cvj+J1/syPcDZ92OPqOJ24AxYSFbVbEYAULxY7wWpo2TMtXJ0L/9Dg8a/h13WsKPEUBg1vi+Oh5QT+YwBl3trMkHFvFgWyJA80Om20AHYCzB4FlD+NxAZIl4Q7IajznC6TIOmxbOCyLVBC+Pmxro4Me48v/WhJuaybzw+o4vr8yCaELkLyZ4jOTUs871Ql3lqJ3zlf/1KXec+POMk5oUZB12IIsXKTGDJfoDARjQXAlFrwx38aEM4MYZkr1iqau+T41nG5IngSY48Jws64TM07AIvIYziPnC8sN8nnq8QL0zcu4a0PxFAVcqnUcGCeku66s3BZRLqtnODZtwAlwyrCAybg3Gieon9F1wjmVwDjQ1szv2B9RLmf/PHbaBWcS7NFh11D2+EGMiQl3q59HzhcaK7BVYID6i+OuDcVTFHDRwTiwVJrOVz8nYt8GnAyPDIOY46UniwnnYQXjQFtT2IYbPGpB3K6u8bfmDBPuliGPnC8p9vID53xhz+9IJEitcUcWhO/4CKHL5/DiV56igBPGOHodxgnhwL7JoJmXm65zZzWfp3vHqyq1Gt6NGhPOkwjGgdcTdb4VG9jywPPFDfOaM1WlhoOjkvCHgZhwt+x45Hxhc+1slg67B27BdQjbUOB24JgI7BN5igIuOjqOXl+g/evvsCHWsCTD1lnhmUaHsgaGwz5RJ7xKHt3cwjjlzdCDtMq2rS10DR2gHzrJ8KkWXmaJCXdjKBrWI+cLS62hoRm4Cgz7wVivA4eq7fFZnDUNCSe8GAbQeYoCThHj6PYK0HgR1tV64tSXszixalVleF9EJo3iySdMeNVYDm9inCAaLpOhrwJdM4tc6Wdx6ha0COjpbVgvpygVe3Zhwt1q453zdauZB+ErbPz0f9FiYXIwwHh3d2mDWQ+0cpIEfG0ArdF6916d6ONxGP1FSC14qXhmFG6NFLXR5+SAJZNhPVa5lBw4nzblmcikUTxfyFOHcU5E4WBMsTLdaz+Z1C1YBTyVik/+n8j1Q/CWCsioUumjzmvfjE3vxytHeAhvaOcLix3GbByP0zER3CApFg04izNQy9Ly+dJGfZlMxfuCpyjgaiAKB2MKlMEiqRRdQEXb/uBoDb2A4jaJcMxuQCow9Py6H+sh0ahuQaBFFOGicAQaC0OVPrJTErG5I9WPFS/uR6gmOqPK3+hGw2tGcXR/5e5t76nKJ9R/FXBx4iFcmPMdHR397+IPDuQAmng084BoVaXOFg4vp8nBNW6QFO8Zz9z1QDPWJNgIP3HixG9/+9sC6gyy4bBqzRIPjjwee1dWG7iHNq+XZ3E6ykksVjHXVozDRviFCxd+/etfX7x4UU+XDUeP7oFgd6x4eQQJpjQi1+QiTR8HpMtCmYHTuCs54iFcmPPt7OxMJBLt7e1PPPGErh6PZjpI/YSSjYsH0ZdsbKrDdmfu1k8zVmQGwnt6ehYsWPDqq69+4xvf0M+xZ8BhVZklHlgpEqmcSgJzlucfYYYENne3OXOXJXmeOKpKlTbtL85AeC6Xmz179t69e2fNmqUf3MmAw5Mbt3FL01afHqZrMB4vVESHV2VxgiWZpCs01N/siF37ZuZ/1YpgPl7E46LempAJMc53aGioqampv7//gw8+aGpqGh4eLqErYvDrRPjYeasxVB1gWA53LeqUfB1gGereN7/5zd27d8PJ50ePHg2F4TIZOgQPJw3hNQO0bVLNtdWBaVZIWPdjis1guC1btqxdu5YQ0traunXr1lAYbuxY8elrYxOPqINlxwqet/ziLNEDo0kB2Udf3FtTN74Y53jy5ElFUUZHR69evaooyqlTp0JRFHQWjEJlG8r4NMDXDHX4pptuOnjwICFk9uzZzz33HGSOAScorJS7rkHRx6BHW1vVZbcMhLe0tCQSCULIhg0blixZEjLDgWOFb95gCMngeSE/wfkuA3yCaUKIwXB6iRDjfN99911FUVRVHRwcVBQlW153yaOZrqI/QtVXsT+qOE1VKf6chi6H+9znPnfgwAFCyMyZM1944QW4zQZVhvT7/yDs/lOVA+vWE0NNWbZsWXt7OyFk/fr1y5cvD5/hoOcSj5e+XqjKGKwzL/uTqkE8uplMUj0rf5zVRIzz1TRtypQpvb29Z8+evfXWW8MydFjJpOkqnP7XlI0aN1auXLl9+3aw4Pnz5yE0gy+okYyXjwP71bjFmINhQxyHVG3fvn3lypWEkBUrVrz00kvjwXBVcx6Q7dAiEXogjunHU1PEOF9CyL59+xYvXrxo0SJoRo2TogD+NyCjTibDm28wFIWLFy/Omzdv9erVmzdv1gEZcPS4gRCC6X/hm7NqBDEQrqrq0qVLW1tbly5dqpa7www41dQJ0j3YlMtfjay/DeEhXJjzJYQMF3+YJR7NMI6fMszhxOM1P8vxU8ly2myEa5o2MDBQxqD/s+FgBP9lWBJh+vjCN8Xgcx20ng9rwkz4ODQc5kU/hci0PsQQqr6X1idxMBvO08+L68tOXdHz+dL3bdX6HXVN2S04T1HAaYnCwZg+yIHyv/G4zbYGoggXheODsWySrLYy2ia44EcwN2hR93kIF9nyNeeZRzMzms939CZw1WlZn5UrJS+KcFE4/rMSEP8L/VbriSNRhIvC8d9wWIOuLtr68etnsc4B1OEhXDpfNybNZumMJ+yMmUxWfNjrBqZ+YXmKAtZKFA7G9E2G72RiMd8GjuDLSdNcOSZEFOGicLBu/su2IzZ1V892xo+HcOl83dtOVem8J3yYDFMo3d10pyOL4Tz3CbDH4CkKOFVROBjTT7n0ZZVPJ7/BJhTlObGqPIgiXBROVSX9vBmJVGxc7Zkq4Pfr02WRzpfDjPk8/aQKtueBHVzg31iMju7BAsZUioYx/GWz1Fnz/5m+exdV90ThcJArOqqqUot4X4edfbAjinBROKLZ58ZrazMvs+UGdQBgO1LPOTUtna8DAzgMks9Tf5rJjLlaOLEO7BePqw8tzcx7nMRieeWmvHITHb7g/MO7SRaVFFX3ROE4ZM67YPpenLbtULM+MM5vtxGiOQ4htDAoinlDAHNYUYSLwjFr6PMd1gVn+o7ALDM1tZq90vn6XCqcJ483QrQdAHQOaQwpqu6JwjHqF4TrTIa2f2Mx50P2pY0Qk6U1L04zAZ9UVOw9YRlVFOGicCwV9esBfOpm6urVVAdMDev13cV2MFIvnW9N/gMUwLgRomjVRNU9UTii8ycIr1CgQxDQJnXWBLbbCNGsFHw466zNC7FFES4Kx5wn/+9Eo076EAY989kCdC+779tKjZ5Moj20DGErL2E4sVbx4CFcDjtUMl7Pq+obIZpS1N/PDBN4PEUBKyIKB2MGToaebDRacyZHn66zP4yyUChuxJ9OF6bOpI1rN98FiCJcFE7gjAWHxbpacFYokGQyqSTo6qSmj2NTL6u/2UE7L/pqpbKBVLU0Wa6qRQvqx/2WA9iwwUO4dL42xAp+hDdCNI3WltKC3lUySR2Cg8OujBryFAWMJQoHYwZRLtZPSjS4YItmzthGiGm79abRKYXYxCNZ5ZbIxIH0/o9d5VcU4aJwXCnvUWBXC85gcD8ajU3vN+7pXCjQ1UrQ9Sm2ppMbhyMROl9DF5U9XigtJ3U2SMxDuHS+HpUc58mA27U+z9MOiacoYFxROBgzuLLugiMR2i21XldUPQuZDI0VieSVmyITB+xbx9URxH3PPc4NZ7G7jZFV2Fa0pvcsFOjceDSqKp+IXfumohC6xbDyCeqD9e6nEdp4zUO4dL5GNn2/1p2vs6maCn15igIGEoWDMYMuw/Jt6JZGo7RllErRDojBF8OallSKjj9C60lRqNDdnc8W4HANhv0kRBEuCiegxrLeY2FMYfebEaqX8rHp/XQ04vqhfLbyfI0x3OoSD+HS+Vbn1K+78B15Ok1rPT4k1aE+PEUBJyEKB2OGRoZuKfat5hWBsJQbvHN5sAKmc2Bk38FoYQUfoggXhVOhXHAuao48wMRozTZvZY5SKdrYLRTov1bjgZUxxq54CJfOd4zHgEjlulwe+3ejFk9RwOmIwsGYYZVVlTZ+9T/rDqk0nBcmjkZpj6TqD/qMFjvgVI2h3wTb6RbU79cUeGqKdL416Q1TAJ6igPMpCgdjStmGAVGEi8KxUdXnR1bfakOPg2Goji8/PIRL58vHfcBi8xQFnBVROBhTyjYMiCJcFI6Nqj4/qnqqJnjeOn25ZJthHsKl87Wl1vFDWBwG+wdYd0wdw7EG5CkKOE1ROBgzmHI6XTrQEizol5KiCBeF4xcPjtItDuzms4V4nA7Npfd8mJz4JB2vZRg1cJSeXSAewqXztWPW+TOYYo1G6SyZdL7OefM9JIwTwhZ1LudpROrOU4exHqJwMGYQ5Vgsf8d9kRuuRm/7mK7t+8KLvnjeAH1e3NfXd+jQIWyqRikKhJoelhlZTQZgWuonsxE+PDzcXTlNwYZTv3zVFRkWjMXjdU2kBjgb4Zqm7d+/H58kxIZTQ7kAPlZV0taWUr5N14ddd0UdVP3SkYdwYS3fo0ePzpgxA45T1Yng0UwHCYUAp5XD1g1ulxkJzCAD4blcrqWlZcqUKVgNBhwcPUSyvs2Zm50YxOePjfCOjg5FUXp7e3WF2HD06CESYMM42IDB1VfHYvPIQ7gw50sI6ejoaFjnm8mURhu6u/3qANFCxVYUenp6Gtb55vOlPRh0C4qtnA7R2AwHFm9M5wvfxBBC6124mjt6kZDOV6diPAhsdbiRnW9ArM5muEZ2vmE3HLUdWx5Onz59A/otXLiwwVu+bDSKjaUUfzUxb7/9dt10kyZNIoRUdb7M7qCmAjKAgQGHhlu1apVuuBtuuOHIkSNVna80nIHe+l06NJyVAozOV9M0Ff1GRkYIIYlEomGHHaz49fi+k4qH7EZFQsjBgwcbdtjBYwNZJefEcCMjI9h2WvHXsGO+Vkx6fN+J4axUYnS+ZrhMJjNt2rSpU6cePnxYf8qjmQ4iBecMMBB+5cqVBx98cOLEiTt27NATYsDR40qBgQE2wjdt2qQoyrp16/QU2XD06FJwywAP4cKcb1WleTSrCihv2jMginBROPbayqc6A6IIF4WjKyYFewZ4CJfO157bkD3lKQo4q6JwMKaUbRgQRbgoHBtV5SPMAA/h0vliJkMv8xQFnHlROBhTyjYMiCJcFI6NqvIRZoCHcOl8MZOhl3mKAs68KByMKWUbBkQRLgrHRlX5CDPAQ7h0vpjJ0Ms8RQFnXhQOxpSyDQOiCBeFY6OqfIQZ4CFcOl/MZOhlnqKAMy8KB2NK2YYBUYSLwrFRVT7CDPAQLp0vZjL0Mk9RwJkXhYMxpWzDgCjCReHYqCofYQZ4CJfOFzMZepmnKODMi8LBmFK2YUAU4aJwbFSVjzADPIRL54uZDL3MUxRw5kXhYEwp2zAginBRODaqykeYAR7CpfPFTIZe5ikKOPOicDCmlG0YEEW4KBwbVeUjzAAP4dL5YiZDL/MUBZx5UTgYU8o2DIgiXBSOjaryEWaAh3DpfDGToZd5igLOvCgcjCllGwZEES4Kx0ZV+QgzwEO4p86XR1GcYSkDA2Y+zXfYuDLjmO+wIctYVW0nil4zjvmONAEPAwY+DZeukOvufGHLS/mvNwxcc801rsxvFdgbbWUqOgOiDJfL5XRMKXjDgFUlqnm/vs63ZvIygGRAMiAZaEwGpPNtTLvLXEsGJAM+MyCdr88GYE7efN47M5SM6CUD5vPevUxdpsXDQF9f36FDh3gQcFyPnG8ul5s9e/bevXtnzZqVy+WwBlJmYKDqee8MODWjaJr29a9/fdeuXQ888EBPT0/N8DJATQbM573XjMIWoLOzM5FItLe3P/HEE2wIMhZm4OjRozNmzDCclIYDuJU9cr5btmxZu3YtIaS1tXXr1q1utZThzQyYT700h+G/89Zbb915552EkFdffXXevHn8gBLBfOplPTgZGhpqamrq7+//4IMPmpqahoeH65FKo2F2dHSEz/m2tLQkEglCyIYNG5YsWdJoNqtHfr1xvs8+++ycOXMIIfv27YOjjuuRl0bDNJx6WY/snzx5UlGU0dHRq1evKopy6tSpeqTSaJihdL7Lli1rb28nhKxfv3758uWNZrN65Ncb57tt27avfvWrhJA33nhj8uTJ9chIA2J64HzfffddRVFUVR0cHFQUJZvNNiDPwrMcSue7fft2aK6vWLHipZdeEk5KAwKaz3uvBwnnzp277bbbNE178cUXV61aVY8kGg1T0zQPnK+maVOmTOnt7T179uytt96qaVqj8VyP/CYSifANO6iqunTp0tbW1qVLl6qqWg9eGgqz6nnvdWLgJz/5ySOPPDJ//vxLly7VKYmGgjWf916n7O/bt2/x4sWLFi06cOBAnZJoKNhMJjNt2rSpU6cePnxYSMY9mnADXQcGBoQoLUE8ZmBgYEA2nTzmXEhyw8WfECgJIpwBT52vcO0loGRAMiAZCCkD0vmG1HBSbcmAZCDcDEjnG277Se0lA5KBkDIgnW9IDSfVlgxIBsLNwP8DynUfTnle3EMAAAAASUVORK5CYII="
    },
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "60f380d7",
   "metadata": {},
   "source": [
    "## Regularization in Neural Networks\n",
    "\n",
    "The number of input and output units in a neural network is generally determined by the dimensionality of the data set, whereas the number $ M $ of hidden units is a free parameter that can be adjusted to give the best predictive performance. Note that $ M $ controls the number of parameters (weights and biases) in the network, and so we might expect that in a maximum likelihood setting there will be an optimum value of $ M $ that gives the best generalization performance, corresponding to the optimum balance between underfitting and overfitting. \n",
    "\n",
    "![image-2.png](attachment:image-2.png)\n",
    "\n",
    "Fig.9 Examples of two-layer networks trained on 10 data points drawn from the sinusoidal data set. The graphs show the result of ﬁtting networks having M = 1, 3 and 10 hidden units, respectively, by minimizing a sum-of-squares error function using a scaled conjugate-gradient algorithm.\n",
    "\n",
    "Figure 5.9 shows an example of the effect of different values of $ M $ for the sinusoidal regression problem.\n",
    "\n",
    "The generalization error, however, is not a simple function of $ M $ due to the presence of local minima in the error function, as illustrated in Fig.10. Here we see the effect of choosing multiple random initializations for the weight vector for a range of values of $ M $. The overall best validation set performance in this case occurred for a particular solution having $ M = 8 $.\n",
    "\n",
    "In practice, one approach to choosing $ M $ is in fact to plot a graph of the kind shown in Fig.10 and then to choose the specific solution having the smallest validation set error. There are, however, other ways to control the complexity of a neural network model in order to avoid overfitting. From our discussion of polynomial curve fitting in Chapter 1, we see that an alternative approach is to choose a relatively large value for $ M $ and then to control complexity by the addition of a regularization term to the error function.\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "Fig.10 Plot of the sum-of-squares test-set error for the polynomial data set versus the number of hidden units in the network, with 30 random starts for each network size, showing the ef- fect of local minima. For each new start, the weight vector was initial- ized by sampling from an isotropic Gaussian distribution having a mean of zero and a variance of 10.\n",
    "\n",
    "\n",
    "The simplest regularizer is the quadratic, giving a regularized error:\n",
    "\n",
    "$$\n",
    "E_{\\text{reg}}(w) = E(w) + \\frac{\\lambda}{2} w^T w\n",
    "$$\n",
    "\n",
    "This regularizer is also known as weight decay and has been discussed at length in Chapter 3. The effective model complexity is then determined by the choice of the regularization coefficient $ \\lambda $. As we have seen previously, this regularizer can be interpreted as the negative logarithm of a zero-mean Gaussian prior distribution over the weight vector $ w $.\n",
    "\n",
    "##  Consistent Gaussian Priors\n",
    "\n",
    "One of the limitations of simple weight decay in the form\n",
    "\n",
    "$$\n",
    "E_{\\text{reg}}(w) = E(w) + \\frac{\\lambda}{2} w^T w\n",
    "$$\n",
    "\n",
    "is that it is inconsistent with certain scaling properties of network mappings. To illustrate this, consider a multilayer perceptron network having two layers of weights and linear output units, which performs a mapping from a set of input variables $ \\{x_i\\} $ to a set of output variables $ \\{y_k\\} $. The activations of the hidden units in the first hidden layer are given by\n",
    "\n",
    "$$\n",
    "z_j = h\\left( \\sum_i w_{ji} x_i + w_j^0 \\right)\n",
    "$$\n",
    "\n",
    "while the activations of the output units are given by\n",
    "\n",
    "$$\n",
    "y_k = \\sum_j w_{kj} z_j + w_k^0\n",
    "$$\n",
    "\n",
    "Suppose we perform a linear transformation of the input data of the form\n",
    "\n",
    "$$\n",
    "x_i \\rightarrow \\tilde{x}_i = a x_i + b\n",
    "$$\n",
    "\n",
    "Then we can arrange for the mapping performed by the network to be unchanged by making a corresponding linear transformation of the weights and biases from the inputs to the units in the hidden layer:\n",
    "\n",
    "$$\n",
    "w_{ji} \\rightarrow \\tilde{w}_{ji} = a w_{ji}\n",
    "$$\n",
    "\n",
    "$$\n",
    "w_j^0 \\rightarrow \\tilde{w}_j^0 = w_j^0 - a \\sum_i w_{ji}\n",
    "$$\n",
    "\n",
    "Similarly, a linear transformation of the output variables of the network of the form\n",
    "\n",
    "$$\n",
    "y_k \\rightarrow \\tilde{y}_k = c y_k + d\n",
    "$$\n",
    "\n",
    "can be achieved by making a transformation of the second-layer weights and biases:\n",
    "\n",
    "$$\n",
    "w_{kj} \\rightarrow \\tilde{w}_{kj} = c w_{kj}\n",
    "$$\n",
    "\n",
    "$$\n",
    "w_k^0 \\rightarrow \\tilde{w}_k^0 = c w_k^0 + d\n",
    "$$\n",
    "\n",
    "If we train one network using the original data and one network using data for which the input and/or target variables are transformed by one of the above linear transformations, then consistency requires that we should obtain equivalent networks that differ only by the linear transformation of the weights as given. Any regularizer should be consistent with this property; otherwise, it arbitrarily favours one solution over another, equivalent one.\n",
    "\n",
    "Clearly, simple weight decay\n",
    "\n",
    "$$\n",
    "E_{\\text{reg}}(w) = E(w) + \\frac{\\lambda}{2} w^T w\n",
    "$$\n",
    "\n",
    "that treats all weights and biases on an equal footing does not satisfy this property. We therefore look for a regularizer which is invariant under the linear transformations. These require that the regularizer should be invariant to rescaling of the weights and to shifts of the biases. Such a regularizer is given by\n",
    "\n",
    "$$\n",
    "E_{\\text{reg}}(w) = \\lambda_1 \\sum_{w \\in W_1} \\|w\\|^2 + \\lambda_2 \\sum_{w \\in W_2} \\|w\\|^2\n",
    "$$\n",
    "\n",
    "where $ W_1 $ denotes the set of weights in the first layer, $ W_2 $ denotes the set of weights in the second layer, and biases are excluded from the summations. This regularizer will remain unchanged under the weight transformations, provided the regularization parameters are rescaled as follows:\n",
    "\n",
    "$$\n",
    "\\lambda_1 \\rightarrow a^{1/2} \\lambda_1\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\lambda_2 \\rightarrow c^{-1/2} \\lambda_2\n",
    "$$\n",
    "\n",
    "The regularizer\n",
    "\n",
    "$$\n",
    "E_{\\text{reg}}(w) \\propto \\exp \\left( -\\frac{1}{2} \\sum_{w \\in W_1} \\|w\\|^2 - \\frac{1}{2} \\sum_{w \\in W_2} \\|w\\|^2 \\right)\n",
    "$$\n",
    "\n",
    "corresponds to a prior of the form\n",
    "\n",
    "$$\n",
    "p(w | \\alpha_1, \\alpha_2) \\propto \\exp \\left( -\\frac{1}{2} \\sum_{w \\in W_1} \\|w\\|^2 - \\frac{1}{2} \\sum_{w \\in W_2} \\|w\\|^2 \\right)\n",
    "$$\n",
    "\n",
    "Note that priors of this form are improper (they cannot be normalized) because the bias parameters are unconstrained. The use of improper priors can lead to difficulties in selecting regularization coefficients and in model comparison within the Bayesian framework, because the corresponding evidence is zero. It is therefore common to include separate priors for the biases, which then break shift invariance, having their own hyperparameters.\n",
    "\n",
    "## Regularization in Neural Networks\n",
    "\n",
    "The number of input and output units in a neural network is generally determined by the dimensionality of the data set, whereas the number \\( M \\) of hidden units is a free parameter that can be adjusted to give the best predictive performance. Note that \\( M \\) controls the number of parameters (weights and biases) in the network, and so we might expect that in a maximum likelihood setting there will be an optimum value of \\( M \\) that gives the best generalization performance, corresponding to the optimum balance between underfitting and overfitting. Figure 5.9 shows an example of the effect of different values of \\( M \\) for the sinusoidal regression problem.\n",
    "\n",
    "The generalization error, however, is not a simple function of $ M $ due to the presence of local minima in the error function, as illustrated in Fig.10. Here we see the effect of choosing multiple random initializations for the weight vector for a range of values of $ M $. The overall best validation set performance in this case occurred for a particular solution having $ M = 8 $.\n",
    "\n",
    "In practice, one approach to choosing $ M $ is in fact to plot a graph of the kind shown in Figure 5.10 and then to choose the specific solution having the smallest validation set error. There are, however, other ways to control the complexity of a neural network model in order to avoid overfitting. From our discussion of polynomial curve fitting in Chapter 1, we see that an alternative approach is to choose a relatively large value for $ M $ and then to control complexity by the addition of a regularization term to the error function.\n",
    "\n",
    "The simplest regularizer is the quadratic, giving a regularized error:\n",
    "\n",
    "$$\n",
    "E_{\\text{reg}}(w) = E(w) + \\frac{\\lambda}{2} w^T w\n",
    "$$\n",
    "\n",
    "This regularizer is also known as weight decay and has been discussed at length in Chapter 3. The effective model complexity is then determined by the choice of the regularization coefficient \\( \\lambda \\). As we have seen previously, this regularizer can be interpreted as the negative logarithm of a zero-mean Gaussian prior distribution over the weight vector \\( w \\).\n",
    "\n",
    "##  Consistent Gaussian Priors\n",
    "\n",
    "One of the limitations of simple weight decay in the form\n",
    "\n",
    "$$\n",
    "E_{\\text{reg}}(w) = E(w) + \\frac{\\lambda}{2} w^T w\n",
    "$$\n",
    "\n",
    "is that it is inconsistent with certain scaling properties of network mappings. To illustrate this, consider a multilayer perceptron network having two layers of weights and linear output units, which performs a mapping from a set of input variables $ \\{x_i\\} $ to a set of output variables $ \\{y_k\\} $. The activations of the hidden units in the first hidden layer are given by\n",
    "\n",
    "$$\n",
    "z_j = h\\left( \\sum_i w_{ji} x_i + w_j^0 \\right)\n",
    "$$\n",
    "\n",
    "while the activations of the output units are given by\n",
    "\n",
    "$$\n",
    "y_k = \\sum_j w_{kj} z_j + w_k^0\n",
    "$$\n",
    "\n",
    "Suppose we perform a linear transformation of the input data of the form\n",
    "\n",
    "$$\n",
    "x_i \\rightarrow \\tilde{x}_i = a x_i + b\n",
    "$$\n",
    "\n",
    "Then we can arrange for the mapping performed by the network to be unchanged by making a corresponding linear transformation of the weights and biases from the inputs to the units in the hidden layer:\n",
    "\n",
    "$$\n",
    "w_{ji} \\rightarrow \\tilde{w}_{ji} = a w_{ji}\n",
    "$$\n",
    "\n",
    "$$\n",
    "w_j^0 \\rightarrow \\tilde{w}_j^0 = w_j^0 - a \\sum_i w_{ji}\n",
    "$$\n",
    "\n",
    "Similarly, a linear transformation of the output variables of the network of the form\n",
    "\n",
    "$$\n",
    "y_k \\rightarrow \\tilde{y}_k = c y_k + d\n",
    "$$\n",
    "\n",
    "can be achieved by making a transformation of the second-layer weights and biases:\n",
    "\n",
    "$$\n",
    "w_{kj} \\rightarrow \\tilde{w}_{kj} = c w_{kj}\n",
    "$$\n",
    "\n",
    "$$\n",
    "w_k^0 \\rightarrow \\tilde{w}_k^0 = c w_k^0 + d\n",
    "$$\n",
    "\n",
    "If we train one network using the original data and one network using data for which the input and/or target variables are transformed by one of the above linear transformations, then consistency requires that we should obtain equivalent networks that differ only by the linear transformation of the weights as given. Any regularizer should be consistent with this property; otherwise, it arbitrarily favours one solution over another, equivalent one.\n",
    "\n",
    "Clearly, simple weight decay\n",
    "\n",
    "$$\n",
    "E_{\\text{reg}}(w) = E(w) + \\frac{\\lambda}{2} w^T w\n",
    "$$\n",
    "\n",
    "that treats all weights and biases on an equal footing does not satisfy this property. We therefore look for a regularizer which is invariant under the linear transformations. These require that the regularizer should be invariant to rescaling of the weights and to shifts of the biases. Such a regularizer is given by\n",
    "\n",
    "$$\n",
    "E_{\\text{reg}}(w) = \\lambda_1 \\sum_{w \\in W_1} \\|w\\|^2 + \\lambda_2 \\sum_{w \\in W_2} \\|w\\|^2\n",
    "$$\n",
    "\n",
    "where $ W_1 $ denotes the set of weights in the first layer, $ W_2 $ denotes the set of weights in the second layer, and biases are excluded from the summations. This regularizer will remain unchanged under the weight transformations, provided the regularization parameters are rescaled as follows:\n",
    "\n",
    "$$\n",
    "\\lambda_1 \\rightarrow a^{1/2} \\lambda_1\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\lambda_2 \\rightarrow c^{-1/2} \\lambda_2\n",
    "$$\n",
    "\n",
    "The regularizer\n",
    "\n",
    "$$\n",
    "E_{\\text{reg}}(w) \\propto \\exp \\left( -\\frac{1}{2} \\sum_{w \\in W_1} \\|w\\|^2 - \\frac{1}{2} \\sum_{w \\in W_2} \\|w\\|^2 \\right)\n",
    "$$\n",
    "\n",
    "corresponds to a prior of the form\n",
    "\n",
    "$$\n",
    "p(w | \\alpha_1, \\alpha_2) \\propto \\exp \\left( -\\frac{1}{2} \\sum_{w \\in W_1} \\|w\\|^2 - \\frac{1}{2} \\sum_{w \\in W_2} \\|w\\|^2 \\right)\n",
    "$$\n",
    "\n",
    "Note that priors of this form are improper (they cannot be normalized) because the bias parameters are unconstrained. The use of improper priors can lead to difficulties in selecting regularization coefficients and in model comparison within the Bayesian framework, because the corresponding evidence is zero. It is therefore common to include separate priors for the biases, which then break shift invariance, having their own hyperparameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16b84030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 102.10469537698627\n",
      "Epoch 100, Loss: 99.50166066932483\n",
      "Epoch 200, Loss: 99.71626484219182\n",
      "Epoch 300, Loss: 99.76359259131844\n",
      "Epoch 400, Loss: 99.77095240638324\n",
      "Epoch 500, Loss: 99.7720215417094\n",
      "Epoch 600, Loss: 99.77215236662506\n",
      "Epoch 700, Loss: 99.77215832013599\n",
      "Epoch 800, Loss: 99.7721539389246\n",
      "Epoch 900, Loss: 99.77215141035673\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "# Sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "# Derivative of sigmoid function\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "# Mean Squared Error Loss Function with Regularization\n",
    "def loss_with_regularization(y_pred, y_true, weights1, weights2, lambda_reg):\n",
    "    mse = sum((y_pred[i] - y_true[i]) ** 2 for i in range(len(y_pred))) / len(y_pred)\n",
    "    \n",
    "    # Weight decay (L2 regularization)\n",
    "    l2_loss = lambda_reg * (sum(w ** 2 for row in weights1 for w in row) +\n",
    "                            sum(w ** 2 for row in weights2 for w in row)) / 2\n",
    "    \n",
    "    return mse + l2_loss\n",
    "\n",
    "# Forward pass of the neural network\n",
    "def forward_pass(X, weights1, weights2):\n",
    "    # Hidden layer activations\n",
    "    z_hidden = [0] * len(weights1[0])\n",
    "    for j in range(len(weights1[0])):\n",
    "        z_hidden[j] = sum(X[i] * weights1[i][j] for i in range(len(X))) + weights1[-1][j]\n",
    "    \n",
    "    a_hidden = [sigmoid(z) for z in z_hidden]\n",
    "    \n",
    "    # Output layer activations\n",
    "    z_output = [0] * len(weights2[0])\n",
    "    for k in range(len(weights2[0])):\n",
    "        z_output[k] = sum(a_hidden[j] * weights2[j][k] for j in range(len(a_hidden))) + weights2[-1][k]\n",
    "    \n",
    "    y_output = z_output  # Linear output (no activation function)\n",
    "    \n",
    "    return a_hidden, z_hidden, y_output\n",
    "\n",
    "# Backpropagation to compute gradients\n",
    "def backpropagate(X, T, a_hidden, z_hidden, y_output, weights1, weights2, lambda_reg):\n",
    "    # Compute output layer errors\n",
    "    delta_output = [0] * len(T)\n",
    "    for k in range(len(T)):\n",
    "        delta_output[k] = (y_output[k] - T[k])\n",
    "    \n",
    "    # Compute gradients for weights2\n",
    "    grad_w2 = [[0] * len(weights2[0]) for _ in range(len(weights2))]\n",
    "    for j in range(len(a_hidden)):  # Loop over hidden layer activations\n",
    "        for k in range(len(weights2[0])):  # Loop over output neurons\n",
    "            grad_w2[j][k] = a_hidden[j] * delta_output[k] + lambda_reg * weights2[j][k]\n",
    "    \n",
    "    # Compute hidden layer errors\n",
    "    delta_hidden = [0] * len(a_hidden)\n",
    "    for j in range(len(a_hidden)):\n",
    "        delta_hidden[j] = sum(weights2[j][k] * delta_output[k] for k in range(len(delta_output))) * sigmoid_derivative(z_hidden[j])\n",
    "    \n",
    "    # Compute gradients for weights1\n",
    "    grad_w1 = [[0] * len(weights1[0]) for _ in range(len(weights1))]\n",
    "    for i in range(len(X)):  # Loop over input features\n",
    "        for j in range(len(a_hidden)):  # Loop over hidden layer neurons\n",
    "            grad_w1[i][j] = X[i] * delta_hidden[j] + lambda_reg * weights1[i][j]\n",
    "    \n",
    "    return grad_w1, grad_w2, delta_output\n",
    "\n",
    "# Update weights using gradient descent\n",
    "def update_weights(weights1, weights2, grad_w1, grad_w2, learning_rate):\n",
    "    for i in range(len(weights1)):\n",
    "        for j in range(len(weights1[0])):\n",
    "            weights1[i][j] -= learning_rate * grad_w1[i][j]\n",
    "    \n",
    "    for j in range(len(weights2)):\n",
    "        for k in range(len(weights2[0])):\n",
    "            weights2[j][k] -= learning_rate * grad_w2[j][k]\n",
    "    \n",
    "    return weights1, weights2\n",
    "\n",
    "# Train the neural network\n",
    "def train_neural_network(X, T, weights1, weights2, lambda_reg, learning_rate, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for i in range(len(X)):\n",
    "            a_hidden, z_hidden, y_output = forward_pass(X[i], weights1, weights2)\n",
    "            \n",
    "            # Compute loss and gradients\n",
    "            loss = loss_with_regularization(y_output, T[i], weights1, weights2, lambda_reg)\n",
    "            total_loss += loss\n",
    "            \n",
    "            grad_w1, grad_w2, delta_output = backpropagate(X[i], T[i], a_hidden, z_hidden, y_output, weights1, weights2, lambda_reg)\n",
    "            \n",
    "            # Update weights\n",
    "            weights1, weights2 = update_weights(weights1, weights2, grad_w1, grad_w2, learning_rate)\n",
    "        \n",
    "        # Print loss every 100 epochs\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {total_loss / len(X)}\")\n",
    "\n",
    "    return weights1, weights2\n",
    "\n",
    "# Initialize weights\n",
    "def initialize_weights(input_size, hidden_size, output_size):\n",
    "    weights1 = [[random.uniform(-0.5, 0.5) for _ in range(hidden_size)] for _ in range(input_size)]  # Weights from input to hidden\n",
    "    weights1.append([random.uniform(-0.5, 0.5) for _ in range(hidden_size)])  # Bias for hidden layer\n",
    "    weights2 = [[random.uniform(-0.5, 0.5) for _ in range(output_size)] for _ in range(hidden_size)]  # Weights from hidden to output\n",
    "    weights2.append([random.uniform(-0.5, 0.5) for _ in range(output_size)])  # Bias for output layer\n",
    "    return weights1, weights2\n",
    "\n",
    "# Example usage\n",
    "input_size = 1  # Example for a single input (e.g., for simple regression)\n",
    "hidden_size = 10\n",
    "output_size = 1  # Example for a single output (regression task)\n",
    "lambda_reg = 0.01\n",
    "learning_rate = 0.1\n",
    "epochs = 1000\n",
    "\n",
    "# Example data (for a regression task)\n",
    "X = [[i] for i in range(-5, 6)]  # Input values\n",
    "T = [[i ** 2] for i in range(-5, 6)]  # Target values (e.g., quadratic function)\n",
    "\n",
    "# Initialize weights\n",
    "weights1, weights2 = initialize_weights(input_size, hidden_size, output_size)\n",
    "\n",
    "# Train the neural network\n",
    "weights1, weights2 = train_neural_network(X, T, weights1, weights2, lambda_reg, learning_rate, epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55388521",
   "metadata": {},
   "source": [
    "## Regularization in Neural Networks\n",
    "\n",
    "The number of input and output units in a neural network is generally determined by the dimensionality of the data set, whereas the number $ M $ of hidden units is a free parameter that can be adjusted to give the best predictive performance. Note that $ M $ controls the number of parameters (weights and biases) in the network, and so we might expect that in a maximum likelihood setting, there will be an optimum value of $ M $ that gives the best generalization performance, corresponding to the optimum balance between underfitting and overfitting.\n",
    "\n",
    "### Regularization Term\n",
    "\n",
    "The simplest regularizer is the quadratic, giving a regularized error of the form:\n",
    "\n",
    "$$\n",
    "E(w) = E(w) + \\frac{\\lambda}{2} \\| w \\|^2\n",
    "$$\n",
    "\n",
    "This regularizer is also known as **weight decay** and has been discussed at length in Chapter 3. The effective model complexity is then determined by the choice of the regularization coefficient $ \\lambda $. As we have seen previously, this regularizer can be interpreted as the negative logarithm of a zero-mean Gaussian prior distribution over the weight vector $ w $.\n",
    "\n",
    "### Consistent Gaussian Priors\n",
    "\n",
    "One of the limitations of simple weight decay in the form above is that it is inconsistent with certain scaling properties of network mappings. To illustrate this, consider a multilayer perceptron network having two layers of weights and linear output units, which performs a mapping from a set of input variables $ \\{ x_i \\} $ to a set of output variables $ \\{ y_k \\} $.\n",
    "\n",
    "The activations of the hidden units in the first hidden layer take the form:\n",
    "\n",
    "$$\n",
    "z_j = \\sum_i w_{ji} x_i + w_{j0}\n",
    "$$\n",
    "\n",
    "while the activations of the output units are given by:\n",
    "\n",
    "$$\n",
    "y_k = \\sum_j w_{kj} z_j + w_{k0}\n",
    "$$\n",
    "\n",
    "Suppose we perform a linear transformation of the input data of the form:\n",
    "\n",
    "$$\n",
    "x_i \\rightarrow \\tilde{x}_i = a x_i + b\n",
    "$$\n",
    "\n",
    "Then we can arrange for the mapping performed by the network to be unchanged by making a corresponding linear transformation of the weights and biases from the inputs to the hidden layer:\n",
    "\n",
    "$$\n",
    "w_{ji} \\rightarrow \\tilde{w}_{ji} = \\frac{w_{ji}}{a}\n",
    "$$\n",
    "$$\n",
    "w_{j0} \\rightarrow \\tilde{w}_{j0} = w_{j0} - w_{ji} \\frac{b}{a}\n",
    "$$\n",
    "\n",
    "Similarly, a linear transformation of the output variables of the network of the form:\n",
    "\n",
    "$$\n",
    "y_k \\rightarrow \\tilde{y}_k = c y_k + d\n",
    "$$\n",
    "\n",
    "can be achieved by making a transformation of the second-layer weights and biases using:\n",
    "\n",
    "$$\n",
    "w_{kj} \\rightarrow \\tilde{w}_{kj} = c w_{kj}\n",
    "$$\n",
    "$$\n",
    "w_{k0} \\rightarrow \\tilde{w}_{k0} = c w_{k0} + d\n",
    "$$\n",
    "\n",
    "If we train one network using the original data and one network using data for which the input and/or target variables are transformed by one of the above linear transformations, then consistency requires that we should obtain equivalent networks that differ only by the linear transformation of the weights as given. Any regularizer should be consistent with this property, otherwise it arbitrarily favors one solution over another, equivalent one.\n",
    "\n",
    "### Invariant Regularizer\n",
    "\n",
    "The regularizer should be invariant to re-scaling of the weights and to shifts of the biases. A suitable regularizer is:\n",
    "\n",
    "$$\n",
    "\\lambda_1 \\sum_{w \\in W_1} w^2 + \\lambda_2 \\sum_{w \\in W_2} w^2\n",
    "$$\n",
    "\n",
    "where $ W_1 $ and $ W_2 $ denote the sets of weights in the first and second layers, respectively, and biases are excluded from the summations. This regularizer will remain unchanged under the weight transformations provided the regularization parameters are re-scaled using:\n",
    "\n",
    "$$\n",
    "\\lambda_1 \\rightarrow a^{1/2} \\lambda_1\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\lambda_2 \\rightarrow c^{-1/2} \\lambda_2\n",
    "$$\n",
    "\n",
    "This regularizer corresponds to a prior of the form:\n",
    "\n",
    "$$\n",
    "p(w | \\alpha_1, \\alpha_2) \\propto \\exp \\left( - \\sum_{w \\in W_1} \\frac{w^2}{2\\alpha_1^2} - \\sum_{w \\in W_2} \\frac{w^2}{2\\alpha_2^2} \\right)\n",
    "$$\n",
    "\n",
    "Note that priors of this form are **improper** (they cannot be normalized) because the bias parameters are unconstrained. The use of improper priors can lead to difficulties in selecting regularization coefficients and in model comparison within the Bayesian framework, because the corresponding evidence is zero. It is therefore common to include separate priors for the biases (which then break shift invariance) having their own hyperparameters.\n",
    "\n",
    "### Automatic Relevance Determination (ARD)\n",
    "\n",
    "We can generalize this idea further by considering priors in which the weights are divided into any number of groups $ W_k $ such that:\n",
    "\n",
    "$$\n",
    "p(w) \\propto \\exp \\left( - \\sum_k \\frac{\\| w_k \\|^2}{2\\alpha_k^2} \\right)\n",
    "$$\n",
    "\n",
    "where $ \\| w_k \\|^2 = \\sum_{j \\in W_k} w_j^2 $. As a special case of this prior, if we choose the groups to correspond to the sets of weights associated with each of the input units, and we optimize the marginal likelihood with respect to the corresponding parameters $ \\alpha_k $, we obtain **automatic relevance determination** (ARD) as discussed in Section 7.2.2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2d5924",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "# Sigmoid function and its derivative\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "# Function for forward pass\n",
    "def forward_pass(X, weights1, weights2):\n",
    "    # Compute activations of the hidden layer\n",
    "    z_hidden = []\n",
    "    a_hidden = []\n",
    "    for i in range(len(weights1)):\n",
    "        z = sum(X[j] * weights1[i][j] for j in range(len(X))) + weights1[i][-1]  # Add bias term\n",
    "        z_hidden.append(z)\n",
    "        a_hidden.append(sigmoid(z))\n",
    "    \n",
    "    # Compute output layer activations\n",
    "    y_output = []\n",
    "    for i in range(len(weights2)):\n",
    "        y = sum(a_hidden[j] * weights2[i][j] for j in range(len(a_hidden))) + weights2[i][-1]  # Add bias term\n",
    "        y_output.append(y)\n",
    "    \n",
    "    return a_hidden, z_hidden, y_output\n",
    "\n",
    "# Backpropagation function\n",
    "def backpropagate(X, T, a_hidden, z_hidden, y_output, weights1, weights2, lambda_reg, alpha1, alpha2):\n",
    "    # Output layer error\n",
    "    delta_output = [y_output[i] - T[i] for i in range(len(T))]\n",
    "    \n",
    "    # Compute gradients for weights2 (output layer)\n",
    "    grad_w2 = [[a_hidden[j] * delta_output[i] + lambda_reg * weights2[i][j] for j in range(len(a_hidden))] for i in range(len(weights2))]\n",
    "    \n",
    "    # Backpropagate error to hidden layer\n",
    "    delta_hidden = [sum(weights2[i][j] * delta_output[i] for i in range(len(weights2))) * sigmoid_derivative(z_hidden[j]) for j in range(len(z_hidden))]\n",
    "    \n",
    "    # Compute gradients for weights1 (hidden layer)\n",
    "    grad_w1 = [[X[i] * delta_hidden[j] + lambda_reg * weights1[j][i] for i in range(len(X))] for j in range(len(weights1))]\n",
    "    \n",
    "    return grad_w1, grad_w2, delta_output\n",
    "\n",
    "# Training function\n",
    "def train_neural_network(X, T, weights1, weights2, alpha1, alpha2, learning_rate=0.01, epochs=100):\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for i in range(len(X)):\n",
    "            # Forward pass\n",
    "            a_hidden, z_hidden, y_output = forward_pass(X[i], weights1, weights2)\n",
    "            \n",
    "            # Compute loss (sum of squared errors)\n",
    "            loss = sum((y_output[j] - T[i][j])**2 for j in range(len(T[i])))\n",
    "            total_loss += loss\n",
    "            \n",
    "            # Backpropagation\n",
    "            grad_w1, grad_w2, delta_output = backpropagate(X[i], T[i], a_hidden, z_hidden, y_output, weights1, weights2, lambda_reg=0, alpha1=alpha1, alpha2=alpha2)\n",
    "            \n",
    "            # Update weights\n",
    "            for j in range(len(weights1)):\n",
    "                for k in range(len(weights1[j])):\n",
    "                    weights1[j][k] -= learning_rate * grad_w1[j][k]\n",
    "            \n",
    "            for j in range(len(weights2)):\n",
    "                for k in range(len(weights2[j])):\n",
    "                    weights2[j][k] -= learning_rate * grad_w2[j][k]\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(X)}\")\n",
    "    \n",
    "    return weights1, weights2\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "# Generate random data for testing\n",
    "X = [[random.uniform(0, 1) for _ in range(5)] for _ in range(10)]  # 10 samples, 5 features each\n",
    "T = [[random.uniform(0, 1)] for _ in range(10)]  # 10 target values, 1 output each\n",
    "\n",
    "# Initialize random weights for the neural network\n",
    "weights1 = [[random.uniform(-0.5, 0.5) for _ in range(6)] for _ in range(10)]  # 10 hidden units, 5 inputs + 1 bias\n",
    "weights2 = [[random.uniform(-0.5, 0.5) for _ in range(11)] for _ in range(1)]  # 1 output unit, 10 hidden units + 1 bias\n",
    "\n",
    "# Define alpha values for the regularizer\n",
    "alpha1 = 0.1\n",
    "alpha2 = 0.1\n",
    "\n",
    "# Train the neural network\n",
    "weights1, weights2 = train_neural_network(X, T, weights1, weights2, alpha1, alpha2, learning_rate=0.01, epochs=100)\n",
    "\n",
    "# The network weights will be updated after training\n",
    "print(\"Trained weights1:\", weights1)\n",
    "print(\"Trained weights2:\", weights2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8905df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "# Sigmoid function and its derivative\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "# Function for forward pass\n",
    "def forward_pass(X, weights1, weights2):\n",
    "    # Compute activations of the hidden layer\n",
    "    z_hidden = []\n",
    "    a_hidden = []\n",
    "    for i in range(len(weights1)):  # For each hidden unit\n",
    "        z = sum(X[j] * weights1[i][j] for j in range(len(X))) + weights1[i][-1]  # Add bias term\n",
    "        z_hidden.append(z)\n",
    "        a_hidden.append(sigmoid(z))\n",
    "    \n",
    "    # Compute output layer activations\n",
    "    y_output = []\n",
    "    for i in range(len(weights2)):  # For each output unit\n",
    "        y = sum(a_hidden[j] * weights2[i][j] for j in range(len(a_hidden))) + weights2[i][-1]  # Add bias term\n",
    "        y_output.append(y)\n",
    "    \n",
    "    return a_hidden, z_hidden, y_output\n",
    "\n",
    "# Backpropagation function\n",
    "def backpropagate(X, T, a_hidden, z_hidden, y_output, weights1, weights2, lambda_reg, alpha1, alpha2):\n",
    "    # Output layer error\n",
    "    delta_output = [y_output[i] - T[i] for i in range(len(T))]\n",
    "    \n",
    "    # Compute gradients for weights2 (output layer)\n",
    "    grad_w2 = [[a_hidden[j] * delta_output[i] + lambda_reg * weights2[i][j] for j in range(len(a_hidden))] for i in range(len(weights2))]\n",
    "    \n",
    "    # Backpropagate error to hidden layer\n",
    "    delta_hidden = [sum(weights2[i][j] * delta_output[i] for i in range(len(weights2))) * sigmoid_derivative(z_hidden[j]) for j in range(len(z_hidden))]\n",
    "    \n",
    "    # Compute gradients for weights1 (hidden layer)\n",
    "    grad_w1 = [[X[i] * delta_hidden[j] + lambda_reg * weights1[j][i] for i in range(len(X))] for j in range(len(weights1))]\n",
    "    \n",
    "    return grad_w1, grad_w2, delta_output\n",
    "\n",
    "# Training function\n",
    "def train_neural_network(X, T, weights1, weights2, alpha1, alpha2, learning_rate=0.01, epochs=100):\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for i in range(len(X)):\n",
    "            # Forward pass\n",
    "            a_hidden, z_hidden, y_output = forward_pass(X[i], weights1, weights2)\n",
    "            \n",
    "            # Compute loss (sum of squared errors)\n",
    "            loss = sum((y_output[j] - T[i][j])**2 for j in range(len(T[i])))\n",
    "            total_loss += loss\n",
    "            \n",
    "            # Backpropagation\n",
    "            grad_w1, grad_w2, delta_output = backpropagate(X[i], T[i], a_hidden, z_hidden, y_output, weights1, weights2, lambda_reg=0, alpha1=alpha1, alpha2=alpha2)\n",
    "            \n",
    "            # Update weights\n",
    "            for j in range(len(weights1)):  # For each hidden unit\n",
    "                for k in range(len(weights1[j])):  # For each input feature + bias\n",
    "                    weights1[j][k] -= learning_rate * grad_w1[j][k]\n",
    "            \n",
    "            for j in range(len(weights2)):  # For each output unit\n",
    "                for k in range(len(weights2[j])):  # For each hidden unit + bias\n",
    "                    weights2[j][k] -= learning_rate * grad_w2[j][k]\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(X)}\")\n",
    "    \n",
    "    return weights1, weights2\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "# Generate random data for testing\n",
    "X = [[random.uniform(0, 1) for _ in range(5)] for _ in range(10)]  # 10 samples, 5 features each\n",
    "T = [[random.uniform(0, 1)] for _ in range(10)]  # 10 target values, 1 output each\n",
    "\n",
    "# Initialize random weights for the neural network\n",
    "weights1 = [[random.uniform(-0.5, 0.5) for _ in range(6)] for _ in range(10)]  # 10 hidden units, 5 inputs + 1 bias\n",
    "weights2 = [[random.uniform(-0.5, 0.5) for _ in range(11)] for _ in range(1)]  # 1 output unit, 10 hidden units + 1 bias\n",
    "\n",
    "# Define alpha values for the regularizer\n",
    "alpha1 = 0.1\n",
    "alpha2 = 0.1\n",
    "\n",
    "# Train the neural network\n",
    "weights1, weights2 = train_neural_network(X, T, weights1, weights2, alpha1, alpha2, learning_rate=0.01, epochs=100)\n",
    "\n",
    "# The network weights will be updated after training\n",
    "print(\"Trained weights1:\", weights1)\n",
    "print(\"Trained weights2:\", weights2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c855071d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
