{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dacb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " * Copyright (c) 2008 Radhamadhab Dalai\n",
    " *\n",
    " * Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    " * of this software and associated documentation files (the \"Software\"), to deal\n",
    " * in the Software without restriction, including without limitation the rights\n",
    " * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    " * copies of the Software, and to permit persons to whom the Software is\n",
    " * furnished to do so, subject to the following conditions:\n",
    " *\n",
    " * The above copyright notice and this permission notice shall be included in\n",
    " * all copies or substantial portions of the Software.\n",
    " *\n",
    " * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    " * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    " * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    " * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    " * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    " * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
    " * THE SOFTWARE.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496487fc",
   "metadata": {},
   "source": [
    "## Bayesian Neural Networks\n",
    "\n",
    "So far, our discussion of neural networks has focused on the use of maximum likelihood to determine the network parameters (weights and biases). Regularized maximum likelihood can be interpreted as a MAP (maximum posterior) approach in which the regularizer can be viewed as the logarithm of a prior parameter distribution. However, in a Bayesian treatment, we need to marginalize over the distribution of parameters in order to make predictions. \n",
    "\n",
    "In Section 3.3, we developed a Bayesian solution for a simple linear regression model under the assumption of Gaussian noise. We saw that the posterior distribution, which is Gaussian, could be evaluated exactly, and the predictive distribution could also be found in closed form. \n",
    "\n",
    "In the case of a multilayered network, the highly nonlinear dependence of the network function on the parameter values means that an exact Bayesian treatment can no longer be found. In fact, the log of the posterior distribution will be non-convex, corresponding to the multiple local minima in the error function. \n",
    "\n",
    "The technique of variational inference, to be discussed in Chapter 10, has been applied to Bayesian neural networks using a factorized Gaussian approximation to the posterior distribution (Hinton and van Camp, 1993) and also using a full-covariance Gaussian (Barber and Bishop, 1998a; Barber and Bishop, 1998b). The most complete treatment, however, has been based on the Laplace approximation (MacKay, 1992c; MacKay, 1992b) and forms the basis for the discussion given here. We will approximate the posterior distribution by a Gaussian, centered at a mode of the true posterior. Furthermore, we shall assume that the covariance of this Gaussian is small so that the network function is approximately linear with respect to the parameters over the region of parameter space for which the posterior probability is significantly nonzero. With these two approximations, we will obtain models that are analogous to the linear regression and classification models discussed in earlier chapters and so we can exploit the results obtained there. We can then make use of the evidence framework to provide point estimates for the hyperparameters and to compare alternative models (for example, networks having different numbers of hidden units). To start with, we shall discuss the regression case and then later consider the modifications needed for solving classification tasks.\n",
    "\n",
    "## Posterior Parameter Distribution\n",
    "\n",
    "Consider the problem of predicting a single continuous target variable $ t $ from a vector $ x $ of inputs (the extension to multiple targets is straightforward). We shall suppose that the conditional distribution $ p(t|x) $ is Gaussian, with an $ x $-dependent mean given by the output of a neural network model $ y(x, w) $, and with precision (inverse variance) $ \\beta $:\n",
    "\n",
    "$$\n",
    "p(t|x, w, \\beta) = \\mathcal{N}(t | y(x, w), \\beta^{-1})\n",
    "$$\n",
    "\n",
    "Similarly, we shall choose a prior distribution over the weights $ w $ that is Gaussian of the form:\n",
    "\n",
    "$$\n",
    "p(w|\\alpha) = \\mathcal{N}(w | 0, \\alpha^{-1} I)\n",
    "$$\n",
    "\n",
    "For an i.i.d. data set of $ N $ observations $ x_1, \\dots, x_N $, with a corresponding set of target values $ D = \\{t_1, \\dots, t_N\\} $, the likelihood function is given by:\n",
    "\n",
    "$$\n",
    "p(D | w, \\beta) = \\prod_{n=1}^{N} \\mathcal{N}(t_n | y(x_n, w), \\beta^{-1})\n",
    "$$\n",
    "\n",
    "The resulting posterior distribution is then:\n",
    "\n",
    "$$\n",
    "p(w | D, \\alpha, \\beta) \\propto p(w|\\alpha) p(D | w, \\beta)\n",
    "$$\n",
    "\n",
    "Which, as a consequence of the nonlinear dependence of $ y(x, w) $ on $ w $, will be non-Gaussian. We can find a Gaussian approximation to the posterior distribution by using the Laplace approximation. To do this, we must first find a (local) maximum of the posterior, and this must be done using iterative numerical optimization. \n",
    "\n",
    "As usual, it is convenient to maximize the logarithm of the posterior, which can be written as:\n",
    "\n",
    "$$\n",
    "\\ln p(w|D) = - \\frac{\\alpha}{2} w^T w - \\sum_{n=1}^{N} \\frac{1}{2} \\beta \\left( y(x_n, w) - t_n \\right)^2 + \\text{constant}\n",
    "$$\n",
    "\n",
    "This corresponds to a regularized sum-of-squares error function. Assuming for the moment that $ \\alpha $ and $ \\beta $ are fixed, we can find a maximum of the posterior, which we denote $ w_{\\text{MAP}} $, by standard nonlinear optimization algorithms such as conjugate gradients, using error backpropagation to evaluate the required derivatives.\n",
    "\n",
    "Having found a mode $ w_{\\text{MAP}} $, we can then build a local Gaussian approximation by evaluating the matrix of second derivatives of the negative log posterior distribution. From the equation above, this is given by:\n",
    "\n",
    "$$\n",
    "A = -\\nabla\\nabla \\ln p(w|D, \\alpha, \\beta) = \\alpha I + \\beta H\n",
    "$$\n",
    "\n",
    "Where $ H $ is the Hessian matrix comprising the second derivatives of the sum-of-squares error function with respect to the components of $ w $. The corresponding Gaussian approximation to the posterior is then:\n",
    "\n",
    "$$\n",
    "q(w|D) = \\mathcal{N}(w | w_{\\text{MAP}}, A^{-1})\n",
    "$$\n",
    "\n",
    "Similarly, the predictive distribution is obtained by marginalizing with respect to this posterior distribution:\n",
    "\n",
    "$$\n",
    "p(t | x, D) = \\int p(t | x, w) q(w | D) dw\n",
    "$$\n",
    "\n",
    "However, even with the Gaussian approximation to the posterior, this integration is still analytically intractable due to the nonlinearity of the network function $ y(x, w) $ as a function of $ w $. \n",
    "\n",
    "To make progress, we now assume that the posterior distribution has small variance compared with the characteristic scales of $ w $ over which $ y(x, w) $ is varying. This allows us to make a Taylor series expansion of the network function around $ w_{\\text{MAP}} $ and retain only the linear terms:\n",
    "\n",
    "$$\n",
    "y(x, w) \\approx y(x, w_{\\text{MAP}}) + g^T (w - w_{\\text{MAP}})\n",
    "$$\n",
    "\n",
    "Where we have defined $ g = \\nabla_w y(x, w) \\big|_{w=w_{\\text{MAP}}} $.\n",
    "\n",
    "With this approximation, we now have a linear-Gaussian model with a Gaussian distribution for $ p(w) $ and a Gaussian for $ p(t|w) $ whose mean is a linear function of $ w $:\n",
    "\n",
    "$$\n",
    "p(t|x, w, \\beta) \\approx \\mathcal{N}(t | y(x, w_{\\text{MAP}}) + g^T (w - w_{\\text{MAP}}), \\beta^{-1})\n",
    "$$\n",
    "\n",
    "We can therefore make use of the general result for the marginal:\n",
    "\n",
    "$$\n",
    "p(t | x, D, \\alpha, \\beta) = \\mathcal{N}(t | y(x, w_{\\text{MAP}}), \\sigma^2(x))\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd0c7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the neural network model\n",
    "class BayesianNeuralNetwork:\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # Initialize weights and biases for a single hidden layer\n",
    "        self.W1 = np.random.randn(input_dim, hidden_dim)\n",
    "        self.b1 = np.zeros((1, hidden_dim))\n",
    "        self.W2 = np.random.randn(hidden_dim, 1)\n",
    "        self.b2 = np.zeros((1, 1))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward pass of the neural network.\"\"\"\n",
    "        self.Z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.A1 = np.tanh(self.Z1)  # Activation function\n",
    "        self.Z2 = np.dot(self.A1, self.W2) + self.b2\n",
    "        return self.Z2\n",
    "\n",
    "    def compute_loss(self, X, y, beta=1.0, alpha=1.0):\n",
    "        \"\"\"Compute the negative log posterior (including prior and likelihood).\"\"\"\n",
    "        # Predict\n",
    "        y_pred = self.forward(X)\n",
    "        \n",
    "        # Likelihood term (sum of squared errors, with precision beta)\n",
    "        likelihood = np.sum((y - y_pred) ** 2)\n",
    "        \n",
    "        # Prior term (L2 regularization on weights)\n",
    "        prior = alpha * (np.sum(self.W1 ** 2) + np.sum(self.W2 ** 2))\n",
    "        \n",
    "        # Combine the terms: negative log posterior\n",
    "        return (0.5 * beta * likelihood) + (0.5 * prior)\n",
    "    \n",
    "    def gradient(self, X, y, beta=1.0, alpha=1.0):\n",
    "        \"\"\"Compute gradients of the negative log posterior.\"\"\"\n",
    "        y_pred = self.forward(X)\n",
    "        \n",
    "        # Gradients for output layer (W2, b2)\n",
    "        dZ2 = y_pred - y\n",
    "        dW2 = np.dot(self.A1.T, dZ2) + alpha * self.W2  # Add L2 regularization term\n",
    "        db2 = np.sum(dZ2, axis=0, keepdims=True)\n",
    "        \n",
    "        # Gradients for hidden layer (W1, b1)\n",
    "        dA1 = np.dot(dZ2, self.W2.T)\n",
    "        dZ1 = dA1 * (1 - self.A1 ** 2)  # Derivative of tanh activation\n",
    "        dW1 = np.dot(X.T, dZ1) + alpha * self.W1  # Add L2 regularization term\n",
    "        db1 = np.sum(dZ1, axis=0, keepdims=True)\n",
    "        \n",
    "        # Return gradients\n",
    "        return dW1, db1, dW2, db2\n",
    "    \n",
    "    def train(self, X_train, y_train, learning_rate=0.01, epochs=1000, beta=1.0, alpha=1.0):\n",
    "        \"\"\"Train the Bayesian Neural Network.\"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            # Compute the gradients\n",
    "            dW1, db1, dW2, db2 = self.gradient(X_train, y_train, beta, alpha)\n",
    "            \n",
    "            # Update weights using gradient descent\n",
    "            self.W1 -= learning_rate * dW1\n",
    "            self.b1 -= learning_rate * db1\n",
    "            self.W2 -= learning_rate * dW2\n",
    "            self.b2 -= learning_rate * db2\n",
    "            \n",
    "            # Print the loss every 100 epochs\n",
    "            if epoch % 100 == 0:\n",
    "                loss = self.compute_loss(X_train, y_train, beta, alpha)\n",
    "                print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions using the trained model.\"\"\"\n",
    "        return self.forward(X)\n",
    "\n",
    "# Generate synthetic regression data\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=0.1, random_state=42)\n",
    "X = X.reshape(-1, 1)  # Reshape for the model\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Instantiate and train the model\n",
    "input_dim = X_train.shape[1]\n",
    "hidden_dim = 10  # Hidden layer size\n",
    "bnn = BayesianNeuralNetwork(input_dim, hidden_dim)\n",
    "\n",
    "# Train the model\n",
    "bnn.train(X_train, y_train, learning_rate=0.01, epochs=2000)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = bnn.predict(X_test)\n",
    "\n",
    "# Plot the results\n",
    "plt.scatter(X_test, y_test, color='blue', label='True Data')\n",
    "plt.plot(X_test, y_pred, color='red', label='Predictions')\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.title(\"Bayesian Neural Network Predictions\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ac2e88",
   "metadata": {},
   "source": [
    "### Predictive Distribution with Input-Dependent Variance\n",
    "\n",
    "The input-dependent variance is given by:\n",
    "\n",
    "$$\n",
    "\\sigma^2(x) = \\beta^{-1} + \\mathbf{g}^\\top \\mathbf{A}^{-1} \\mathbf{g}. \\tag{5.173}\n",
    "$$\n",
    "\n",
    "We observe that the predictive distribution $ p(t | \\mathbf{x}, \\mathcal{D}) $ is a Gaussian, where the mean is given by the network function $ y(\\mathbf{x}, \\mathbf{w}_{\\text{MAP}}) $, with the parameters set to their Maximum A Posteriori (MAP) values. \n",
    "\n",
    "The variance of the predictive distribution has two components:\n",
    "\n",
    "1. **Intrinsic Noise**: \n",
    "   $$ \\beta^{-1} $$\n",
    "   This term arises from the intrinsic noise on the target variable.\n",
    "\n",
    "2. **Uncertainty in the Model Parameters**:\n",
    "   $$ \\mathbf{g}^\\top \\mathbf{A}^{-1} \\mathbf{g} $$\n",
    "   This is an $\\mathbf{x}$-dependent term that expresses the uncertainty in the interpolant due to the uncertainty in the model parameters $ \\mathbf{w} $.\n",
    "\n",
    "This predictive distribution can be compared to the corresponding predictive distribution for the linear regression model, given by:\n",
    "\n",
    "- Mean:\n",
    "  $$\n",
    "  \\mu(x) = \\mathbf{x}^\\top \\mathbf{w}_{\\text{MAP}}\n",
    "  $$\n",
    "\n",
    "- Variance:\n",
    "  $$\n",
    "  \\sigma^2(x) = \\beta^{-1} + \\mathbf{x}^\\top \\mathbf{S} \\mathbf{x}.\n",
    "  $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b380b898",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
