{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89150b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " * Copyright (c) 2018 Radhamadhab Dalai\n",
    " *\n",
    " * Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    " * of this software and associated documentation files (the \"Software\"), to deal\n",
    " * in the Software without restriction, including without limitation the rights\n",
    " * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    " * copies of the Software, and to permit persons to whom the Software is\n",
    " * furnished to do so, subject to the following conditions:\n",
    " *\n",
    " * The above copyright notice and this permission notice shall be included in\n",
    " * all copies or substantial portions of the Software.\n",
    " *\n",
    " * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    " * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    " * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    " * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    " * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    " * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
    " * THE SOFTWARE.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dd09bc",
   "metadata": {},
   "source": [
    "Connectionist Temporal Classification (CTC)\n",
    "CTC addresses the challenges of labeling unsegmented sequence data directly with RNNs. Here's a brief overview:\n",
    "\n",
    "Given a set S of training examples where each example consists of a pair of sequences (x, z) with  x = ($x_1$, $\\ldots$, $x_T$) and  z = ($z_1$, $\\ldots$, $z_U$), where  U $\\leq$ T , the sequences x and z are not aligned initially due to their differing lengths.\n",
    "\n",
    "CTC Method:\n",
    "Softmax Layer: A softmax layer is used to define an output distribution\n",
    "ğ‘ƒ(ğ‘˜|ğ‘¡)\n",
    "at each time step t along the input sequence x. This distribution covers K phonemes plus an extra blank symbol  $\\emptyset$, making the softmax layer size ( K + 1 ).\n",
    "\n",
    "Bidirectional RNNs: Typically, RNNs trained with CTC are bidirectional to ensure each\n",
    "ğ‘ƒ(ğ‘˜|ğ‘¡)\n",
    "depends on the entire input sequence x.\n",
    "\n",
    "Output Vector: The unnormalized output vector\n",
    "ğ‘¦ğ‘¡\n",
    "at time step  t  is defined as:\n",
    "ğ‘¦ğ‘¡=ğ‘Šâ†’ğ‘â„ğ‘¡â†’ğ‘+ğ‘Šâ†ğ‘â„ğ‘¡â†ğ‘+ğ‘ğ‘¦\n",
    "where:\n",
    "\n",
    "$ W_{\\rightarrow N} $ and $ W_{\\leftarrow N} $ are weight matrices from forward and backward hidden states to the output gate.\n",
    "( h_{\\rightarrow N}^t ) and ( h_{\\leftarrow N}^t ) are forward and backward hidden states at time step ( t ).\n",
    "( b_y ) is the bias.\n",
    "Output Probability\n",
    "ğ‘ƒ(ğ‘˜|ğ‘¡)\n",
    ":\n",
    "ğ‘ƒ(ğ‘˜|ğ‘¡)=exp(ğ‘¦ğ‘¡[ğ‘˜])âˆ‘ğ¾+1ğ‘˜â€²=1exp(ğ‘¦ğ‘¡[ğ‘˜â€²])\n",
    "where ( y_t[k] ) is the ( k )-th element of ( y_t ).\n",
    "\n",
    "S-LSTM Network\n",
    "The S-LSTM network extends LSTM to handle longer-term dependencies and more complex input structures:\n",
    "\n",
    "S-LSTM Memory Block:\n",
    "Each S-LSTM memory block contains:\n",
    "\n",
    "One input gate\n",
    "ğ‘–ğ‘¡\n",
    "One output gate\n",
    "ğ‘œğ‘¡\n",
    "Multiple forget gates depending on the number of children of a node\n",
    "Forward Computation:\n",
    "Input Gate\n",
    "ğ‘–ğ‘¡\n",
    ":\n",
    "ğ‘–ğ‘¡=ğœ(ğ‘Šğ‘–ğ¿â„â„ğ¿ğ‘¡âˆ’1+ğ‘Šğ‘–ğ‘…â„â„ğ‘…ğ‘¡âˆ’1+ğ‘Šğ‘–ğ¿ğ‘ğ‘ğ¿ğ‘¡âˆ’1+ğ‘Šğ‘–ğ‘…ğ‘ğ‘ğ‘…ğ‘¡âˆ’1+ğ‘ğ‘–)\n",
    "where:\n",
    "( \\sigma ) is the logistic sigmoid function.\n",
    "( h_{Lt-1}, h_{Rt-1} ) are hidden vectors of the left and right children.\n",
    "( c_{Lt-1}, c_{Rt-1} ) are cell vectors of the left and right children.\n",
    "( W ) and ( b ) are weight matrices and biases.\n",
    "These networks enhance the capabilities of traditional RNNs and LSTMs by addressing alignment issues and handling more complex input structures efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b73feab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 20)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class CTC:\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        self.input_size = input_size\n",
    "        self.num_classes = num_classes\n",
    "        self.softmax = np.zeros((num_classes + 1, input_size))  # Including blank symbol\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Assuming x is the input sequence tensor of shape (input_size, sequence_length)\n",
    "        sequence_length = x.shape[1]\n",
    "        outputs = np.zeros((self.num_classes + 1, sequence_length))\n",
    "\n",
    "        for t in range(sequence_length):\n",
    "            # Example softmax computation (replace with actual logits)\n",
    "            logits_t = np.random.rand(self.num_classes + 1)\n",
    "            outputs[:, t] = np.exp(logits_t) / np.sum(np.exp(logits_t))\n",
    "\n",
    "        return outputs\n",
    "\n",
    "# Example usage:\n",
    "input_size = 10\n",
    "num_classes = 5\n",
    "ctc = CTC(input_size, num_classes)\n",
    "\n",
    "# Example input sequence (10-dimensional input, length 20)\n",
    "input_sequence = np.random.rand(input_size, 20)\n",
    "\n",
    "# Forward pass\n",
    "output_sequence = ctc.forward(input_sequence)\n",
    "print(output_sequence.shape)  # Example output shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89ae30c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class S_LSTM_Cell:\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.weights = {\n",
    "            'W_lh': np.random.randn(hidden_size, hidden_size),\n",
    "            'W_rh': np.random.randn(hidden_size, hidden_size),\n",
    "            'W_lc': np.random.randn(hidden_size, hidden_size),\n",
    "            'W_rc': np.random.randn(hidden_size, hidden_size),\n",
    "            'W_i': np.random.randn(hidden_size, input_size)\n",
    "        }\n",
    "        self.biases = {\n",
    "            'b_i': np.zeros((hidden_size, 1))\n",
    "        }\n",
    "\n",
    "    def forward(self, h_l_prev, h_r_prev, c_l_prev, c_r_prev, x):\n",
    "        i_t = sigmoid(\n",
    "            np.dot(self.weights['W_lh'], h_l_prev) +\n",
    "            np.dot(self.weights['W_rh'], h_r_prev) +\n",
    "            np.dot(self.weights['W_lc'], c_l_prev) +\n",
    "            np.dot(self.weights['W_rc'], c_r_prev) +\n",
    "            np.dot(self.weights['W_i'], x) +\n",
    "            self.biases['b_i']\n",
    "        )\n",
    "        return i_t\n",
    "\n",
    "# Example usage:\n",
    "input_size = 5  # Example input size\n",
    "hidden_size = 10  # Example hidden size\n",
    "s_lstm_cell = S_LSTM_Cell(input_size, hidden_size)\n",
    "\n",
    "# Example inputs (previous hidden and cell states)\n",
    "h_l_prev = np.random.randn(hidden_size, 1)  # Shape (10, 1)\n",
    "h_r_prev = np.random.randn(hidden_size, 1)  # Shape (10, 1)\n",
    "c_l_prev = np.random.randn(hidden_size, 1)  # Shape (10, 1)\n",
    "c_r_prev = np.random.randn(hidden_size, 1)  # Shape (10, 1)\n",
    "x = np.random.randn(input_size, 1)  # Shape (5, 1)\n",
    "\n",
    "# Forward pass through S-LSTM cell\n",
    "i_t = s_lstm_cell.forward(h_l_prev, h_r_prev, c_l_prev, c_r_prev, x)\n",
    "print(i_t.shape)  # Example output shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de45690",
   "metadata": {},
   "source": [
    "Each S-LSTM memory block contains one input gate and one output gate, but different from LSTM, S-LSTM has two or more forget gates. The number of forget gates depends on the number of children of a node. For two children, their hidden vectors are denoted as \\( h^L_{t-1} \\) for the left child and \\( h^R_{t-1} \\) for the right child. These hidden vectors are taken in as the inputs of the current block.\n",
    "\n",
    "The forward computation of a S-LSTM memory block is specified as follows:\n",
    "\n",
    "1. **Input gate**: The input gate \\( i_t \\) contains four resources of information: the hidden vectors \\( h^L_{t-1} \\) and \\( h^R_{t-1} \\) and cell vectors \\( c^L_{t-1} \\) and \\( c^R_{t-1} \\) of its two children, i.e.,\n",
    "\n",
    "$$\n",
    "i_t = \\sigma \\left( W^L_{hi} h^L_{t-1} + W^R_{hi} h^R_{t-1} + W^L_{ci} c^L_{t-1} + W^R_{ci} c^R_{t-1} + b_i \\right)\n",
    "$$\n",
    "\n",
    "where \\( \\sigma \\) is the element-wise logistic function used to confine the gating signals to be in the range of [0, 1].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d72d3961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.43207905678063\n",
      "Epoch 100, Loss: 1.388094958975622\n",
      "Epoch 200, Loss: 1.3739687215329668\n",
      "Epoch 300, Loss: 1.4106310090744245\n",
      "Epoch 400, Loss: 1.3708422935239144\n",
      "Epoch 500, Loss: 1.379349831149973\n",
      "Epoch 600, Loss: 1.3817452737316587\n",
      "Epoch 700, Loss: 1.3979765672000388\n",
      "Epoch 800, Loss: 1.3964823016505372\n",
      "Epoch 900, Loss: 1.3550234442664653\n",
      "Training complete\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "class S_LSTM_Cell:\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.weights = {\n",
    "            'W_lh': np.random.randn(hidden_size, hidden_size),\n",
    "            'W_rh': np.random.randn(hidden_size, hidden_size),\n",
    "            'W_lc': np.random.randn(hidden_size, hidden_size),\n",
    "            'W_rc': np.random.randn(hidden_size, hidden_size),\n",
    "            'W_i': np.random.randn(hidden_size, input_size)\n",
    "        }\n",
    "        self.biases = {\n",
    "            'b_i': np.zeros((hidden_size, 1))\n",
    "        }\n",
    "\n",
    "    def forward(self, h_l_prev, h_r_prev, c_l_prev, c_r_prev, x):\n",
    "        # Compute the input gate\n",
    "        i_t = sigmoid(\n",
    "            np.dot(self.weights['W_lh'], h_l_prev) +\n",
    "            np.dot(self.weights['W_rh'], h_r_prev) +\n",
    "            np.dot(self.weights['W_lc'], c_l_prev) +\n",
    "            np.dot(self.weights['W_rc'], c_r_prev) +\n",
    "            np.dot(self.weights['W_i'], x) +\n",
    "            self.biases['b_i']\n",
    "        )\n",
    "        return i_t\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        # Placeholder for backward pass (gradient computation)\n",
    "        pass\n",
    "\n",
    "    def update_weights(self, lr):\n",
    "        # Placeholder for weight update\n",
    "        pass\n",
    "\n",
    "# Example usage:\n",
    "input_size = 5  # Example input size\n",
    "hidden_size = 10  # Example hidden size\n",
    "s_lstm_cell = S_LSTM_Cell(input_size, hidden_size)\n",
    "\n",
    "# Generate synthetic dataset\n",
    "num_samples = 100\n",
    "X = np.random.randn(num_samples, input_size, 1)  # Inputs\n",
    "y = np.random.randn(num_samples, hidden_size, 1)  # Targets\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for i in range(num_samples):\n",
    "        # Example inputs (previous hidden and cell states)\n",
    "        h_l_prev = np.random.randn(hidden_size, 1)  # Shape (10, 1)\n",
    "        h_r_prev = np.random.randn(hidden_size, 1)  # Shape (10, 1)\n",
    "        c_l_prev = np.random.randn(hidden_size, 1)  # Shape (10, 1)\n",
    "        c_r_prev = np.random.randn(hidden_size, 1)  # Shape (10, 1)\n",
    "        x = X[i]  # Current input\n",
    "\n",
    "        # Forward pass through S-LSTM cell\n",
    "        i_t = s_lstm_cell.forward(h_l_prev, h_r_prev, c_l_prev, c_r_prev, x)\n",
    "\n",
    "        # Compute loss (Mean Squared Error)\n",
    "        loss = np.mean((i_t - y[i]) ** 2)\n",
    "        total_loss += loss\n",
    "\n",
    "        # Backward pass (compute gradients)\n",
    "        grad_output = 2 * (i_t - y[i]) / y[i].size\n",
    "        s_lstm_cell.backward(grad_output)\n",
    "\n",
    "        # Update weights\n",
    "        s_lstm_cell.update_weights(learning_rate)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {total_loss / num_samples}')\n",
    "\n",
    "print('Training complete')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6ef0e5",
   "metadata": {},
   "source": [
    "2. **Forget gate**: The above four sources of information are also used to form the gating signals for the left forget gate \\( f^L_{t-1} \\) and right forget gate \\( f^R_{t-1} \\) via different weight matrices:\n",
    "\n",
    "$$\n",
    "f^L_t = \\sigma \\left( W^L_{hfl} h^L_{t-1} + W^R_{hfl} h^R_{t-1} + W^L_{cfl} c^L_{t-1} + W^R_{cfl} c^R_{t-1} + b_{fl} \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "f^R_t = \\sigma \\left( W^L_{hfr} h^L_{t-1} + W^R_{hfr} h^R_{t-1} + W^L_{cfr} c^L_{t-1} + W^R_{cfr} c^R_{t-1} + b_{fr} \\right)\n",
    "$$\n",
    "\n",
    "3. **Cell gate**: The cell here considers the copies from both childrenâ€™s cell vectors \\( (c^L_{t-1}, c^R_{t-1}) \\), gated with separated forget gates. The left and right forget gates can be controlled independently, allowing the pass-through of information from childrenâ€™s cell vectors:\n",
    "\n",
    "$$\n",
    "x_t = W^L_{hx} h^L_{t-1} + W^R_{hx} h^R_{t-1} + b_x\n",
    "$$\n",
    "\n",
    "$$\n",
    "c_t = f^L_t \\odot c^L_{t-1} + f^R_t \\odot c^R_{t-1} + i_t \\odot \\tanh(x_t)\n",
    "$$\n",
    "\n",
    "4. **Output gate**: The output gate \\( o_t \\) considers the hidden vectors from the children and the current cell vector:\n",
    "\n",
    "$$\n",
    "o_t = \\sigma \\left( W^L_{ho} h^L_{t-1} + W^R_{ho} h^R_{t-1} + W_{co} c_t + b_o \\right)\n",
    "$$\n",
    "\n",
    "5. **Hidden state**: The hidden vector \\( h_t \\) and the cell vector \\( c_t \\) of the current block are passed to the parent and are used depending on if the current block is a left or right child of its parent:\n",
    "\n",
    "$$\n",
    "h_t = o_t \\odot \\tanh(c_t)\n",
    "$$\n",
    "\n",
    "The backward computation of a S-LSTM memory block uses backpropagation over structures:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial o_t}{\\partial h_t} = \\frac{\\partial h_t}{\\partial o_t}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial x_t}{\\partial o_t} = \\frac{\\partial h_t}{\\partial o_t} \\odot \\tanh(c_t) \\sigma' (o_t)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f^L_t}{\\partial x_t} = \\frac{\\partial c_t}{\\partial x_t} \\odot c^L_{t-1} \\sigma' (f^L_t)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91bd7603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden state: [[ 7.32646942e-02]\n",
      " [ 2.93306077e-01]\n",
      " [-1.18412585e-02]\n",
      " [ 1.91031134e-01]\n",
      " [-1.36843695e-03]\n",
      " [-8.30990722e-01]\n",
      " [ 6.37431207e-05]\n",
      " [ 1.47682563e-02]\n",
      " [-9.44697639e-01]\n",
      " [ 1.52057216e-02]]\n",
      "Cell state: [[ 0.52161983]\n",
      " [ 0.76274031]\n",
      " [-0.01184249]\n",
      " [ 2.66960687]\n",
      " [-0.29943821]\n",
      " [-1.19132949]\n",
      " [ 1.36305189]\n",
      " [ 0.44871409]\n",
      " [-3.04636773]\n",
      " [ 1.40694624]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class S_LSTM_Cell:\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.weights = {\n",
    "            'W_lh': np.random.randn(hidden_size, hidden_size),\n",
    "            'W_rh': np.random.randn(hidden_size, hidden_size),\n",
    "            'W_lc': np.random.randn(hidden_size, hidden_size),\n",
    "            'W_rc': np.random.randn(hidden_size, hidden_size),\n",
    "            'W_hi': np.random.randn(hidden_size, hidden_size * 2),  # for h_l and h_r concatenated\n",
    "            'W_ci': np.random.randn(hidden_size, hidden_size * 2),  # for c_l and c_r concatenated\n",
    "            'W_ho': np.random.randn(hidden_size, hidden_size * 2),  # for h_l and h_r concatenated\n",
    "            'W_co': np.random.randn(hidden_size, hidden_size),\n",
    "            'W_hfl': np.random.randn(hidden_size, hidden_size * 2),  # for h_l and h_r concatenated\n",
    "            'W_cfl': np.random.randn(hidden_size, hidden_size * 2),  # for c_l and c_r concatenated\n",
    "            'W_hfr': np.random.randn(hidden_size, hidden_size * 2),  # for h_l and h_r concatenated\n",
    "            'W_cfr': np.random.randn(hidden_size, hidden_size * 2)   # for c_l and c_r concatenated\n",
    "        }\n",
    "        \n",
    "        self.biases = {\n",
    "            'b_i': np.zeros((hidden_size, 1)),\n",
    "            'b_o': np.zeros((hidden_size, 1)),\n",
    "            'b_fl': np.zeros((hidden_size, 1)),\n",
    "            'b_fr': np.zeros((hidden_size, 1))\n",
    "        }\n",
    "\n",
    "    def forward(self, h_l_prev, h_r_prev, c_l_prev, c_r_prev):\n",
    "        # Concatenate left and right hidden states and cell states\n",
    "        h_concat = np.concatenate((h_l_prev, h_r_prev), axis=0)\n",
    "        c_concat = np.concatenate((c_l_prev, c_r_prev), axis=0)\n",
    "        \n",
    "        # Input gate\n",
    "        i_t = sigmoid(\n",
    "            np.dot(self.weights['W_hi'], h_concat) +\n",
    "            np.dot(self.weights['W_ci'], c_concat) +\n",
    "            self.biases['b_i']\n",
    "        )\n",
    "        \n",
    "        # Forget gates\n",
    "        f_l_t = sigmoid(\n",
    "            np.dot(self.weights['W_hfl'], h_concat) +\n",
    "            np.dot(self.weights['W_cfl'], c_concat) +\n",
    "            self.biases['b_fl']\n",
    "        )\n",
    "        \n",
    "        f_r_t = sigmoid(\n",
    "            np.dot(self.weights['W_hfr'], h_concat) +\n",
    "            np.dot(self.weights['W_cfr'], c_concat) +\n",
    "            self.biases['b_fr']\n",
    "        )\n",
    "        \n",
    "        # Cell gate\n",
    "        x_t = np.dot(self.weights['W_lh'], h_l_prev) + np.dot(self.weights['W_rh'], h_r_prev)\n",
    "        c_t = f_l_t * c_l_prev + f_r_t * c_r_prev + i_t * np.tanh(x_t)\n",
    "        \n",
    "        # Output gate\n",
    "        o_t = sigmoid(\n",
    "            np.dot(self.weights['W_ho'], h_concat) +\n",
    "            np.dot(self.weights['W_co'], c_t) +\n",
    "            self.biases['b_o']\n",
    "        )\n",
    "        \n",
    "        # Hidden state\n",
    "        h_t = o_t * np.tanh(c_t)\n",
    "        \n",
    "        return h_t, c_t\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Example usage:\n",
    "input_size = 5  # Example input size\n",
    "hidden_size = 10  # Example hidden size\n",
    "\n",
    "s_lstm_cell = S_LSTM_Cell(input_size, hidden_size)\n",
    "\n",
    "# Example inputs (previous hidden and cell states)\n",
    "h_l_prev = np.random.randn(hidden_size, 1)\n",
    "h_r_prev = np.random.randn(hidden_size, 1)\n",
    "c_l_prev = np.random.randn(hidden_size, 1)\n",
    "c_r_prev = np.random.randn(hidden_size, 1)\n",
    "\n",
    "# Forward pass through S-LSTM cell\n",
    "h_t, c_t = s_lstm_cell.forward(h_l_prev, h_r_prev, c_l_prev, c_r_prev)\n",
    "print(\"Hidden state:\", h_t)\n",
    "print(\"Cell state:\", c_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec0578a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
