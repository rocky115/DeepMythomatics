{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0930bbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " * Copyright (c) 2018 Radhamadhab Dalai\n",
    " *\n",
    " * Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    " * of this software and associated documentation files (the \"Software\"), to deal\n",
    " * in the Software without restriction, including without limitation the rights\n",
    " * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    " * copies of the Software, and to permit persons to whom the Software is\n",
    " * furnished to do so, subject to the following conditions:\n",
    " *\n",
    " * The above copyright notice and this permission notice shall be included in\n",
    " * all copies or substantial portions of the Software.\n",
    " *\n",
    " * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    " * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    " * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    " * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    " * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    " * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
    " * THE SOFTWARE.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47dc0f0",
   "metadata": {},
   "source": [
    "The LSTM cell has several gates and memory units to control the flow of information. Let's denote:\n",
    "\n",
    "- $ x_t $ as the input vector at time step $ t $,\n",
    "- $ h_{t-1} $ as the previous hidden state (output) at time step $ t-1 $,\n",
    "- $ c_{t-1} $ as the previous cell state at time step $ t-1 $,\n",
    "- $ h_t $ as the current hidden state (output) at time step $ t $,\n",
    "- $ c_t $ as the current cell state at time step $ t $.\n",
    "\n",
    "The LSTM cell consists of the following components:\n",
    "\n",
    "1. Forget Gate:\n",
    "   - The forget gate decides what information to discard from the cell state.\n",
    "   - It takes $ x_t $ and $ h_{t-1} $ as inputs and produces a forget gate activation vector $ f_t $ using a sigmoid activation function.\n",
    "   - Mathematically, the forget gate is defined as:\n",
    "     $ f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f) $\n",
    "     where $ W_f $ is the weight matrix and $ b_f $ is the bias vector for the forget gate.\n",
    "\n",
    "2. Input Gate:\n",
    "   - The input gate decides what new information to store in the cell state.\n",
    "   - It takes $ x_t $ and $ h_{t-1} $ as inputs and produces an input gate activation vector $ i_t $ and a candidate cell state update vector $ \\tilde{c}_t $ using sigmoid and tanh activation functions, respectively.\n",
    "   - Mathematically, the input gate and candidate cell state update are defined as:\n",
    "     $ i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i) $\n",
    "     $ \\tilde{c}_t = \\tanh(W_c \\cdot [h_{t-1}, x_t] + b_c) $\n",
    "\n",
    "3. Update Cell State:\n",
    "   - The update cell state computes the new cell state by combining the previous cell state $ c_{t-1} $ with the information selected by the forget gate and the information to be added by the input gate.\n",
    "   - Mathematically, the new cell state $ c_t $ is computed as:\n",
    "     $ c_t = f_t \\cdot c_{t-1} + i_t \\cdot \\tilde{c}_t $\n",
    "\n",
    "4. Output Gate:\n",
    "   - The output gate decides what information to output from the cell state.\n",
    "   - It takes $ x_t $ and $ h_{t-1} $ as inputs and produces an output gate activation vector $ o_t $ and the next hidden state $ h_t $ using sigmoid and tanh activation functions, respectively.\n",
    "   - Mathematically, the output gate and the next hidden state are defined as:\n",
    "     $ o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o) $\n",
    "     $ h_t = o_t \\cdot \\tanh(c_t) $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2ee19f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " * Copyright (c) 2018 Radhamadhab Dalai\n",
    " *\n",
    " * Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    " * of this software and associated documentation files (the \"Software\"), to deal\n",
    " * in the Software without restriction, including without limitation the rights\n",
    " * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    " * copies of the Software, and to permit persons to whom the Software is\n",
    " * furnished to do so, subject to the following conditions:\n",
    " *\n",
    " * The above copyright notice and this permission notice shall be included in\n",
    " * all copies or substantial portions of the Software.\n",
    " *\n",
    " * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    " * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    " * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    " * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    " * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    " * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
    " * THE SOFTWARE.\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class LSTMCell:\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.W_f = np.random.randn(hidden_size, input_size + hidden_size)\n",
    "        self.b_f = np.zeros((hidden_size, 1))\n",
    "        \n",
    "        self.W_i = np.random.randn(hidden_size, input_size + hidden_size)\n",
    "        self.b_i = np.zeros((hidden_size, 1))\n",
    "        \n",
    "        self.W_c = np.random.randn(hidden_size, input_size + hidden_size)\n",
    "        self.b_c = np.zeros((hidden_size, 1))\n",
    "        \n",
    "        self.W_o = np.random.randn(hidden_size, input_size + hidden_size)\n",
    "        self.b_o = np.zeros((hidden_size, 1))\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    def forward(self, x_t, h_prev, c_prev):\n",
    "        # Concatenate input and previous hidden state\n",
    "        concat_input = np.vstack((h_prev, x_t))\n",
    "        \n",
    "        # Forget gate\n",
    "        f_t = self.sigmoid(np.dot(self.W_f, concat_input) + self.b_f)\n",
    "        \n",
    "        # Input gate\n",
    "        i_t = self.sigmoid(np.dot(self.W_i, concat_input) + self.b_i)\n",
    "        \n",
    "        # Candidate cell state update\n",
    "        tilde_c_t = self.tanh(np.dot(self.W_c, concat_input) + self.b_c)\n",
    "        \n",
    "        # Update cell state\n",
    "        c_t = f_t * c_prev + i_t * tilde_c_t\n",
    "        \n",
    "        # Output gate\n",
    "        o_t = self.sigmoid(np.dot(self.W_o, concat_input) + self.b_o)\n",
    "        \n",
    "        # Update hidden state\n",
    "        h_t = o_t * self.tanh(c_t)\n",
    "        \n",
    "        return h_t, c_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b973af8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (2,20) (2,10) (2,20) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5088/2365346715.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_5088/2365346715.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X_train, Y_train, epochs, lr)\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0mY_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}/{epochs}, Loss: {loss}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_5088/2365346715.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, X, Y, Y_pred, lr)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0mdy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mdW_y\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh_forward\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m             \u001b[0mdb_y\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (2,20) (2,10) (2,20) "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LSTMCell:\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.W_f = np.random.randn(hidden_size, input_size + hidden_size)\n",
    "        self.b_f = np.zeros((hidden_size, 1))\n",
    "        \n",
    "        self.W_i = np.random.randn(hidden_size, input_size + hidden_size)\n",
    "        self.b_i = np.zeros((hidden_size, 1))\n",
    "        \n",
    "        self.W_c = np.random.randn(hidden_size, input_size + hidden_size)\n",
    "        self.b_c = np.zeros((hidden_size, 1))\n",
    "        \n",
    "        self.W_o = np.random.randn(hidden_size, input_size + hidden_size)\n",
    "        self.b_o = np.zeros((hidden_size, 1))\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    def forward(self, x_t, h_prev, c_prev):\n",
    "        # Concatenate input and previous hidden state\n",
    "        concat_input = np.vstack((h_prev, x_t))\n",
    "        \n",
    "        # Forget gate\n",
    "        f_t = self.sigmoid(np.dot(self.W_f, concat_input) + self.b_f)\n",
    "        \n",
    "        # Input gate\n",
    "        i_t = self.sigmoid(np.dot(self.W_i, concat_input) + self.b_i)\n",
    "        \n",
    "        # Candidate cell state update\n",
    "        tilde_c_t = self.tanh(np.dot(self.W_c, concat_input) + self.b_c)\n",
    "        \n",
    "        # Update cell state\n",
    "        c_t = f_t * c_prev + i_t * tilde_c_t\n",
    "        \n",
    "        # Output gate\n",
    "        o_t = self.sigmoid(np.dot(self.W_o, concat_input) + self.b_o)\n",
    "        \n",
    "        # Update hidden state\n",
    "        h_t = o_t * self.tanh(c_t)\n",
    "        \n",
    "        return h_t, c_t, f_t, i_t, tilde_c_t, o_t\n",
    "\n",
    "class BidirectionalLSTM:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.lstm_forward = LSTMCell(input_size, hidden_size)\n",
    "        self.lstm_backward = LSTMCell(input_size, hidden_size)\n",
    "        \n",
    "        self.W_y = np.random.randn(output_size, 2 * hidden_size)\n",
    "        self.b_y = np.zeros((output_size, 1))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        T = X.shape[1]\n",
    "        \n",
    "        h_forward = np.zeros((self.hidden_size, T))\n",
    "        c_forward = np.zeros((self.hidden_size, T))\n",
    "        f_forward = np.zeros((self.hidden_size, T))\n",
    "        i_forward = np.zeros((self.hidden_size, T))\n",
    "        tilde_c_forward = np.zeros((self.hidden_size, T))\n",
    "        o_forward = np.zeros((self.hidden_size, T))\n",
    "        \n",
    "        h_backward = np.zeros((self.hidden_size, T))\n",
    "        c_backward = np.zeros((self.hidden_size, T))\n",
    "        f_backward = np.zeros((self.hidden_size, T))\n",
    "        i_backward = np.zeros((self.hidden_size, T))\n",
    "        tilde_c_backward = np.zeros((self.hidden_size, T))\n",
    "        o_backward = np.zeros((self.hidden_size, T))\n",
    "        \n",
    "        h_t = np.zeros((self.hidden_size, 1))\n",
    "        c_t = np.zeros((self.hidden_size, 1))\n",
    "        \n",
    "        # Forward LSTM\n",
    "        for t in range(T):\n",
    "            x_t = X[:, t].reshape(-1, 1)\n",
    "            h_t, c_t, f_t, i_t, tilde_c_t, o_t = self.lstm_forward.forward(x_t, h_t, c_t)\n",
    "            h_forward[:, t] = h_t.ravel()\n",
    "            c_forward[:, t] = c_t.ravel()\n",
    "            f_forward[:, t] = f_t.ravel()\n",
    "            i_forward[:, t] = i_t.ravel()\n",
    "            tilde_c_forward[:, t] = tilde_c_t.ravel()\n",
    "            o_forward[:, t] = o_t.ravel()\n",
    "        \n",
    "        h_t = np.zeros((self.hidden_size, 1))\n",
    "        c_t = np.zeros((self.hidden_size, 1))\n",
    "        \n",
    "        # Backward LSTM\n",
    "        for t in reversed(range(T)):\n",
    "            x_t = X[:, t].reshape(-1, 1)\n",
    "            h_t, c_t, f_t, i_t, tilde_c_t, o_t = self.lstm_backward.forward(x_t, h_t, c_t)\n",
    "            h_backward[:, t] = h_t.ravel()\n",
    "            c_backward[:, t] = c_t.ravel()\n",
    "            f_backward[:, t] = f_t.ravel()\n",
    "            i_backward[:, t] = i_t.ravel()\n",
    "            tilde_c_backward[:, t] = tilde_c_t.ravel()\n",
    "            o_backward[:, t] = o_t.ravel()\n",
    "        \n",
    "        h = np.vstack((h_forward, h_backward))\n",
    "        \n",
    "        y = self.softmax(np.dot(self.W_y, h) + self.b_y)\n",
    "        \n",
    "        self.h_forward = h_forward\n",
    "        self.c_forward = c_forward\n",
    "        self.f_forward = f_forward\n",
    "        self.i_forward = i_forward\n",
    "        self.tilde_c_forward = tilde_c_forward\n",
    "        self.o_forward = o_forward\n",
    "        self.h_backward = h_backward\n",
    "        self.c_backward = c_backward\n",
    "        self.f_backward = f_backward\n",
    "        self.i_backward = i_backward\n",
    "        self.tilde_c_backward = tilde_c_backward\n",
    "        self.o_backward = o_backward\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=0, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=0, keepdims=True)\n",
    "    \n",
    "    def compute_loss(self, Y, Y_pred):\n",
    "        return -np.sum(Y * np.log(Y_pred))\n",
    "    \n",
    "    def backward(self, X, Y, Y_pred, lr=0.001):\n",
    "        T = X.shape[1]\n",
    "        \n",
    "        dW_y = np.zeros_like(self.W_y)\n",
    "        db_y = np.zeros_like(self.b_y)\n",
    "        \n",
    "        dh_forward = np.zeros_like(self.h_forward)\n",
    "        dh_backward = np.zeros_like(self.h_backward)\n",
    "        \n",
    "        dc_forward = np.zeros_like(self.c_forward)\n",
    "        dc_backward = np.zeros_like(self.c_backward)\n",
    "        \n",
    "        for t in range(T):\n",
    "            dy = Y_pred[:, t].reshape(-1, 1) - Y[:, t].reshape(-1, 1)\n",
    "            dW_y += np.dot(dy, self.h_forward[:, t].reshape(1, -1))\n",
    "            db_y += dy\n",
    "        \n",
    "        # Backpropagation through time for forward LSTM\n",
    "        for t in reversed(range(T)):\n",
    "            dy = Y_pred[:, t].reshape(-1, 1) - Y[:, t].reshape(-1, 1)\n",
    "            do = dy * np.tanh(self.c_forward[:, t].reshape(-1, 1))\n",
    "            dc = do * self.o_forward[:, t].reshape(-1, 1) * (1 - np.tanh(self.c_forward[:, t].reshape(-1, 1)) ** 2)\n",
    "            dc += dc_forward[:, t].reshape(-1, 1)\n",
    "            \n",
    "            di = dc * self.tilde_c_forward[:, t].reshape(-1, 1)\n",
    "            df = dc * self.c_forward[:, t-1].reshape(-1, 1) if t > 0 else np.zeros_like(dc)\n",
    "            dtilde_c = dc * self.i_forward[:, t].reshape(-1, 1)\n",
    "            \n",
    "            di_input = di * self.i_forward[:, t].reshape(-1, 1) * (1 - self.i_forward[:, t].reshape(-1, 1))\n",
    "            df_input = df * self.f_forward[:, t].reshape(-1, 1) * (1 - self.f_forward[:, t].reshape(-1, 1))\n",
    "            do_input = do * self.o_forward[:, t].reshape(-1, 1) * (1 - self.o_forward[:, t].reshape(-1, 1))\n",
    "            dtilde_c_input = dtilde_c * (1 - self.tilde_c_forward[:, t].reshape(-1, 1) ** 2)\n",
    "            \n",
    "            dW_i = np.dot(di_input, np.vstack((self.h_forward[:, t-1].reshape(-1, 1), X[:, t].reshape(-1, 1))).T)\n",
    "            dW_f = np.dot(df_input, np.vstack((self.h_forward[:, t-1].reshape(-1, 1), X[:, t].reshape(-1, 1))).T)\n",
    "            dW_o = np.dot(do_input, np.vstack((self.h_forward[:, t-1].reshape(-1, 1), X[:, t].reshape(-1, 1))).T)\n",
    "            dW_c = np.dot(dtilde_c_input, np.vstack((self.h_forward[:, t-1].reshape(-1, 1), X[:, t].reshape(-1, 1))).T)\n",
    "            \n",
    "            dc_forward[:, t-1] = dc * self.f_forward[:, t].reshape(-1, 1) if t > 0 else np.zeros_like(dc)\n",
    "            dh_forward[:, t-1] = (np.dot(self.lstm_forward.W_i.T, di_input) +\n",
    "                                  np.dot(self.lstm_forward.W_f.T, df_input) +\n",
    "                                  np.dot(self.lstm_forward.W_o.T, do_input) +\n",
    "                                  np.dot(self.lstm_forward.W_c.T, dtilde_c_input))[:self.hidden_size]\n",
    "        \n",
    "        # Backpropagation through time for backward LSTM\n",
    "        for t in range(T):\n",
    "            dy = Y_pred[:, t].reshape(-1, 1) - Y[:, t].reshape(-1, 1)\n",
    "            do = dy * np.tanh(self.c_backward[:, t].reshape(-1, 1))\n",
    "            dc = do * self.o_backward[:, t].reshape(-1, 1) * (1 - np.tanh(self.c_backward[:, t].reshape(-1, 1)) ** 2)\n",
    "            dc += dc_backward[:, t].reshape(-1, 1)\n",
    "            \n",
    "            di = dc * self.tilde_c_backward[:, t].reshape(-1, 1)\n",
    "            df = dc * self.c_backward[:, t+1].reshape(-1, 1) if t < T-1 else np.zeros_like(dc)\n",
    "            dtilde_c = dc * self.i_backward[:, t].reshape(-1, 1)\n",
    "            \n",
    "            di_input = di * self.i_backward[:, t].reshape(-1, 1) * (1 - self.i_backward[:, t].reshape(-1, 1))\n",
    "            df_input = df * self.f_backward[:, t].reshape(-1, 1) * (1 - self.f_backward[:, t].reshape(-1, 1))\n",
    "            do_input = do * self.o_backward[:, t].reshape(-1, 1) * (1 - self.o_backward[:, t].reshape(-1, 1))\n",
    "            dtilde_c_input = dtilde_c * (1 - self.tilde_c_backward[:, t].reshape(-1, 1) ** 2)\n",
    "            \n",
    "            dW_i = np.dot(di_input, np.vstack((self.h_backward[:, t+1].reshape(-1, 1), X[:, t].reshape(-1, 1))).T)\n",
    "            dW_f = np.dot(df_input, np.vstack((self.h_backward[:, t+1].reshape(-1, 1), X[:, t].reshape(-1, 1))).T)\n",
    "            dW_o = np.dot(do_input, np.vstack((self.h_backward[:, t+1].reshape(-1, 1), X[:, t].reshape(-1, 1))).T)\n",
    "            dW_c = np.dot(dtilde_c_input, np.vstack((self.h_backward[:, t+1].reshape(-1, 1), X[:, t].reshape(-1, 1))).T)\n",
    "            \n",
    "            dc_backward[:, t+1] = dc * self.f_backward[:, t].reshape(-1, 1) if t < T-1 else np.zeros_like(dc)\n",
    "            dh_backward[:, t+1] = (np.dot(self.lstm_backward.W_i.T, di_input) +\n",
    "                                   np.dot(self.lstm_backward.W_f.T, df_input) +\n",
    "                                   np.dot(self.lstm_backward.W_o.T, do_input) +\n",
    "                                   np.dot(self.lstm_backward.W_c.T, dtilde_c_input))[:self.hidden_size]\n",
    "        \n",
    "        self.W_y -= lr * dW_y\n",
    "        self.b_y -= lr * db_y\n",
    "    \n",
    "    def train(self, X_train, Y_train, epochs=100, lr=0.001):\n",
    "        for epoch in range(epochs):\n",
    "            Y_pred = self.forward(X_train)\n",
    "            loss = self.compute_loss(Y_train, Y_pred)\n",
    "            self.backward(X_train, Y_train, Y_pred, lr)\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss}\")\n",
    "\n",
    "# Example usage:\n",
    "input_size = 9  # Each image is 3x3, flattened to a vector of size 9\n",
    "hidden_size = 10\n",
    "output_size = 2  # Two classes: 0 and 1\n",
    "\n",
    "# Create a BidirectionalLSTM instance\n",
    "model = BidirectionalLSTM(input_size, hidden_size, output_size)\n",
    "\n",
    "# Sample input data (2 samples of 3x3 images flattened to 9xT vectors, with T=1)\n",
    "X_train = np.array([\n",
    "    [1, 1, 1, 1, 0, 1, 1, 1, 1],  # Digit '0'\n",
    "    [0, 1, 0, 0, 1, 0, 0, 1, 0]   # Digit '1'\n",
    "]).T\n",
    "\n",
    "# Sample target output (one-hot encoded)\n",
    "Y_train = np.array([\n",
    "    [1, 0],  # Label for '0'\n",
    "    [0, 1]   # Label for '1'\n",
    "]).T\n",
    "\n",
    "# Train the model\n",
    "model.train(X_train, Y_train, epochs=1000, lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ce54882",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (2,1) and (10,2) not aligned: 1 (dim 1) != 10 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5088/3789832589.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_5088/3789832589.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X_train, Y_train, epochs, lr)\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0mY_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}/{epochs}, Loss: {loss}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_5088/3789832589.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, X, Y, Y_pred, lr)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0mdy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mdW_y\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh_forward\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh_backward\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m             \u001b[0mdb_y\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (2,1) and (10,2) not aligned: 1 (dim 1) != 10 (dim 0)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LSTMCell:\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.W_f = np.random.randn(hidden_size, input_size + hidden_size)\n",
    "        self.b_f = np.zeros((hidden_size, 1))\n",
    "        \n",
    "        self.W_i = np.random.randn(hidden_size, input_size + hidden_size)\n",
    "        self.b_i = np.zeros((hidden_size, 1))\n",
    "        \n",
    "        self.W_c = np.random.randn(hidden_size, input_size + hidden_size)\n",
    "        self.b_c = np.zeros((hidden_size, 1))\n",
    "        \n",
    "        self.W_o = np.random.randn(hidden_size, input_size + hidden_size)\n",
    "        self.b_o = np.zeros((hidden_size, 1))\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    def forward(self, x_t, h_prev, c_prev):\n",
    "        # Concatenate input and previous hidden state\n",
    "        concat_input = np.vstack((h_prev, x_t))\n",
    "        \n",
    "        # Forget gate\n",
    "        f_t = self.sigmoid(np.dot(self.W_f, concat_input) + self.b_f)\n",
    "        \n",
    "        # Input gate\n",
    "        i_t = self.sigmoid(np.dot(self.W_i, concat_input) + self.b_i)\n",
    "        \n",
    "        # Candidate cell state update\n",
    "        tilde_c_t = self.tanh(np.dot(self.W_c, concat_input) + self.b_c)\n",
    "        \n",
    "        # Update cell state\n",
    "        c_t = f_t * c_prev + i_t * tilde_c_t\n",
    "        \n",
    "        # Output gate\n",
    "        o_t = self.sigmoid(np.dot(self.W_o, concat_input) + self.b_o)\n",
    "        \n",
    "        # Update hidden state\n",
    "        h_t = o_t * self.tanh(c_t)\n",
    "        \n",
    "        return h_t, c_t, f_t, i_t, tilde_c_t, o_t\n",
    "\n",
    "class BidirectionalLSTM:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.lstm_forward = LSTMCell(input_size, hidden_size)\n",
    "        self.lstm_backward = LSTMCell(input_size, hidden_size)\n",
    "        \n",
    "        self.W_y = np.random.randn(output_size, 2 * hidden_size)\n",
    "        self.b_y = np.zeros((output_size, 1))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        T = X.shape[1]\n",
    "        \n",
    "        h_forward = np.zeros((self.hidden_size, T))\n",
    "        c_forward = np.zeros((self.hidden_size, T))\n",
    "        f_forward = np.zeros((self.hidden_size, T))\n",
    "        i_forward = np.zeros((self.hidden_size, T))\n",
    "        tilde_c_forward = np.zeros((self.hidden_size, T))\n",
    "        o_forward = np.zeros((self.hidden_size, T))\n",
    "        \n",
    "        h_backward = np.zeros((self.hidden_size, T))\n",
    "        c_backward = np.zeros((self.hidden_size, T))\n",
    "        f_backward = np.zeros((self.hidden_size, T))\n",
    "        i_backward = np.zeros((self.hidden_size, T))\n",
    "        tilde_c_backward = np.zeros((self.hidden_size, T))\n",
    "        o_backward = np.zeros((self.hidden_size, T))\n",
    "        \n",
    "        h_t = np.zeros((self.hidden_size, 1))\n",
    "        c_t = np.zeros((self.hidden_size, 1))\n",
    "        \n",
    "        # Forward LSTM\n",
    "        for t in range(T):\n",
    "            x_t = X[:, t].reshape(-1, 1)\n",
    "            h_t, c_t, f_t, i_t, tilde_c_t, o_t = self.lstm_forward.forward(x_t, h_t, c_t)\n",
    "            h_forward[:, t] = h_t.ravel()\n",
    "            c_forward[:, t] = c_t.ravel()\n",
    "            f_forward[:, t] = f_t.ravel()\n",
    "            i_forward[:, t] = i_t.ravel()\n",
    "            tilde_c_forward[:, t] = tilde_c_t.ravel()\n",
    "            o_forward[:, t] = o_t.ravel()\n",
    "        \n",
    "        h_t = np.zeros((self.hidden_size, 1))\n",
    "        c_t = np.zeros((self.hidden_size, 1))\n",
    "        \n",
    "        # Backward LSTM\n",
    "        for t in reversed(range(T)):\n",
    "            x_t = X[:, t].reshape(-1, 1)\n",
    "            h_t, c_t, f_t, i_t, tilde_c_t, o_t = self.lstm_backward.forward(x_t, h_t, c_t)\n",
    "            h_backward[:, t] = h_t.ravel()\n",
    "            c_backward[:, t] = c_t.ravel()\n",
    "            f_backward[:, t] = f_t.ravel()\n",
    "            i_backward[:, t] = i_t.ravel()\n",
    "            tilde_c_backward[:, t] = tilde_c_t.ravel()\n",
    "            o_backward[:, t] = o_t.ravel()\n",
    "        \n",
    "        h = np.vstack((h_forward, h_backward))\n",
    "        \n",
    "        y = self.softmax(np.dot(self.W_y, h) + self.b_y)\n",
    "        \n",
    "        self.h_forward = h_forward\n",
    "        self.c_forward = c_forward\n",
    "        self.f_forward = f_forward\n",
    "        self.i_forward = i_forward\n",
    "        self.tilde_c_forward = tilde_c_forward\n",
    "        self.o_forward = o_forward\n",
    "        self.h_backward = h_backward\n",
    "        self.c_backward = c_backward\n",
    "        self.f_backward = f_backward\n",
    "        self.i_backward = i_backward\n",
    "        self.tilde_c_backward = tilde_c_backward\n",
    "        self.o_backward = o_backward\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=0, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=0, keepdims=True)\n",
    "    \n",
    "    def compute_loss(self, Y, Y_pred):\n",
    "        return -np.sum(Y * np.log(Y_pred))\n",
    "    \n",
    "    def backward(self, X, Y, Y_pred, lr=0.001):\n",
    "        T = X.shape[1]\n",
    "        \n",
    "        dW_y = np.zeros_like(self.W_y)\n",
    "        db_y = np.zeros_like(self.b_y)\n",
    "        \n",
    "        dh_forward = np.zeros((self.hidden_size, T))\n",
    "        dh_backward = np.zeros((self.hidden_size, T))\n",
    "        \n",
    "        dc_forward = np.zeros((self.hidden_size, T))\n",
    "        dc_backward = np.zeros((self.hidden_size, T))\n",
    "        \n",
    "        for t in range(T):\n",
    "            dy = Y_pred[:, t].reshape(-1, 1) - Y[:, t].reshape(-1, 1)\n",
    "            dW_y += np.dot(dy, np.vstack((self.h_forward[:, t], self.h_backward[:, t])).T)\n",
    "            db_y += dy\n",
    "        \n",
    "        # Backpropagation through time for forward LSTM\n",
    "        for t in reversed(range(T)):\n",
    "            dy = Y_pred[:, t].reshape(-1, 1) - Y[:, t].reshape(-1, 1)\n",
    "            do = dy * np.tanh(self.c_forward[:, t].reshape(-1, 1))\n",
    "            dc = do * self.o_forward[:, t].reshape(-1, 1) * (1 - np.tanh(self.c_forward[:, t].reshape(-1, 1)) ** 2)\n",
    "            dc += dc_forward[:, t].reshape(-1, 1)\n",
    "            \n",
    "            di = dc * self.tilde_c_forward[:, t].reshape(-1, 1)\n",
    "            df = dc * self.c_forward[:, t-1].reshape(-1, 1) if t > 0 else np.zeros_like(dc)\n",
    "            dtilde_c = dc * self.i_forward[:, t].reshape(-1, 1)\n",
    "            \n",
    "            di_input = di * self.i_forward[:, t].reshape(-1, 1) * (1 - self.i_forward[:, t].reshape(-1, 1))\n",
    "            df_input = df * self.f_forward[:, t].reshape(-1, 1) * (1 - self.f_forward[:, t].reshape(-1, 1))\n",
    "            do_input = do * self.o_forward[:, t].reshape(-1, 1) * (1 - self.o_forward[:, t].reshape(-1, 1))\n",
    "            dtilde_c_input = dtilde_c * (1 - self.tilde_c_forward[:, t].reshape(-1, 1) ** 2)\n",
    "            \n",
    "            concat_input = np.vstack((self.h_forward[:, t-1].reshape(-1, 1) if t > 0 else np.zeros((self.hidden_size, 1)), X[:, t].reshape(-1, 1)))\n",
    "            \n",
    "            dW_i = np.dot(di_input, concat_input.T)\n",
    "            dW_f = np.dot(df_input, concat_input.T)\n",
    "            dW_o = np.dot(do_input, concat_input.T)\n",
    "            dW_c = np.dot(dtilde_c_input, concat_input.T)\n",
    "            \n",
    "            dc_forward[:, t-1] = dc * self.f_forward[:, t].reshape(-1, 1) if t > 0 else np.zeros_like(dc)\n",
    "            dh_forward[:, t-1] = (np.dot(self.lstm_forward.W_i.T, di_input) +\n",
    "                                  np.dot(self.lstm_forward.W_f.T, df_input) +\n",
    "                                  np.dot(self.lstm_forward.W_o.T, do_input) +\n",
    "                                  np.dot(self.lstm_forward.W_c.T, dtilde_c_input))[:self.hidden_size]\n",
    "        \n",
    "        # Backpropagation through time for backward LSTM\n",
    "        for t in range(T):\n",
    "            dy = Y_pred[:, t].reshape(-1, 1) - Y[:, t].reshape(-1, 1)\n",
    "            do = dy * np.tanh(self.c_backward[:, t].reshape(-1, 1))\n",
    "            dc = do * self.o_backward[:, t].reshape(-1, 1) * (1 - np.tanh(self.c_backward[:, t].reshape(-1, 1)) ** 2)\n",
    "            dc += dc_backward[:, t].reshape(-1, 1)\n",
    "            \n",
    "            di = dc * self.tilde_c_backward[:, t].reshape(-1, 1)\n",
    "            df = dc * self.c_backward[:, t+1].reshape(-1, 1) if t < T-1 else np.zeros_like(dc)\n",
    "            dtilde_c = dc * self.i_backward[:, t].reshape(-1, 1)\n",
    "            \n",
    "            di_input = di * self.i_backward[:, t].reshape(-1, 1) * (1 - self.i_backward[:, t].reshape(-1, 1))\n",
    "            df_input = df * self.f_backward[:, t].reshape(-1, 1) * (1 - self.f_backward[:, t].reshape(-1, 1))\n",
    "            do_input = do * self.o_backward[:, t].reshape(-1, 1) * (1 - self.o_backward[:, t].reshape(-1, 1))\n",
    "            dtilde_c_input = dtilde_c * (1 - self.tilde_c_backward[:, t].reshape(-1, 1) ** 2)\n",
    "            \n",
    "            concat_input = np.vstack((self.h_backward[:, t+1].reshape(-1, 1) if t < T-1 else np.zeros((self.hidden_size, 1)), X[:, t].reshape(-1, 1)))\n",
    "            \n",
    "            dW_i = np.dot(di_input, concat_input.T)\n",
    "            dW_f = np.dot(df_input, concat_input.T)\n",
    "            dW_o = np.dot(do_input, concat_input.T)\n",
    "            dW_c = np.dot(dtilde_c_input, concat_input.T)\n",
    "            \n",
    "            dc_backward[:, t+1] = dc * self.f_backward[:, t].reshape(-1, 1) if t < T-1 else np.zeros_like(dc)\n",
    "            dh_backward[:, t+1] = (np.dot(self.lstm_backward.W_i.T, di_input) +\n",
    "                                   np.dot(self.lstm_backward.W_f.T, df_input) +\n",
    "                                   np.dot(self.lstm_backward.W_o.T, do_input) +\n",
    "                                   np.dot(self.lstm_backward.W_c.T, dtilde_c_input))[:self.hidden_size]\n",
    "        \n",
    "        self.W_y -= lr * dW_y\n",
    "        self.b_y -= lr * db_y\n",
    "    \n",
    "    def train(self, X_train, Y_train, epochs=100, lr=0.001):\n",
    "        for epoch in range(epochs):\n",
    "            Y_pred = self.forward(X_train)\n",
    "            loss = self.compute_loss(Y_train, Y_pred)\n",
    "            self.backward(X_train, Y_train, Y_pred, lr)\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss}\")\n",
    "\n",
    "# Example usage:\n",
    "input_size = 9  # Each image is 3x3, flattened to a vector of size 9\n",
    "hidden_size = 10\n",
    "output_size = 2  # Two classes: 0 and 1\n",
    "\n",
    "# Create a BidirectionalLSTM instance\n",
    "model = BidirectionalLSTM(input_size, hidden_size, output_size)\n",
    "\n",
    "# Sample input data (2 samples of 3x3 images flattened to 9xT vectors, with T=1)\n",
    "X_train = np.array([\n",
    "    [1, 1, 1, 1, 0, 1, 1, 1, 1],  # Digit '0'\n",
    "    [0, 1, 0, 0, 1, 0, 0, 1, 0]   # Digit '1'\n",
    "]).T\n",
    "\n",
    "# Sample target output (one-hot encoded)\n",
    "Y_train = np.array([\n",
    "    [1, 0],  # Label for '0'\n",
    "    [0, 1]   # Label for '1'\n",
    "]).T\n",
    "\n",
    "# Train the model\n",
    "model.train(X_train, Y_train, epochs=1000, lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24b7ae72",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (2,1) (10,1) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4914/2382533587.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;31m# print(self.h_forward[:, t])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_4914/2382533587.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X_train, Y_train, epochs, lr)\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0mY_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}/{epochs}, Loss: {loss}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4914/2382533587.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, X, Y, Y_pred, lr)\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;31m# do = dy_broadcasted * np.tanh(self.c_forward[:, t].reshape(-1, 1).T)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m             \u001b[0mdo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdy\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_forward\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0mdc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdo\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mo_forward\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_forward\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (2,1) (10,1) "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LSTMCell:\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.W_f = np.random.randn(hidden_size, input_size + hidden_size)\n",
    "        self.b_f = np.zeros((hidden_size, 1))\n",
    "        \n",
    "        self.W_i = np.random.randn(hidden_size, input_size + hidden_size)\n",
    "        self.b_i = np.zeros((hidden_size, 1))\n",
    "        \n",
    "        self.W_c = np.random.randn(hidden_size, input_size + hidden_size)\n",
    "        self.b_c = np.zeros((hidden_size, 1))\n",
    "        \n",
    "        self.W_o = np.random.randn(hidden_size, input_size + hidden_size)\n",
    "        self.b_o = np.zeros((hidden_size, 1))\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    def forward(self, x_t, h_prev, c_prev):\n",
    "        # Concatenate input and previous hidden state\n",
    "        concat_input = np.vstack((h_prev, x_t))\n",
    "        \n",
    "        # Forget gate\n",
    "        f_t = self.sigmoid(np.dot(self.W_f, concat_input) + self.b_f)\n",
    "        \n",
    "        # Input gate\n",
    "        i_t = self.sigmoid(np.dot(self.W_i, concat_input) + self.b_i)\n",
    "        \n",
    "        # Candidate cell state update\n",
    "        tilde_c_t = self.tanh(np.dot(self.W_c, concat_input) + self.b_c)\n",
    "        \n",
    "        # Update cell state\n",
    "        c_t = f_t * c_prev + i_t * tilde_c_t\n",
    "        \n",
    "        # Output gate\n",
    "        o_t = self.sigmoid(np.dot(self.W_o, concat_input) + self.b_o)\n",
    "        \n",
    "        # Update hidden state\n",
    "        h_t = o_t * self.tanh(c_t)\n",
    "        \n",
    "        return h_t, c_t, f_t, i_t, tilde_c_t, o_t\n",
    "\n",
    "class BidirectionalLSTM:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.lstm_forward = LSTMCell(input_size, hidden_size)\n",
    "        self.lstm_backward = LSTMCell(input_size, hidden_size)\n",
    "        \n",
    "        self.W_y = np.random.randn(output_size, 2 * hidden_size)\n",
    "        self.b_y = np.zeros((output_size, 1))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        T = X.shape[1]\n",
    "        \n",
    "        h_forward = np.zeros((self.hidden_size, T))\n",
    "        c_forward = np.zeros((self.hidden_size, T))\n",
    "        f_forward = np.zeros((self.hidden_size, T))\n",
    "        i_forward = np.zeros((self.hidden_size, T))\n",
    "        tilde_c_forward = np.zeros((self.hidden_size, T))\n",
    "        o_forward = np.zeros((self.hidden_size, T))\n",
    "        \n",
    "        h_backward = np.zeros((self.hidden_size, T))\n",
    "        c_backward = np.zeros((self.hidden_size, T))\n",
    "        f_backward = np.zeros((self.hidden_size, T))\n",
    "        i_backward = np.zeros((self.hidden_size, T))\n",
    "        tilde_c_backward = np.zeros((self.hidden_size, T))\n",
    "        o_backward = np.zeros((self.hidden_size, T))\n",
    "        \n",
    "        h_t = np.zeros((self.hidden_size, 1))\n",
    "        c_t = np.zeros((self.hidden_size, 1))\n",
    "        \n",
    "        # Forward LSTM\n",
    "        for t in range(T):\n",
    "            x_t = X[:, t].reshape(-1, 1)\n",
    "            h_t, c_t, f_t, i_t, tilde_c_t, o_t = self.lstm_forward.forward(x_t, h_t, c_t)\n",
    "            h_forward[:, t] = h_t.ravel()\n",
    "            c_forward[:, t] = c_t.ravel()\n",
    "            f_forward[:, t] = f_t.ravel()\n",
    "            i_forward[:, t] = i_t.ravel()\n",
    "            tilde_c_forward[:, t] = tilde_c_t.ravel()\n",
    "            o_forward[:, t] = o_t.ravel()\n",
    "        \n",
    "        h_t = np.zeros((self.hidden_size, 1))\n",
    "        c_t = np.zeros((self.hidden_size, 1))\n",
    "        \n",
    "        # Backward LSTM\n",
    "        for t in reversed(range(T)):\n",
    "            x_t = X[:, t].reshape(-1, 1)\n",
    "            h_t, c_t, f_t, i_t, tilde_c_t, o_t = self.lstm_backward.forward(x_t, h_t, c_t)\n",
    "            h_backward[:, t] = h_t.ravel()\n",
    "            c_backward[:, t] = c_t.ravel()\n",
    "            f_backward[:, t] = f_t.ravel()\n",
    "            i_backward[:, t] = i_t.ravel()\n",
    "            tilde_c_backward[:, t] = tilde_c_t.ravel()\n",
    "            o_backward[:, t] = o_t.ravel()\n",
    "        \n",
    "        h = np.vstack((h_forward, h_backward))\n",
    "        \n",
    "        y = self.softmax(np.dot(self.W_y, h) + self.b_y)\n",
    "        \n",
    "        self.h_forward = h_forward\n",
    "        self.c_forward = c_forward\n",
    "        self.f_forward = f_forward\n",
    "        self.i_forward = i_forward\n",
    "        self.tilde_c_forward = tilde_c_forward\n",
    "        self.o_forward = o_forward\n",
    "        self.h_backward = h_backward\n",
    "        self.c_backward = c_backward\n",
    "        self.f_backward = f_backward\n",
    "        self.i_backward = i_backward\n",
    "        self.tilde_c_backward = tilde_c_backward\n",
    "        self.o_backward = o_backward\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=0, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=0, keepdims=True)\n",
    "    \n",
    "    def compute_loss(self, Y, Y_pred):\n",
    "        return -np.sum(Y * np.log(Y_pred))\n",
    "    \n",
    "    def backward(self, X, Y, Y_pred, lr=0.001):\n",
    "        T = X.shape[1]\n",
    "        \n",
    "        dW_y = np.zeros_like(self.W_y)\n",
    "        db_y = np.zeros_like(self.b_y)\n",
    "        \n",
    "        dh_forward = np.zeros((self.hidden_size, T))\n",
    "        dh_backward = np.zeros((self.hidden_size, T))\n",
    "        \n",
    "        dc_forward = np.zeros((self.hidden_size, T))\n",
    "        dc_backward = np.zeros((self.hidden_size, T))\n",
    "       \n",
    "        for t in range(T):\n",
    "            #print((self.h_forward[:, t]).shape)\n",
    "            #print((self.h_backward[:, t]).shape)\n",
    "           \n",
    "            dy = (Y_pred[:, t].reshape(-1, 1) - Y[:, t].reshape(-1, 1))\n",
    "            #print(dW_y.shape) # (2,20)\n",
    "            #print(np.vstack((self.h_forward[:, t], self.h_backward[:, t])).T)\n",
    "            #dW_y += np.dot(dy.T, np.vstack((self.h_forward[:, t], self.h_backward[:, t])))\n",
    "            \n",
    "           # Concatenate forward and backward hidden states along the columns\n",
    "        \n",
    "            h_concat = np.hstack((self.h_forward[:, t], self.h_backward[:, t]))\n",
    "# Transpose h_concat to align dimensions properly\n",
    "            h_concat_transposed = h_concat.T.reshape(1, -1)  # Shape: (1, 20)\n",
    "# Now shape is (20, 2)\n",
    "\n",
    "# Perform dot product with dy\n",
    "            dW_y += np.dot(dy, h_concat_transposed)  \n",
    "            \n",
    "            db_y += dy\n",
    "        \n",
    "        # Backpropagation through time for forward LSTM\n",
    "        for t in reversed(range(T)):\n",
    "            dy = (Y_pred[:, t].reshape(-1, 1) - Y[:, t].reshape(-1, 1)) # (2,1) and (10,1)\n",
    "            # Reshape dy to match the shape of self.c_forward[:, t]\n",
    "            # dy_broadcasted = np.broadcast_to(dy, self.c_forward[:, t].shape)  # Assuming self.c_forward[:, t].shape is (10, 1)\n",
    "\n",
    "# Perform element-wise multiplication\n",
    "            # do = dy_broadcasted * np.tanh(self.c_forward[:, t].reshape(-1, 1).T)\n",
    "            \n",
    "            do = dy * np.tanh(self.c_forward[:, t].reshape(-1, 1))\n",
    "            \n",
    "            dc = do * self.o_forward[:, t].reshape(-1, 1) * (1 - np.tanh(self.c_forward[:, t].reshape(-1, 1)) ** 2)\n",
    "            dc += dc_forward[:, t].reshape(-1, 1)\n",
    "            \n",
    "            di = dc * self.tilde_c_forward[:, t].reshape(-1, 1)\n",
    "            df = dc * self.c_forward[:, t-1].reshape(-1, 1) if t > 0 else np.zeros_like(dc)\n",
    "            dtilde_c = dc * self.i_forward[:, t].reshape(-1, 1)\n",
    "            \n",
    "            di_input = di * self.i_forward[:, t].reshape(-1, 1) * (1 - self.i_forward[:, t].reshape(-1, 1))\n",
    "            df_input = df * self.f_forward[:, t].reshape(-1, 1) * (1 - self.f_forward[:, t].reshape(-1, 1))\n",
    "            do_input = do * self.o_forward[:, t].reshape(-1, 1) * (1 - self.o_forward[:, t].reshape(-1, 1))\n",
    "            dtilde_c_input = dtilde_c * (1 - self.tilde_c_forward[:, t].reshape(-1, 1) ** 2)\n",
    "            \n",
    "            concat_input = np.vstack((self.h_forward[:, t-1].reshape(-1, 1) if t > 0 else np.zeros((self.hidden_size, 1)), X[:, t].reshape(-1, 1)))\n",
    "            \n",
    "            self.lstm_forward.W_i -= lr * np.dot(di_input, concat_input.T)\n",
    "            self.lstm_forward.W_f -= lr * np.dot(df_input, concat_input.T)\n",
    "            self.lstm_forward.W_o -= lr * np.dot(do_input, concat_input.T)\n",
    "            self.lstm_forward.W_c -= lr * np.dot(dtilde_c_input, concat_input.T)\n",
    "            \n",
    "            dc_forward[:, t-1] = (dc * self.f_forward[:, t].reshape(-1, 1)).reshape(-1) if t > 0 else np.zeros_like(dc)\n",
    "            dh_forward[:, t-1] = (np.dot(self.lstm_forward.W_i.T, di_input) +\n",
    "                                  np.dot(self.lstm_forward.W_f.T, df_input) +\n",
    "                                  np.dot(self.lstm_forward.W_o.T, do_input) +\n",
    "                                  np.dot(self.lstm_forward.W_c.T, dtilde_c_input))[:self.hidden_size].reshape(-1) if t > 0 else np.zeros_like(dc).reshape(-1)\n",
    "        \n",
    "        # Backpropagation through time for backward LSTM\n",
    "        for t in range(T):\n",
    "#             dy = (Y_pred[:, t].reshape(-1, 1) - Y[:, t].reshape(-1, 1))\n",
    "#             do = dy * np.tanh(self.c_backward[:, t].reshape(-1, 1))\n",
    "#             dc = do * self.o_backward[:, t].reshape(-1, 1) * (1 - np.tanh(self.c_backward[:, t].reshape(-1, 1)) ** 2)\n",
    "#             dc += dc_backward[:, t].reshape(-1, 1)\n",
    "            \n",
    "#             di = dc * self.tilde_c_backward[:, t].reshape(-1, 1)\n",
    "#             df = dc * self.c_backward[:, t+1].reshape(-1, 1) if t < T-1 else np.zeros_like(dc)\n",
    "#             dtilde_c = dc * self.i_backward[:, t].reshape(-1, 1)\n",
    "            \n",
    "#             di_input = di * self.i_backward[:, t].reshape(-1, 1) * (1 - self.i_backward[:, t].reshape(-1, 1))\n",
    "#             df_input = df * self.f_backward[:, t].reshape(-1, 1) * (1 - self.f_backward[:, t].reshape(-1, 1))\n",
    "#             do_input = do * self.o_backward[:, t].reshape(-1, 1) * (1 - self.o_backward[:, t].reshape(-1, 1))\n",
    "#             dtilde_c_input = dtilde_c * (1 - self.tilde_c_backward[:, t].reshape(-1, 1) ** 2)\n",
    "            \n",
    "#             concat_input = np.vstack((self.h_backward[:, t+1].reshape(-1, 1) if t < T-1 else np.zeros((self.hidden_size, 1)), X[:, t].reshape(-1, 1)))\n",
    "            \n",
    "#             self.lstm_backward.W_i -= lr * np.dot(di_input, concat_input.T)\n",
    "#             self.lstm_backward.W_f -= lr * np.dot(df_input, concat_input.T)\n",
    "#             self.lstm_backward.W_o -= lr * np.dot(do_input, concat_input.T)\n",
    "#             self.lstm_backward.W_c -= lr * np.dot(dtilde_c_input, concat_input.T)\n",
    "            \n",
    "#             dc_backward[:, t+1] = (dc * self.f_backward[:, t].reshape(-1, 1)).reshape(-1) if t < T-1 else np.zeros_like(dc)\n",
    "#             dh_backward[:, t+1] = (np.dot(self.lstm_backward.W_i.T, di_input) +\n",
    "#                                    np.dot(self.lstm_backward.W_f.T, df_input) +\n",
    "#                                    np.dot(self.lstm_backward.W_o.T, do_input) +\n",
    "#                                    np.dot(self.lstm_backward.W_c.T, dtilde_c_input))[:self.hidden_size].reshape(-1) if t < T-1 else np.zeros_like(dc).reshape(-1)\n",
    "        \n",
    "#         self.W_y -= lr * dW_y\n",
    "#         self.b_y -= lr * db_y\n",
    "           # Backprop through time for forward direction\n",
    "            dh_forward = np.dot(self.W_hy[:, :self.hidden_size].T, dy[:, t].reshape(-1, 1)) + dh_forward\n",
    "            dtanh_forward = tanh_derivative(self.h_forward[:, t].reshape(-1, 1))\n",
    "            dW_xh += np.dot(dh_forward * dtanh_forward, X[:, t].reshape(1, -1))\n",
    "            db_h += dh_forward * dtanh_forward\n",
    "            if t > 0:\n",
    "                dW_hh += np.dot(dh_forward * dtanh_forward, self.h_forward[:, t-1].reshape(1, -1))\n",
    "            dh_forward = np.dot(self.W_hh.T, dh_forward * dtanh_forward)\n",
    "\n",
    "            # Backprop through time for backward direction\n",
    "            dh_backward = np.dot(self.W_hy[:, self.hidden_size:].T, dy[:, T-t-1].reshape(-1, 1)) + dh_backward\n",
    "            dtanh_backward = tanh_derivative(self.h_backward[:, T-t-1].reshape(-1, 1))\n",
    "            dW_xh += np.dot(dh_backward * dtanh_backward, X[:, T-t-1].reshape(1, -1))\n",
    "            db_h += dh_backward * dtanh_backward\n",
    "            if T-t-1 < T-1:\n",
    "                dW_hh += np.dot(dh_backward * dtanh_backward, self.h_backward[:, T-t].reshape(1, -1))\n",
    "            dh_backward = np.dot(self.W_hh.T, dh_backward * dtanh_backward)\n",
    "\n",
    "            # Update output layer weights\n",
    "            h_concat_transposed = np.vstack((self.h_forward[:, t], self.h_backward[:, t])).reshape(1, -1)\n",
    "            dW_hy += np.dot(dy[:, t].reshape(-1, 1), h_concat_transposed)\n",
    "\n",
    "        # Update parameters\n",
    "        self.W_xh -= lr * dW_xh\n",
    "        self.W_hh -= lr * dW_hh\n",
    "        self.W_hy -= lr * dW_hy\n",
    "        self.b_h -= lr * db_h\n",
    "        self.b_y -= lr * db_y\n",
    "def backward(self, X, y_true, y_pred, lr):\n",
    "    T = X.shape[1]\n",
    "    dy = y_pred - y_true  # Assuming y_true is the target output\n",
    "\n",
    "    dW_xh = np.zeros_like(self.W_xh)\n",
    "            dW_hh = np.zeros_like(self.W_hh)\n",
    "            dW_hy = np.zeros_like(self.W_hy)\n",
    "            db_h = np.zeros_like(self.b_h)\n",
    "            db_y = np.zeros_like(self.b_y)\n",
    "\n",
    "            dh_forward = np.zeros((self.hidden_size, 1))\n",
    "            dh_backward = np.zeros((self.hidden_size, 1))\n",
    "\n",
    "            for t in range(T):\n",
    "                # Backprop through time for forward direction\n",
    "                dh_forward = np.dot(self.W_hy[:, :self.hidden_size].T, dy[:, t].reshape(-1, 1)) + dh_forward\n",
    "                dtanh_forward = tanh_derivative(self.h_forward[:, t].reshape(-1, 1))\n",
    "                dW_xh += np.dot(dh_forward * dtanh_forward, X[:, t].reshape(1, -1))\n",
    "                db_h += dh_forward * dtanh_forward\n",
    "                if t > 0:\n",
    "                    dW_hh += np.dot(dh_forward * dtanh_forward, self.h_forward[:, t-1].reshape(1, -1))\n",
    "                dh_forward = np.dot(self.W_hh.T, dh_forward * dtanh_forward)\n",
    "\n",
    "                # Backprop through time for backward direction\n",
    "                dh_backward = np.dot(self.W_hy[:, self.hidden_size:].T, dy[:, T-t-1].reshape(-1, 1)) + dh_backward\n",
    "                dtanh_backward = tanh_derivative(self.h_backward[:, T-t-1].reshape(-1, 1))\n",
    "                dW_xh += np.dot(dh_backward * dtanh_backward, X[:, T-t-1].reshape(1, -1))\n",
    "                db_h += dh_backward * dtanh_backward\n",
    "                if T-t-1 < T-1:\n",
    "                    dW_hh += np.dot(dh_backward * dtanh_backward, self.h_backward[:, T-t].reshape(1, -1))\n",
    "                dh_backward = np.dot(self.W_hh.T, dh_backward * dtanh_backward)\n",
    "\n",
    "                # Update output layer weights\n",
    "                h_concat_transposed = np.vstack((self.h_forward[:, t], self.h_backward[:, t])).reshape(1, -1)\n",
    "                dW_hy += np.dot(dy[:, t].reshape(-1, 1), h_concat_transposed)\n",
    "\n",
    "            # Update parameters\n",
    "            self.W_xh -= lr * dW_xh\n",
    "            self.W_hh -= lr * dW_hh\n",
    "            self.W_hy -= lr * dW_hy\n",
    "            self.b_h -= lr * db_h\n",
    "            self.b_y -= lr * db_y\n",
    "\n",
    "def train(self, X_train, Y_train, epochs=100, lr=0.001):\n",
    "    for epoch in range(epochs):\n",
    "        Y_pred = self.forward(X_train)\n",
    "        loss = self.compute_loss(Y_train, Y_pred)\n",
    "        self.backward(X_train, Y_train, Y_pred, lr)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss}\")\n",
    "\n",
    "# Example usage:\n",
    "input_size = 9  # Each image is 3x3, flattened to a vector of size 9\n",
    "hidden_size = 10\n",
    "output_size = 2  # Two classes: 0 and 1\n",
    "\n",
    "# Create a BidirectionalLSTM instance\n",
    "model = BidirectionalLSTM(input_size, hidden_size, output_size)\n",
    "\n",
    "# Sample input data (2 samples of 3x3 images flattened to 9xT vectors, with T=1)\n",
    "X_train = np.array([\n",
    "    [1, 1, 1, 1, 0, 1, 1, 1, 1],  # Digit '0'\n",
    "    [0, 1, 0, 0, 1, 0, 0, 1, 0]   # Digit '1'\n",
    "]).T\n",
    "\n",
    "# Sample target output (one-hot encoded)\n",
    "Y_train = np.array([\n",
    "    [1, 0],  # Label for '0'\n",
    "    [0, 1]   # Label for '1'\n",
    "]).T\n",
    "\n",
    "# Train the model\n",
    "# print(self.h_forward[:, t])\n",
    "\n",
    "model.train(X_train, Y_train, epochs=1000, lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a24e5131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of A: (2, 1)\n",
      "Shape of B: (10, 2)\n",
      "Shape of C: (1, 10)\n",
      "Resulting matrix C:\n",
      " [[ 0.06273332  0.47446912  3.60165141 -0.45661903 -1.55865188 -1.02723517\n",
      "   0.88714056  1.90474578  1.27699197 -1.26093308]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example matrices\n",
    "A = np.random.randn(2, 1)  # Shape (2, 1)\n",
    "B = np.random.randn(10, 2) # Shape (10, 2)\n",
    "\n",
    "# Transpose A to align dimensions correctly\n",
    "C = np.dot(A.T, B.T)  # Dot product of shape (1, 2) and (10, 2)\n",
    "\n",
    "print(\"Shape of A:\", A.shape)\n",
    "print(\"Shape of B:\", B.shape)\n",
    "print(\"Shape of C:\", C.shape)\n",
    "print(\"Resulting matrix C:\\n\", C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8bb5fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n",
      "(10, 1)\n",
      "(10,)\n",
      "(10,)\n",
      "(10, 1)\n",
      "(10,)\n",
      "(10,)\n",
      "(10, 1)\n",
      "(10,)\n",
      "(10,)\n",
      "(10, 1)\n",
      "(10,)\n",
      "(10,)\n",
      "(10, 1)\n",
      "(10,)\n",
      "(10,)\n",
      "(10, 1)\n",
      "(10,)\n",
      "(10,)\n",
      "(10, 1)\n",
      "(10,)\n",
      "(10,)\n",
      "(10, 1)\n",
      "(10,)\n",
      "(10,)\n",
      "(10, 1)\n",
      "(10,)\n",
      "(10,)\n",
      "(10, 1)\n",
      "(10,)\n",
      "(10,)\n",
      "(10, 1)\n",
      "(10,)\n",
      "(10,)\n",
      "(10, 1)\n",
      "(10,)\n",
      "(10,)\n",
      "(10, 1)\n",
      "(10,)\n",
      "(10,)\n",
      "(10, 1)\n",
      "(10,)\n",
      "(10,)\n",
      "(10, 1)\n",
      "(10,)\n",
      "(10,)\n",
      "(10, 1)\n",
      "(10,)\n",
      "(10,)\n",
      "(10, 1)\n",
      "(10,)\n",
      "(10,)\n",
      "(10, 1)\n",
      "(10,)\n",
      "(10,)\n",
      "(10, 1)\n",
      "(10,)\n",
      "(10,)\n",
      "(10, 1)\n",
      "(10,)\n",
      "(10,)\n",
      "(10,)\n",
      "(10,)\n",
      "(10,)\n",
      "(10,)\n",
      "(10,)\n",
      "(10,)\n",
      "(10,)\n",
      "(10,)\n",
      "(10,)\n",
      "(10,)\n",
      "(10,)\n",
      "(10,)\n",
      "(10,)\n",
      "(10,)\n",
      "(10,)\n",
      "(10,)\n",
      "(10,)\n",
      "(10,)\n",
      "(10,)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 20 is out of bounds for axis 1 with size 20",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13059/2618611870.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;31m# Create and train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0mrnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBidirectionalRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_13059/2618611870.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, y, epochs, learning_rate)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_backward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_backward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_backward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_backward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_13059/2618611870.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, X, y, y_pred, h_forward, h_backward, c_forward, c_backward, learning_rate)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mdo_backward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdh_backward\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_backward\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0mdc_backward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdh_backward\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_backward\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0mdi_backward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdc_backward\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mU_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_backward\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb_c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0mdf_backward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdc_backward\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mc_backward\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 20 is out of bounds for axis 1 with size 20"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Activation functions\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x) ** 2\n",
    "\n",
    "# Bidirectional RNN with LSTM\n",
    "class BidirectionalRNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size, sequence_length):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.W_f = np.random.randn(hidden_size, input_size)\n",
    "        self.W_i = np.random.randn(hidden_size, input_size)\n",
    "        self.W_c = np.random.randn(hidden_size, input_size)\n",
    "        self.W_o = np.random.randn(hidden_size, input_size)\n",
    "        \n",
    "        self.U_f = np.random.randn(hidden_size, hidden_size)\n",
    "        self.U_i = np.random.randn(hidden_size, hidden_size)\n",
    "        self.U_c = np.random.randn(hidden_size, hidden_size)\n",
    "        self.U_o = np.random.randn(hidden_size, hidden_size)\n",
    "        \n",
    "        self.b_f = np.zeros((hidden_size, 1))\n",
    "        self.b_i = np.zeros((hidden_size, 1))\n",
    "        self.b_c = np.zeros((hidden_size, 1))\n",
    "        self.b_o = np.zeros((hidden_size, 1))\n",
    "        \n",
    "        self.V_y = np.random.randn(output_size, 2 * hidden_size)\n",
    "        self.b_y = np.zeros((output_size, 1))\n",
    "\n",
    "    def forward(self, X):\n",
    "        h_forward = np.zeros((self.hidden_size, self.sequence_length))\n",
    "        c_forward = np.zeros((self.hidden_size, self.sequence_length))\n",
    "        \n",
    "        h_backward = np.zeros((self.hidden_size, self.sequence_length))\n",
    "        c_backward = np.zeros((self.hidden_size, self.sequence_length))\n",
    "        \n",
    "        for t in range(self.sequence_length):\n",
    "            x_t = X[:, t].reshape(-1, 1)\n",
    "            h_prev_forward = h_forward[:, t-1].reshape(-1, 1) if t > 0 else np.zeros((self.hidden_size, 1))\n",
    "\n",
    "            f_t = sigmoid(np.dot(self.W_f, x_t) + np.dot(self.U_f, h_prev_forward) + self.b_f)\n",
    "            i_t = sigmoid(np.dot(self.W_i, x_t) + np.dot(self.U_i, h_prev_forward) + self.b_i)\n",
    "            o_t = sigmoid(np.dot(self.W_o, x_t) + np.dot(self.U_o, h_prev_forward) + self.b_o)\n",
    "            c_hat_t = tanh(np.dot(self.W_c, x_t) + np.dot(self.U_c, h_prev_forward) + self.b_c)\n",
    "            t1 = c_forward[:,t].reshape(-1)\n",
    "            print(t1.shape)\n",
    "            #print((f_t * c_forward[:, t-1].reshape(-1,1))\n",
    "            t2 = f_t * c_forward[:, t-1].reshape(-1,1)\n",
    "            print(t2.shape)      \n",
    "            t1[:] = np.squeeze(t2)\n",
    "            print(t1.shape)\n",
    "            c_forward[:, t] = t1\n",
    "            #c_forward[:, t] = (f_t * c_forward[:, t-1].reshape(-1) if t > 0 else f_t) + i_t * c_hat_t\n",
    "            #h_forward[:, :t] = o_t * tanh(c_forward[:, t])\n",
    "\n",
    "        for t in range(self.sequence_length - 1, -1, -1):\n",
    "            x_t = X[:, t].reshape(-1, 1)\n",
    "            h_prev_backward = h_backward[:, t+1].reshape(-1, 1) if t < self.sequence_length - 1 else np.zeros((self.hidden_size, 1))\n",
    "\n",
    "            f_t = sigmoid(np.dot(self.W_f, x_t) + np.dot(self.U_f, h_prev_backward) + self.b_f)\n",
    "            i_t = sigmoid(np.dot(self.W_i, x_t) + np.dot(self.U_i, h_prev_backward) + self.b_i)\n",
    "            o_t = sigmoid(np.dot(self.W_o, x_t) + np.dot(self.U_o, h_prev_backward) + self.b_o)\n",
    "            c_hat_t = tanh(np.dot(self.W_c, x_t) + np.dot(self.U_c, h_prev_backward) + self.b_c)\n",
    "            print(c_backward[:, t].shape)\n",
    "            t_t = (f_t * c_backward[:, t+1].reshape(-1, 1) if t < self.sequence_length - 1 else f_t) + i_t * c_hat_t\n",
    "            c_backward[:, t] = t_t.flatten()\n",
    "            t2 = o_t * tanh(c_backward[:, t])\n",
    "            t2_reshaped = t2.reshape(-1, 1)[:10].flatten()  # Ensure it becomes (10,)\n",
    "            h_backward[:, t] = t2_reshaped\n",
    "            #h_backward[:, t] = o_t * tanh(c_backward[:, t])\n",
    " \n",
    "\n",
    "        h_concat = np.concatenate((h_forward[:, -1], h_backward[:, 0])).reshape(-1, 1)\n",
    "        y_pred = sigmoid(np.dot(self.V_y, h_concat) + self.b_y)\n",
    "        return y_pred, h_forward, h_backward, c_forward, c_backward\n",
    "\n",
    "    def backward(self, X, y, y_pred, h_forward, h_backward, c_forward, c_backward, learning_rate):\n",
    "        dV_y = np.zeros_like(self.V_y)\n",
    "        db_y = np.zeros_like(self.b_y)\n",
    "        \n",
    "        dW_f = np.zeros_like(self.W_f)\n",
    "        dW_i = np.zeros_like(self.W_i)\n",
    "        dW_c = np.zeros_like(self.W_c)\n",
    "        dW_o = np.zeros_like(self.W_o)\n",
    "        \n",
    "        dU_f = np.zeros_like(self.U_f)\n",
    "        dU_i = np.zeros_like(self.U_i)\n",
    "        dU_c = np.zeros_like(self.U_c)\n",
    "        dU_o = np.zeros_like(self.U_o)\n",
    "        \n",
    "        db_f = np.zeros_like(self.b_f)\n",
    "        db_i = np.zeros_like(self.b_i)\n",
    "        db_c = np.zeros_like(self.b_c)\n",
    "        db_o = np.zeros_like(self.b_o)\n",
    "\n",
    "        dy = y_pred - y.reshape(-1, 1)\n",
    "        dV_y += np.dot(dy, np.concatenate((h_forward[:, -1], h_backward[:, 0])).reshape(1, -1))\n",
    "        db_y += dy\n",
    "\n",
    "        dh_forward = np.dot(self.V_y[:, :self.hidden_size].T, dy)\n",
    "        dh_backward = np.dot(self.V_y[:, self.hidden_size:].T, dy)\n",
    "\n",
    "        for t in range(self.sequence_length - 1, -1, -1):\n",
    "            do_forward = dh_forward * tanh(c_forward[:, t].reshape(-1, 1))\n",
    "            dc_forward = dh_forward * (1 - tanh(c_forward[:, t].reshape(-1, 1)) ** 2)\n",
    "            di_forward = dc_forward * tanh(np.dot(self.W_c, X[:, t].reshape(-1, 1)) + np.dot(self.U_c, h_forward[:, t-1].reshape(-1, 1)) + self.b_c)\n",
    "            df_forward = dc_forward * c_forward[:, t-1].reshape(-1, 1)\n",
    "            \n",
    "            dW_o += np.dot(do_forward, X[:, t].reshape(1, -1))\n",
    "            dU_o += np.dot(do_forward, h_forward[:, t-1].reshape(1, -1))\n",
    "            db_o += do_forward\n",
    "\n",
    "            dW_i += np.dot(di_forward, X[:, t].reshape(1, -1))\n",
    "            dU_i += np.dot(di_forward, h_forward[:, t-1].reshape(1, -1))\n",
    "            db_i += di_forward\n",
    "\n",
    "            dW_f += np.dot(df_forward, X[:, t].reshape(1, -1))\n",
    "            dU_f += np.dot(df_forward, h_forward[:, t-1].reshape(1, -1))\n",
    "            db_f += df_forward\n",
    "\n",
    "        for t in range(self.sequence_length):\n",
    "            do_backward = dh_backward * tanh(c_backward[:, t].reshape(-1, 1))\n",
    "            dc_backward = dh_backward * (1 - tanh(c_backward[:, t].reshape(-1, 1)) ** 2)\n",
    "            di_backward = dc_backward * tanh(np.dot(self.W_c, X[:, t].reshape(-1, 1)) + np.dot(self.U_c, h_backward[:, t+1].reshape(-1, 1)) + self.b_c)\n",
    "            df_backward = dc_backward * c_backward[:, t+1].reshape(-1, 1)\n",
    "            \n",
    "            dW_o += np.dot(do_backward, X[:, t].reshape(1, -1))\n",
    "            dU_o += np.dot(do_backward, h_backward[:, t+1].reshape(1, -1))\n",
    "            db_o += do_backward\n",
    "\n",
    "            dW_i += np.dot(di_backward, X[:, t].reshape(1, -1))\n",
    "            dU_i += np.dot(di_backward, h_backward[:, t+1].reshape(1, -1))\n",
    "            db_i += di_backward\n",
    "\n",
    "            dW_f += np.dot(df_backward, X[:, t].reshape(1, -1))\n",
    "            dU_f += np.dot(df_backward, h_backward[:, t+1].reshape(1, -1))\n",
    "            db_f += df_backward\n",
    "\n",
    "        self.V_y -= learning_rate * dV_y\n",
    "        self.b_y -= learning_rate * db_y\n",
    "\n",
    "        self.W_f -= learning_rate * dW_f\n",
    "        self.W_i -= learning_rate * dW_i\n",
    "        self.W_c -= learning_rate * dW_c\n",
    "        self.W_o -= learning_rate * dW_o\n",
    "\n",
    "        self.U_f -= learning_rate * dU_f\n",
    "        self.U_i -= learning_rate * dU_i\n",
    "        self.U_c -= learning_rate * dU_c\n",
    "        self.U_o -= learning_rate * dU_o\n",
    "\n",
    "        self.b_f -= learning_rate * db_f\n",
    "        self.b_i -= learning_rate * db_i\n",
    "        self.b_c -= learning_rate * db_c\n",
    "        self.b_o -= learning_rate * db_o\n",
    "\n",
    "    def train(self, X, y, epochs, learning_rate):\n",
    "        for epoch in range(epochs):\n",
    "            y_pred, h_forward, h_backward, c_forward, c_backward = self.forward(X)\n",
    "            self.backward(X, y, y_pred, h_forward, h_backward, c_forward, c_backward, learning_rate)\n",
    "            if epoch % 10 == 0:\n",
    "                loss = np.mean((y - y_pred) ** 2)\n",
    "                print(f'Epoch {epoch}, Loss: {loss}')\n",
    "\n",
    "# Example usage\n",
    "input_size = 5\n",
    "hidden_size = 10\n",
    "output_size = 1\n",
    "sequence_length = 20\n",
    "learning_rate = 0.001\n",
    "epochs = 100\n",
    "\n",
    "# Dummy data\n",
    "X = np.random.randn(input_size, sequence_length)\n",
    "y = np.random.randint(0, 2, (output_size, 1))\n",
    "\n",
    "# Create and train the model\n",
    "rnn = BidirectionalRNN(input_size, hidden_size, output_size, sequence_length)\n",
    "rnn.train(X, y, epochs, learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f420b472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After assigning row: [ 1.28727281 -1.18136797  1.02094269 -1.84678749  1.27751904 -0.02640973\n",
      "  0.2429473   0.56807779 -1.78637454  0.06674648]\n",
      "After assigning column: [ 1.28727281  0.71817259 -0.42793566  0.78615642  0.81391894  1.25202391\n",
      " -1.24249576  0.07097557  0.20145723  0.30219445]\n",
      "After flattening and slicing: [ 1.28727281 -1.18136797  1.02094269 -1.84678749  1.27751904 -0.02640973\n",
      "  0.2429473   0.56807779 -1.78637454  0.06674648]\n",
      "After reshaping and assigning: (10,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example arrays\n",
    "t1 = np.random.randn(10)  # Shape (10,)\n",
    "t2 = np.random.randn(10, 10)  # Shape (10, 10)\n",
    "\n",
    "# Case 1: Assigning a row\n",
    "t1[:] = t2[0, :]  # Assign the first row of t2 to t1\n",
    "print('After assigning row:', t1)\n",
    "\n",
    "# Case 2: Assigning a column\n",
    "t1[:] = t2[:, 0]  # Assign the first column of t2 to t1\n",
    "print('After assigning column:', t1)\n",
    "\n",
    "# Case 3: Flatten and take first 10 elements\n",
    "t1 = t2.flatten()[:10]  # Flatten t2 and take first 10 elements\n",
    "print('After flattening and slicing:', t1)\n",
    "\n",
    "# Case 4: Reshaping and assigning\n",
    "t2_reshaped = t2.reshape(-1, 1)[:10].flatten()  # Ensure it becomes (10,)\n",
    "t1[:] = t2_reshaped\n",
    "print('After reshaping and assigning:', t1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a23f6fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dc_backward shape: (10, 1)\n",
      "dot_Wc_X shape: (10, 1)\n",
      "dot_Uc_hb shape: (10, 1)\n",
      "b_c shape: (10, 1)\n",
      "sum_dot_products shape: (10, 1)\n",
      "tanh_sum shape: (10, 1)\n",
      "di_backward shape: (10, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example dimensions\n",
    "n_hidden = 10  # Number of hidden units\n",
    "n_input = 5    # Number of input features\n",
    "batch_size = 20  # Batch size, assumed here as 20 for the example\n",
    "\n",
    "# Randomly initialize matrices for demonstration purposes\n",
    "W_c = np.random.randn(n_hidden, n_input)\n",
    "U_c = np.random.randn(n_hidden, n_hidden)\n",
    "b_c = np.random.randn(n_hidden, 1)\n",
    "X = np.random.randn(n_input, batch_size)  # Input batch of size 20\n",
    "h_backward = np.random.randn(n_hidden, batch_size + 1)  # Hidden states including initial state\n",
    "dc_backward = np.random.randn(n_hidden, batch_size)  # Example dc_backward\n",
    "\n",
    "# Example timestep t (ensure t is within bounds)\n",
    "t = 0  # Starting index, should be between 0 and batch_size-1\n",
    "\n",
    "# Ensure t is within valid range\n",
    "if t < 0 or t >= batch_size:\n",
    "    raise ValueError(f\"Time step t should be in range [0, {batch_size-1}]\")\n",
    "\n",
    "# Ensure all shapes are compatible\n",
    "x_t_reshaped = X[:, t].reshape(-1, 1)  # Shape (n_input, 1)\n",
    "h_b_t1_reshaped = h_backward[:, t+1].reshape(-1, 1)  # Shape (n_hidden, 1)\n",
    "\n",
    "# Calculate dot products\n",
    "dot_Wc_X = np.dot(W_c, x_t_reshaped)  # Shape (n_hidden, 1)\n",
    "dot_Uc_hb = np.dot(U_c, h_b_t1_reshaped)  # Shape (n_hidden, 1)\n",
    "\n",
    "# Sum the components and add bias\n",
    "sum_dot_products = dot_Wc_X + dot_Uc_hb + b_c  # Shape (n_hidden, 1)\n",
    "\n",
    "# Apply tanh\n",
    "tanh_sum = np.tanh(sum_dot_products)  # Shape (n_hidden, 1)\n",
    "\n",
    "# Element-wise multiply with dc_backward\n",
    "di_backward = dc_backward[:, t].reshape(-1, 1) * tanh_sum  # Shape (n_hidden, 1)\n",
    "\n",
    "# Printing for verification\n",
    "print(\"dc_backward shape:\", dc_backward[:, t].reshape(-1, 1).shape)\n",
    "print(\"dot_Wc_X shape:\", dot_Wc_X.shape)\n",
    "print(\"dot_Uc_hb shape:\", dot_Uc_hb.shape)\n",
    "print(\"b_c shape:\", b_c.shape)\n",
    "print(\"sum_dot_products shape:\", sum_dot_products.shape)\n",
    "print(\"tanh_sum shape:\", tanh_sum.shape)\n",
    "print(\"di_backward shape:\", di_backward.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e90cf8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Y: [[  1.30163509   7.07544671  -7.05588653   3.96047734   2.38241803\n",
      "    5.17756802   1.56250443  -5.78920478  -4.41848448  -3.83722165\n",
      "    3.43541901   6.49849044  20.19723144  -8.13955026   0.95239785]\n",
      " [ -2.69841934   2.29325906  -2.31885708   2.4509893   -5.4886059\n",
      "   -9.08251322  -9.67782239   4.38749734   9.79895852   9.21639647\n",
      "    5.704967    -6.16941026 -10.14077588  -0.18198193   6.23166498]\n",
      " [  1.13965882  -0.22977437  -1.67931229 -12.53216329  -7.33665196\n",
      "   -5.82198187  -6.29288702   0.67500261  -0.89203674  10.32818604\n",
      "  -11.68439997  -6.51663573  -3.45511747  -2.54123866   1.54084556]\n",
      " [ -6.9566914   -1.74641181  -7.60329237  -7.06665351   0.953177\n",
      "    4.29658088  -1.33672038   6.47574353   6.30524103  -2.01889714\n",
      "   -4.70632023  -3.92074232  -9.69582973  -1.554269    -0.40463853]\n",
      " [ -5.33752486   1.36616355   7.44670578   3.80909187  -4.37499531\n",
      "   -7.14967499  -1.34761871  10.26028012  -0.15742397  13.67016264\n",
      "    0.43516504  -1.50984267 -12.18702335  -2.34640604  -3.5354282 ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class BidirectionalRNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size, seq_length):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "        # Initialize weights\n",
    "        self.W_xh_forward = np.random.randn(hidden_size, input_size)\n",
    "        self.W_hh_forward = np.random.randn(hidden_size, hidden_size)\n",
    "        self.b_h_forward = np.zeros((hidden_size, 1))\n",
    "\n",
    "        self.W_xh_backward = np.random.randn(hidden_size, input_size)\n",
    "        self.W_hh_backward = np.random.randn(hidden_size, hidden_size)\n",
    "        self.b_h_backward = np.zeros((hidden_size, 1))\n",
    "\n",
    "        self.W_hy = np.random.randn(output_size, 2 * hidden_size)\n",
    "        self.b_y = np.zeros((output_size, 1))\n",
    "\n",
    "    def forward(self, X):\n",
    "        h_forward = np.zeros((self.hidden_size, self.seq_length))\n",
    "        h_backward = np.zeros((self.hidden_size, self.seq_length))\n",
    "        Y = np.zeros((self.output_size, self.seq_length))\n",
    "\n",
    "        # Forward pass\n",
    "        for t in range(self.seq_length):\n",
    "            x_t = X[:, t].reshape(-1, 1)\n",
    "            if t == 0:\n",
    "                h_forward[:, t] = np.tanh(np.dot(self.W_xh_forward, x_t) + self.b_h_forward).flatten()\n",
    "            else:\n",
    "                h_forward[:, t] = np.tanh(np.dot(self.W_xh_forward, x_t) + np.dot(self.W_hh_forward, h_forward[:, t-1].reshape(-1, 1)) + self.b_h_forward).flatten()\n",
    "\n",
    "        # Backward pass\n",
    "        for t in reversed(range(self.seq_length)):\n",
    "            x_t = X[:, t].reshape(-1, 1)\n",
    "            if t == self.seq_length - 1:\n",
    "                h_backward[:, t] = np.tanh(np.dot(self.W_xh_backward, x_t) + self.b_h_backward).flatten()\n",
    "            else:\n",
    "                h_backward[:, t] = np.tanh(np.dot(self.W_xh_backward, x_t) + np.dot(self.W_hh_backward, h_backward[:, t+1].reshape(-1, 1)) + self.b_h_backward).flatten()\n",
    "\n",
    "        # Output layer\n",
    "        for t in range(self.seq_length):\n",
    "            h_concat = np.concatenate((h_forward[:, t], h_backward[:, t])).reshape(-1, 1)\n",
    "            Y[:, t] = np.dot(self.W_hy, h_concat).flatten() + self.b_y.flatten()\n",
    "\n",
    "        return Y\n",
    "\n",
    "# Example usage\n",
    "input_size = 10\n",
    "hidden_size = 20\n",
    "output_size = 5\n",
    "seq_length = 15\n",
    "\n",
    "X = np.random.randn(input_size, seq_length)\n",
    "\n",
    "rnn = BidirectionalRNN(input_size, hidden_size, output_size, seq_length)\n",
    "Y = rnn.forward(X)\n",
    "\n",
    "print(\"Output Y:\", Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f21d8014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 771.6388182602528\n",
      "Epoch 2/10, Loss: 222.70415716326218\n",
      "Epoch 3/10, Loss: 116.27330771888519\n",
      "Epoch 4/10, Loss: 90.4364523962641\n",
      "Epoch 5/10, Loss: 81.5900988975763\n",
      "Epoch 6/10, Loss: 78.54778902845628\n",
      "Epoch 7/10, Loss: 76.81286575409018\n",
      "Epoch 8/10, Loss: 76.2742182929642\n",
      "Epoch 9/10, Loss: 75.83716311236157\n",
      "Epoch 10/10, Loss: 75.715586036493\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SimpleBidirectionalRNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size, seq_length, learning_rate=0.001):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.seq_length = seq_length\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Initialize weights\n",
    "        self.W_xh_forward = np.random.randn(hidden_size, input_size)\n",
    "        self.W_hh_forward = np.random.randn(hidden_size, hidden_size)\n",
    "        self.b_h_forward = np.zeros((hidden_size, 1))\n",
    "\n",
    "        self.W_xh_backward = np.random.randn(hidden_size, input_size)\n",
    "        self.W_hh_backward = np.random.randn(hidden_size, hidden_size)\n",
    "        self.b_h_backward = np.zeros((hidden_size, 1))\n",
    "\n",
    "        self.W_hy = np.random.randn(output_size, 2 * hidden_size)\n",
    "        self.b_y = np.zeros((output_size, 1))\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.h_forward = np.zeros((self.hidden_size, self.seq_length))\n",
    "        self.h_backward = np.zeros((self.hidden_size, self.seq_length))\n",
    "        self.Y = np.zeros((self.output_size, self.seq_length))\n",
    "\n",
    "        # Forward pass\n",
    "        for t in range(self.seq_length):\n",
    "            x_t = X[:, t].reshape(-1, 1)\n",
    "            if t == 0:\n",
    "                self.h_forward[:, t] = np.tanh(np.dot(self.W_xh_forward, x_t) + self.b_h_forward).flatten()\n",
    "            else:\n",
    "                self.h_forward[:, t] = np.tanh(np.dot(self.W_xh_forward, x_t) + np.dot(self.W_hh_forward, self.h_forward[:, t-1].reshape(-1, 1)) + self.b_h_forward).flatten()\n",
    "\n",
    "        # Backward pass\n",
    "        for t in reversed(range(self.seq_length)):\n",
    "            x_t = X[:, t].reshape(-1, 1)\n",
    "            if t == self.seq_length - 1:\n",
    "                self.h_backward[:, t] = np.tanh(np.dot(self.W_xh_backward, x_t) + self.b_h_backward).flatten()\n",
    "            else:\n",
    "                self.h_backward[:, t] = np.tanh(np.dot(self.W_xh_backward, x_t) + np.dot(self.W_hh_backward, self.h_backward[:, t+1].reshape(-1, 1)) + self.b_h_backward).flatten()\n",
    "\n",
    "        # Output layer\n",
    "        for t in range(self.seq_length):\n",
    "            h_concat = np.concatenate((self.h_forward[:, t], self.h_backward[:, t])).reshape(-1, 1)\n",
    "            self.Y[:, t] = (np.dot(self.W_hy, h_concat) + self.b_y).flatten()\n",
    "\n",
    "        return self.Y\n",
    "\n",
    "    def backward(self, X, Y_true):\n",
    "        dW_xh_forward = np.zeros_like(self.W_xh_forward)\n",
    "        dW_hh_forward = np.zeros_like(self.W_hh_forward)\n",
    "        db_h_forward = np.zeros_like(self.b_h_forward)\n",
    "\n",
    "        dW_xh_backward = np.zeros_like(self.W_xh_backward)\n",
    "        dW_hh_backward = np.zeros_like(self.W_hh_backward)\n",
    "        db_h_backward = np.zeros_like(self.b_h_backward)\n",
    "\n",
    "        dW_hy = np.zeros_like(self.W_hy)\n",
    "        db_y = np.zeros_like(self.b_y)\n",
    "\n",
    "        dY = self.Y - Y_true\n",
    "\n",
    "        for t in range(self.seq_length):\n",
    "            dy = dY[:, t].reshape(-1, 1)\n",
    "            h_concat = np.concatenate((self.h_forward[:, t], self.h_backward[:, t])).reshape(-1, 1)\n",
    "\n",
    "            dW_hy += np.dot(dy, h_concat.T)\n",
    "            db_y += dy\n",
    "\n",
    "            dh_forward = np.dot(self.W_hy[:, :self.hidden_size].T, dy)\n",
    "            for k in range(t, -1, -1):\n",
    "                dh_forward = dh_forward * (1 - self.h_forward[:, k].reshape(-1, 1)**2)\n",
    "                dW_xh_forward += np.dot(dh_forward, X[:, k].reshape(1, -1))\n",
    "                db_h_forward += dh_forward\n",
    "                if k != 0:\n",
    "                    dW_hh_forward += np.dot(dh_forward, self.h_forward[:, k-1].reshape(1, -1))\n",
    "                    dh_forward = np.dot(self.W_hh_forward.T, dh_forward)\n",
    "\n",
    "            dh_backward = np.dot(self.W_hy[:, self.hidden_size:].T, dy)\n",
    "            for k in range(t, self.seq_length):\n",
    "                dh_backward = dh_backward * (1 - self.h_backward[:, k].reshape(-1, 1)**2)\n",
    "                dW_xh_backward += np.dot(dh_backward, X[:, k].reshape(1, -1))\n",
    "                db_h_backward += dh_backward\n",
    "                if k != self.seq_length - 1:\n",
    "                    dW_hh_backward += np.dot(dh_backward, self.h_backward[:, k+1].reshape(1, -1))\n",
    "                    dh_backward = np.dot(self.W_hh_backward.T, dh_backward)\n",
    "\n",
    "        self.W_xh_forward -= self.learning_rate * dW_xh_forward\n",
    "        self.W_hh_forward -= self.learning_rate * dW_hh_forward\n",
    "        self.b_h_forward -= self.learning_rate * db_h_forward\n",
    "\n",
    "        self.W_xh_backward -= self.learning_rate * dW_xh_backward\n",
    "        self.W_hh_backward -= self.learning_rate * dW_hh_backward\n",
    "        self.b_h_backward -= self.learning_rate * db_h_backward\n",
    "\n",
    "        self.W_hy -= self.learning_rate * dW_hy\n",
    "        self.b_y -= self.learning_rate * db_y\n",
    "\n",
    "    def train(self, X_train, Y_train, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            loss = 0\n",
    "            for X, Y in zip(X_train, Y_train):\n",
    "                Y_pred = self.forward(X)\n",
    "                self.backward(X, Y)\n",
    "                loss += np.sum((Y_pred - Y) ** 2)\n",
    "            print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss / len(X_train)}')\n",
    "\n",
    "# Example usage\n",
    "input_size = 10\n",
    "hidden_size = 20\n",
    "output_size = 5\n",
    "seq_length = 15\n",
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "\n",
    "# Generate random training data\n",
    "X_train = [np.random.randn(input_size, seq_length) for _ in range(100)]\n",
    "Y_train = [np.random.randn(output_size, seq_length) for _ in range(100)]\n",
    "\n",
    "rnn = SimpleBidirectionalRNN(input_size, hidden_size, output_size, seq_length, learning_rate)\n",
    "rnn.train(X_train, Y_train, epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a514c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SimpleBiLSTM:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.001):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Initialize weights and biases for forward LSTM\n",
    "        self.W_f = np.random.randn(hidden_size, hidden_size + input_size)\n",
    "        self.b_f = np.zeros((hidden_size, 1))\n",
    "\n",
    "        self.W_i = np.random.randn(hidden_size, hidden_size + input_size)\n",
    "        self.b_i = np.zeros((hidden_size, 1))\n",
    "\n",
    "        self.W_c = np.random.randn(hidden_size, hidden_size + input_size)\n",
    "        self.b_c = np.zeros((hidden_size, 1))\n",
    "\n",
    "        self.W_o = np.random.randn(hidden_size, hidden_size + input_size)\n",
    "        self.b_o = np.zeros((hidden_size, 1))\n",
    "\n",
    "        # Initialize weights and biases for backward LSTM\n",
    "        self.W_f_back = np.random.randn(hidden_size, hidden_size + input_size)\n",
    "        self.b_f_back = np.zeros((hidden_size, 1))\n",
    "\n",
    "        self.W_i_back = np.random.randn(hidden_size, hidden_size + input_size)\n",
    "        self.b_i_back = np.zeros((hidden_size, 1))\n",
    "\n",
    "        self.W_c_back = np.random.randn(hidden_size, hidden_size + input_size)\n",
    "        self.b_c_back = np.zeros((hidden_size, 1))\n",
    "\n",
    "        self.W_o_back = np.random.randn(hidden_size, hidden_size + input_size)\n",
    "        self.b_o_back = np.zeros((hidden_size, 1))\n",
    "\n",
    "        # Initialize weights and biases for output layer\n",
    "        self.W_y = np.random.randn(output_size, 2 * hidden_size)\n",
    "        self.b_y = np.zeros((output_size, 1))\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def forward(self, X):\n",
    "        T = X.shape[1]  # Length of the sequence\n",
    "\n",
    "        # Initialize hidden and cell states for forward and backward passes\n",
    "        self.h_forward = np.zeros((self.hidden_size, T))\n",
    "        self.c_forward = np.zeros((self.hidden_size, T))\n",
    "        self.h_backward = np.zeros((self.hidden_size, T))\n",
    "        self.c_backward = np.zeros((self.hidden_size, T))\n",
    "\n",
    "        # Forward pass\n",
    "        for t in range(T):\n",
    "            x_t = X[:, t].reshape(-1, 1)\n",
    "            combined = np.vstack((self.h_forward[:, t-1].reshape(-1, 1), x_t)) if t > 0 else np.vstack((np.zeros((self.hidden_size, 1)), x_t))\n",
    "\n",
    "            f_t = self.sigmoid(np.dot(self.W_f, combined) + self.b_f)\n",
    "            i_t = self.sigmoid(np.dot(self.W_i, combined) + self.b_i)\n",
    "            c_tilde = np.tanh(np.dot(self.W_c, combined) + self.b_c)\n",
    "            self.c_forward[:, t] = f_t.flatten() * self.c_forward[:, t-1] + i_t.flatten() * c_tilde.flatten() if t > 0 else i_t.flatten() * c_tilde.flatten()\n",
    "            o_t = self.sigmoid(np.dot(self.W_o, combined) + self.b_o)\n",
    "            self.h_forward[:, t] = o_t.flatten() * np.tanh(self.c_forward[:, t])\n",
    "\n",
    "        # Backward pass\n",
    "        for t in reversed(range(T)):\n",
    "            x_t = X[:, t].reshape(-1, 1)\n",
    "            combined = np.vstack((self.h_backward[:, t+1].reshape(-1, 1), x_t)) if t < T-1 else np.vstack((np.zeros((self.hidden_size, 1)), x_t))\n",
    "\n",
    "            f_t = self.sigmoid(np.dot(self.W_f_back, combined) + self.b_f_back)\n",
    "            i_t = self.sigmoid(np.dot(self.W_i_back, combined) + self.b_i_back)\n",
    "            c_tilde = np.tanh(np.dot(self.W_c_back, combined) + self.b_c_back)\n",
    "            self.c_backward[:, t] = f_t.flatten() * self.c_backward[:, t+1] + i_t.flatten() * c_tilde.flatten() if t < T-1 else i_t.flatten() * c_tilde.flatten()\n",
    "            o_t = self.sigmoid(np.dot(self.W_o_back, combined) + self.b_o_back)\n",
    "            self.h_backward[:, t] = o_t.flatten() * np.tanh(self.c_backward[:, t])\n",
    "\n",
    "        # Concatenate forward and backward hidden states\n",
    "        self.h_concat = np.vstack((self.h_forward, self.h_backward))\n",
    "\n",
    "        # Output layer\n",
    "        y_pred = np.dot(self.W_y, self.h_concat) + self.b_y\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    def train(self, X_train, Y_train, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            for X, Y_true in zip(X_train, Y_train):\n",
    "                # Forward pass\n",
    "                Y_pred = self.forward(X)\n",
    "\n",
    "                # Backpropagation through time (BPTT) and gradient descent\n",
    "                # Update weights using gradients (not shown for simplicity)\n",
    "                pass\n",
    "\n",
    "# Example usage:\n",
    "input_size = 10\n",
    "hidden_size = 20\n",
    "output_size = 5\n",
    "seq_length = 15\n",
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "\n",
    "# Generate random training data\n",
    "X_train = [np.random.randn(input_size, seq_length) for _ in range(100)]\n",
    "Y_train = [np.random.randn(output_size, seq_length) for _ in range(100)]\n",
    "\n",
    "# Create and train the BiLSTM model\n",
    "bilstm = SimpleBiLSTM(input_size, hidden_size, output_size, learning_rate)\n",
    "bilstm.train(X_train, Y_train, epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b9e6258",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (20,) (300,) (20,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6711/1304487484.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;31m# Create and train the BiLSTM model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0mbilstm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleBiLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m \u001b[0mbilstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_6711/1304487484.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X_train, Y_train, epochs)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m                 \u001b[0;31m# Backpropagation through time (BPTT) and gradient descent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_backward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_backward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;31m# Example usage:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_6711/1304487484.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, X, Y_true, Y_pred, h_forward, c_forward, h_backward, c_backward)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mdy_backward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0mdh_forward\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdy_forward\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m             \u001b[0mdh_backward\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdy_backward\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (20,) (300,) (20,) "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SimpleBiLSTM:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.001):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Initialize weights and biases for forward LSTM\n",
    "        self.W_f = np.random.randn(hidden_size, hidden_size + input_size)\n",
    "        self.b_f = np.zeros((hidden_size, 1))\n",
    "\n",
    "        self.W_i = np.random.randn(hidden_size, hidden_size + input_size)\n",
    "        self.b_i = np.zeros((hidden_size, 1))\n",
    "\n",
    "        self.W_c = np.random.randn(hidden_size, hidden_size + input_size)\n",
    "        self.b_c = np.zeros((hidden_size, 1))\n",
    "\n",
    "        self.W_o = np.random.randn(hidden_size, hidden_size + input_size)\n",
    "        self.b_o = np.zeros((hidden_size, 1))\n",
    "\n",
    "        # Initialize weights and biases for backward LSTM\n",
    "        self.W_f_back = np.random.randn(hidden_size, hidden_size + input_size)\n",
    "        self.b_f_back = np.zeros((hidden_size, 1))\n",
    "\n",
    "        self.W_i_back = np.random.randn(hidden_size, hidden_size + input_size)\n",
    "        self.b_i_back = np.zeros((hidden_size, 1))\n",
    "\n",
    "        self.W_c_back = np.random.randn(hidden_size, hidden_size + input_size)\n",
    "        self.b_c_back = np.zeros((hidden_size, 1))\n",
    "\n",
    "        self.W_o_back = np.random.randn(hidden_size, hidden_size + input_size)\n",
    "        self.b_o_back = np.zeros((hidden_size, 1))\n",
    "\n",
    "        # Initialize weights and biases for output layer\n",
    "        self.W_y = np.random.randn(output_size, 2 * hidden_size)\n",
    "        self.b_y = np.zeros((output_size, 1))\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def forward(self, X):\n",
    "        T = X.shape[1]  # Length of the sequence\n",
    "\n",
    "        # Initialize hidden and cell states for forward and backward passes\n",
    "        h_forward = np.zeros((self.hidden_size, T))\n",
    "        c_forward = np.zeros((self.hidden_size, T))\n",
    "        h_backward = np.zeros((self.hidden_size, T))\n",
    "        c_backward = np.zeros((self.hidden_size, T))\n",
    "\n",
    "        # Forward pass\n",
    "        for t in range(T):\n",
    "            x_t = X[:, t].reshape(-1, 1)\n",
    "            combined = np.concatenate((h_forward[:, t-1:t], x_t), axis=0) if t > 0 else np.concatenate((np.zeros((self.hidden_size, 1)), x_t), axis=0)\n",
    "\n",
    "            f_t = self.sigmoid(np.dot(self.W_f, combined) + self.b_f)\n",
    "            i_t = self.sigmoid(np.dot(self.W_i, combined) + self.b_i)\n",
    "            c_tilde = self.tanh(np.dot(self.W_c, combined) + self.b_c)\n",
    "            c_forward[:, t:t+1] = f_t * c_forward[:, t-1:t] + i_t * c_tilde if t > 0 else i_t * c_tilde\n",
    "            o_t = self.sigmoid(np.dot(self.W_o, combined) + self.b_o)\n",
    "            h_forward[:, t:t+1] = o_t * self.tanh(c_forward[:, t:t+1])\n",
    "\n",
    "        # Backward pass\n",
    "        for t in reversed(range(T)):\n",
    "            x_t = X[:, t].reshape(-1, 1)\n",
    "            combined = np.concatenate((h_backward[:, t+1:t+2], x_t), axis=0) if t < T-1 else np.concatenate((np.zeros((self.hidden_size, 1)), x_t), axis=0)\n",
    "\n",
    "            f_t = self.sigmoid(np.dot(self.W_f_back, combined) + self.b_f_back)\n",
    "            i_t = self.sigmoid(np.dot(self.W_i_back, combined) + self.b_i_back)\n",
    "            c_tilde = self.tanh(np.dot(self.W_c_back, combined) + self.b_c_back)\n",
    "            c_backward[:, t:t+1] = f_t * c_backward[:, t+1:t+2] + i_t * c_tilde if t < T-1 else i_t * c_tilde\n",
    "            o_t = self.sigmoid(np.dot(self.W_o_back, combined) + self.b_o_back)\n",
    "            h_backward[:, t:t+1] = o_t * self.tanh(c_backward[:, t:t+1])\n",
    "\n",
    "        # Concatenate forward and backward hidden states\n",
    "        h_concat = np.concatenate((h_forward, h_backward), axis=0)\n",
    "\n",
    "        # Output layer\n",
    "        y_pred = np.dot(self.W_y, h_concat) + self.b_y\n",
    "\n",
    "        return y_pred, h_forward, c_forward, h_backward, c_backward\n",
    "\n",
    "    def backward(self, X, Y_true, Y_pred, h_forward, c_forward, h_backward, c_backward):\n",
    "        T = X.shape[1]\n",
    "        dW_y = np.zeros_like(self.W_y)\n",
    "        db_y = np.zeros_like(self.b_y)\n",
    "\n",
    "        dh_forward = np.zeros_like(h_forward)\n",
    "        dc_forward = np.zeros_like(c_forward)\n",
    "\n",
    "        dh_backward = np.zeros_like(h_backward)\n",
    "        dc_backward = np.zeros_like(c_backward)\n",
    "\n",
    "        dW_f = np.zeros_like(self.W_f)\n",
    "        db_f = np.zeros_like(self.b_f)\n",
    "\n",
    "        dW_i = np.zeros_like(self.W_i)\n",
    "        db_i = np.zeros_like(self.b_i)\n",
    "\n",
    "        dW_c = np.zeros_like(self.W_c)\n",
    "        db_c = np.zeros_like(self.b_c)\n",
    "\n",
    "        dW_o = np.zeros_like(self.W_o)\n",
    "        db_o = np.zeros_like(self.b_o)\n",
    "\n",
    "        dW_f_back = np.zeros_like(self.W_f_back)\n",
    "        db_f_back = np.zeros_like(self.b_f_back)\n",
    "\n",
    "        dW_i_back = np.zeros_like(self.W_i_back)\n",
    "        db_i_back = np.zeros_like(self.b_i_back)\n",
    "\n",
    "        dW_c_back = np.zeros_like(self.W_c_back)\n",
    "        db_c_back = np.zeros_like(self.b_c_back)\n",
    "\n",
    "        dW_o_back = np.zeros_like(self.W_o_back)\n",
    "        db_o_back = np.zeros_like(self.b_o_back)\n",
    "\n",
    "        dy = Y_pred - Y_true\n",
    "\n",
    "        dW_y += np.dot(dy, np.concatenate((h_forward, h_backward), axis=0).T)\n",
    "        db_y += np.sum(dy, axis=1, keepdims=True)\n",
    "\n",
    "        for t in reversed(range(T)):\n",
    "            dy_forward = np.dot(self.W_y[:, :self.hidden_size].T, dy)\n",
    "            dy_backward = np.dot(self.W_y[:, self.hidden_size:].T, dy)\n",
    "\n",
    "            dh_forward[:, t] += dy_forward.flatten()\n",
    "            dh_backward[:, t] += dy_backward.flatten()\n",
    "\n",
    "            do_forward = dh_forward[:, t] * np.tanh(c_forward[:, t])\n",
    "            dc_forward[:, t] += dh_forward[:, t] * self.sigmoid(h_forward[:, t]) * (1 - np.tanh(c_forward[:, t]) ** 2)\n",
    "\n",
    "            do_backward = dh_backward[:, t] * np.tanh(c_backward[:, t])\n",
    "            dc_backward[:, t] += dh_backward[:, t] * self.sigmoid(h_backward[:, t]) * (1 - np.tanh(c_backward[:, t]) ** 2)\n",
    "\n",
    "            dW_o += np.dot(do_forward * self.sigmoid(h_forward[:, t]) * (1 - self.sigmoid(h_forward[:, t])), np.concatenate((h_forward[:, t-1:t], X[:, t:t+1]), axis=0).T)\n",
    "            db_o += np.sum(do_forward * self.sigmoid(h_forward[:, t]) * (1 - self.sigmoid(h_forward[:, t])), axis=1, keepdims=True)\n",
    "\n",
    "            dW_o_back += np.dot(do_backward * self.sigmoid(h_backward[:, t]) * (1 - self.sigmoid(h_backward[:, t])), np.concatenate((h_backward[:, t+1:t+2], X[:, t:t+1]), axis=0).T)\n",
    "            db_o_back += np.sum(do_backward * self.sigmoid(h_backward[:, t]) * (1 - self.sigmoid(h_backward[:, t])), axis=1, keepdims=True)\n",
    "\n",
    "            df_forward = dc_forward[:, t] * c_forward[:, t-1]\n",
    "            di_forward = dc_forward[:, t] * np.tanh(c_forward[:, t])\n",
    "\n",
    "            df_backward = dc_backward[:, t] * c_backward[:, t+1]\n",
    "            di_backward = dc_backward[:, t] * np.tanh(c_backward[:, t])\n",
    "\n",
    "            dW_f += np.dot(df_forward * self.sigmoid(h_forward[:, t]) * (1 - self.sigmoid(h_forward[:, t])), np.concatenate((h_forward[:, t-1:t], X[:, t:t+1]), axis=0).T)\n",
    "            db_f += np.sum(df_forward * self.sigmoid(h_forward[:, t]) * (1 - self.sigmoid(h_forward[:, t])), axis=1, keepdims=True)\n",
    "\n",
    "            dW_f_back += np.dot(df_backward * self.sigmoid(h_backward[:, t]) * (1 - self.sigmoid(h_backward[:, t])), np.concatenate((h_backward[:, t+1:t+2], X[:, t:t+1]), axis=0).T)\n",
    "            db_f_back += np.sum(df_backward * self.sigmoid(h_backward[:, t]) * (1 - self.sigmoid(h_backward[:, t])), axis=1, keepdims=True)\n",
    "\n",
    "            dW_i += np.dot(di_forward * self.sigmoid(h_forward[:, t]) * (1 - self.sigmoid(h_forward[:, t])), np.concatenate((h_forward[:, t-1:t], X[:, t:t+1]), axis=0).T)\n",
    "            db_i += np.sum(di_forward * self.sigmoid(h_forward[:, t]) * (1 - self.sigmoid(h_forward[:, t])), axis=1, keepdims=True)\n",
    "\n",
    "            dW_i_back += np.dot(di_backward * self.sigmoid(h_backward[:, t]) * (1 - self.sigmoid(h_backward[:, t])), np.concatenate((h_backward[:, t+1:t+2], X[:, t:t+1]), axis=0).T)\n",
    "            db_i_back += np.sum(di_backward * self.sigmoid(h_backward[:, t]) * (1 - self.sigmoid(h_backward[:, t])), axis=1, keepdims=True)\n",
    "\n",
    "            dc_forward[:, t-1] = dc_forward[:, t] * self.sigmoid(h_forward[:, t])\n",
    "            dc_backward[:, t+1] = dc_backward[:, t] * self.sigmoid(h_backward[:, t])\n",
    "\n",
    "            dh_forward[:, t-1] = np.dot(self.W_f[:, :self.hidden_size].T, df_forward) + np.dot(self.W_i[:, :self.hidden_size].T, di_forward)\n",
    "            dh_backward[:, t+1] = np.dot(self.W_f_back[:, :self.hidden_size].T, df_backward) + np.dot(self.W_i_back[:, :self.hidden_size].T, di_backward)\n",
    "\n",
    "        self.W_y -= self.learning_rate * dW_y\n",
    "        self.b_y -= self.learning_rate * db_y\n",
    "\n",
    "        self.W_o -= self.learning_rate * dW_o\n",
    "        self.b_o -= self.learning_rate * db_o\n",
    "\n",
    "        self.W_o_back -= self.learning_rate * dW_o_back\n",
    "        self.b_o_back -= self.learning_rate * db_o_back\n",
    "\n",
    "        self.W_f -= self.learning_rate * dW_f\n",
    "        self.b_f -= self.learning_rate * db_f\n",
    "\n",
    "        self.W_f_back -= self.learning_rate * dW_f_back\n",
    "        self.b_f_back -= self.learning_rate * db_f_back\n",
    "\n",
    "        self.W_i -= self.learning_rate * dW_i\n",
    "        self.b_i -= self.learning_rate * db_i\n",
    "\n",
    "        self.W_i_back -= self.learning_rate * dW_i_back\n",
    "        self.b_i_back -= self.learning_rate * db_i_back\n",
    "\n",
    "    def train(self, X_train, Y_train, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            for X, Y_true in zip(X_train, Y_train):\n",
    "                # Forward pass\n",
    "                Y_pred, h_forward, c_forward, h_backward, c_backward = self.forward(X)\n",
    "\n",
    "                # Backpropagation through time (BPTT) and gradient descent\n",
    "                self.backward(X, Y_true, Y_pred, h_forward, c_forward, h_backward, c_backward)\n",
    "\n",
    "# Example usage:\n",
    "input_size = 10\n",
    "hidden_size = 20\n",
    "output_size = 5\n",
    "seq_length = 15\n",
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "\n",
    "# Generate random training data\n",
    "X_train = [np.random.randn(input_size, seq_length) for _ in range(100)]\n",
    "Y_train = [np.random.randn(output_size, seq_length) for _ in range(100)]\n",
    "\n",
    "# Create and train the BiLSTM model\n",
    "bilstm = SimpleBiLSTM(input_size, hidden_size, output_size, learning_rate)\n",
    "bilstm.train(X_train, Y_train, epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03f9a822",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'method' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6711/3949774592.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0mbilstm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleBiLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m \u001b[0mbilstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_6711/3949774592.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, Y, epochs)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_6711/3949774592.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, X, Y, y_pred)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m             \u001b[0mh_concat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh_backward\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m             \u001b[0mdW_y\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_concat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0mdb_y\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'method' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x) ** 2\n",
    "\n",
    "class SimpleBiLSTM:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.001):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Initialize weights and biases for forward LSTM\n",
    "        self.W_f = np.random.randn(hidden_size, hidden_size + input_size)\n",
    "        self.b_f = np.zeros((hidden_size, 1))\n",
    "\n",
    "        self.W_i = np.random.randn(hidden_size, hidden_size + input_size)\n",
    "        self.b_i = np.zeros((hidden_size, 1))\n",
    "\n",
    "        self.W_c = np.random.randn(hidden_size, hidden_size + input_size)\n",
    "        self.b_c = np.zeros((hidden_size, 1))\n",
    "\n",
    "        self.W_o = np.random.randn(hidden_size, hidden_size + input_size)\n",
    "        self.b_o = np.zeros((hidden_size, 1))\n",
    "\n",
    "        # Initialize weights and biases for backward LSTM\n",
    "        self.W_f_back = np.random.randn(hidden_size, hidden_size + input_size)\n",
    "        self.b_f_back = np.zeros((hidden_size, 1))\n",
    "\n",
    "        self.W_i_back = np.random.randn(hidden_size, hidden_size + input_size)\n",
    "        self.b_i_back = np.zeros((hidden_size, 1))\n",
    "\n",
    "        self.W_c_back = np.random.randn(hidden_size, hidden_size + input_size)\n",
    "        self.b_c_back = np.zeros((hidden_size, 1))\n",
    "\n",
    "        self.W_o_back = np.random.randn(hidden_size, hidden_size + input_size)\n",
    "        self.b_o_back = np.zeros((hidden_size, 1))\n",
    "\n",
    "        # Initialize output layer weights\n",
    "        self.W_y = np.random.randn(output_size, 2 * hidden_size)\n",
    "        self.b_y = np.zeros((output_size, 1))\n",
    "\n",
    "    def forward_step(self, x_t, h_prev, c_prev, W_f, W_i, W_c, W_o, b_f, b_i, b_c, b_o):\n",
    "        concat = np.vstack((h_prev, x_t))\n",
    "        \n",
    "        f_t = sigmoid(np.dot(W_f, concat) + b_f)\n",
    "        i_t = sigmoid(np.dot(W_i, concat) + b_i)\n",
    "        c_hat_t = np.tanh(np.dot(W_c, concat) + b_c)\n",
    "        c_t = f_t * c_prev + i_t * c_hat_t\n",
    "        o_t = sigmoid(np.dot(W_o, concat) + b_o)\n",
    "        h_t = o_t * np.tanh(c_t)\n",
    "        \n",
    "        return h_t, c_t\n",
    "\n",
    "    def forward(self, X):\n",
    "        T = X.shape[1]\n",
    "        \n",
    "        h_forward = np.zeros((self.hidden_size, T))\n",
    "        c_forward = np.zeros((self.hidden_size, T))\n",
    "        \n",
    "        h_backward = np.zeros((self.hidden_size, T))\n",
    "        c_backward = np.zeros((self.hidden_size, T))\n",
    "\n",
    "        h_prev_forward = np.zeros((self.hidden_size, 1))\n",
    "        c_prev_forward = np.zeros((self.hidden_size, 1))\n",
    "        \n",
    "        h_prev_backward = np.zeros((self.hidden_size, 1))\n",
    "        c_prev_backward = np.zeros((self.hidden_size, 1))\n",
    "\n",
    "        # Forward pass\n",
    "        for t in range(T):\n",
    "            h_t, c_t = self.forward_step(X[:, t].reshape(-1, 1), h_prev_forward, c_prev_forward,\n",
    "                                         self.W_f, self.W_i, self.W_c, self.W_o,\n",
    "                                         self.b_f, self.b_i, self.b_c, self.b_o)\n",
    "            h_forward[:, t] = h_t.flatten()\n",
    "            c_forward[:, t] = c_t.flatten()\n",
    "            h_prev_forward, c_prev_forward = h_t, c_t\n",
    "\n",
    "        # Backward pass\n",
    "        for t in reversed(range(T)):\n",
    "            h_t, c_t = self.forward_step(X[:, t].reshape(-1, 1), h_prev_backward, c_prev_backward,\n",
    "                                         self.W_f_back, self.W_i_back, self.W_c_back, self.W_o_back,\n",
    "                                         self.b_f_back, self.b_i_back, self.b_c_back, self.b_o_back)\n",
    "            h_backward[:, t] = h_t.flatten()\n",
    "            c_backward[:, t] = c_t.flatten()\n",
    "            h_prev_backward, c_prev_backward = h_t, c_t\n",
    "\n",
    "        # Concatenate forward and backward hidden states\n",
    "        h_concat = np.vstack((h_forward, h_backward))\n",
    "        \n",
    "        # Output layer\n",
    "        y = sigmoid(np.dot(self.W_y, h_concat) + self.b_y)\n",
    "        \n",
    "        return y\n",
    "\n",
    "    def backward(self, X, Y, y_pred):\n",
    "        T = X.shape[1]\n",
    "        \n",
    "        dW_y = np.zeros_like(self.W_y)\n",
    "        db_y = np.zeros_like(self.b_y)\n",
    "        \n",
    "        dW_f = np.zeros_like(self.W_f)\n",
    "        dW_i = np.zeros_like(self.W_i)\n",
    "        dW_c = np.zeros_like(self.W_c)\n",
    "        dW_o = np.zeros_like(self.W_o)\n",
    "        db_f = np.zeros_like(self.b_f)\n",
    "        db_i = np.zeros_like(self.b_i)\n",
    "        db_c = np.zeros_like(self.b_c)\n",
    "        db_o = np.zeros_like(self.b_o)\n",
    "        \n",
    "        dW_f_back = np.zeros_like(self.W_f_back)\n",
    "        dW_i_back = np.zeros_like(self.W_i_back)\n",
    "        dW_c_back = np.zeros_like(self.W_c_back)\n",
    "        dW_o_back = np.zeros_like(self.W_o_back)\n",
    "        db_f_back = np.zeros_like(self.b_f_back)\n",
    "        db_i_back = np.zeros_like(self.b_i_back)\n",
    "        db_c_back = np.zeros_like(self.b_c_back)\n",
    "        db_o_back = np.zeros_like(self.b_o_back)\n",
    "\n",
    "        dy = y_pred - Y\n",
    "\n",
    "        for t in range(T):\n",
    "            h_concat = np.vstack((self.forward[:, t], self.h_backward[:, t])).reshape(-1, 1)\n",
    "            dW_y += np.dot(dy[:, t].reshape(-1, 1), h_concat.T)\n",
    "            db_y += dy[:, t].reshape(-1, 1)\n",
    "\n",
    "        # Backpropagation through time (BPTT) for LSTM (Forward and Backward)\n",
    "        # (Update weights using gradients not shown for simplicity)\n",
    "        \n",
    "        # Update weights (example for W_f)\n",
    "        self.W_f -= self.learning_rate * dW_f\n",
    "        self.b_f -= self.learning_rate * db_f\n",
    "\n",
    "        self.W_i -= self.learning_rate * dW_i\n",
    "        self.b_i -= self.learning_rate * db_i\n",
    "\n",
    "        self.W_c -= self.learning_rate * dW_c\n",
    "        self.b_c -= self.learning_rate * db_c\n",
    "\n",
    "        self.W_o -= self.learning_rate * dW_o\n",
    "        self.b_o -= self.learning_rate * db_o\n",
    "\n",
    "        self.W_f_back -= self.learning_rate * dW_f_back\n",
    "        self.b_f_back -= self.learning_rate * db_f_back\n",
    "\n",
    "        self.W_i_back -= self.learning_rate * dW_i_back\n",
    "        self.b_i_back -= self.learning_rate * db_i_back\n",
    "\n",
    "        self.W_c_back -= self.learning_rate * dW_c_back\n",
    "        self.b_c_back -= self.learning_rate * db_c_back\n",
    "\n",
    "        self.W_o_back -= self.learning_rate * dW_o_back\n",
    "        self.b_o_back -= self.learning_rate * db_o_back\n",
    "\n",
    "        self.W_y -= self.learning_rate * dW_y\n",
    "        self.b_y -= self.learning_rate * db_y\n",
    "\n",
    "    def train(self, X, Y, epochs=100):\n",
    "        for epoch in range(epochs):\n",
    "            y_pred = self.forward(X)\n",
    "            self.backward(X, Y, y_pred)\n",
    "            if epoch % 10 == 0:\n",
    "                loss = np.mean((Y - y_pred) ** 2)\n",
    "                print(f'Epoch {epoch}, Loss: {loss}')\n",
    "\n",
    "# Example usage with random input and output\n",
    "input_size = 10\n",
    "hidden_size = 20\n",
    "output_size = 2\n",
    "\n",
    "X = np.random.randn(input_size, 5)  # Random input with 5 time steps\n",
    "Y = np.random.randint(0, 2, (output_size, 5))  # Random binary output\n",
    "\n",
    "bilstm = SimpleBiLSTM(input_size, hidden_size, output_size)\n",
    "bilstm.train(X, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb564025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.44897024083686593\n",
      "Epoch 10, Loss: 0.4447017142233314\n",
      "Epoch 20, Loss: 0.44046608249831304\n",
      "Epoch 30, Loss: 0.43626574772327753\n",
      "Epoch 40, Loss: 0.4321029901705102\n",
      "Epoch 50, Loss: 0.42797996053771137\n",
      "Epoch 60, Loss: 0.42389867320455143\n",
      "Epoch 70, Loss: 0.4198610005948521\n",
      "Epoch 80, Loss: 0.4158686686857023\n",
      "Epoch 90, Loss: 0.41192325368289273\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x) ** 2\n",
    "\n",
    "class SimpleBiLSTM:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.001):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Initialize weights and biases for forward LSTM\n",
    "        self.W_f = np.random.randn(hidden_size, hidden_size + input_size)\n",
    "        self.b_f = np.zeros((hidden_size, 1))\n",
    "\n",
    "        self.W_i = np.random.randn(hidden_size, hidden_size + input_size)\n",
    "        self.b_i = np.zeros((hidden_size, 1))\n",
    "\n",
    "        self.W_c = np.random.randn(hidden_size, hidden_size + input_size)\n",
    "        self.b_c = np.zeros((hidden_size, 1))\n",
    "\n",
    "        self.W_o = np.random.randn(hidden_size, hidden_size + input_size)\n",
    "        self.b_o = np.zeros((hidden_size, 1))\n",
    "\n",
    "        # Initialize weights and biases for backward LSTM\n",
    "        self.W_f_back = np.random.randn(hidden_size, hidden_size + input_size)\n",
    "        self.b_f_back = np.zeros((hidden_size, 1))\n",
    "\n",
    "        self.W_i_back = np.random.randn(hidden_size, hidden_size + input_size)\n",
    "        self.b_i_back = np.zeros((hidden_size, 1))\n",
    "\n",
    "        self.W_c_back = np.random.randn(hidden_size, hidden_size + input_size)\n",
    "        self.b_c_back = np.zeros((hidden_size, 1))\n",
    "\n",
    "        self.W_o_back = np.random.randn(hidden_size, hidden_size + input_size)\n",
    "        self.b_o_back = np.zeros((hidden_size, 1))\n",
    "\n",
    "        # Initialize output layer weights\n",
    "        self.W_y = np.random.randn(output_size, 2 * hidden_size)\n",
    "        self.b_y = np.zeros((output_size, 1))\n",
    "\n",
    "    def forward_step(self, x_t, h_prev, c_prev, W_f, W_i, W_c, W_o, b_f, b_i, b_c, b_o):\n",
    "        concat = np.vstack((h_prev, x_t))\n",
    "        \n",
    "        f_t = sigmoid(np.dot(W_f, concat) + b_f)\n",
    "        i_t = sigmoid(np.dot(W_i, concat) + b_i)\n",
    "        c_hat_t = np.tanh(np.dot(W_c, concat) + b_c)\n",
    "        c_t = f_t * c_prev + i_t * c_hat_t\n",
    "        o_t = sigmoid(np.dot(W_o, concat) + b_o)\n",
    "        h_t = o_t * np.tanh(c_t)\n",
    "        \n",
    "        return h_t, c_t\n",
    "\n",
    "    def forward(self, X):\n",
    "        T = X.shape[1]\n",
    "        \n",
    "        self.h_forward = np.zeros((self.hidden_size, T))\n",
    "        self.c_forward = np.zeros((self.hidden_size, T))\n",
    "        \n",
    "        self.h_backward = np.zeros((self.hidden_size, T))\n",
    "        self.c_backward = np.zeros((self.hidden_size, T))\n",
    "\n",
    "        h_prev_forward = np.zeros((self.hidden_size, 1))\n",
    "        c_prev_forward = np.zeros((self.hidden_size, 1))\n",
    "        \n",
    "        h_prev_backward = np.zeros((self.hidden_size, 1))\n",
    "        c_prev_backward = np.zeros((self.hidden_size, 1))\n",
    "\n",
    "        # Forward pass\n",
    "        for t in range(T):\n",
    "            h_t, c_t = self.forward_step(X[:, t].reshape(-1, 1), h_prev_forward, c_prev_forward,\n",
    "                                         self.W_f, self.W_i, self.W_c, self.W_o,\n",
    "                                         self.b_f, self.b_i, self.b_c, self.b_o)\n",
    "            self.h_forward[:, t] = h_t.flatten()\n",
    "            self.c_forward[:, t] = c_t.flatten()\n",
    "            h_prev_forward, c_prev_forward = h_t, c_t\n",
    "\n",
    "        # Backward pass\n",
    "        for t in reversed(range(T)):\n",
    "            h_t, c_t = self.forward_step(X[:, t].reshape(-1, 1), h_prev_backward, c_prev_backward,\n",
    "                                         self.W_f_back, self.W_i_back, self.W_c_back, self.W_o_back,\n",
    "                                         self.b_f_back, self.b_i_back, self.b_c_back, self.b_o_back)\n",
    "            self.h_backward[:, t] = h_t.flatten()\n",
    "            self.c_backward[:, t] = c_t.flatten()\n",
    "            h_prev_backward, c_prev_backward = h_t, c_t\n",
    "\n",
    "        # Concatenate forward and backward hidden states\n",
    "        h_concat = np.vstack((self.h_forward, self.h_backward))\n",
    "        \n",
    "        # Output layer\n",
    "        y = sigmoid(np.dot(self.W_y, h_concat) + self.b_y)\n",
    "        \n",
    "        return y\n",
    "\n",
    "    def backward(self, X, Y, y_pred):\n",
    "        T = X.shape[1]\n",
    "        \n",
    "        dW_y = np.zeros_like(self.W_y)\n",
    "        db_y = np.zeros_like(self.b_y)\n",
    "        \n",
    "        dW_f = np.zeros_like(self.W_f)\n",
    "        dW_i = np.zeros_like(self.W_i)\n",
    "        dW_c = np.zeros_like(self.W_c)\n",
    "        dW_o = np.zeros_like(self.W_o)\n",
    "        db_f = np.zeros_like(self.b_f)\n",
    "        db_i = np.zeros_like(self.b_i)\n",
    "        db_c = np.zeros_like(self.b_c)\n",
    "        db_o = np.zeros_like(self.b_o)\n",
    "        \n",
    "        dW_f_back = np.zeros_like(self.W_f_back)\n",
    "        dW_i_back = np.zeros_like(self.W_i_back)\n",
    "        dW_c_back = np.zeros_like(self.W_c_back)\n",
    "        dW_o_back = np.zeros_like(self.W_o_back)\n",
    "        db_f_back = np.zeros_like(self.b_f_back)\n",
    "        db_i_back = np.zeros_like(self.b_i_back)\n",
    "        db_c_back = np.zeros_like(self.b_c_back)\n",
    "        db_o_back = np.zeros_like(self.b_o_back)\n",
    "\n",
    "        dy = y_pred - Y\n",
    "\n",
    "        for t in range(T):\n",
    "            h_concat = np.vstack((self.h_forward[:, t], self.h_backward[:, t])).reshape(-1, 1)\n",
    "            dW_y += np.dot(dy[:, t].reshape(-1, 1), h_concat.T)\n",
    "            db_y += dy[:, t].reshape(-1, 1)\n",
    "\n",
    "        # Backpropagation through time (BPTT) for LSTM (Forward and Backward)\n",
    "        # (Update weights using gradients not shown for simplicity)\n",
    "        \n",
    "        # Update weights (example for W_f)\n",
    "        self.W_f -= self.learning_rate * dW_f\n",
    "        self.b_f -= self.learning_rate * db_f\n",
    "\n",
    "        self.W_i -= self.learning_rate * dW_i\n",
    "        self.b_i -= self.learning_rate * db_i\n",
    "\n",
    "        self.W_c -= self.learning_rate * dW_c\n",
    "        self.b_c -= self.learning_rate * db_c\n",
    "\n",
    "        self.W_o -= self.learning_rate * dW_o\n",
    "        self.b_o -= self.learning_rate * db_o\n",
    "\n",
    "        self.W_f_back -= self.learning_rate * dW_f_back\n",
    "        self.b_f_back -= self.learning_rate * db_f_back\n",
    "\n",
    "        self.W_i_back -= self.learning_rate * dW_i_back\n",
    "        self.b_i_back -= self.learning_rate * db_i_back\n",
    "\n",
    "        self.W_c_back -= self.learning_rate * dW_c_back\n",
    "        self.b_c_back -= self.learning_rate * db_c_back\n",
    "\n",
    "        self.W_o_back -= self.learning_rate * dW_o_back\n",
    "        self.b_o_back -= self.learning_rate * db_o_back\n",
    "\n",
    "        self.W_y -= self.learning_rate * dW_y\n",
    "        self.b_y -= self.learning_rate * db_y\n",
    "\n",
    "    def train(self, X, Y, epochs=100):\n",
    "        for epoch in range(epochs):\n",
    "            y_pred = self.forward(X)\n",
    "            self.backward(X, Y, y_pred)\n",
    "            if epoch % 10 == 0:\n",
    "                loss = np.mean((Y - y_pred) ** 2)\n",
    "                print(f'Epoch {epoch}, Loss: {loss}')\n",
    "\n",
    "# Example usage with random input and output\n",
    "input_size = 10\n",
    "hidden_size = 20\n",
    "output_size = 2\n",
    "\n",
    "X = np.random.randn(input_size, 5)  # Random input with 5 time steps\n",
    "Y = np.random.randint(0, 2, (output_size, 5))  # Random binary output\n",
    "\n",
    "bilstm = SimpleBiLSTM(input_size, hidden_size, output_size)\n",
    "bilstm.train(X, Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaf7492",
   "metadata": {},
   "source": [
    "### Connectionist Temporal Classification (CTC)\n",
    "\n",
    "CTC addresses the challenges of labeling unsegmented sequence data directly with RNNs. Here's a brief overview:\n",
    "\n",
    "Given a set \\( S \\) of training examples where each example consists of a pair of sequences \\( (x, z) \\) with \\( x = (x_1, \\ldots, x_T) \\) and \\( z = (z_1, \\ldots, z_U) \\), where \\( U \\leq T \\), the sequences \\( x \\) and \\( z \\) are not aligned initially due to their differing lengths.\n",
    "\n",
    "#### CTC Method:\n",
    "\n",
    "1. **Softmax Layer**: A softmax layer is used to define an output distribution $$ P(k|t) $$ at each time step \\( t \\) along the input sequence \\( x \\). This distribution covers \\( K \\) phonemes plus an extra blank symbol \\( \\emptyset \\), making the softmax layer size \\( K + 1 \\).\n",
    "\n",
    "2. **Bidirectional RNNs**: Typically, RNNs trained with CTC are bidirectional to ensure each $$ P(k|t) $$ depends on the entire input sequence \\( x \\).\n",
    "\n",
    "3. **Output Vector**: The unnormalized output vector $$ y_t $$ at time step \\( t \\) is defined as:\n",
    "   $$ y_t = W_{\\rightarrow N} h_{\\rightarrow N}^t + W_{\\leftarrow N} h_{\\leftarrow N}^t + b_y $$\n",
    "   where:\n",
    "   - \\( W_{\\rightarrow N} \\) and \\( W_{\\leftarrow N} \\) are weight matrices from forward and backward hidden states to the output gate.\n",
    "   - \\( h_{\\rightarrow N}^t \\) and \\( h_{\\leftarrow N}^t \\) are forward and backward hidden states at time step \\( t \\).\n",
    "   - \\( b_y \\) is the bias.\n",
    "\n",
    "4. **Output Probability $$ P(k|t) $$**:\n",
    "   $$ P(k|t) = \\frac{\\exp(y_t[k])}{\\sum_{k'=1}^{K+1} \\exp(y_t[k'])} $$\n",
    "   where \\( y_t[k] \\) is the \\( k \\)-th element of \\( y_t \\).\n",
    "\n",
    "### S-LSTM Network\n",
    "\n",
    "The S-LSTM network extends LSTM to handle longer-term dependencies and more complex input structures:\n",
    "\n",
    "#### S-LSTM Memory Block:\n",
    "\n",
    "Each S-LSTM memory block contains:\n",
    "- One input gate $$ i_t $$\n",
    "- One output gate $$ o_t $$\n",
    "- Multiple forget gates depending on the number of children of a node\n",
    "\n",
    "#### Forward Computation:\n",
    "\n",
    "1. **Input Gate $$ i_t $$**:\n",
    "   $$ i_t = \\sigma(W_{Lh}^i h_{Lt-1} + W_{Rh}^i h_{Rt-1} + W_{Lc}^i c_{Lt-1} + W_{Rc}^i c_{Rt-1} + b_i) $$\n",
    "   where:\n",
    "   - \\( \\sigma \\) is the logistic sigmoid function.\n",
    "   - \\( h_{Lt-1}, h_{Rt-1} \\) are hidden vectors of the left and right children.\n",
    "   - \\( c_{Lt-1}, c_{Rt-1} \\) are cell vectors of the left and right children.\n",
    "   - \\( W \\) and \\( b \\) are weight matrices and biases.\n",
    "\n",
    "These networks enhance the capabilities of traditional RNNs and LSTMs by addressing alignment issues and handling more complex input structures efficiently.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe1e31ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 20)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class CTC:\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        self.input_size = input_size\n",
    "        self.num_classes = num_classes\n",
    "        self.softmax = np.zeros((num_classes + 1, input_size))  # Including blank symbol\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Assuming x is the input sequence tensor of shape (input_size, sequence_length)\n",
    "        sequence_length = x.shape[1]\n",
    "        outputs = np.zeros((self.num_classes + 1, sequence_length))\n",
    "\n",
    "        for t in range(sequence_length):\n",
    "            # Example softmax computation (replace with actual logits)\n",
    "            logits_t = np.random.rand(self.num_classes + 1)\n",
    "            outputs[:, t] = np.exp(logits_t) / np.sum(np.exp(logits_t))\n",
    "\n",
    "        return outputs\n",
    "\n",
    "# Example usage:\n",
    "input_size = 10\n",
    "num_classes = 5\n",
    "ctc = CTC(input_size, num_classes)\n",
    "\n",
    "# Example input sequence (10-dimensional input, length 20)\n",
    "input_sequence = np.random.rand(input_size, 20)\n",
    "\n",
    "# Forward pass\n",
    "output_sequence = ctc.forward(input_sequence)\n",
    "print(output_sequence.shape)  # Example output shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "445101fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (10,5) and (10,1) not aligned: 5 (dim 1) != 10 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6711/3265978671.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# Forward pass through S-LSTM cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mi_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms_lstm_cell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_l_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_r_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_l_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_r_prev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Example output shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_6711/3265978671.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, h_l_prev, h_r_prev, c_l_prev, c_r_prev)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W_rh'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_r_prev\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W_lc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_l_prev\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W_rc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_r_prev\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbiases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b_i'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         )\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (10,5) and (10,1) not aligned: 5 (dim 1) != 10 (dim 0)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class S_LSTM_Cell:\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.weights = {\n",
    "            'W_lh': np.random.randn(hidden_size, input_size),\n",
    "            'W_rh': np.random.randn(hidden_size, input_size),\n",
    "            'W_lc': np.random.randn(hidden_size, input_size),\n",
    "            'W_rc': np.random.randn(hidden_size, input_size)\n",
    "        }\n",
    "        self.biases = {\n",
    "            'b_i': np.zeros((hidden_size, 1))\n",
    "        }\n",
    "\n",
    "    def forward(self, h_l_prev, h_r_prev, c_l_prev, c_r_prev):\n",
    "        i_t = sigmoid(\n",
    "            np.dot(self.weights['W_lh'], h_l_prev) +\n",
    "            np.dot(self.weights['W_rh'], h_r_prev) +\n",
    "            np.dot(self.weights['W_lc'], c_l_prev) +\n",
    "            np.dot(self.weights['W_rc'], c_r_prev) +\n",
    "            self.biases['b_i']\n",
    "        )\n",
    "        return i_t\n",
    "\n",
    "# Example usage:\n",
    "input_size = 5  # Example input size\n",
    "hidden_size = 10  # Example hidden size\n",
    "s_lstm_cell = S_LSTM_Cell(input_size, hidden_size)\n",
    "\n",
    "# Example inputs (previous hidden and cell states)\n",
    "h_l_prev = np.random.randn(hidden_size, 1)\n",
    "h_r_prev = np.random.randn(hidden_size, 1)\n",
    "c_l_prev = np.random.randn(hidden_size, 1)\n",
    "c_r_prev = np.random.randn(hidden_size, 1)\n",
    "\n",
    "# Forward pass through S-LSTM cell\n",
    "i_t = s_lstm_cell.forward(h_l_prev, h_r_prev, c_l_prev, c_r_prev)\n",
    "print(i_t.shape)  # Example output shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d88433f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
