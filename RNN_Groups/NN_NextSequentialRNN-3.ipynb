{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d54bfc1",
   "metadata": {},
   "source": [
    "The equations and descriptions you've provided outline the dynamics and architecture of recurrent neural networks (RNNs) with a forget gate. Let's break down the key components and equations mentioned:\n",
    "\n",
    "### Equations for RNNs with Forget Gate:\n",
    "\n",
    "1. **Input Gate** ($i_t$):\n",
    "   $$i_t = \\sigma(W_{ix} x_t + W_{ih}h_{t-1} + W_{ic}c_{t-1} + b_i)$$\n",
    "\n",
    "2. **Forget Gate** ($f_t$):\n",
    "   $$f_t = \\sigma(W_{fx}x_t + W_{fh}h_{t-1} + W_{fc}c_{t-1} + b_f)$$\n",
    "\n",
    "3. **Cell State Update** ($c_t$):\n",
    "   $$c_t = f_t c_{t-1} + i_t \\tanh(W_{cx}x_t + W_{ch}h_{t-1} + b_c)$$\n",
    "\n",
    "4. **Output Gate** ($y_t$):\n",
    "   $$y_t = \\sigma(W_{ox}x_t + W_{oh}h_{t-1} + W_{oc}c_t + b_o)$$\n",
    "\n",
    "5. **Hidden State Update** ($h_t$):\n",
    "   $$h_t = y_t \\tanh(c_t)$$\n",
    "\n",
    "### Network Parameters:\n",
    "- $x_t$: Input vector to the network unit.\n",
    "- $f_t$, $i_t$, $y_t$, $h_t$, $c_t$: Gate and state vectors.\n",
    "- $W_{ix}$, $W_{fx}$, $W_{ox}$, $W_{cx}$: Weight matrices from input vector to respective gates.\n",
    "- $W_{ih}$, $W_{fh}$, $W_{oh}$, $W_{ch}$: Weight matrices from input gate to hidden gate.\n",
    "- $W_{ic}$, $W_{fc}$, $W_{oc}$: Weight matrices from cell to respective gates.\n",
    "- $b_i$, $b_f$, $b_o$, $b_c$: Bias vectors.\n",
    "- $\\sigma(\\cdot)$: Logistic sigmoid function.\n",
    "- $\\tanh(\\cdot)$: Hyperbolic tangent function.\n",
    "\n",
    "### Recurrent Computation:\n",
    "\n",
    "The recurrent computation is described by a deterministic transition from the previous hidden state to the current hidden state:\n",
    "$$h_{t}^{l} = f(T_{n,n} h_{t}^{l-1} + T_{n,n} h_{t-1}^{l})$$\n",
    "where $f$ is typically either the sigmoid or hyperbolic tangent activation function.\n",
    "\n",
    "### Jordan Network vs. Elman Network:\n",
    "\n",
    "1. **Jordan Network**: In a Jordan network, the output from the previous time step is used as an input along with the current input. The hidden state is computed using both the input and the previous output.\n",
    "\n",
    "2. **Elman Network**: An Elman network, introduced by Jeff Elman, has a simpler structure where the hidden state at the previous time step is directly fed back into the network. It is widely used for capturing sequential dependencies.\n",
    "\n",
    "These networks are fundamental in sequence modeling tasks and have various applications in natural language processing, time series prediction, and more. Choosing between Jordan and Elman networks depends on the specific requirements and characteristics of the problem being addressed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7680a2cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() takes 2 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10021/1404861337.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Perform forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mh_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn_forget_gate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_prev_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_prev_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Display the output hidden state and cell state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() takes 2 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "# Define the sample input\n",
    "# Assuming input_size = 3, hidden_size = 4, output_size = 2\n",
    "# Randomly initialize input, previous hidden state, and previous cell state\n",
    "x_sample = np.random.randn(3, 5)  # Shape: (input_size, time_steps)\n",
    "h_prev_sample = np.random.randn(4, 1)  # Shape: (hidden_size, 1)\n",
    "c_prev_sample = np.random.randn(4, 1)  # Shape: (hidden_size, 1)\n",
    "\n",
    "# Create an instance of RNNForgetGate\n",
    "rnn_forget_gate = RNNForgetGate(input_size=3, hidden_size=4, output_size=2)\n",
    "\n",
    "# Perform forward pass\n",
    "h_t, c_t = rnn_forget_gate.forward(x_sample, h_prev_sample, c_prev_sample)\n",
    "\n",
    "# Display the output hidden state and cell state\n",
    "print(\"Output hidden state (h_t):\\n\", h_t)\n",
    "print(\"\\nOutput cell state (c_t):\\n\", c_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1a1407d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dW_ix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10021/581857142.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;31m# Print the gradients (for demonstration purposes)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Gradient for W_ix:\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW_ix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Gradient for W_fx:\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW_fx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Gradient for W_ox:\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW_ox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dW_ix' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Activation functions and their derivatives\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x) ** 2\n",
    "\n",
    "class RNNForgetGate:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.W_ix = np.random.randn(hidden_size, input_size)\n",
    "        self.W_ih = np.random.randn(hidden_size, hidden_size)\n",
    "        self.W_ic = np.random.randn(hidden_size, hidden_size)\n",
    "        self.b_i = np.zeros((hidden_size, 1))\n",
    "        \n",
    "        self.W_fx = np.random.randn(hidden_size, input_size)\n",
    "        self.W_fh = np.random.randn(hidden_size, hidden_size)\n",
    "        self.W_fc = np.random.randn(hidden_size, hidden_size)\n",
    "        self.b_f = np.zeros((hidden_size, 1))\n",
    "        \n",
    "        self.W_cx = np.random.randn(hidden_size, input_size)\n",
    "        self.W_ch = np.random.randn(hidden_size, hidden_size)\n",
    "        self.b_c = np.zeros((hidden_size, 1))\n",
    "        \n",
    "        self.W_ox = np.random.randn(hidden_size, input_size)\n",
    "        self.W_oh = np.random.randn(hidden_size, hidden_size)\n",
    "        self.W_oc = np.random.randn(hidden_size, hidden_size)\n",
    "        self.b_o = np.zeros((hidden_size, 1))\n",
    "        \n",
    "        self.W_y = np.random.randn(output_size, hidden_size)\n",
    "        self.b_y = np.zeros((output_size, 1))\n",
    "        \n",
    "        self.h = np.zeros((hidden_size, 1))\n",
    "        self.c = np.zeros((hidden_size, 1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        T = x.shape[1]\n",
    "        h_seq = np.zeros((self.hidden_size, T))\n",
    "        c_seq = np.zeros((self.hidden_size, T))\n",
    "        y_seq = np.zeros((self.output_size, T))\n",
    "        \n",
    "        i_seq = np.zeros((self.hidden_size, T))\n",
    "        f_seq = np.zeros((self.hidden_size, T))\n",
    "        o_seq = np.zeros((self.hidden_size, T))\n",
    "        \n",
    "        for t in range(T):\n",
    "            x_t = x[:, t].reshape(-1, 1)\n",
    "            i_t = sigmoid(np.dot(self.W_ix, x_t) + np.dot(self.W_ih, self.h) + np.dot(self.W_ic, self.c) + self.b_i)\n",
    "            f_t = sigmoid(np.dot(self.W_fx, x_t) + np.dot(self.W_fh, self.h) + np.dot(self.W_fc, self.c) + self.b_f)\n",
    "            c_t = f_t * self.c + i_t * np.tanh(np.dot(self.W_cx, x_t) + np.dot(self.W_ch, self.h) + self.b_c)\n",
    "            o_t = sigmoid(np.dot(self.W_ox, x_t) + np.dot(self.W_oh, self.h) + np.dot(self.W_oc, c_t) + self.b_o)\n",
    "            h_t = o_t * np.tanh(c_t)\n",
    "            y_t = sigmoid(np.dot(self.W_y, h_t) + self.b_y)\n",
    "            \n",
    "            self.h = h_t\n",
    "            self.c = c_t\n",
    "            \n",
    "            h_seq[:, t] = h_t.ravel()\n",
    "            c_seq[:, t] = c_t.ravel()\n",
    "            y_seq[:, t] = y_t.ravel()\n",
    "            \n",
    "            i_seq[:, t] = i_t.ravel()\n",
    "            f_seq[:, t] = f_t.ravel()\n",
    "            o_seq[:, t] = o_t.ravel()\n",
    "        \n",
    "        return h_seq, c_seq, y_seq, i_seq, f_seq, o_seq\n",
    "    \n",
    "    def backward(self, x, target, h_seq, c_seq, y_seq, i_seq, f_seq, o_seq, lr=0.01):\n",
    "        T = x.shape[1]\n",
    "        dW_ix = np.zeros_like(self.W_ix)\n",
    "        dW_ih = np.zeros_like(self.W_ih)\n",
    "        dW_ic = np.zeros_like(self.W_ic)\n",
    "        db_i = np.zeros_like(self.b_i)\n",
    "        \n",
    "        dW_fx = np.zeros_like(self.W_fx)\n",
    "        dW_fh = np.zeros_like(self.W_fh)\n",
    "        dW_fc = np.zeros_like(self.W_fc)\n",
    "        db_f = np.zeros_like(self.b_f)\n",
    "        \n",
    "        dW_cx = np.zeros_like(self.W_cx)\n",
    "        dW_ch = np.zeros_like(self.W_ch)\n",
    "        db_c = np.zeros_like(self.b_c)\n",
    "        \n",
    "        dW_ox = np.zeros_like(self.W_ox)\n",
    "        dW_oh = np.zeros_like(self.W_oh)\n",
    "        dW_oc = np.zeros_like(self.W_oc)\n",
    "        db_o = np.zeros_like(self.b_o)\n",
    "        \n",
    "        dW_y = np.zeros_like(self.W_y)\n",
    "        db_y = np.zeros_like(self.b_y)\n",
    "        \n",
    "        dh_next = np.zeros((self.hidden_size, 1))\n",
    "        dc_next = np.zeros((self.hidden_size, 1))\n",
    "        \n",
    "        for t in reversed(range(T)):\n",
    "            x_t = x[:, t].reshape(-1, 1)\n",
    "            y_t = target[:, t].reshape(-1, 1)\n",
    "            h_t = h_seq[:, t].reshape(-1, 1)\n",
    "            c_t = c_seq[:, t].reshape(-1, 1)\n",
    "            y_pred = y_seq[:, t].reshape(-1, 1)\n",
    "            i_t = i_seq[:, t].reshape(-1, 1)\n",
    "            f_t = f_seq[:, t].reshape(-1, 1)\n",
    "            o_t = o_seq[:, t].reshape(-1, 1)\n",
    "            \n",
    "            dy = y_pred - y_t\n",
    "            dW_y += np.dot(dy, h_t.T)\n",
    "            db_y += dy\n",
    "            \n",
    "            do = dy * sigmoid_derivative(y_pred) * np.tanh(c_t)\n",
    "            dW_ox += np.dot(do, x_t.T)\n",
    "            dW_oh += np.dot(do, self.h.T)\n",
    "            dW_oc += np.dot(do, c_t.T)\n",
    "            db_o += do\n",
    "            \n",
    "            dh = np.dot(self.W_oh.T, do) + dh_next\n",
    "            dc = np.dot(self.W_oc.T, do) * tanh_derivative(c_t) + dc_next\n",
    "            di = dc * np.tanh(c_t) * sigmoid_derivative(np.dot(self.W_ix, x_t) + np.dot(self.W_ih, self.h) + np.dot(self.W_ic, c_t) + self.b_i)\n",
    "            dW_ix += np.dot(di, x_t.T)\n",
    "            dW_ih += np.dot(di, self.h.T)\n",
    "            dW_ic += np.dot(di, c_t.T)\n",
    "            db_i += di\n",
    "            \n",
    "            df = dc * self.c * sigmoid_derivative(np.dot(self.W_fx, x_t) + np.dot(self.W_fh, self.h) + np.dot(self.W_fc, c_t) + self.b_f)\n",
    "            dW_fx += np.dot(df, x_t.T)\n",
    "            dW_fh += np.dot(df, self.h.T)\n",
    "            dW_fc += np.dot(df, c_t.T)\n",
    "            db_f += df\n",
    "            \n",
    "            dc_next = f_t * dc\n",
    "            dh_next = np.dot(self.W_ih.T, di) + np.dot(self.W_fh.T, df)\n",
    "        \n",
    "        self.W_ix -= lr * dW_ix\n",
    "        self.W_ih -= lr * dW_ih\n",
    "        self.W_ic -= lr * dW_ic\n",
    "        self.b_i -= lr * db_i\n",
    "        \n",
    "        self.W_fx -= lr * dW_fx\n",
    "        self.W_fh -= lr * dW_fh\n",
    "        self.W_fc -= lr * dW_fc\n",
    "        self.b_f -= lr * db_f\n",
    "        \n",
    "        self.W_cx -= lr * dW_cx\n",
    "        self.W_ch -= lr * dW_ch\n",
    "        self.b_c -= lr * db_c\n",
    "        \n",
    "        self.W_ox -= lr * dW_ox\n",
    "        self.W_oh -= lr * dW_oh\n",
    "        self.W_oc -= lr * dW_oc\n",
    "        self.b_o -= lr * db_o\n",
    "        \n",
    "        self.W_y -= lr * dW_y\n",
    "        self.b_y -= lr * db_y\n",
    "\n",
    "# Example usage\n",
    "input_size = 10\n",
    "hidden_size = 10\n",
    "output_size = 10\n",
    "sequence_length = 12\n",
    "\n",
    "# Create an RNN instance\n",
    "rnn = RNNForgetGate(input_size, hidden_size, output_size)\n",
    "\n",
    "# Generate random input matrix\n",
    "x = np.random.randn(input_size, sequence_length)\n",
    "\n",
    "# Forward pass\n",
    "h_seq, c_seq, y_seq, i_seq, f_seq, o_seq = rnn.forward(x)\n",
    "\n",
    "# Assuming target is the target output for simplicity in this example\n",
    "target = np.random.randn(output_size, sequence_length)\n",
    "\n",
    "# Backward pass\n",
    "rnn.backward(x, target, h_seq, c_seq, y_seq, i_seq, f_seq, o_seq, lr=0.01)\n",
    "# Perform backward pass and get gradients\n",
    "#dW_ix, dW_fx, dW_ox, dW_cx, dW_ih, dW_fh, dW_oh, dW_ch, db_i, db_f, db_o, db_c = rnn.backward(X, h, c, dy)\n",
    "\n",
    "# Print the gradients (for demonstration purposes)\n",
    "print(\"Gradient for W_ix:\\n\", dW_ix)\n",
    "print(\"Gradient for W_fx:\\n\", dW_fx)\n",
    "print(\"Gradient for W_ox:\\n\", dW_ox)\n",
    "print(\"Gradient for W_cx:\\n\", dW_cx)\n",
    "print(\"Gradient for W_ih:\\n\", dW_ih)\n",
    "print(\"Gradient for W_fh:\\n\", dW_fh)\n",
    "print(\"Gradient for W_oh:\\n\", dW_oh)\n",
    "print(\"Gradient for W_ch:\\n\", dW_ch)\n",
    "print(\"Gradient for b_i:\\n\", db_i)\n",
    "print(\"Gradient for b_f:\\n\", db_f)\n",
    "print(\"Gradient for b_o:\\n\", db_o)\n",
    "print(\"Gradient for b_c:\\n\", db_c)\n",
    "\n",
    "# Print the output after forward pass\n",
    "print(\"Output after forward pass:\")\n",
    "print(y_seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c843a4bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (5,10) and (5,1) not aligned: 10 (dim 1) != 5 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10021/3282351662.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;31m# Perform backward pass and get gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m \u001b[0mdW_ix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW_fx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW_ox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW_cx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW_ih\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW_fh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW_oh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW_ch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb_o\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn_forget_gate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;31m# Print the gradients (for demonstration purposes)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_10021/3282351662.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, X, h, c, dy)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;31m#do = dy[:, t] * np.tanh(c[:, t])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mdW_ox\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msigmoid_derivative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (5,10) and (5,1) not aligned: 10 (dim 1) != 5 (dim 0)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the RNN class with forget gate\n",
    "class RNNForgetGate:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # Initialize parameters\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.W_ix = np.random.randn(hidden_size, input_size)\n",
    "        self.W_fx = np.random.randn(hidden_size, input_size)\n",
    "        self.W_ox = np.random.randn(hidden_size, input_size)\n",
    "        self.W_cx = np.random.randn(hidden_size, input_size)\n",
    "        self.W_ih = np.random.randn(hidden_size, hidden_size)\n",
    "        self.W_fh = np.random.randn(hidden_size, hidden_size)\n",
    "        self.W_oh = np.random.randn(hidden_size, hidden_size)\n",
    "        self.W_ch = np.random.randn(hidden_size, hidden_size)\n",
    "        self.b_i = np.zeros((hidden_size, 1))\n",
    "        self.b_f = np.zeros((hidden_size, 1))\n",
    "        self.b_o = np.zeros((hidden_size, 1))\n",
    "        self.b_c = np.zeros((hidden_size, 1))\n",
    "\n",
    "    def backward(self, X, h, c, dy):\n",
    "        # Initialize gradients\n",
    "        dW_ix, dW_fx, dW_ox, dW_cx = np.zeros_like(self.W_ix), np.zeros_like(self.W_fx), np.zeros_like(self.W_ox), np.zeros_like(self.W_cx)\n",
    "        dW_ih, dW_fh, dW_oh, dW_ch = np.zeros_like(self.W_ih), np.zeros_like(self.W_fh), np.zeros_like(self.W_oh), np.zeros_like(self.W_ch)\n",
    "        db_i, db_f, db_o, db_c = np.zeros_like(self.b_i), np.zeros_like(self.b_f), np.zeros_like(self.b_o), np.zeros_like(self.b_c)\n",
    "        dh_next = np.zeros_like(h[:, 0])\n",
    "        dc_next = np.zeros_like(c[:, 0])\n",
    "\n",
    "        # Loop backward through time steps\n",
    "        for t in reversed(range(len(X))):\n",
    "            # Compute total gradient\n",
    "            dh = h[:, t] + dh_next\n",
    "\n",
    "            # Compute gradient o# Compute gradient of output gate\n",
    "            do = np.dot(dy[:, t].reshape(-1, 1), np.tanh(c[:, t]).reshape(1, -1))\n",
    "\n",
    "            \n",
    "            #do = dy[:, t] * np.tanh(c[:, t])\n",
    "            dW_ox += np.dot(do * sigmoid_derivative(h[:, t]), X[:, t].reshape(-1, 1))\n",
    "\n",
    "\n",
    "            #dW_ox += np.dot(do * sigmoid_derivative(h[:, t]), X[:, t].T)\n",
    "            dW_oh += np.dot(do * sigmoid_derivative(h[:, t]), h[:, t-1].T)\n",
    "            db_o += np.sum(do * sigmoid_derivative(h[:, t]), axis=1, keepdims=True)\n",
    "\n",
    "            # Compute gradient of cell state\n",
    "            dc = dh * sigmoid(f[:, t])\n",
    "            dc += dc_next\n",
    "            dc_prev = dc * f[:, t]\n",
    "            dW_cx += np.dot(dc_prev * sigmoid_derivative(h[:, t]), X[:, t].T)\n",
    "            dW_ch += np.dot(dc_prev * sigmoid_derivative(h[:, t]), h[:, t-1].T)\n",
    "            db_c += np.sum(dc_prev * sigmoid_derivative(h[:, t]), axis=1, keepdims=True)\n",
    "\n",
    "            # Compute gradient of input gate\n",
    "            di = dc * g[:, t]\n",
    "            dW_ix += np.dot(di * sigmoid_derivative(h[:, t]), X[:, t].T)\n",
    "            dW_ih += np.dot(di * sigmoid_derivative(h[:, t]), h[:, t-1].T)\n",
    "            db_i += np.sum(di * sigmoid_derivative(h[:, t]), axis=1, keepdims=True)\n",
    "\n",
    "            # Compute gradient of forget gate\n",
    "            df = dc * c[:, t-1]\n",
    "            dW_fx += np.dot(df * sigmoid_derivative(h[:, t]), X[:, t].T)\n",
    "            dW_fh += np.dot(df * sigmoid_derivative(h[:, t]), h[:, t-1].T)\n",
    "            db_f += np.sum(df * sigmoid_derivative(h[:, t]), axis=1, keepdims=True)\n",
    "\n",
    "            # Compute gradient for next hidden state and cell state\n",
    "            dh_next = np.dot(self.W_ih.T, di) + np.dot(self.W_fh.T, df) + np.dot(self.W_oh.T, do) + np.dot(self.W_ch.T, dc)\n",
    "            dc_next = dc * f[:, t]\n",
    "\n",
    "        # Return gradients\n",
    "        return dW_ix, dW_fx, dW_ox, dW_cx, dW_ih, dW_fh, dW_oh, dW_ch, db_i, db_f, db_o, db_c\n",
    "\n",
    "# Sigmoid activation function and its derivative\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Generate sample input data\n",
    "X = np.random.randn(5, 10)  # 5 input features, 10 time steps\n",
    "h = np.random.randn(10, 10)  # Hidden state, 10 hidden units, 10 time steps\n",
    "c = np.random.randn(10, 10)  # Cell state, 10 hidden units, 10 time steps\n",
    "dy = np.random.randn(5, 10)  # Gradient of loss with respect to output, 5 output units, 10 time steps\n",
    "\n",
    "# Initialize RNN with forget gate\n",
    "rnn_forget_gate = RNNForgetGate(input_size=5, hidden_size=10, output_size=5)\n",
    "\n",
    "# Perform backward pass and get gradients\n",
    "dW_ix, dW_fx, dW_ox, dW_cx, dW_ih, dW_fh, dW_oh, dW_ch, db_i, db_f, db_o, db_c = rnn_forget_gate.backward(X, h, c, dy)\n",
    "\n",
    "# Print the gradients (for demonstration purposes)\n",
    "print(\"Gradient for W_ix:\\n\", dW_ix)\n",
    "print(\"Gradient for W_fx:\\n\", dW_fx)\n",
    "print(\"Gradient for W_ox:\\n\", dW_ox)\n",
    "print(\"Gradient for W_cx:\\n\", dW_cx)\n",
    "print(\"Gradient for W_ih:\\n\", dW_ih)\n",
    "print(\"Gradient for W_fh:\\n\", dW_fh)\n",
    "print(\"Gradient for W_oh:\\n\", dW_oh)\n",
    "print(\"Gradient for W_ch:\\n\", dW_ch)\n",
    "print(\"Gradient for b_i:\\n\", db_i)\n",
    "print(\"Gradient for b_f:\\n\", db_f)\n",
    "print(\"Gradient for b_o:\\n\", db_o)\n",
    "print(\"Gradient for b_c:\\n\", db_c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38e854f",
   "metadata": {},
   "source": [
    "# Optical Character Recognition (OCR) with Recurrent Neural Network (RNN)\n",
    "\n",
    "1. **Model Input**: Let's denote our input data as \\( X = \\{x^{(1)}, x^{(2)}, ..., x^{(T)}\\} \\), where \\( T \\) is the number of time steps (or sequence length) and \\( x^{(t)} \\) represents the input image at time step \\( t \\).\n",
    "\n",
    "2. **Model Output**: Similarly, let's denote the output (or prediction) as \\( Y = \\{y^{(1)}, y^{(2)}, ..., y^{(T)}\\} \\), where \\( y^{(t)} \\) represents the predicted label at time step \\( t \\).\n",
    "\n",
    "3. **RNN Model**: The RNN processes the input sequence \\( X \\) and generates the output sequence \\( Y \\). At each time step \\( t \\), the RNN takes the current input \\( x^{(t)} \\) and the previous hidden state \\( h^{(t-1)} \\) to compute the next hidden state \\( h^{(t)} \\) and the output \\( y^{(t)} \\).\n",
    "\n",
    "   Mathematically, this can be represented as:\n",
    "   $$\n",
    "   h^{(t)} = \\text{RNN}(x^{(t)}, h^{(t-1)})\n",
    "   $$\n",
    "   $$\n",
    "   y^{(t)} = \\text{softmax}(W_{\\text{out}} h^{(t)} + b_{\\text{out}})\n",
    "   $$\n",
    "   where \\( \\text{RNN} \\) represents the recurrent neural network cell (such as LSTM or GRU), \\( W_{\\text{out}} \\) and \\( b_{\\text{out}} \\) are the weight matrix and bias vector for the output layer, and \\( \\text{softmax} \\) is the softmax activation function.\n",
    "\n",
    "4. **Loss Function**: We typically use the cross-entropy loss function to measure the difference between the predicted output and the ground truth labels:\n",
    "   $$\n",
    "   \\mathcal{L} = -\\frac{1}{T} \\sum_{t=1}^{T} \\sum_{i} y_i^{(t)} \\log(\\hat{y}_i^{(t)})\n",
    "   $$\n",
    "   where \\( \\hat{y}^{(t)} \\) is the predicted probability distribution over classes at time step \\( t \\), and \\( y^{(t)} \\) is the true probability distribution (one-hot encoded) over classes at time step \\( t \\).\n",
    "\n",
    "5. **Training**: During training, we minimize the loss function \\( \\mathcal{L} \\) with respect to the model parameters (weights and biases) using gradient descent-based optimization algorithms like Adam or SGD.\n",
    "\n",
    "6. **Inference**: During inference, we feed the input image sequence \\( X \\) into the trained model, and the model generates the output sequence \\( Y \\). We can then decode the output sequence to obtain the final predicted labels for the characters in the input image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "225e5acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated hidden state shape: (10, 1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (10,25) and (1,1) not aligned: 25 (dim 1) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10021/3723874944.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0my_pred_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_A_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m     \u001b[0my_pred_B\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_B_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0my_pred_C\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_C_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_10021/3723874944.py\u001b[0m in \u001b[0;36mforward_pass\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mx_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Reshape input for matrix multiplication\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWxh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_t\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWhh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbh\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Hidden state update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mby\u001b[0m  \u001b[0;31m# Output computation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (10,25) and (1,1) not aligned: 25 (dim 1) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sample input image (binary representation)\n",
    "# Assume each image is represented as a 2D array (e.g., 28x28 pixels)\n",
    "# For simplicity, let's assume a 5x5 image for each character\n",
    "import numpy as np\n",
    "\n",
    "# Define the dimensions\n",
    "input_size = 25\n",
    "hidden_size = 10\n",
    "\n",
    "# Initialize the weight matrices and biases\n",
    "Wxh = np.random.randn(hidden_size, input_size)\n",
    "Whh = np.random.randn(hidden_size, hidden_size)\n",
    "bh = np.zeros((hidden_size, 1))\n",
    "\n",
    "# Define the input vector\n",
    "x_t = np.random.randn(input_size, 1)\n",
    "\n",
    "# Initialize the hidden state\n",
    "h = np.zeros((hidden_size, 1))\n",
    "\n",
    "# Update the hidden state\n",
    "h = np.tanh(np.dot(Wxh, x_t) + np.dot(Whh, h) + bh)\n",
    "\n",
    "print(\"Updated hidden state shape:\", h.shape)\n",
    "\n",
    "\n",
    "\n",
    "image_A = np.array([[0, 1, 1, 1, 0],\n",
    "                     [1, 0, 0, 0, 1],\n",
    "                     [1, 1, 1, 1, 1],\n",
    "                     [1, 0, 0, 0, 1],\n",
    "                     [1, 0, 0, 0, 1]])\n",
    "\n",
    "image_B = np.array([[1, 1, 1, 1, 0],\n",
    "                     [1, 0, 0, 0, 1],\n",
    "                     [1, 1, 1, 1, 0],\n",
    "                     [1, 0, 0, 0, 1],\n",
    "                     [1, 1, 1, 1, 0]])\n",
    "\n",
    "image_C = np.array([[0, 1, 1, 1, 0],\n",
    "                     [1, 0, 0, 0, 1],\n",
    "                     [1, 0, 0, 0, 0],\n",
    "                     [1, 0, 0, 0, 1],\n",
    "                     [0, 1, 1, 1, 0]])\n",
    "\n",
    "# Flatten each image to a 1D array\n",
    "# In a real OCR system, you would preprocess the images and extract relevant features\n",
    "image_A_flat = image_A.flatten()\n",
    "image_B_flat = image_B.flatten()\n",
    "image_C_flat = image_C.flatten()\n",
    "\n",
    "# Sample output labels (one-hot encoded)\n",
    "label_A = np.array([1, 0, 0])  # A\n",
    "label_B = np.array([0, 1, 0])  # B\n",
    "label_C = np.array([0, 0, 1])  # C\n",
    "\n",
    "# Model parameters (weights and biases)\n",
    "input_size = 25  # Size of flattened image\n",
    "hidden_size = 10  # Number of hidden units\n",
    "output_size = 3  # Number of output classes\n",
    "\n",
    "# Initialize model parameters (weights and biases)\n",
    "Wxh = np.random.randn(hidden_size, input_size)  # Input-to-hidden weights\n",
    "Whh = np.random.randn(hidden_size, hidden_size)  # Hidden-to-hidden weights\n",
    "Why = np.random.randn(output_size, hidden_size)  # Hidden-to-output weights\n",
    "bh = np.zeros((hidden_size, 1))  # Hidden bias\n",
    "by = np.zeros((output_size, 1))  # Output bias\n",
    "\n",
    "# Learning rate\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Forward pass function\n",
    "def forward_pass(x):\n",
    "    bh = np.zeros((hidden_size, 1))  # Correcting the shape of the bias term\n",
    "\n",
    "    h = np.zeros((hidden_size, 1))  # Initialize hidden state\n",
    "    for t in range(len(x)):\n",
    "        x_t = x[t].reshape(-1, 1)  # Reshape input for matrix multiplication\n",
    "        h = np.tanh(np.dot(Wxh, x_t) + np.dot(Whh, h) + bh)  # Hidden state update\n",
    "    y = np.dot(Why, h) + by  # Output computation\n",
    "    return y, h\n",
    "\n",
    "# Softmax function for converting raw scores to probabilities\n",
    "def softmax(x):\n",
    "    exp_scores = np.exp(x - np.max(x))  # Subtracting max for numerical stability\n",
    "    return exp_scores / np.sum(exp_scores)\n",
    "\n",
    "# Loss function (cross-entropy)\n",
    "def cross_entropy_loss(y_pred, y_true):\n",
    "    return -np.sum(y_true * np.log(y_pred))\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    y_pred_A, _ = forward_pass(image_A_flat)\n",
    "    y_pred_B, _ = forward_pass(image_B_flat)\n",
    "    y_pred_C, _ = forward_pass(image_C_flat)\n",
    "\n",
    "    # Compute loss\n",
    "    loss_A = cross_entropy_loss(softmax(y_pred_A), label_A)\n",
    "    loss_B = cross_entropy_loss(softmax(y_pred_B), label_B)\n",
    "    loss_C = cross_entropy_loss(softmax(y_pred_C), label_C)\n",
    "    total_loss = (loss_A + loss_B + loss_C) / 3\n",
    "\n",
    "    # Backpropagation\n",
    "    dy_A = softmax(y_pred_A) - label_A\n",
    "    dy_B = softmax(y_pred_B) - label_B\n",
    "    dy_C = softmax(y_pred_C) - label_C\n",
    "\n",
    "    dWhy_A = np.dot(dy_A, np.tanh(h_A).T)\n",
    "    dWhy_B = np.dot(dy_B, np.tanh(h_B).T)\n",
    "    dWhy_C = np.dot(dy_C, np.tanh(h_C).T)\n",
    "\n",
    "    dby_A = dy_A\n",
    "    dby_B = dy_B\n",
    "    dby_C = dy_C\n",
    "\n",
    "    # Update weights and biases\n",
    "    Why -= learning_rate * (dWhy_A + dWhy_B + dWhy_C) / 3\n",
    "    by -= learning_rate * (dby_A + dby_B + dby_C) / 3\n",
    "\n",
    "    # Print loss\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch}: Loss = {total_loss}')\n",
    "\n",
    "# Test the model on sample images after training\n",
    "print(\"Test Output for Image A:\", softmax(forward_pass(image_A_flat)[0]))\n",
    "print(\"Test Output for Image B:\", softmax(forward_pass(image_B_flat)[0]))\n",
    "print(\"Test Output for Image C:\", softmax(forward_pass(image_C_flat)[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf503a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
