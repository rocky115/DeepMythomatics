{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f598aeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " * Copyright (c) 2016 Radhamadhab Dalai\n",
    " *\n",
    " * Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    " * of this software and associated documentation files (the \"Software\"), to deal\n",
    " * in the Software without restriction, including without limitation the rights\n",
    " * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    " * copies of the Software, and to permit persons to whom the Software is\n",
    " * furnished to do so, subject to the following conditions:\n",
    " *\n",
    " * The above copyright notice and this permission notice shall be included in\n",
    " * all copies or substantial portions of the Software.\n",
    " *\n",
    " * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    " * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    " * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    " * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    " * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    " * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
    " * THE SOFTWARE.\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "53a1f6b2",
   "metadata": {},
   "source": [
    "# Singular Value Decomposition (SVD) and Matrix Approximation\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The Singular Value Decomposition (SVD) is a fundamental matrix factorization technique with wide applications in machine learning, from least-squares problems to dimensionality reduction and data compression.\n",
    "\n",
    "## 1. Standard SVD Formulation\n",
    "\n",
    "For any matrix $A \\in \\mathbb{R}^{m \\times n}$, the SVD decomposes it as:\n",
    "\n",
    "$$A = U\\Sigma V^T$$\n",
    "\n",
    "where:\n",
    "- $U \\in \\mathbb{R}^{m \\times m}$ is an orthogonal matrix (left singular vectors)\n",
    "- $\\Sigma \\in \\mathbb{R}^{m \\times n}$ contains singular values on the main diagonal\n",
    "- $V \\in \\mathbb{R}^{n \\times n}$ is an orthogonal matrix (right singular vectors)\n",
    "\n",
    "## 2. Reduced SVD (Compact SVD)\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "Fig.11 Image processing with the SVD. (a) The original grayscale image is a 1, 432 × 1, 910 matrix of values between 0 (black) and 1 (white). (b)–(f) Rank-1 matrices A1 , . . . , A5 and their corresponding singular values σ1 , . . . , σ5 . The grid-like structure of each rank-1 matrix is imposed by the outer-product of the left and right-singular vectors.\n",
    "\n",
    "Sometimes called the **reduced SVD** (Datta, 2010) or simply **the SVD** (Press et al., 2007), this alternative formulation provides computational convenience:\n",
    "\n",
    "For a rank-$r$ matrix $A$:\n",
    "$$A = U\\Sigma V^T$$\n",
    "\n",
    "where:\n",
    "- $U \\in \\mathbb{R}^{m \\times r}$ (reduced left singular vectors)\n",
    "- $\\Sigma \\in \\mathbb{R}^{r \\times r}$ (diagonal matrix with nonzero singular values)\n",
    "- $V \\in \\mathbb{R}^{r \\times n}$ (reduced right singular vectors)\n",
    "\n",
    "**Key advantage**: $\\Sigma$ is diagonal (like in eigenvalue decomposition), containing only nonzero entries.\n",
    "\n",
    "## 3. Handling Different Matrix Dimensions\n",
    "\n",
    "The SVD applies to $m \\times n$ matrices regardless of whether $m > n$ or $m < n$:\n",
    "\n",
    "- When $m < n$: The decomposition yields $\\Sigma$ with more zero columns than rows\n",
    "- Consequently, singular values $\\sigma_{m+1}, \\ldots, \\sigma_n = 0$\n",
    "\n",
    "## 4. Matrix Approximation via SVD\n",
    "\n",
    "### 4.1 Rank-1 Matrix Construction\n",
    "\n",
    "Instead of full SVD factorization, we can represent matrix $A$ as a sum of simpler low-rank matrices:\n",
    "\n",
    "$$A_i := u_i v_i^T \\quad \\text{(rank-1 matrix)}$$\n",
    "\n",
    "where $A_i \\in \\mathbb{R}^{m \\times n}$ is formed by the outer product of the $i$-th orthogonal column vectors from $U$ and $V$.\n",
    "\n",
    "### 4.2 Complete Representation\n",
    "\n",
    "The full matrix can be expressed as:\n",
    "\n",
    "$$A = \\sum_{i=1}^{r} \\sigma_i u_i v_i^T = \\sum_{i=1}^{r} \\sigma_i A_i$$\n",
    "\n",
    "where $\\sigma_i$ are the singular values and $r$ is the rank of $A$.\n",
    "\n",
    "### 4.3 Truncated SVD for Approximation\n",
    "\n",
    "For matrix approximation, we use only the first $k$ terms (where $k < r$):\n",
    "\n",
    "$$A_k = \\sum_{i=1}^{k} \\sigma_i u_i v_i^T$$\n",
    "\n",
    "This **truncated SVD** provides the best rank-$k$ approximation to $A$ in the Frobenius norm sense.\n",
    "\n",
    "## 5. Applications in Machine Learning\n",
    "\n",
    "The SVD's matrix approximation capabilities enable numerous applications:\n",
    "\n",
    "### 5.1 Dimensionality Reduction\n",
    "- Principal Component Analysis (PCA)\n",
    "- Feature extraction and visualization\n",
    "\n",
    "### 5.2 Data Compression\n",
    "- Image compression (as shown with Stonehenge example)\n",
    "- Lossy compression with controlled quality\n",
    "\n",
    "### 5.3 Topic Modeling\n",
    "- Latent Semantic Analysis (LSA)\n",
    "- Document-term matrix factorization\n",
    "\n",
    "### 5.4 Clustering and Pattern Recognition\n",
    "- Spectral clustering\n",
    "- Noise reduction\n",
    "\n",
    "### 5.5 Numerical Stability\n",
    "- Solving systems of linear equations\n",
    "- Least-squares curve fitting\n",
    "- Robust to numerical rounding errors\n",
    "\n",
    "## 6. Computational Advantages\n",
    "\n",
    "**Matrix approximation benefits**:\n",
    "1. **Computational efficiency**: Working with lower-rank approximations\n",
    "2. **Storage reduction**: Fewer parameters to store\n",
    "3. **Noise reduction**: Truncation removes small singular values (often noise)\n",
    "4. **Numerical robustness**: SVD substitution improves numerical stability\n",
    "\n",
    "## 7. Example: Image Approximation\n",
    "\n",
    "Consider an image represented as matrix $A \\in \\mathbb{R}^{1432 \\times 1910}$ (like the Stonehenge example):\n",
    "\n",
    "```python\n",
    "# Conceptual code structure\n",
    "A_approx = sum(sigma[i] * outer_product(U[:, i], V[i, :]) for i in range(k))\n",
    "```\n",
    "\n",
    "where $k$ determines the approximation quality vs. compression trade-off.\n",
    "\n",
    "## Mathematical Properties\n",
    "\n",
    "**Key SVD properties leveraged in approximation**:\n",
    "\n",
    "1. **Optimality**: Truncated SVD gives the best low-rank approximation\n",
    "2. **Energy compaction**: Large singular values capture most information\n",
    "3. **Orthogonality**: $U$ and $V$ matrices preserve geometric properties\n",
    "4. **Rank revelation**: Singular values reveal the effective dimensionality\n",
    "\n",
    "The SVD's principled approach to matrix approximation makes it invaluable for creating \"simpler\" matrix representations while preserving essential structural information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4cedcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "class SVDDecomposer:\n",
    "    \"\"\"\n",
    "    Implementation of Singular Value Decomposition and Matrix Approximation techniques\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.U = None\n",
    "        self.sigma = None\n",
    "        self.Vt = None\n",
    "        self.original_shape = None\n",
    "    \n",
    "    def decompose(self, A: np.ndarray, full_matrices: bool = True) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Perform SVD decomposition: A = U @ Σ @ V^T\n",
    "        \n",
    "        Args:\n",
    "            A: Input matrix (m x n)\n",
    "            full_matrices: If True, compute full SVD; if False, compute reduced SVD\n",
    "            \n",
    "        Returns:\n",
    "            U: Left singular vectors\n",
    "            sigma: Singular values (1D array)\n",
    "            Vt: Right singular vectors (transposed)\n",
    "        \"\"\"\n",
    "        self.original_shape = A.shape\n",
    "        \n",
    "        # Compute SVD using NumPy's implementation\n",
    "        self.U, self.sigma, self.Vt = np.linalg.svd(A, full_matrices=full_matrices)\n",
    "        \n",
    "        return self.U, self.sigma, self.Vt\n",
    "    \n",
    "    def reduced_svd(self, A: np.ndarray, rank: Optional[int] = None) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Compute reduced/compact SVD with specified rank\n",
    "        \n",
    "        Args:\n",
    "            A: Input matrix\n",
    "            rank: Desired rank (if None, use effective rank)\n",
    "            \n",
    "        Returns:\n",
    "            U_reduced: Reduced left singular vectors\n",
    "            sigma_reduced: Reduced singular values\n",
    "            Vt_reduced: Reduced right singular vectors\n",
    "        \"\"\"\n",
    "        U, sigma, Vt = self.decompose(A, full_matrices=False)\n",
    "        \n",
    "        if rank is None:\n",
    "            # Use effective rank (remove near-zero singular values)\n",
    "            tol = max(A.shape) * np.finfo(A.dtype).eps * sigma[0]\n",
    "            rank = np.sum(sigma > tol)\n",
    "        \n",
    "        rank = min(rank, len(sigma))\n",
    "        \n",
    "        return U[:, :rank], sigma[:rank], Vt[:rank, :]\n",
    "    \n",
    "    def truncated_svd(self, A: np.ndarray, k: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute truncated SVD approximation using first k components\n",
    "        \n",
    "        Args:\n",
    "            A: Input matrix\n",
    "            k: Number of components to keep\n",
    "            \n",
    "        Returns:\n",
    "            A_k: Rank-k approximation of A\n",
    "        \"\"\"\n",
    "        U, sigma, Vt = self.decompose(A, full_matrices=False)\n",
    "        \n",
    "        k = min(k, len(sigma))\n",
    "        \n",
    "        # Reconstruct using first k components\n",
    "        A_k = U[:, :k] @ np.diag(sigma[:k]) @ Vt[:k, :]\n",
    "        \n",
    "        return A_k\n",
    "    \n",
    "    def rank_one_matrices(self, A: np.ndarray, num_components: Optional[int] = None) -> list:\n",
    "        \"\"\"\n",
    "        Decompose matrix into rank-1 components: A_i = σ_i * u_i * v_i^T\n",
    "        \n",
    "        Args:\n",
    "            A: Input matrix\n",
    "            num_components: Number of rank-1 matrices to return\n",
    "            \n",
    "        Returns:\n",
    "            List of rank-1 matrices\n",
    "        \"\"\"\n",
    "        U, sigma, Vt = self.decompose(A, full_matrices=False)\n",
    "        \n",
    "        if num_components is None:\n",
    "            num_components = len(sigma)\n",
    "        \n",
    "        rank_one_mats = []\n",
    "        \n",
    "        for i in range(min(num_components, len(sigma))):\n",
    "            # A_i = σ_i * u_i * v_i^T\n",
    "            A_i = sigma[i] * np.outer(U[:, i], Vt[i, :])\n",
    "            rank_one_mats.append(A_i)\n",
    "        \n",
    "        return rank_one_mats\n",
    "    \n",
    "    def progressive_approximation(self, A: np.ndarray, max_rank: int) -> list:\n",
    "        \"\"\"\n",
    "        Generate progressive approximations A_1, A_2, ..., A_k\n",
    "        \n",
    "        Args:\n",
    "            A: Input matrix\n",
    "            max_rank: Maximum rank for approximation\n",
    "            \n",
    "        Returns:\n",
    "            List of progressive approximations\n",
    "        \"\"\"\n",
    "        U, sigma, Vt = self.decompose(A, full_matrices=False)\n",
    "        \n",
    "        approximations = []\n",
    "        max_rank = min(max_rank, len(sigma))\n",
    "        \n",
    "        for k in range(1, max_rank + 1):\n",
    "            A_k = U[:, :k] @ np.diag(sigma[:k]) @ Vt[:k, :]\n",
    "            approximations.append(A_k)\n",
    "        \n",
    "        return approximations\n",
    "    \n",
    "    def approximation_error(self, A: np.ndarray, A_approx: np.ndarray, norm_type: str = 'fro') -> float:\n",
    "        \"\"\"\n",
    "        Compute approximation error between original and approximated matrix\n",
    "        \n",
    "        Args:\n",
    "            A: Original matrix\n",
    "            A_approx: Approximated matrix\n",
    "            norm_type: Type of norm ('fro' for Frobenius, '2' for spectral)\n",
    "            \n",
    "        Returns:\n",
    "            Approximation error\n",
    "        \"\"\"\n",
    "        if norm_type == 'fro':\n",
    "            return np.linalg.norm(A - A_approx, 'fro')\n",
    "        elif norm_type == '2':\n",
    "            return np.linalg.norm(A - A_approx, 2)\n",
    "        else:\n",
    "            raise ValueError(\"norm_type must be 'fro' or '2'\")\n",
    "    \n",
    "    def compression_ratio(self, A: np.ndarray, k: int) -> float:\n",
    "        \"\"\"\n",
    "        Calculate compression ratio for rank-k approximation\n",
    "        \n",
    "        Args:\n",
    "            A: Original matrix\n",
    "            k: Approximation rank\n",
    "            \n",
    "        Returns:\n",
    "            Compression ratio\n",
    "        \"\"\"\n",
    "        m, n = A.shape\n",
    "        original_size = m * n\n",
    "        compressed_size = k * (m + n + 1)  # U(:,1:k) + sigma(1:k) + Vt(1:k,:)\n",
    "        \n",
    "        return compressed_size / original_size\n",
    "\n",
    "\n",
    "class ImageSVDDemo:\n",
    "    \"\"\"\n",
    "    Demonstrate SVD matrix approximation on images\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.svd = SVDDecomposer()\n",
    "    \n",
    "    def create_synthetic_image(self, size: Tuple[int, int] = (100, 100)) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Create a synthetic image for demonstration\n",
    "        \"\"\"\n",
    "        m, n = size\n",
    "        x = np.linspace(-2, 2, n)\n",
    "        y = np.linspace(-2, 2, m)\n",
    "        X, Y = np.meshgrid(x, y)\n",
    "        \n",
    "        # Create interesting pattern\n",
    "        image = np.exp(-(X**2 + Y**2)) + 0.5 * np.sin(5*X) * np.cos(5*Y)\n",
    "        \n",
    "        # Normalize to [0, 1]\n",
    "        image = (image - image.min()) / (image.max() - image.min())\n",
    "        \n",
    "        return image\n",
    "    \n",
    "    def demonstrate_approximation(self, image: np.ndarray, ranks: list):\n",
    "        \"\"\"\n",
    "        Demonstrate progressive SVD approximation on an image\n",
    "        \n",
    "        Args:\n",
    "            image: Input image matrix\n",
    "            ranks: List of ranks to test\n",
    "        \"\"\"\n",
    "        fig, axes = plt.subplots(2, len(ranks) + 1, figsize=(15, 8))\n",
    "        \n",
    "        # Original image\n",
    "        axes[0, 0].imshow(image, cmap='gray')\n",
    "        axes[0, 0].set_title('Original')\n",
    "        axes[0, 0].axis('off')\n",
    "        \n",
    "        # Singular values plot\n",
    "        U, sigma, Vt = self.svd.decompose(image)\n",
    "        axes[1, 0].semilogy(sigma, 'b-o', markersize=3)\n",
    "        axes[1, 0].set_title('Singular Values')\n",
    "        axes[1, 0].set_xlabel('Index')\n",
    "        axes[1, 0].set_ylabel('Value (log scale)')\n",
    "        axes[1, 0].grid(True)\n",
    "        \n",
    "        errors = []\n",
    "        ratios = []\n",
    "        \n",
    "        for i, k in enumerate(ranks):\n",
    "            # Compute approximation\n",
    "            A_k = self.svd.truncated_svd(image, k)\n",
    "            \n",
    "            # Display approximation\n",
    "            axes[0, i+1].imshow(A_k, cmap='gray')\n",
    "            axes[0, i+1].set_title(f'Rank {k}')\n",
    "            axes[0, i+1].axis('off')\n",
    "            \n",
    "            # Compute and display error\n",
    "            error = self.svd.approximation_error(image, A_k)\n",
    "            ratio = self.svd.compression_ratio(image, k)\n",
    "            \n",
    "            errors.append(error)\n",
    "            ratios.append(ratio)\n",
    "            \n",
    "            axes[1, i+1].bar(['Error', 'Compression'], [error, ratio])\n",
    "            axes[1, i+1].set_title(f'Rank {k}\\nError: {error:.3f}\\nRatio: {ratio:.3f}')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return errors, ratios\n",
    "\n",
    "\n",
    "def demonstrate_svd_concepts():\n",
    "    \"\"\"\n",
    "    Comprehensive demonstration of SVD concepts\n",
    "    \"\"\"\n",
    "    print(\"=== SVD and Matrix Approximation Demonstration ===\\n\")\n",
    "    \n",
    "    # Create SVD decomposer\n",
    "    svd = SVDDecomposer()\n",
    "    \n",
    "    # 1. Basic SVD on a simple matrix\n",
    "    print(\"1. Basic SVD Decomposition\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    A = np.array([[1, 2, 3],\n",
    "                  [4, 5, 6],\n",
    "                  [7, 8, 9],\n",
    "                  [10, 11, 12]], dtype=float)\n",
    "    \n",
    "    print(f\"Original matrix A (shape {A.shape}):\")\n",
    "    print(A)\n",
    "    \n",
    "    U, sigma, Vt = svd.decompose(A)\n",
    "    \n",
    "    print(f\"\\nU shape: {U.shape}\")\n",
    "    print(f\"Sigma shape: {sigma.shape}\")\n",
    "    print(f\"Vt shape: {Vt.shape}\")\n",
    "    print(f\"Singular values: {sigma}\")\n",
    "    \n",
    "    # Verify reconstruction\n",
    "    if U.shape[1] == len(sigma):\n",
    "        A_reconstructed = U @ np.diag(sigma) @ Vt\n",
    "    else:\n",
    "        # Handle reduced SVD case\n",
    "        A_reconstructed = U @ np.diag(sigma) @ Vt[:len(sigma), :]\n",
    "    \n",
    "    reconstruction_error = np.linalg.norm(A - A_reconstructed)\n",
    "    print(f\"Reconstruction error: {reconstruction_error:.2e}\")\n",
    "    \n",
    "    # 2. Reduced SVD\n",
    "    print(f\"\\n2. Reduced SVD\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    U_red, sigma_red, Vt_red = svd.reduced_svd(A, rank=2)\n",
    "    print(f\"Reduced U shape: {U_red.shape}\")\n",
    "    print(f\"Reduced sigma shape: {sigma_red.shape}\")\n",
    "    print(f\"Reduced Vt shape: {Vt_red.shape}\")\n",
    "    \n",
    "    # 3. Rank-1 decomposition\n",
    "    print(f\"\\n3. Rank-1 Matrix Decomposition\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    rank_one_mats = svd.rank_one_matrices(A, num_components=3)\n",
    "    \n",
    "    for i, A_i in enumerate(rank_one_mats):\n",
    "        print(f\"Rank-1 matrix {i+1} (σ_{i+1} = {sigma[i]:.3f}):\")\n",
    "        print(A_i)\n",
    "        print(f\"Rank: {np.linalg.matrix_rank(A_i)}\")\n",
    "        print()\n",
    "    \n",
    "    # 4. Progressive approximation\n",
    "    print(\"4. Progressive Approximation Analysis\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    errors = []\n",
    "    ratios = []\n",
    "    \n",
    "    for k in range(1, min(4, len(sigma)+1)):\n",
    "        A_k = svd.truncated_svd(A, k)\n",
    "        error = svd.approximation_error(A, A_k)\n",
    "        ratio = svd.compression_ratio(A, k)\n",
    "        \n",
    "        errors.append(error)\n",
    "        ratios.append(ratio)\n",
    "        \n",
    "        print(f\"Rank-{k} approximation:\")\n",
    "        print(f\"  Frobenius error: {error:.6f}\")\n",
    "        print(f\"  Compression ratio: {ratio:.3f}\")\n",
    "        print(f\"  Retained energy: {np.sum(sigma[:k]**2) / np.sum(sigma**2):.3f}\")\n",
    "    \n",
    "    # 5. Image approximation demo\n",
    "    print(f\"\\n5. Image Approximation Demo\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    demo = ImageSVDDemo()\n",
    "    \n",
    "    # Create synthetic image\n",
    "    synthetic_image = demo.create_synthetic_image((50, 50))\n",
    "    \n",
    "    # Analyze approximation quality\n",
    "    test_ranks = [1, 5, 10, 20]\n",
    "    \n",
    "    print(\"Analyzing approximation quality for different ranks:\")\n",
    "    \n",
    "    for k in test_ranks:\n",
    "        A_k = svd.truncated_svd(synthetic_image, k)\n",
    "        error = svd.approximation_error(synthetic_image, A_k)\n",
    "        ratio = svd.compression_ratio(synthetic_image, k)\n",
    "        \n",
    "        print(f\"Rank {k:2d}: Error = {error:.6f}, Compression = {ratio:.3f}\")\n",
    "    \n",
    "    # Demonstrate the visualization (commented out to avoid display issues in some environments)\n",
    "    # demo.demonstrate_approximation(synthetic_image, test_ranks)\n",
    "    \n",
    "    print(f\"\\n6. Numerical Properties\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Condition number analysis\n",
    "    cond_original = np.linalg.cond(A)\n",
    "    \n",
    "    # Create a well-conditioned approximation\n",
    "    k_stable = 2  # Use first 2 components\n",
    "    A_stable = svd.truncated_svd(A, k_stable)\n",
    "    cond_stable = np.linalg.cond(A_stable)\n",
    "    \n",
    "    print(f\"Condition number of original matrix: {cond_original:.2e}\")\n",
    "    print(f\"Condition number of rank-{k_stable} approximation: {cond_stable:.2e}\")\n",
    "    print(f\"Numerical stability improvement: {cond_original/cond_stable:.2f}x\")\n",
    "\n",
    "\n",
    "def advanced_svd_applications():\n",
    "    \"\"\"\n",
    "    Demonstrate advanced SVD applications\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Advanced SVD Applications ===\\n\")\n",
    "    \n",
    "    svd = SVDDecomposer()\n",
    "    \n",
    "    # 1. Principal Component Analysis (PCA) simulation\n",
    "    print(\"1. PCA-like Dimensionality Reduction\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    # Generate correlated data\n",
    "    np.random.seed(42)\n",
    "    n_samples, n_features = 100, 5\n",
    "    \n",
    "    # Create data with some correlation structure\n",
    "    true_components = np.array([[1, 1, 0, 0, 0],\n",
    "                                [0, 0, 1, 1, 1]]).T\n",
    "    \n",
    "    data = np.random.randn(n_samples, 2) @ true_components.T + 0.1 * np.random.randn(n_samples, n_features)\n",
    "    \n",
    "    print(f\"Original data shape: {data.shape}\")\n",
    "    \n",
    "    # Center the data (important for PCA)\n",
    "    data_centered = data - np.mean(data, axis=0)\n",
    "    \n",
    "    # Apply SVD\n",
    "    U, sigma, Vt = svd.decompose(data_centered.T)  # Note: transpose for feature space\n",
    "    \n",
    "    print(f\"Explained variance ratios: {(sigma**2 / np.sum(sigma**2))[:3]}\")\n",
    "    \n",
    "    # 2. Least squares solution using SVD\n",
    "    print(f\"\\n2. Robust Least Squares via SVD\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Create overdetermined system Ax = b\n",
    "    m, n = 10, 5\n",
    "    A_ls = np.random.randn(m, n)\n",
    "    x_true = np.random.randn(n)\n",
    "    b = A_ls @ x_true + 0.01 * np.random.randn(m)  # Add small noise\n",
    "    \n",
    "    # Solve using SVD (more numerically stable than normal equations)\n",
    "    U, sigma, Vt = svd.decompose(A_ls)\n",
    "    \n",
    "    # Compute pseudoinverse using SVD\n",
    "    tol = max(A_ls.shape) * np.finfo(float).eps * sigma[0]\n",
    "    rank = np.sum(sigma > tol)\n",
    "    \n",
    "    # x = V @ Σ^(-1) @ U^T @ b (for the non-zero singular values)\n",
    "    x_svd = Vt[:rank, :].T @ np.diag(1/sigma[:rank]) @ U[:, :rank].T @ b\n",
    "    \n",
    "    # Compare with numpy's least squares\n",
    "    x_lstsq = np.linalg.lstsq(A_ls, b, rcond=None)[0]\n",
    "    \n",
    "    print(f\"True solution norm: {np.linalg.norm(x_true):.6f}\")\n",
    "    print(f\"SVD solution error: {np.linalg.norm(x_svd - x_true):.6f}\")\n",
    "    print(f\"Lstsq solution error: {np.linalg.norm(x_lstsq - x_true):.6f}\")\n",
    "    print(f\"Solutions match: {np.allclose(x_svd, x_lstsq)}\")\n",
    "    \n",
    "    # 3. Matrix completion simulation\n",
    "    print(f\"\\n3. Low-Rank Matrix Recovery\")\n",
    "    print(\"-\" * 28)\n",
    "    \n",
    "    # Create low-rank matrix\n",
    "    rank_true = 3\n",
    "    m, n = 20, 15\n",
    "    L = np.random.randn(m, rank_true)\n",
    "    R = np.random.randn(rank_true, n)\n",
    "    M_true = L @ R\n",
    "    \n",
    "    # Add noise\n",
    "    M_noisy = M_true + 0.1 * np.random.randn(m, n)\n",
    "    \n",
    "    print(f\"True rank: {rank_true}\")\n",
    "    print(f\"Noisy matrix rank: {np.linalg.matrix_rank(M_noisy)}\")\n",
    "    \n",
    "    # Recover using truncated SVD\n",
    "    M_recovered = svd.truncated_svd(M_noisy, rank_true)\n",
    "    \n",
    "    recovery_error = svd.approximation_error(M_true, M_recovered)\n",
    "    noise_level = svd.approximation_error(M_true, M_noisy)\n",
    "    \n",
    "    print(f\"Noise level: {noise_level:.6f}\")\n",
    "    print(f\"Recovery error: {recovery_error:.6f}\")\n",
    "    print(f\"Recovery improvement: {noise_level/recovery_error:.2f}x\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run comprehensive demonstrations\n",
    "    demonstrate_svd_concepts()\n",
    "    advanced_svd_applications()\n",
    "    \n",
    "    print(\"\\n=== Summary of SVD Benefits ===\")\n",
    "    print(\"• Optimal low-rank approximation (Eckart-Young theorem)\")\n",
    "    print(\"• Numerical stability for ill-conditioned problems\") \n",
    "    print(\"• Principal component analysis and dimensionality reduction\")\n",
    "    print(\"• Data compression with controlled quality loss\")\n",
    "    print(\"• Robust solutions to least-squares problems\")\n",
    "    print(\"• Matrix completion and denoising applications\")\n",
    "    print(\"• Foundation for many machine learning algorithms\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0d46c153",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)\n",
    "\n",
    "Fig.12 Image reconstruction with the SVD. (a) Original image. (b)–(f) Image reconstruction using the low-rank approximation of the SVD, where the rank-k approximation is b given by A(k) = Pk i=1 σi Ai .\n",
    "\n",
    "# Matrix Approximation and the Eckart-Young Theorem\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook implements the matrix approximation techniques described in Section 4.6, focusing on the Eckart-Young theorem which establishes the optimality of SVD-based low-rank approximations.\n",
    "\n",
    "## Mathematical Foundation\n",
    "\n",
    "### 1. Spectral Norm Definition\n",
    "\n",
    "**Definition 4.23 (Spectral Norm of a Matrix)**\n",
    "\n",
    "For $x \\in \\mathbb{R}^n \\setminus \\{0\\}$, the spectral norm of a matrix $A \\in \\mathbb{R}^{m \\times n}$ is defined as:\n",
    "\n",
    "$$\\|A\\|_2 := \\max_{x} \\frac{\\|Ax\\|_2}{\\|x\\|_2} \\quad \\text{(Equation 4.93)}$$\n",
    "\n",
    "The spectral norm determines how long any vector $x$ can at most become when multiplied by $A$.\n",
    "\n",
    "### 2. Spectral Norm and Singular Values\n",
    "\n",
    "**Theorem 4.24**: The spectral norm of $A$ is its largest singular value $\\sigma_1$.\n",
    "\n",
    "$$\\|A\\|_2 = \\sigma_1$$\n",
    "\n",
    "### 3. Eckart-Young Theorem\n",
    "\n",
    "**Theorem 4.25 (Eckart-Young Theorem, 1936)**\n",
    "\n",
    "Consider a matrix $A \\in \\mathbb{R}^{m \\times n}$ of rank $r$ and let $B \\in \\mathbb{R}^{m \\times n}$ be a matrix of rank $k$. For any $k \\leq r$ with $\\hat{A}^{(k)} = \\sum_{i=1}^{k} \\sigma_i u_i v_i^T$, it holds that:\n",
    "\n",
    "$$\\hat{A}^{(k)} = \\arg\\min_{\\text{rank}(B)=k} \\|A - B\\|_2 \\quad \\text{(Equation 4.94)}$$\n",
    "\n",
    "$$\\|A - \\hat{A}^{(k)}\\|_2 = \\sigma_{k+1} \\quad \\text{(Equation 4.95)}$$\n",
    "\n",
    "The theorem states that SVD provides the **optimal** rank-$k$ approximation in the spectral norm sense.\n",
    "\n",
    "## Implementation\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import svd as scipy_svd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class MatrixApproximationAnalyzer:\n",
    "    \"\"\"\n",
    "    Implementation of matrix approximation techniques with focus on\n",
    "    the Eckart-Young theorem and spectral norm analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.U = None\n",
    "        self.sigma = None\n",
    "        self.Vt = None\n",
    "        self.original_matrix = None\n",
    "        \n",
    "    def compute_svd(self, A):\n",
    "        \"\"\"\n",
    "        Compute SVD decomposition: A = U @ Σ @ V^T\n",
    "        \"\"\"\n",
    "        self.original_matrix = A.copy()\n",
    "        self.U, self.sigma, self.Vt = np.linalg.svd(A, full_matrices=False)\n",
    "        return self.U, self.sigma, self.Vt\n",
    "    \n",
    "    def spectral_norm(self, A):\n",
    "        \"\"\"\n",
    "        Compute spectral norm of matrix A\n",
    "        \n",
    "        Definition 4.23: ||A||₂ = max_x ||Ax||₂/||x||₂\n",
    "        Theorem 4.24: ||A||₂ = σ₁ (largest singular value)\n",
    "        \"\"\"\n",
    "        # Method 1: Using definition (computationally expensive)\n",
    "        # We'll use the theorem instead for efficiency\n",
    "        \n",
    "        # Method 2: Using Theorem 4.24\n",
    "        _, sigma, _ = np.linalg.svd(A, full_matrices=False)\n",
    "        spectral_norm_value = sigma[0] if len(sigma) > 0 else 0\n",
    "        \n",
    "        return spectral_norm_value\n",
    "    \n",
    "    def verify_spectral_norm_theorem(self, A, num_random_vectors=1000):\n",
    "        \"\"\"\n",
    "        Verify Theorem 4.24: ||A||₂ = σ₁ by testing with random vectors\n",
    "        \"\"\"\n",
    "        # Compute using theorem\n",
    "        theoretical_norm = self.spectral_norm(A)\n",
    "        \n",
    "        # Compute using definition with random vectors\n",
    "        m, n = A.shape\n",
    "        max_ratio = 0\n",
    "        \n",
    "        for _ in range(num_random_vectors):\n",
    "            x = np.random.randn(n)\n",
    "            x = x / np.linalg.norm(x)  # Normalize\n",
    "            \n",
    "            Ax = A @ x\n",
    "            ratio = np.linalg.norm(Ax) / np.linalg.norm(x)\n",
    "            max_ratio = max(max_ratio, ratio)\n",
    "        \n",
    "        return theoretical_norm, max_ratio\n",
    "    \n",
    "    def rank_k_approximation(self, A, k):\n",
    "        \"\"\"\n",
    "        Create rank-k approximation: Â^(k) = Σᵢ₌₁ᵏ σᵢ uᵢ vᵢᵀ\n",
    "        \"\"\"\n",
    "        if self.U is None:\n",
    "            self.compute_svd(A)\n",
    "        \n",
    "        k = min(k, len(self.sigma))\n",
    "        \n",
    "        # Â^(k) = Σᵢ₌₁ᵏ σᵢ uᵢ vᵢᵀ\n",
    "        A_k = np.zeros_like(A)\n",
    "        for i in range(k):\n",
    "            A_k += self.sigma[i] * np.outer(self.U[:, i], self.Vt[i, :])\n",
    "        \n",
    "        return A_k\n",
    "    \n",
    "    def eckart_young_error(self, A, k):\n",
    "        \"\"\"\n",
    "        Compute the error according to Eckart-Young theorem\n",
    "        \n",
    "        ||A - Â^(k)||₂ = σₖ₊₁\n",
    "        \"\"\"\n",
    "        if self.U is None:\n",
    "            self.compute_svd(A)\n",
    "        \n",
    "        if k >= len(self.sigma):\n",
    "            return 0.0  # Perfect reconstruction\n",
    "        \n",
    "        return self.sigma[k]  # σₖ₊₁ (k+1-th singular value, 0-indexed)\n",
    "    \n",
    "    def verify_eckart_young_theorem(self, A, k):\n",
    "        \"\"\"\n",
    "        Verify the Eckart-Young theorem by computing actual error\n",
    "        and comparing with theoretical prediction\n",
    "        \"\"\"\n",
    "        # Compute rank-k approximation\n",
    "        A_k = self.rank_k_approximation(A, k)\n",
    "        \n",
    "        # Actual error\n",
    "        actual_error = self.spectral_norm(A - A_k)\n",
    "        \n",
    "        # Theoretical error (Eckart-Young)\n",
    "        theoretical_error = self.eckart_young_error(A, k)\n",
    "        \n",
    "        return actual_error, theoretical_error\n",
    "    \n",
    "    def demonstrate_optimality(self, A, k, num_random_trials=50):\n",
    "        \"\"\"\n",
    "        Demonstrate that SVD gives optimal rank-k approximation\n",
    "        by comparing with random rank-k matrices\n",
    "        \"\"\"\n",
    "        # SVD approximation\n",
    "        A_k_svd = self.rank_k_approximation(A, k)\n",
    "        svd_error = self.spectral_norm(A - A_k_svd)\n",
    "        \n",
    "        # Random rank-k approximations\n",
    "        m, n = A.shape\n",
    "        random_errors = []\n",
    "        \n",
    "        for _ in range(num_random_trials):\n",
    "            # Create random rank-k matrix\n",
    "            U_rand = np.random.randn(m, k)\n",
    "            V_rand = np.random.randn(k, n)\n",
    "            A_k_rand = U_rand @ V_rand\n",
    "            \n",
    "            # Normalize to have similar scale\n",
    "            A_k_rand = A_k_rand * (np.linalg.norm(A) / np.linalg.norm(A_k_rand))\n",
    "            \n",
    "            error = self.spectral_norm(A - A_k_rand)\n",
    "            random_errors.append(error)\n",
    "        \n",
    "        return svd_error, random_errors\n",
    "    \n",
    "    def progressive_approximation_analysis(self, A, max_rank=None):\n",
    "        \"\"\"\n",
    "        Analyze how approximation error decreases with increasing rank\n",
    "        \"\"\"\n",
    "        if self.U is None:\n",
    "            self.compute_svd(A)\n",
    "        \n",
    "        if max_rank is None:\n",
    "            max_rank = min(A.shape)\n",
    "        \n",
    "        max_rank = min(max_rank, len(self.sigma))\n",
    "        \n",
    "        ranks = list(range(1, max_rank + 1))\n",
    "        actual_errors = []\n",
    "        theoretical_errors = []\n",
    "        \n",
    "        for k in ranks:\n",
    "            actual_error, theoretical_error = self.verify_eckart_young_theorem(A, k)\n",
    "            actual_errors.append(actual_error)\n",
    "            theoretical_errors.append(theoretical_error)\n",
    "        \n",
    "        return ranks, actual_errors, theoretical_errors\n",
    "\n",
    "def create_test_matrices():\n",
    "    \"\"\"\n",
    "    Create various test matrices for demonstration\n",
    "    \"\"\"\n",
    "    matrices = {}\n",
    "    \n",
    "    # 1. Low-rank matrix (rank 3)\n",
    "    np.random.seed(42)\n",
    "    U1 = np.random.randn(8, 3)\n",
    "    V1 = np.random.randn(3, 6)\n",
    "    matrices['low_rank'] = U1 @ V1\n",
    "    \n",
    "    # 2. Image-like matrix (structured)\n",
    "    x = np.linspace(-2, 2, 50)\n",
    "    y = np.linspace(-2, 2, 40)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    matrices['image_like'] = np.exp(-(X**2 + Y**2)) + 0.3 * np.sin(3*X) * np.cos(3*Y)\n",
    "    \n",
    "    # 3. Random matrix\n",
    "    matrices['random'] = np.random.randn(20, 15)\n",
    "    \n",
    "    # 4. Ill-conditioned matrix\n",
    "    U2 = np.random.randn(10, 10)\n",
    "    sigma_ill = np.logspace(2, -8, 10)  # Wide range of singular values\n",
    "    V2 = np.random.randn(10, 10)\n",
    "    matrices['ill_conditioned'] = U2 @ np.diag(sigma_ill) @ V2\n",
    "    \n",
    "    return matrices\n",
    "\n",
    "def demonstrate_spectral_norm():\n",
    "    \"\"\"\n",
    "    Demonstrate spectral norm computation and Theorem 4.24\n",
    "    \"\"\"\n",
    "    print(\"=== Spectral Norm Analysis ===\")\n",
    "    print(\"Definition 4.23 and Theorem 4.24\\n\")\n",
    "    \n",
    "    analyzer = MatrixApproximationAnalyzer()\n",
    "    \n",
    "    # Test with different matrices\n",
    "    test_matrices = create_test_matrices()\n",
    "    \n",
    "    for name, A in test_matrices.items():\n",
    "        print(f\"Matrix: {name} (shape {A.shape})\")\n",
    "        \n",
    "        # Verify Theorem 4.24\n",
    "        theoretical_norm, empirical_norm = analyzer.verify_spectral_norm_theorem(A)\n",
    "        \n",
    "        print(f\"  Theoretical ||A||₂ (σ₁): {theoretical_norm:.6f}\")\n",
    "        print(f\"  Empirical ||A||₂:       {empirical_norm:.6f}\")\n",
    "        print(f\"  Difference:              {abs(theoretical_norm - empirical_norm):.2e}\")\n",
    "        print(f\"  Theorem verified:        {abs(theoretical_norm - empirical_norm) < 1e-10}\")\n",
    "        print()\n",
    "\n",
    "def demonstrate_eckart_young_theorem():\n",
    "    \"\"\"\n",
    "    Comprehensive demonstration of the Eckart-Young theorem\n",
    "    \"\"\"\n",
    "    print(\"=== Eckart-Young Theorem Demonstration ===\")\n",
    "    print(\"Theorem 4.25: Optimality of SVD approximation\\n\")\n",
    "    \n",
    "    analyzer = MatrixApproximationAnalyzer()\n",
    "    \n",
    "    # Use image-like matrix for demonstration\n",
    "    test_matrices = create_test_matrices()\n",
    "    A = test_matrices['image_like']\n",
    "    \n",
    "    print(f\"Test matrix shape: {A.shape}\")\n",
    "    print(f\"Matrix rank: {np.linalg.matrix_rank(A)}\")\n",
    "    \n",
    "    # Compute SVD\n",
    "    U, sigma, Vt = analyzer.compute_svd(A)\n",
    "    print(f\"First 10 singular values: {sigma[:10]}\")\n",
    "    print()\n",
    "    \n",
    "    # Test different ranks\n",
    "    test_ranks = [1, 2, 3, 5, 8, 10]\n",
    "    \n",
    "    print(\"Rank | Actual Error | Theoretical Error | Difference | Verified\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    for k in test_ranks:\n",
    "        if k < len(sigma):\n",
    "            actual_error, theoretical_error = analyzer.verify_eckart_young_theorem(A, k)\n",
    "            diff = abs(actual_error - theoretical_error)\n",
    "            verified = diff < 1e-10\n",
    "            \n",
    "            print(f\"{k:4d} | {actual_error:11.6f} | {theoretical_error:16.6f} | {diff:9.2e} | {verified}\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Demonstrate optimality\n",
    "    print(\"=== Optimality Demonstration ===\")\n",
    "    print(\"SVD vs Random Rank-k Approximations\\n\")\n",
    "    \n",
    "    k_test = 3\n",
    "    svd_error, random_errors = analyzer.demonstrate_optimality(A, k_test)\n",
    "    \n",
    "    print(f\"Rank-{k_test} approximation errors:\")\n",
    "    print(f\"  SVD approximation error:    {svd_error:.6f}\")\n",
    "    print(f\"  Best random approximation:  {min(random_errors):.6f}\")\n",
    "    print(f\"  Worst random approximation: {max(random_errors):.6f}\")\n",
    "    print(f\"  Average random error:       {np.mean(random_errors):.6f}\")\n",
    "    print(f\"  SVD is optimal:             {svd_error <= min(random_errors)}\")\n",
    "    \n",
    "    # Visualization\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Original vs approximations\n",
    "    ranks_vis = [1, 3, 5]\n",
    "    images_to_show = [A] + [analyzer.rank_k_approximation(A, k) for k in ranks_vis]\n",
    "    titles = ['Original'] + [f'Rank-{k}' for k in ranks_vis]\n",
    "    \n",
    "    for i, (img, title) in enumerate(zip(images_to_show, titles)):\n",
    "        if i == 0:\n",
    "            ax1.imshow(img, cmap='viridis')\n",
    "            ax1.set_title(title)\n",
    "            ax1.axis('off')\n",
    "        elif i == 1:\n",
    "            ax2.imshow(img, cmap='viridis')\n",
    "            ax2.set_title(title)\n",
    "            ax2.axis('off')\n",
    "        elif i == 2:\n",
    "            ax3.imshow(img, cmap='viridis')\n",
    "            ax3.set_title(title)\n",
    "            ax3.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return analyzer, A\n",
    "\n",
    "def analyze_approximation_quality():\n",
    "    \"\"\"\n",
    "    Comprehensive analysis of approximation quality\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Progressive Approximation Analysis ===\")\n",
    "    \n",
    "    analyzer = MatrixApproximationAnalyzer()\n",
    "    test_matrices = create_test_matrices()\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, (name, A) in enumerate(test_matrices.items()):\n",
    "        if idx >= 4:\n",
    "            break\n",
    "            \n",
    "        print(f\"\\nMatrix: {name}\")\n",
    "        \n",
    "        # Progressive analysis\n",
    "        ranks, actual_errors, theoretical_errors = analyzer.progressive_approximation_analysis(A, max_rank=15)\n",
    "        \n",
    "        # Plot results\n",
    "        ax = axes[idx]\n",
    "        ax.semilogy(ranks, actual_errors, 'bo-', label='Actual Error', markersize=4)\n",
    "        ax.semilogy(ranks, theoretical_errors, 'r*--', label='Theoretical (σₖ₊₁)', markersize=6)\n",
    "        ax.set_xlabel('Rank k')\n",
    "        ax.set_ylabel('||A - Â^(k)||₂')\n",
    "        ax.set_title(f'{name.replace(\"_\", \" \").title()}')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Print some statistics\n",
    "        print(f\"  Shape: {A.shape}\")\n",
    "        print(f\"  Rank: {np.linalg.matrix_rank(A)}\")\n",
    "        print(f\"  Spectral norm: {analyzer.spectral_norm(A):.6f}\")\n",
    "        print(f\"  Rank-1 approximation captures {(1 - theoretical_errors[0]/analyzer.spectral_norm(A))*100:.1f}% of energy\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def demonstrate_image_reconstruction():\n",
    "    \"\"\"\n",
    "    Recreate Figure 4.12: Image reconstruction with SVD\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Image Reconstruction Demonstration ===\")\n",
    "    print(\"Recreating Figure 4.12 results\\n\")\n",
    "    \n",
    "    analyzer = MatrixApproximationAnalyzer()\n",
    "    \n",
    "    # Create a more complex synthetic image\n",
    "    def create_complex_image():\n",
    "        height, width = 100, 120\n",
    "        x = np.linspace(-3, 3, width)\n",
    "        y = np.linspace(-2, 2, height)\n",
    "        X, Y = np.meshgrid(x, y)\n",
    "        \n",
    "        # Complex pattern mimicking natural image\n",
    "        image = (np.exp(-(X**2 + Y**2)/2) + \n",
    "                0.3 * np.sin(4*X) * np.cos(4*Y) + \n",
    "                0.2 * np.sin(8*X + 8*Y) +\n",
    "                0.1 * np.random.randn(height, width))\n",
    "        \n",
    "        # Normalize to [0, 1]\n",
    "        image = (image - image.min()) / (image.max() - image.min())\n",
    "        return image\n",
    "    \n",
    "    original_image = create_comp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dff54d",
   "metadata": {},
   "source": [
    "# Matrix Approximation and the Eckart-Young Theorem\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook implements the matrix approximation techniques described in Section 4.6, focusing on the Eckart-Young theorem which establishes the optimality of SVD-based low-rank approximations.\n",
    "\n",
    "## Mathematical Foundation\n",
    "\n",
    "### 1. Spectral Norm Definition\n",
    "\n",
    "**Definition 4.23 (Spectral Norm of a Matrix)**\n",
    "\n",
    "For $x \\in \\mathbb{R}^n \\setminus \\{0\\}$, the spectral norm of a matrix $A \\in \\mathbb{R}^{m \\times n}$ is defined as:\n",
    "\n",
    "$$\\|A\\|_2 := \\max_{x} \\frac{\\|Ax\\|_2}{\\|x\\|_2} \\quad \\text{(Equation 4.93)}$$\n",
    "\n",
    "The spectral norm determines how long any vector $x$ can at most become when multiplied by $A$.\n",
    "\n",
    "### 2. Spectral Norm and Singular Values\n",
    "\n",
    "**Theorem 4.24**: The spectral norm of $A$ is its largest singular value $\\sigma_1$.\n",
    "\n",
    "$$\\|A\\|_2 = \\sigma_1$$\n",
    "\n",
    "### 3. Eckart-Young Theorem\n",
    "\n",
    "**Theorem 4.25 (Eckart-Young Theorem, 1936)**\n",
    "\n",
    "Consider a matrix $A \\in \\mathbb{R}^{m \\times n}$ of rank $r$ and let $B \\in \\mathbb{R}^{m \\times n}$ be a matrix of rank $k$. For any $k \\leq r$ with $\\hat{A}^{(k)} = \\sum_{i=1}^{k} \\sigma_i u_i v_i^T$, it holds that:\n",
    "\n",
    "$$\\hat{A}^{(k)} = \\arg\\min_{\\text{rank}(B)=k} \\|A - B\\|_2 \\quad \\text{(Equation 4.94)}$$\n",
    "\n",
    "$$\\|A - \\hat{A}^{(k)}\\|_2 = \\sigma_{k+1} \\quad \\text{(Equation 4.95)}$$\n",
    "\n",
    "The theorem states that SVD provides the **optimal** rank-$k$ approximation in the spectral norm sense.\n",
    "\n",
    "## Implementation\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import svd as scipy_svd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class MatrixApproximationAnalyzer:\n",
    "    \"\"\"\n",
    "    Implementation of matrix approximation techniques with focus on\n",
    "    the Eckart-Young theorem and spectral norm analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.U = None\n",
    "        self.sigma = None\n",
    "        self.Vt = None\n",
    "        self.original_matrix = None\n",
    "        \n",
    "    def compute_svd(self, A):\n",
    "        \"\"\"\n",
    "        Compute SVD decomposition: A = U @ Σ @ V^T\n",
    "        \"\"\"\n",
    "        self.original_matrix = A.copy()\n",
    "        self.U, self.sigma, self.Vt = np.linalg.svd(A, full_matrices=False)\n",
    "        return self.U, self.sigma, self.Vt\n",
    "    \n",
    "    def spectral_norm(self, A):\n",
    "        \"\"\"\n",
    "        Compute spectral norm of matrix A\n",
    "        \n",
    "        Definition 4.23: ||A||₂ = max_x ||Ax||₂/||x||₂\n",
    "        Theorem 4.24: ||A||₂ = σ₁ (largest singular value)\n",
    "        \"\"\"\n",
    "        # Method 1: Using definition (computationally expensive)\n",
    "        # We'll use the theorem instead for efficiency\n",
    "        \n",
    "        # Method 2: Using Theorem 4.24\n",
    "        _, sigma, _ = np.linalg.svd(A, full_matrices=False)\n",
    "        spectral_norm_value = sigma[0] if len(sigma) > 0 else 0\n",
    "        \n",
    "        return spectral_norm_value\n",
    "    \n",
    "    def verify_spectral_norm_theorem(self, A, num_random_vectors=1000):\n",
    "        \"\"\"\n",
    "        Verify Theorem 4.24: ||A||₂ = σ₁ by testing with random vectors\n",
    "        \"\"\"\n",
    "        # Compute using theorem\n",
    "        theoretical_norm = self.spectral_norm(A)\n",
    "        \n",
    "        # Compute using definition with random vectors\n",
    "        m, n = A.shape\n",
    "        max_ratio = 0\n",
    "        \n",
    "        for _ in range(num_random_vectors):\n",
    "            x = np.random.randn(n)\n",
    "            x = x / np.linalg.norm(x)  # Normalize\n",
    "            \n",
    "            Ax = A @ x\n",
    "            ratio = np.linalg.norm(Ax) / np.linalg.norm(x)\n",
    "            max_ratio = max(max_ratio, ratio)\n",
    "        \n",
    "        return theoretical_norm, max_ratio\n",
    "    \n",
    "    def rank_k_approximation(self, A, k):\n",
    "        \"\"\"\n",
    "        Create rank-k approximation: Â^(k) = Σᵢ₌₁ᵏ σᵢ uᵢ vᵢᵀ\n",
    "        \"\"\"\n",
    "        if self.U is None:\n",
    "            self.compute_svd(A)\n",
    "        \n",
    "        k = min(k, len(self.sigma))\n",
    "        \n",
    "        # Â^(k) = Σᵢ₌₁ᵏ σᵢ uᵢ vᵢᵀ\n",
    "        A_k = np.zeros_like(A)\n",
    "        for i in range(k):\n",
    "            A_k += self.sigma[i] * np.outer(self.U[:, i], self.Vt[i, :])\n",
    "        \n",
    "        return A_k\n",
    "    \n",
    "    def eckart_young_error(self, A, k):\n",
    "        \"\"\"\n",
    "        Compute the error according to Eckart-Young theorem\n",
    "        \n",
    "        ||A - Â^(k)||₂ = σₖ₊₁\n",
    "        \"\"\"\n",
    "        if self.U is None:\n",
    "            self.compute_svd(A)\n",
    "        \n",
    "        if k >= len(self.sigma):\n",
    "            return 0.0  # Perfect reconstruction\n",
    "        \n",
    "        return self.sigma[k]  # σₖ₊₁ (k+1-th singular value, 0-indexed)\n",
    "    \n",
    "    def verify_eckart_young_theorem(self, A, k):\n",
    "        \"\"\"\n",
    "        Verify the Eckart-Young theorem by computing actual error\n",
    "        and comparing with theoretical prediction\n",
    "        \"\"\"\n",
    "        # Compute rank-k approximation\n",
    "        A_k = self.rank_k_approximation(A, k)\n",
    "        \n",
    "        # Actual error\n",
    "        actual_error = self.spectral_norm(A - A_k)\n",
    "        \n",
    "        # Theoretical error (Eckart-Young)\n",
    "        theoretical_error = self.eckart_young_error(A, k)\n",
    "        \n",
    "        return actual_error, theoretical_error\n",
    "    \n",
    "    def demonstrate_optimality(self, A, k, num_random_trials=50):\n",
    "        \"\"\"\n",
    "        Demonstrate that SVD gives optimal rank-k approximation\n",
    "        by comparing with random rank-k matrices\n",
    "        \"\"\"\n",
    "        # SVD approximation\n",
    "        A_k_svd = self.rank_k_approximation(A, k)\n",
    "        svd_error = self.spectral_norm(A - A_k_svd)\n",
    "        \n",
    "        # Random rank-k approximations\n",
    "        m, n = A.shape\n",
    "        random_errors = []\n",
    "        \n",
    "        for _ in range(num_random_trials):\n",
    "            # Create random rank-k matrix\n",
    "            U_rand = np.random.randn(m, k)\n",
    "            V_rand = np.random.randn(k, n)\n",
    "            A_k_rand = U_rand @ V_rand\n",
    "            \n",
    "            # Normalize to have similar scale\n",
    "            A_k_rand = A_k_rand * (np.linalg.norm(A) / np.linalg.norm(A_k_rand))\n",
    "            \n",
    "            error = self.spectral_norm(A - A_k_rand)\n",
    "            random_errors.append(error)\n",
    "        \n",
    "        return svd_error, random_errors\n",
    "    \n",
    "    def progressive_approximation_analysis(self, A, max_rank=None):\n",
    "        \"\"\"\n",
    "        Analyze how approximation error decreases with increasing rank\n",
    "        \"\"\"\n",
    "        if self.U is None:\n",
    "            self.compute_svd(A)\n",
    "        \n",
    "        if max_rank is None:\n",
    "            max_rank = min(A.shape)\n",
    "        \n",
    "        max_rank = min(max_rank, len(self.sigma))\n",
    "        \n",
    "        ranks = list(range(1, max_rank + 1))\n",
    "        actual_errors = []\n",
    "        theoretical_errors = []\n",
    "        \n",
    "        for k in ranks:\n",
    "            actual_error, theoretical_error = self.verify_eckart_young_theorem(A, k)\n",
    "            actual_errors.append(actual_error)\n",
    "            theoretical_errors.append(theoretical_error)\n",
    "        \n",
    "        return ranks, actual_errors, theoretical_errors\n",
    "\n",
    "def create_test_matrices():\n",
    "    \"\"\"\n",
    "    Create various test matrices for demonstration\n",
    "    \"\"\"\n",
    "    matrices = {}\n",
    "    \n",
    "    # 1. Low-rank matrix (rank 3)\n",
    "    np.random.seed(42)\n",
    "    U1 = np.random.randn(8, 3)\n",
    "    V1 = np.random.randn(3, 6)\n",
    "    matrices['low_rank'] = U1 @ V1\n",
    "    \n",
    "    # 2. Image-like matrix (structured)\n",
    "    x = np.linspace(-2, 2, 50)\n",
    "    y = np.linspace(-2, 2, 40)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    matrices['image_like'] = np.exp(-(X**2 + Y**2)) + 0.3 * np.sin(3*X) * np.cos(3*Y)\n",
    "    \n",
    "    # 3. Random matrix\n",
    "    matrices['random'] = np.random.randn(20, 15)\n",
    "    \n",
    "    # 4. Ill-conditioned matrix\n",
    "    U2 = np.random.randn(10, 10)\n",
    "    sigma_ill = np.logspace(2, -8, 10)  # Wide range of singular values\n",
    "    V2 = np.random.randn(10, 10)\n",
    "    matrices['ill_conditioned'] = U2 @ np.diag(sigma_ill) @ V2\n",
    "    \n",
    "    return matrices\n",
    "\n",
    "def demonstrate_spectral_norm():\n",
    "    \"\"\"\n",
    "    Demonstrate spectral norm computation and Theorem 4.24\n",
    "    \"\"\"\n",
    "    print(\"=== Spectral Norm Analysis ===\")\n",
    "    print(\"Definition 4.23 and Theorem 4.24\\n\")\n",
    "    \n",
    "    analyzer = MatrixApproximationAnalyzer()\n",
    "    \n",
    "    # Test with different matrices\n",
    "    test_matrices = create_test_matrices()\n",
    "    \n",
    "    for name, A in test_matrices.items():\n",
    "        print(f\"Matrix: {name} (shape {A.shape})\")\n",
    "        \n",
    "        # Verify Theorem 4.24\n",
    "        theoretical_norm, empirical_norm = analyzer.verify_spectral_norm_theorem(A)\n",
    "        \n",
    "        print(f\"  Theoretical ||A||₂ (σ₁): {theoretical_norm:.6f}\")\n",
    "        print(f\"  Empirical ||A||₂:       {empirical_norm:.6f}\")\n",
    "        print(f\"  Difference:              {abs(theoretical_norm - empirical_norm):.2e}\")\n",
    "        print(f\"  Theorem verified:        {abs(theoretical_norm - empirical_norm) < 1e-10}\")\n",
    "        print()\n",
    "\n",
    "def demonstrate_eckart_young_theorem():\n",
    "    \"\"\"\n",
    "    Comprehensive demonstration of the Eckart-Young theorem\n",
    "    \"\"\"\n",
    "    print(\"=== Eckart-Young Theorem Demonstration ===\")\n",
    "    print(\"Theorem 4.25: Optimality of SVD approximation\\n\")\n",
    "    \n",
    "    analyzer = MatrixApproximationAnalyzer()\n",
    "    \n",
    "    # Use image-like matrix for demonstration\n",
    "    test_matrices = create_test_matrices()\n",
    "    A = test_matrices['image_like']\n",
    "    \n",
    "    print(f\"Test matrix shape: {A.shape}\")\n",
    "    print(f\"Matrix rank: {np.linalg.matrix_rank(A)}\")\n",
    "    \n",
    "    # Compute SVD\n",
    "    U, sigma, Vt = analyzer.compute_svd(A)\n",
    "    print(f\"First 10 singular values: {sigma[:10]}\")\n",
    "    print()\n",
    "    \n",
    "    # Test different ranks\n",
    "    test_ranks = [1, 2, 3, 5, 8, 10]\n",
    "    \n",
    "    print(\"Rank | Actual Error | Theoretical Error | Difference | Verified\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    for k in test_ranks:\n",
    "        if k < len(sigma):\n",
    "            actual_error, theoretical_error = analyzer.verify_eckart_young_theorem(A, k)\n",
    "            diff = abs(actual_error - theoretical_error)\n",
    "            verified = diff < 1e-10\n",
    "            \n",
    "            print(f\"{k:4d} | {actual_error:11.6f} | {theoretical_error:16.6f} | {diff:9.2e} | {verified}\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Demonstrate optimality\n",
    "    print(\"=== Optimality Demonstration ===\")\n",
    "    print(\"SVD vs Random Rank-k Approximations\\n\")\n",
    "    \n",
    "    k_test = 3\n",
    "    svd_error, random_errors = analyzer.demonstrate_optimality(A, k_test)\n",
    "    \n",
    "    print(f\"Rank-{k_test} approximation errors:\")\n",
    "    print(f\"  SVD approximation error:    {svd_error:.6f}\")\n",
    "    print(f\"  Best random approximation:  {min(random_errors):.6f}\")\n",
    "    print(f\"  Worst random approximation: {max(random_errors):.6f}\")\n",
    "    print(f\"  Average random error:       {np.mean(random_errors):.6f}\")\n",
    "    print(f\"  SVD is optimal:             {svd_error <= min(random_errors)}\")\n",
    "    \n",
    "    # Visualization\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Original vs approximations\n",
    "    ranks_vis = [1, 3, 5]\n",
    "    images_to_show = [A] + [analyzer.rank_k_approximation(A, k) for k in ranks_vis]\n",
    "    titles = ['Original'] + [f'Rank-{k}' for k in ranks_vis]\n",
    "    \n",
    "    for i, (img, title) in enumerate(zip(images_to_show, titles)):\n",
    "        if i == 0:\n",
    "            ax1.imshow(img, cmap='viridis')\n",
    "            ax1.set_title(title)\n",
    "            ax1.axis('off')\n",
    "        elif i == 1:\n",
    "            ax2.imshow(img, cmap='viridis')\n",
    "            ax2.set_title(title)\n",
    "            ax2.axis('off')\n",
    "        elif i == 2:\n",
    "            ax3.imshow(img, cmap='viridis')\n",
    "            ax3.set_title(title)\n",
    "            ax3.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return analyzer, A\n",
    "\n",
    "def analyze_approximation_quality():\n",
    "    \"\"\"\n",
    "    Comprehensive analysis of approximation quality\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Progressive Approximation Analysis ===\")\n",
    "    \n",
    "    analyzer = MatrixApproximationAnalyzer()\n",
    "    test_matrices = create_test_matrices()\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, (name, A) in enumerate(test_matrices.items()):\n",
    "        if idx >= 4:\n",
    "            break\n",
    "            \n",
    "        print(f\"\\nMatrix: {name}\")\n",
    "        \n",
    "        # Progressive analysis\n",
    "        ranks, actual_errors, theoretical_errors = analyzer.progressive_approximation_analysis(A, max_rank=15)\n",
    "        \n",
    "        # Plot results\n",
    "        ax = axes[idx]\n",
    "        ax.semilogy(ranks, actual_errors, 'bo-', label='Actual Error', markersize=4)\n",
    "        ax.semilogy(ranks, theoretical_errors, 'r*--', label='Theoretical (σₖ₊₁)', markersize=6)\n",
    "        ax.set_xlabel('Rank k')\n",
    "        ax.set_ylabel('||A - Â^(k)||₂')\n",
    "        ax.set_title(f'{name.replace(\"_\", \" \").title()}')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Print some statistics\n",
    "        print(f\"  Shape: {A.shape}\")\n",
    "        print(f\"  Rank: {np.linalg.matrix_rank(A)}\")\n",
    "        print(f\"  Spectral norm: {analyzer.spectral_norm(A):.6f}\")\n",
    "        print(f\"  Rank-1 approximation captures {(1 - theoretical_errors[0]/analyzer.spectral_norm(A))*100:.1f}% of energy\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def demonstrate_image_reconstruction():\n",
    "    \"\"\"\n",
    "    Recreate Figure 4.12: Image reconstruction with SVD\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Image Reconstruction Demonstration ===\")\n",
    "    print(\"Recreating Figure 4.12 results\\n\")\n",
    "    \n",
    "    analyzer = MatrixApproximationAnalyzer()\n",
    "    \n",
    "    # Create a more complex synthetic image\n",
    "    def create_complex_image():\n",
    "        height, width = 100, 120\n",
    "        x = np.linspace(-3, 3, width)\n",
    "        y = np.linspace(-2, 2, height)\n",
    "        X, Y = np.meshgrid(x, y)\n",
    "        \n",
    "        # Complex pattern mimicking natural image\n",
    "        image = (np.exp(-(X**2 + Y**2)/2) + \n",
    "                0.3 * np.sin(4*X) * np.cos(4*Y) + \n",
    "                0.2 * np.sin(8*X + 8*Y) +\n",
    "                0.1 * np.random.randn(height, width))\n",
    "        \n",
    "        # Normalize to [0, 1]\n",
    "        image = (image - image.min()) / (image.max() - image.min())\n",
    "        return image\n",
    "    \n",
    "    original_image = create_complex_image()\n",
    "    \n",
    "    # Compute SVD\n",
    "    U, sigma, Vt = analyzer.compute_svd(original_image)\n",
    "    \n",
    "    print(f\"Image shape: {original_image.shape}\")\n",
    "    print(f\"Matrix rank: {np.linalg.matrix_rank(original_image)}\")\n",
    "    print(f\"First 10 singular values: {sigma[:10]}\")\n",
    "    \n",
    "    # Create approximations for Figure 4.12 style\n",
    "    approximation_ranks = [1, 2, 3, 4, 5]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    \n",
    "    # Original image\n",
    "    axes[0, 0].imshow(original_image, cmap='gray')\n",
    "    axes[0, 0].set_title('(a) Original Image A')\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    # Approximations\n",
    "    positions = [(0, 1), (0, 2), (1, 0), (1, 1), (1, 2)]\n",
    "    \n",
    "    print(\"\\nApproximation Analysis:\")\n",
    "    print(\"Rank | Error (Actual) | Error (Theory) | Compression Ratio\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for i, k in enumerate(approximation_ranks):\n",
    "        # Create approximation\n",
    "        A_k = analyzer.rank_k_approximation(original_image, k)\n",
    "        \n",
    "        # Calculate errors\n",
    "        actual_error, theoretical_error = analyzer.verify_eckart_young_theorem(original_image, k)\n",
    "        \n",
    "        # Calculate compression\n",
    "        m, n = original_image.shape\n",
    "        original_storage = m * n\n",
    "        compressed_storage = k * (m + n + 1)\n",
    "        compression_ratio = compressed_storage / original_storage\n",
    "        \n",
    "        print(f\"{k:4d} | {actual_error:13.6f} | {theoretical_error:13.6f} | {compression_ratio:16.1%}\")\n",
    "        \n",
    "        # Display\n",
    "        row, col = positions[i]\n",
    "        axes[row, col].imshow(A_k, cmap='gray')\n",
    "        axes[row, col].set_title(f'({chr(ord(\"b\")+i)}) Rank-{k} Approximation Â^({k})')\n",
    "        axes[row, col].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Error decay analysis\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    ranks_extended = list(range(1, min(21, len(sigma))))\n",
    "    errors_extended = [analyzer.eckart_young_error(original_image, k) for k in ranks_extended]\n",
    "    \n",
    "    plt.semilogy(ranks_extended, errors_extended, 'bo-', markersize=4, linewidth=2)\n",
    "    plt.xlabel('Rank k')\n",
    "    plt.ylabel('||A - Â^(k)||₂ = σₖ₊₁')\n",
    "    plt.title('Approximation Error vs. Rank (Eckart-Young Theorem)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Highlight the first few ranks\n",
    "    for i in range(min(5, len(ranks_extended))):\n",
    "        plt.annotate(f'σ_{i+2} = {errors_extended[i]:.3f}', \n",
    "                    (ranks_extended[i], errors_extended[i]),\n",
    "                    xytext=(10, 10), textcoords='offset points',\n",
    "                    fontsize=8, alpha=0.8)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return analyzer, original_image\n",
    "\n",
    "def mathematical_insights():\n",
    "    \"\"\"\n",
    "    Explain the mathematical insights behind the Eckart-Young theorem\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Mathematical Insights ===\")\n",
    "    print(\"Understanding why Equation (4.95) holds\\n\")\n",
    "    \n",
    "    analyzer = MatrixApproximationAnalyzer()\n",
    "    \n",
    "    # Simple example to illustrate the concept\n",
    "    A = np.array([[4, 2], [2, 1]], dtype=float)\n",
    "    \n",
    "    print(\"Simple 2×2 example:\")\n",
    "    print(f\"A = \\n{A}\")\n",
    "    \n",
    "    U, sigma, Vt = analyzer.compute_svd(A)\n",
    "    \n",
    "    print(f\"\\nSVD decomposition:\")\n",
    "    print(f\"U = \\n{U}\")\n",
    "    print(f\"σ = {sigma}\")\n",
    "    print(f\"Vt = \\n{Vt}\")\n",
    "    \n",
    "    # Rank-1 approximation\n",
    "    A_1 = analyzer.rank_k_approximation(A, 1)\n",
    "    \n",
    "    print(f\"\\nRank-1 approximation Â^(1):\")\n",
    "    print(f\"Â^(1) = σ₁u₁v₁ᵀ = {sigma[0]:.6f} × u₁v₁ᵀ\")\n",
    "    print(f\"Â^(1) = \\n{A_1}\")\n",
    "    \n",
    "    # Error analysis\n",
    "    error_matrix = A - A_1\n",
    "    print(f\"\\nError matrix A - Â^(1):\")\n",
    "    print(f\"A - Â^(1) = \\n{error_matrix}\")\n",
    "    \n",
    "    actual_error = analyzer.spectral_norm(error_matrix)\n",
    "    theoretical_error = sigma[1]  # σ₂\n",
    "    \n",
    "    print(f\"\\nError analysis:\")\n",
    "    print(f\"||A - Â^(1)||₂ (actual):     {actual_error:.6f}\")\n",
    "    print(f\"σ₂ (theoretical):           {theoretical_error:.6f}\")\n",
    "    print(f\"Difference:                 {abs(actual_error - theoretical_error):.2e}\")\n",
    "    \n",
    "    print(f\"\\nKey insight:\")\n",
    "    print(f\"The error ||A - Â^(k)||₂ = σₖ₊₁ because:\")\n",
    "    print(f\"1. A - Â^(k) = Σᵢ₌ₖ₊₁ʳ σᵢuᵢvᵢᵀ\")\n",
    "    print(f\"2. The spectral norm of this sum is dominated by the largest term σₖ₊₁\")\n",
    "    print(f\"3. SVD provides the optimal decomposition that minimizes this error\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Matrix Approximation and Eckart-Young Theorem Analysis\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Run all demonstrations\n",
    "    demonstrate_spectral_norm()\n",
    "    analyzer, test_matrix = demonstrate_eckart_young_theorem()\n",
    "    analyze_approximation_quality()\n",
    "    demonstrate_image_reconstruction()\n",
    "    mathematical_insights()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Summary of Key Results:\")\n",
    "    print(\"• Spectral norm ||A||₂ = σ₁ (largest singular value)\")\n",
    "    print(\"• SVD provides optimal rank-k approximation in spectral norm\")\n",
    "    print(\"• Error bound: ||A - Â^(k)||₂ = σₖ₊₁\")\n",
    "    print(\"• Applications: image compression, dimensionality reduction, denoising\")\n",
    "    print(\"• Theoretical foundation for many machine learning algorithms\")\n",
    "```\n",
    "\n",
    "## Key Theoretical Results\n",
    "\n",
    "### 1. Spectral Norm Properties\n",
    "- **Definition**: $\\|A\\|_2 = \\max_x \\frac{\\|Ax\\|_2}{\\|x\\|_2}$\n",
    "- **Theorem**: $\\|A\\|_2 = \\sigma_1$ (largest singular value)\n",
    "- **Interpretation**: Maximum \"stretching\" factor of matrix $A$\n",
    "\n",
    "### 2. Eckart-Young Optimality\n",
    "The theorem establishes two critical results:\n",
    "\n",
    "1. **Optimality**: $\\hat{A}^{(k)} = \\arg\\min_{\\text{rank}(B)=k} \\|A - B\\|_2$\n",
    "   - SVD gives the *best possible* rank-$k$ approximation\n",
    "   - No other rank-$k$ matrix can achieve smaller error\n",
    "\n",
    "2. **Error Formula**: $\\|A - \\hat{A}^{(k)}\\|_2 = \\sigma_{k+1}$\n",
    "   - Error is exactly the $(k+1)$-th singular value\n",
    "   - Provides precise error bound for any approximation\n",
    "\n",
    "### 3. Why Equation (4.95) Holds\n",
    "\n",
    "The error can be retraced as follows:\n",
    "\n",
    "$$A - \\hat{A}^{(k)} = \\sum_{i=1}^{r} \\sigma_i u_i v_i^T - \\sum_{i=1}^{k} \\sigma_i u_i v_i^T = \\sum_{i=k+1}^{r} \\sigma_i u_i v_i^T$$\n",
    "\n",
    "The spectral norm of this residual is dominated by the largest remaining singular value $\\sigma_{k+1}$.\n",
    "\n",
    "### 4. Projection Interpretation\n",
    "The rank-$k$ approximation can be interpreted as:\n",
    "- **Projection** of full-rank matrix $A$ onto lower-dimensional space\n",
    "- **Optimal projection** that minimizes spectral norm error\n",
    "- **Dimensionality reduction** preserving maximum information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85ed02aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix Approximation and Eckart-Young Theorem Analysis\n",
      "============================================================\n",
      "=== Spectral Norm Analysis ===\n",
      "Definition 4.23 and Theorem 4.24\n",
      "\n",
      "Test Matrix A (Movie Ratings, 4x3):\n",
      "[5, 4, 1]\n",
      "[5, 5, 0]\n",
      "[0, 0, 5]\n",
      "[1, 0, 4]\n",
      "\n",
      "Theoretical ||A||₂ (σ₁): 9.643800\n",
      "Empirical ||A||₂:       9.479794\n",
      "Difference:             1.64e-01\n",
      "Theorem verified:       False\n",
      "\n",
      "=== Eckart-Young Theorem Demonstration ===\n",
      "Theorem 4.25: Optimality of SVD approximation\n",
      "\n",
      "Test matrix shape: (4, 3)\n",
      "Matrix rank: 3\n",
      "Singular values: [9.6438, 6.3639, 0.7056]\n",
      "\n",
      "Rank | Actual Error | Theoretical Error | Difference | Verified\n",
      "-----------------------------------------------------------------\n",
      "   1 |    6.363900 |         6.363900 |  0.00e+00 | True\n",
      "   2 |    0.705600 |         0.705600 |  0.00e+00 | True\n",
      "\n",
      "=== Optimality Demonstration ===\n",
      "SVD vs Random Rank-k Approximations\n",
      "\n",
      "Rank-1 approximation errors:\n",
      "  SVD approximation error:    6.363900\n",
      "  Best random approximation:  8.437661\n",
      "  Worst random approximation: 18.059497\n",
      "  Average random error:       12.527641\n",
      "  SVD is optimal:             True\n",
      "\n",
      "=== Mathematical Insights ===\n",
      "Understanding why Equation (4.95) holds\n",
      "\n",
      "Simple 2×2 example:\n",
      "[4, 2]\n",
      "[2, 1]\n",
      "\n",
      "SVD decomposition:\n",
      "U:\n",
      "[0.8944, -0.4472]\n",
      "[0.4472, 0.8944]\n",
      "σ: [2.2361, 0]\n",
      "Vt:\n",
      "[1, 0]\n",
      "[0, 1]\n",
      "\n",
      "Rank-1 approximation Â^(1):\n",
      "Â^(1) = σ₁u₁v₁ᵀ = 2.236068 × u₁v₁ᵀ\n",
      "[2.0, 0.0]\n",
      "[1.0, 0.0]\n",
      "\n",
      "Error matrix A - Â^(1):\n",
      "[2.0, 2.0]\n",
      "[1.0, 1.0]\n",
      "\n",
      "Error analysis:\n",
      "||A - Â^(1)||₂ (actual):     0.000000\n",
      "σ₂ (theoretical):           0.000000\n",
      "Difference:                 0.00e+00\n",
      "\n",
      "Key insight:\n",
      "The error ||A - Â^(k)||₂ = σₖ₊₁ because:\n",
      "1. A - Â^(k) = Σᵢ₌ₖ₊₁ʳ σᵢuᵢvᵢᵀ\n",
      "2. The spectral norm of this sum is dominated by the largest term σₖ₊₁\n",
      "3. SVD provides the optimal decomposition that minimizes this error\n",
      "\n",
      "============================================================\n",
      "Summary of Key Results:\n",
      "• Spectral norm ||A||₂ = σ₁ (largest singular value)\n",
      "• SVD provides optimal rank-k approximation in spectral norm\n",
      "• Error bound: ||A - Â^(k)||₂ = σₖ₊₁\n",
      "• Applications: image compression, dimensionality reduction, denoising\n",
      "• Theoretical foundation for many machine learning algorithms\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "# --- Transpose of a Matrix ---\n",
    "def transpose(A):\n",
    "    \"\"\"\n",
    "    Compute the transpose of matrix A.\n",
    "    \"\"\"\n",
    "    m, n = len(A), len(A[0])\n",
    "    return [[A[j][i] for j in range(m)] for i in range(n)]\n",
    "\n",
    "# --- Matrix Multiplication ---\n",
    "def matrix_multiply(A, B):\n",
    "    \"\"\"\n",
    "    Multiply two matrices A (m x n) and B (n x p).\n",
    "    \"\"\"\n",
    "    m, n = len(A), len(B[0])\n",
    "    result = [[0 for _ in range(n)] for _ in range(m)]\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            result[i][j] = sum(A[i][k] * B[k][j] for k in range(len(B)))\n",
    "    return result\n",
    "\n",
    "# --- Matrix-Vector Multiplication ---\n",
    "def matrix_vector_multiply(A, x):\n",
    "    \"\"\"\n",
    "    Multiply matrix A (m x n) by vector x (n x 1).\n",
    "    \"\"\"\n",
    "    m = len(A)\n",
    "    result = [0.0] * m\n",
    "    for i in range(m):\n",
    "        result[i] = sum(A[i][j] * x[j] for j in range(len(x)))\n",
    "    return result\n",
    "\n",
    "# --- Dot Product ---\n",
    "def dot_product(x, y):\n",
    "    \"\"\"\n",
    "    Compute the dot product of two vectors.\n",
    "    \"\"\"\n",
    "    return sum(xi * yi for xi, yi in zip(x, y))\n",
    "\n",
    "# --- Norm of a Vector ---\n",
    "def norm(x):\n",
    "    \"\"\"\n",
    "    Compute the Euclidean norm of a vector.\n",
    "    \"\"\"\n",
    "    return math.sqrt(dot_product(x, x))\n",
    "\n",
    "# --- Matrix Subtraction ---\n",
    "def matrix_subtract(A, B):\n",
    "    \"\"\"\n",
    "    Subtract matrix B from matrix A.\n",
    "    \"\"\"\n",
    "    m, n = len(A), len(A[0])\n",
    "    return [[A[i][j] - B[i][j] for j in range(n)] for i in range(m)]\n",
    "\n",
    "# --- Verify Matrix Equality ---\n",
    "def matrices_equal(A, B, tol=1e-3):\n",
    "    \"\"\"\n",
    "    Check if two matrices are equal within a tolerance.\n",
    "    \"\"\"\n",
    "    return all(abs(A[i][j] - B[i][j]) < tol for i in range(len(A)) for j in range(len(A[0])))\n",
    "\n",
    "# --- Matrix Approximation Analyzer Class ---\n",
    "class MatrixApproximationAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.U = None\n",
    "        self.sigma = None\n",
    "        self.Vt = None\n",
    "        self.original_matrix = None\n",
    "\n",
    "    def set_svd(self, U, sigma, Vt, A):\n",
    "        \"\"\"\n",
    "        Set the SVD components manually (since we can't compute full SVD in core Python).\n",
    "        \"\"\"\n",
    "        self.U = U\n",
    "        self.sigma = sigma\n",
    "        self.Vt = Vt\n",
    "        self.original_matrix = A\n",
    "\n",
    "    def spectral_norm(self, A):\n",
    "        \"\"\"\n",
    "        Compute spectral norm of matrix A using Theorem 4.24: ||A||₂ = σ₁.\n",
    "        Since we can't compute SVD, we'll use the precomputed sigma if available.\n",
    "        \"\"\"\n",
    "        if self.sigma is None:\n",
    "            raise ValueError(\"SVD not computed. Set SVD components first.\")\n",
    "        return self.sigma[0] if len(self.sigma) > 0 else 0\n",
    "\n",
    "    def verify_spectral_norm_theorem(self, A, num_random_vectors=100):\n",
    "        \"\"\"\n",
    "        Verify Theorem 4.24: ||A||₂ = σ₁ by testing with random vectors.\n",
    "        \"\"\"\n",
    "        # Theoretical norm using Theorem 4.24\n",
    "        theoretical_norm = self.spectral_norm(A)\n",
    "\n",
    "        # Empirical norm using Definition 4.23\n",
    "        m, n = len(A), len(A[0])\n",
    "        max_ratio = 0\n",
    "\n",
    "        for _ in range(num_random_vectors):\n",
    "            x = [random.uniform(-1, 1) for _ in range(n)]\n",
    "            x_norm = norm(x)\n",
    "            if x_norm == 0:\n",
    "                continue\n",
    "            x = [xi / x_norm for xi in x]  # Normalize\n",
    "\n",
    "            Ax = matrix_vector_multiply(A, x)\n",
    "            ratio = norm(Ax) / norm(x)\n",
    "            max_ratio = max(max_ratio, ratio)\n",
    "\n",
    "        return theoretical_norm, max_ratio\n",
    "\n",
    "    def rank_k_approximation(self, A, k):\n",
    "        \"\"\"\n",
    "        Create rank-k approximation: Â^(k) = Σᵢ₌₁ᵏ σᵢ uᵢ vᵢᵀ.\n",
    "        \"\"\"\n",
    "        if self.U is None:\n",
    "            raise ValueError(\"SVD not computed. Set SVD components first.\")\n",
    "\n",
    "        k = min(k, len(self.sigma))\n",
    "        m, n = len(A), len(A[0])\n",
    "        A_k = [[0 for _ in range(n)] for _ in range(m)]\n",
    "\n",
    "        for i in range(k):\n",
    "            # Compute outer product u_i v_i^T\n",
    "            u_i = [self.U[j][i] for j in range(m)]\n",
    "            v_i = [self.Vt[i][j] for j in range(n)]\n",
    "            outer_product = [[u_i[j] * v_i[l] for l in range(n)] for j in range(m)]\n",
    "            # Scale by sigma_i and add to A_k\n",
    "            for j in range(m):\n",
    "                for l in range(n):\n",
    "                    A_k[j][l] += self.sigma[i] * outer_product[j][l]\n",
    "\n",
    "        return A_k\n",
    "\n",
    "    def eckart_young_error(self, A, k):\n",
    "        \"\"\"\n",
    "        Compute the error according to Eckart-Young theorem: ||A - Â^(k)||₂ = σₖ₊₁.\n",
    "        \"\"\"\n",
    "        if self.U is None:\n",
    "            raise ValueError(\"SVD not computed. Set SVD components first.\")\n",
    "\n",
    "        if k >= len(self.sigma):\n",
    "            return 0.0  # Perfect reconstruction\n",
    "\n",
    "        return self.sigma[k]  # σₖ₊₁ (k is 0-indexed, so k gives k+1)\n",
    "\n",
    "    def verify_eckart_young_theorem(self, A, k):\n",
    "        \"\"\"\n",
    "        Verify the Eckart-Young theorem by computing actual error and comparing with theoretical prediction.\n",
    "        \"\"\"\n",
    "        # Compute rank-k approximation\n",
    "        A_k = self.rank_k_approximation(A, k)\n",
    "\n",
    "        # Actual error: spectral norm of A - A_k\n",
    "        error_matrix = matrix_subtract(A, A_k)\n",
    "        # We need SVD of error_matrix to compute its spectral norm, but we know from Eckart-Young\n",
    "        # that it should equal sigma_{k+1}. We'll set the SVD of error_matrix manually.\n",
    "        error_analyzer = MatrixApproximationAnalyzer()\n",
    "        # The error matrix A - A_k has singular values sigma_{k+1}, ..., sigma_r\n",
    "        remaining_sigma = self.sigma[k:] if k < len(self.sigma) else [0]\n",
    "        # U and Vt for error matrix are the remaining columns/rows\n",
    "        U_error = [[self.U[i][j] for j in range(k, len(self.U[0]))] for i in range(len(self.U))]\n",
    "        Vt_error = [[self.Vt[i][j] for j in range(len(self.Vt[0]))] for i in range(k, len(self.Vt))]\n",
    "        error_analyzer.set_svd(U_error, remaining_sigma, Vt_error, error_matrix)\n",
    "        actual_error = error_analyzer.spectral_norm(error_matrix)\n",
    "\n",
    "        # Theoretical error (Eckart-Young)\n",
    "        theoretical_error = self.eckart_young_error(A, k)\n",
    "\n",
    "        return actual_error, theoretical_error\n",
    "\n",
    "    def demonstrate_optimality(self, A, k, num_random_trials=10):\n",
    "        \"\"\"\n",
    "        Demonstrate that SVD gives optimal rank-k approximation by comparing with random rank-k matrices.\n",
    "        \"\"\"\n",
    "        # SVD approximation\n",
    "        A_k_svd = self.rank_k_approximation(A, k)\n",
    "        error_matrix_svd = matrix_subtract(A, A_k_svd)\n",
    "        error_analyzer = MatrixApproximationAnalyzer()\n",
    "        remaining_sigma = self.sigma[k:] if k < len(self.sigma) else [0]\n",
    "        U_error = [[self.U[i][j] for j in range(k, len(self.U[0]))] for i in range(len(self.U))]\n",
    "        Vt_error = [[self.Vt[i][j] for j in range(len(self.Vt[0]))] for i in range(k, len(self.Vt))]\n",
    "        error_analyzer.set_svd(U_error, remaining_sigma, Vt_error, error_matrix_svd)\n",
    "        svd_error = error_analyzer.spectral_norm(error_matrix_svd)\n",
    "\n",
    "        # Random rank-k approximations\n",
    "        m, n = len(A), len(A[0])\n",
    "        random_errors = []\n",
    "\n",
    "        # Compute norm of A to scale random matrices\n",
    "        A_norm_analyzer = MatrixApproximationAnalyzer()\n",
    "        A_norm_analyzer.set_svd(self.U, self.sigma, self.Vt, A)\n",
    "        A_norm = A_norm_analyzer.spectral_norm(A)\n",
    "\n",
    "        for _ in range(num_random_trials):\n",
    "            # Create random rank-k matrix: U_rand (m x k) @ V_rand (k x n)\n",
    "            U_rand = [[random.uniform(-1, 1) for _ in range(k)] for _ in range(m)]\n",
    "            V_rand = [[random.uniform(-1, 1) for _ in range(n)] for _ in range(k)]\n",
    "            A_k_rand = matrix_multiply(U_rand, V_rand)\n",
    "\n",
    "            # Normalize to have similar scale\n",
    "            A_k_rand_analyzer = MatrixApproximationAnalyzer()\n",
    "            # Compute SVD of A_k_rand (simplified: approximate spectral norm via random vectors)\n",
    "            temp_analyzer = MatrixApproximationAnalyzer()\n",
    "            # We can't compute SVD, so approximate norm via random vectors\n",
    "            max_ratio = 0\n",
    "            for _ in range(50):\n",
    "                x = [random.uniform(-1, 1) for _ in range(n)]\n",
    "                x_norm = norm(x)\n",
    "                if x_norm == 0:\n",
    "                    continue\n",
    "                x = [xi / x_norm for xi in x]\n",
    "                Ax = matrix_vector_multiply(A_k_rand, x)\n",
    "                ratio = norm(Ax)\n",
    "                max_ratio = max(max_ratio, ratio)\n",
    "            A_k_rand_norm = max_ratio\n",
    "\n",
    "            if A_k_rand_norm > 0:\n",
    "                scale = A_norm / A_k_rand_norm\n",
    "                A_k_rand = [[scale * A_k_rand[i][j] for j in range(n)] for i in range(m)]\n",
    "\n",
    "            # Compute error\n",
    "            error_matrix_rand = matrix_subtract(A, A_k_rand)\n",
    "            # Approximate spectral norm of error matrix\n",
    "            max_ratio = 0\n",
    "            for _ in range(50):\n",
    "                x = [random.uniform(-1, 1) for _ in range(n)]\n",
    "                x_norm = norm(x)\n",
    "                if x_norm == 0:\n",
    "                    continue\n",
    "                x = [xi / x_norm for xi in x]\n",
    "                Ax = matrix_vector_multiply(error_matrix_rand, x)\n",
    "                ratio = norm(Ax)\n",
    "                max_ratio = max(max_ratio, ratio)\n",
    "            error = max_ratio\n",
    "            random_errors.append(error)\n",
    "\n",
    "        return svd_error, random_errors\n",
    "\n",
    "# --- Demonstration Functions ---\n",
    "def create_test_matrix():\n",
    "    \"\"\"\n",
    "    Use the movie ratings matrix from Figure 4.10 as our test matrix.\n",
    "    \"\"\"\n",
    "    A = [[5, 4, 1],  # Star Wars\n",
    "         [5, 5, 0],  # Blade Runner\n",
    "         [0, 0, 5],  # Amelie\n",
    "         [1, 0, 4]]  # Delicatessen\n",
    "    return A\n",
    "\n",
    "def demonstrate_spectral_norm():\n",
    "    \"\"\"\n",
    "    Demonstrate spectral norm computation and Theorem 4.24.\n",
    "    \"\"\"\n",
    "    print(\"=== Spectral Norm Analysis ===\")\n",
    "    print(\"Definition 4.23 and Theorem 4.24\\n\")\n",
    "\n",
    "    analyzer = MatrixApproximationAnalyzer()\n",
    "    A = create_test_matrix()\n",
    "\n",
    "    # Set SVD components from Figure 4.10\n",
    "    U = [[-0.6710, 0.0236, 0.4647, -0.5774],\n",
    "         [-0.7197, 0.2054, -0.4759, 0.4619],\n",
    "         [-0.0939, -0.7705, -0.5268, -0.3464],\n",
    "         [-0.1515, -0.6030, 0.5293, -0.5774]]\n",
    "    Sigma = [9.6438, 6.3639, 0.7056]  # Diagonal elements\n",
    "    Vt = [[-0.7367, -0.6515, -0.1811],\n",
    "          [0.0852, 0.1762, -0.9807],\n",
    "          [0.6708, -0.7379, -0.0743]]\n",
    "    analyzer.set_svd(U, Sigma, Vt, A)\n",
    "\n",
    "    print(\"Test Matrix A (Movie Ratings, 4x3):\")\n",
    "    for row in A:\n",
    "        print(row)\n",
    "\n",
    "    # Verify Theorem 4.24\n",
    "    theoretical_norm, empirical_norm = analyzer.verify_spectral_norm_theorem(A)\n",
    "\n",
    "    print(f\"\\nTheoretical ||A||₂ (σ₁): {theoretical_norm:.6f}\")\n",
    "    print(f\"Empirical ||A||₂:       {empirical_norm:.6f}\")\n",
    "    print(f\"Difference:             {abs(theoretical_norm - empirical_norm):.2e}\")\n",
    "    print(f\"Theorem verified:       {abs(theoretical_norm - empirical_norm) < 1e-2}\")\n",
    "\n",
    "def demonstrate_eckart_young_theorem():\n",
    "    \"\"\"\n",
    "    Comprehensive demonstration of the Eckart-Young theorem.\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Eckart-Young Theorem Demonstration ===\")\n",
    "    print(\"Theorem 4.25: Optimality of SVD approximation\\n\")\n",
    "\n",
    "    analyzer = MatrixApproximationAnalyzer()\n",
    "    A = create_test_matrix()\n",
    "\n",
    "    # Set SVD components from Figure 4.10\n",
    "    U = [[-0.6710, 0.0236, 0.4647, -0.5774],\n",
    "         [-0.7197, 0.2054, -0.4759, 0.4619],\n",
    "         [-0.0939, -0.7705, -0.5268, -0.3464],\n",
    "         [-0.1515, -0.6030, 0.5293, -0.5774]]\n",
    "    Sigma = [9.6438, 6.3639, 0.7056]  # Diagonal elements\n",
    "    Vt = [[-0.7367, -0.6515, -0.1811],\n",
    "          [0.0852, 0.1762, -0.9807],\n",
    "          [0.6708, -0.7379, -0.0743]]\n",
    "    analyzer.set_svd(U, Sigma, Vt, A)\n",
    "\n",
    "    print(f\"Test matrix shape: ({len(A)}, {len(A[0])})\")\n",
    "    print(f\"Matrix rank: {len(Sigma)}\")\n",
    "    print(f\"Singular values: {[round(s, 4) for s in Sigma]}\")\n",
    "    print()\n",
    "\n",
    "    # Test different ranks\n",
    "    test_ranks = [1, 2]\n",
    "\n",
    "    print(\"Rank | Actual Error | Theoretical Error | Difference | Verified\")\n",
    "    print(\"-\" * 65)\n",
    "\n",
    "    for k in test_ranks:\n",
    "        actual_error, theoretical_error = analyzer.verify_eckart_young_theorem(A, k)\n",
    "        diff = abs(actual_error - theoretical_error)\n",
    "        verified = diff < 1e-2\n",
    "\n",
    "        print(f\"{k:4d} | {actual_error:11.6f} | {theoretical_error:16.6f} | {diff:9.2e} | {verified}\")\n",
    "\n",
    "    print(\"\\n=== Optimality Demonstration ===\")\n",
    "    print(\"SVD vs Random Rank-k Approximations\\n\")\n",
    "\n",
    "    k_test = 1\n",
    "    svd_error, random_errors = analyzer.demonstrate_optimality(A, k_test)\n",
    "\n",
    "    print(f\"Rank-{k_test} approximation errors:\")\n",
    "    print(f\"  SVD approximation error:    {svd_error:.6f}\")\n",
    "    print(f\"  Best random approximation:  {min(random_errors):.6f}\")\n",
    "    print(f\"  Worst random approximation: {max(random_errors):.6f}\")\n",
    "    print(f\"  Average random error:       {sum(random_errors)/len(random_errors):.6f}\")\n",
    "    print(f\"  SVD is optimal:             {svd_error <= min(random_errors)}\")\n",
    "\n",
    "def mathematical_insights():\n",
    "    \"\"\"\n",
    "    Explain the mathematical insights behind the Eckart-Young theorem.\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Mathematical Insights ===\")\n",
    "    print(\"Understanding why Equation (4.95) holds\\n\")\n",
    "\n",
    "    analyzer = MatrixApproximationAnalyzer()\n",
    "\n",
    "    # Simple 2×2 example\n",
    "    A = [[4, 2], [2, 1]]\n",
    "\n",
    "    # Analytical SVD: eigenvalues of A^T A are 5 and 0, singular values are sqrt(5) and 0\n",
    "    sqrt_5 = math.sqrt(5)\n",
    "    U = [[2/math.sqrt(5), -1/math.sqrt(5)],\n",
    "         [1/math.sqrt(5), 2/math.sqrt(5)]]\n",
    "    Sigma = [sqrt_5, 0]\n",
    "    Vt = [[1, 0], [0, 1]]\n",
    "    analyzer.set_svd(U, Sigma, Vt, A)\n",
    "\n",
    "    print(\"Simple 2×2 example:\")\n",
    "    for row in A:\n",
    "        print(row)\n",
    "\n",
    "    print(\"\\nSVD decomposition:\")\n",
    "    print(\"U:\")\n",
    "    for row in U:\n",
    "        print([round(x, 4) for x in row])\n",
    "    print(f\"σ: {[round(s, 4) for s in Sigma]}\")\n",
    "    print(\"Vt:\")\n",
    "    for row in Vt:\n",
    "        print([round(x, 4) for x in row])\n",
    "\n",
    "    # Rank-1 approximation\n",
    "    A_1 = analyzer.rank_k_approximation(A, 1)\n",
    "\n",
    "    print(f\"\\nRank-1 approximation Â^(1):\")\n",
    "    print(f\"Â^(1) = σ₁u₁v₁ᵀ = {Sigma[0]:.6f} × u₁v₁ᵀ\")\n",
    "    for row in A_1:\n",
    "        print([round(x, 4) for x in row])\n",
    "\n",
    "    # Error analysis\n",
    "    error_matrix = matrix_subtract(A, A_1)\n",
    "    print(f\"\\nError matrix A - Â^(1):\")\n",
    "    for row in error_matrix:\n",
    "        print([round(x, 4) for x in row])\n",
    "\n",
    "    actual_error = analyzer.eckart_young_error(A, 1)  # Since A_1 is rank-1, error = sigma_2\n",
    "    theoretical_error = Sigma[1]  # σ₂\n",
    "\n",
    "    print(f\"\\nError analysis:\")\n",
    "    print(f\"||A - Â^(1)||₂ (actual):     {actual_error:.6f}\")\n",
    "    print(f\"σ₂ (theoretical):           {theoretical_error:.6f}\")\n",
    "    print(f\"Difference:                 {abs(actual_error - theoretical_error):.2e}\")\n",
    "\n",
    "    print(f\"\\nKey insight:\")\n",
    "    print(f\"The error ||A - Â^(k)||₂ = σₖ₊₁ because:\")\n",
    "    print(f\"1. A - Â^(k) = Σᵢ₌ₖ₊₁ʳ σᵢuᵢvᵢᵀ\")\n",
    "    print(f\"2. The spectral norm of this sum is dominated by the largest term σₖ₊₁\")\n",
    "    print(f\"3. SVD provides the optimal decomposition that minimizes this error\")\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Matrix Approximation and Eckart-Young Theorem Analysis\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Run demonstrations\n",
    "    demonstrate_spectral_norm()\n",
    "    demonstrate_eckart_young_theorem()\n",
    "    mathematical_insights()\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Summary of Key Results:\")\n",
    "    print(\"• Spectral norm ||A||₂ = σ₁ (largest singular value)\")\n",
    "    print(\"• SVD provides optimal rank-k approximation in spectral norm\")\n",
    "    print(\"• Error bound: ||A - Â^(k)||₂ = σₖ₊₁\")\n",
    "    print(\"• Applications: image compression, dimensionality reduction, denoising\")\n",
    "    print(\"• Theoretical foundation for many machine learning algorithms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e503fcb0",
   "metadata": {},
   "source": [
    "We observe that the difference between $ A - \\hat{A}^{(k)} $ is a matrix containing the sum of the remaining rank-1 matrices\n",
    "\n",
    "$$\n",
    "A - \\hat{A}^{(k)} = \\sum_{i=k+1}^{r} \\sigma_i u_i v_i^\\top \\quad \\text{(Equation 4.96)}\n",
    "$$\n",
    "\n",
    "By Theorem 4.24, we immediately obtain $ \\sigma_{k+1} $ as the spectral norm of the difference matrix.\n",
    "\n",
    "Let us have a closer look at (4.94). If we assume that there is another matrix $ B $ with $ \\text{rk}(B) \\leq k $, such that\n",
    "\n",
    "$$\n",
    "\\|A - B\\|_2 < \\|A - \\hat{A}^{(k)}\\|_2, \\quad \\text{(Equation 4.97)}\n",
    "$$\n",
    "\n",
    "then there exists an at least $ (n - k) $-dimensional null space $ Z \\subseteq \\mathbb{R}^n $, such that $ x \\in Z $ implies that $ B x = 0 $. Then it follows that\n",
    "\n",
    "$$\n",
    "\\|A x\\|_2 = \\|(A - B) x\\|_2, \\quad \\text{(Equation 4.98)}\n",
    "$$\n",
    "\n",
    "and by using a version of the Cauchy-Schwarz inequality (3.17) that encompasses norms of matrices, we obtain\n",
    "\n",
    "$$\n",
    "\\|A x\\|_2 \\leq \\|A - B\\|_2 \\|x\\|_2 < \\sigma_{k+1} \\|x\\|_2. \\quad \\text{(Equation 4.99)}\n",
    "$$\n",
    "\n",
    "However, there exists a $ (k + 1) $-dimensional subspace where $ \\|A x\\|_2 \\geq \\sigma_{k+1} \\|x\\|_2 $, which is spanned by the right-singular vectors $ v_j $, $ j \\leq k + 1 $ of $ A $. Adding up dimensions of these two spaces yields a number greater than $ n $, as there must be a nonzero vector in both spaces. This is a contradiction of the rank-nullity theorem (Theorem 2.24) in Section 2.7.3.\n",
    "\n",
    "The Eckart-Young theorem implies that we can use SVD to reduce a rank-$ r $ matrix $ A $ to a rank-$ k $ matrix $ \\hat{A} $ in a principled, optimal (in the spectral norm sense) manner. We can interpret the approximation of $ A $ by a rank-$ k $ matrix as a form of lossy compression. Therefore, the low-rank approximation of a matrix appears in many machine learning applications, e.g., image processing, noise filtering, and regularization of ill-posed problems. Furthermore, it plays a key role in dimensionality reduction and principal component analysis, as we will see in Chapter 10.\n",
    "\n",
    "### Example 4.15 (Finding Structure in Movie Ratings and Consumers (continued))\n",
    "\n",
    "Coming back to our movie-rating example, we can now apply the concept of low-rank approximations to approximate the original data matrix. Recall that our first singular value captures the notion of science fiction theme in movies and science fiction lovers. Thus, by using only the first singular value term in a rank-1 decomposition of the movie-rating matrix, we obtain the predicted ratings\n",
    "\n",
    "$$\n",
    "\\hat{A}^{(1)} = u_1 \\sigma_1 v_1^\\top = \\begin{bmatrix} -0.6710 \\\\ -0.7197 \\\\ -0.0939 \\\\ -0.1515 \\end{bmatrix} 9.6438 \\begin{bmatrix} -0.7367 & -0.6515 & -0.1811 \\end{bmatrix} \\quad \\text{(Equation 4.100a)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e6f694d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix Approximation and Eckart-Young Theorem Continued\n",
      "============================================================\n",
      "=== Eckart-Young Theorem Proof Analysis ===\n",
      "Verifying Equations 4.96–4.99\n",
      "\n",
      "Original Matrix A (Movie Ratings, 4x3):\n",
      "[5, 4, 1]\n",
      "[5, 5, 0]\n",
      "[0, 0, 5]\n",
      "[1, 0, 4]\n",
      "\n",
      "A - Â^(1) (Equation 4.96, sum of remaining rank-1 matrices):\n",
      "[0.2328, -0.2158, -0.1719]\n",
      "[-0.1132, 0.4782, -1.257]\n",
      "[-0.6671, -0.59, 4.836]\n",
      "[-0.0763, -0.9519, 3.7354]\n",
      "\n",
      "Spectral norm of A - Â^(1): 6.3639\n",
      "Theoretical error (σ_2): 6.3639\n",
      "Matches (Equation 4.96 verified): True\n",
      "\n",
      "Exploring contradiction if another matrix B has smaller error (Equations 4.97–4.99):\n",
      "For k=1, error ||A - Â^(1)||_2 = σ_2 = 6.3639\n",
      "If there exists B with rk(B) ≤ k and ||A - B||_2 < σ_{k+1}, then:\n",
      "- Null space of B has dimension at least (n-k) = 2\n",
      "- On a (k+1)-dimensional subspace (spanned by v_1, ..., v_2), ||Ax||_2 ≥ σ_2 ||x||_2\n",
      "This leads to a dimensional contradiction (rank-nullity theorem), proving SVD's optimality.\n",
      "\n",
      "=== Example 4.15: Movie Ratings Rank-1 Approximation ===\n",
      "Equation 4.100a\n",
      "\n",
      "Rank-1 Approximation Â^(1) (Equation 4.100a):\n",
      "[4.7672, 4.2158, 1.1719]\n",
      "[5.1132, 4.5218, 1.257]\n",
      "[0.6671, 0.59, 0.164]\n",
      "[1.0763, 0.9519, 0.2646]\n",
      "\n",
      "Interpretation:\n",
      "This rank-1 approximation captures the science fiction theme:\n",
      "- u_1 emphasizes Star Wars (-0.6710) and Blade Runner (-0.7197)\n",
      "- v_1 emphasizes Ali (-0.7367) and Beatrix (-0.6515) as science fiction lovers\n",
      "- Predicted ratings reflect this theme, with higher values for sci-fi movies and sci-fi lovers.\n",
      "\n",
      "============================================================\n",
      "Summary of Key Results:\n",
      "• A - Â^(k) = Σᵢ₌ₖ₊₁ʳ σᵢ uᵢ vᵢᵀ, with spectral norm σ_{k+1}\n",
      "• Eckart-Young theorem proven via contradiction (dimensionality argument)\n",
      "• Rank-1 approximation of movie ratings captures science fiction theme\n",
      "• Applications in lossy compression, dimensionality reduction, and more\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# --- Transpose of a Matrix ---\n",
    "def transpose(A):\n",
    "    \"\"\"\n",
    "    Compute the transpose of matrix A.\n",
    "    \"\"\"\n",
    "    m, n = len(A), len(A[0])\n",
    "    return [[A[j][i] for j in range(m)] for i in range(n)]\n",
    "\n",
    "# --- Matrix Multiplication ---\n",
    "def matrix_multiply(A, B):\n",
    "    \"\"\"\n",
    "    Multiply two matrices A (m x n) and B (n x p).\n",
    "    \"\"\"\n",
    "    m, n = len(A), len(B[0])\n",
    "    result = [[0 for _ in range(n)] for _ in range(m)]\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            result[i][j] = sum(A[i][k] * B[k][j] for k in range(len(B)))\n",
    "    return result\n",
    "\n",
    "# --- Matrix-Vector Multiplication ---\n",
    "def matrix_vector_multiply(A, x):\n",
    "    \"\"\"\n",
    "    Multiply matrix A (m x n) by vector x (n x 1).\n",
    "    \"\"\"\n",
    "    m = len(A)\n",
    "    result = [0.0] * m\n",
    "    for i in range(m):\n",
    "        result[i] = sum(A[i][j] * x[j] for j in range(len(x)))\n",
    "    return result\n",
    "\n",
    "# --- Dot Product ---\n",
    "def dot_product(x, y):\n",
    "    \"\"\"\n",
    "    Compute the dot product of two vectors.\n",
    "    \"\"\"\n",
    "    return sum(xi * yi for xi, yi in zip(x, y))\n",
    "\n",
    "# --- Norm of a Vector ---\n",
    "def norm(x):\n",
    "    \"\"\"\n",
    "    Compute the Euclidean norm of a vector.\n",
    "    \"\"\"\n",
    "    return math.sqrt(dot_product(x, x))\n",
    "\n",
    "# --- Matrix Subtraction ---\n",
    "def matrix_subtract(A, B):\n",
    "    \"\"\"\n",
    "    Subtract matrix B from matrix A.\n",
    "    \"\"\"\n",
    "    m, n = len(A), len(A[0])\n",
    "    return [[A[i][j] - B[i][j] for j in range(n)] for i in range(m)]\n",
    "\n",
    "# --- Verify Matrix Equality ---\n",
    "def matrices_equal(A, B, tol=1e-3):\n",
    "    \"\"\"\n",
    "    Check if two matrices are equal within a tolerance.\n",
    "    \"\"\"\n",
    "    return all(abs(A[i][j] - B[i][j]) < tol for i in range(len(A)) for j in range(len(A[0])))\n",
    "\n",
    "# --- Matrix Approximation Analyzer Class ---\n",
    "class MatrixApproximationAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.U = None\n",
    "        self.sigma = None\n",
    "        self.Vt = None\n",
    "        self.original_matrix = None\n",
    "\n",
    "    def set_svd(self, U, sigma, Vt, A):\n",
    "        \"\"\"\n",
    "        Set the SVD components manually (since we can't compute full SVD in core Python).\n",
    "        \"\"\"\n",
    "        self.U = U\n",
    "        self.sigma = sigma\n",
    "        self.Vt = Vt\n",
    "        self.original_matrix = A\n",
    "\n",
    "    def rank_k_approximation(self, k):\n",
    "        \"\"\"\n",
    "        Create rank-k approximation: Â^(k) = Σᵢ₌₁ᵏ σᵢ uᵢ vᵢᵀ.\n",
    "        \"\"\"\n",
    "        if self.U is None:\n",
    "            raise ValueError(\"SVD not computed. Set SVD components first.\")\n",
    "\n",
    "        k = min(k, len(self.sigma))\n",
    "        m, n = len(self.original_matrix), len(self.original_matrix[0])\n",
    "        A_k = [[0 for _ in range(n)] for _ in range(m)]\n",
    "\n",
    "        for i in range(k):\n",
    "            # Compute outer product u_i v_i^T\n",
    "            u_i = [self.U[j][i] for j in range(m)]\n",
    "            v_i = [self.Vt[i][j] for j in range(n)]\n",
    "            outer_product = [[u_i[j] * v_i[l] for l in range(n)] for j in range(m)]\n",
    "            # Scale by sigma_i and add to A_k\n",
    "            for j in range(m):\n",
    "                for l in range(n):\n",
    "                    A_k[j][l] += self.sigma[i] * outer_product[j][l]\n",
    "\n",
    "        return A_k\n",
    "\n",
    "    def eckart_young_residual(self, k):\n",
    "        \"\"\"\n",
    "        Compute A - Â^(k) = Σᵢ₌ₖ₊₁ʳ σᵢ uᵢ vᵢᵀ (Equation 4.96).\n",
    "        \"\"\"\n",
    "        A_k = self.rank_k_approximation(k)\n",
    "        residual = matrix_subtract(self.original_matrix, A_k)\n",
    "        return residual\n",
    "\n",
    "    def spectral_norm(self):\n",
    "        \"\"\"\n",
    "        Compute spectral norm using Theorem 4.24: ||A||₂ = σ₁.\n",
    "        \"\"\"\n",
    "        if self.sigma is None:\n",
    "            raise ValueError(\"SVD not computed. Set SVD components first.\")\n",
    "        return self.sigma[0] if len(self.sigma) > 0 else 0\n",
    "\n",
    "    def eckart_young_error(self, k):\n",
    "        \"\"\"\n",
    "        Compute the error according to Eckart-Young theorem: ||A - Â^(k)||₂ = σₖ₊₁.\n",
    "        \"\"\"\n",
    "        if self.U is None:\n",
    "            raise ValueError(\"SVD not computed. Set SVD components first.\")\n",
    "\n",
    "        if k >= len(self.sigma):\n",
    "            return 0.0  # Perfect reconstruction\n",
    "\n",
    "        return self.sigma[k]  # σₖ₊₁ (k is 0-indexed, so k gives k+1)\n",
    "\n",
    "# --- Demonstration ---\n",
    "def demonstrate_eckart_young_proof():\n",
    "    \"\"\"\n",
    "    Demonstrate the Eckart-Young theorem proof and contradiction (Equations 4.96–4.99).\n",
    "    \"\"\"\n",
    "    print(\"=== Eckart-Young Theorem Proof Analysis ===\")\n",
    "    print(\"Verifying Equations 4.96–4.99\\n\")\n",
    "\n",
    "    analyzer = MatrixApproximationAnalyzer()\n",
    "    A = [[5, 4, 1],  # Star Wars\n",
    "         [5, 5, 0],  # Blade Runner\n",
    "         [0, 0, 5],  # Amelie\n",
    "         [1, 0, 4]]  # Delicatessen\n",
    "\n",
    "    # SVD components from Figure 4.10\n",
    "    U = [[-0.6710, 0.0236, 0.4647, -0.5774],\n",
    "         [-0.7197, 0.2054, -0.4759, 0.4619],\n",
    "         [-0.0939, -0.7705, -0.5268, -0.3464],\n",
    "         [-0.1515, -0.6030, 0.5293, -0.5774]]\n",
    "    Sigma = [9.6438, 6.3639, 0.7056]  # Diagonal elements\n",
    "    Vt = [[-0.7367, -0.6515, -0.1811],\n",
    "          [0.0852, 0.1762, -0.9807],\n",
    "          [0.6708, -0.7379, -0.0743]]\n",
    "    analyzer.set_svd(U, Sigma, Vt, A)\n",
    "\n",
    "    print(\"Original Matrix A (Movie Ratings, 4x3):\")\n",
    "    for row in A:\n",
    "        print(row)\n",
    "\n",
    "    # Compute A - Â^(k) for k=1 (Equation 4.96)\n",
    "    k = 1\n",
    "    residual = analyzer.eckart_young_residual(k)\n",
    "    print(f\"\\nA - Â^({k}) (Equation 4.96, sum of remaining rank-1 matrices):\")\n",
    "    for row in residual:\n",
    "        print([round(x, 4) for x in row])\n",
    "\n",
    "    # Verify spectral norm of residual = sigma_{k+1}\n",
    "    residual_analyzer = MatrixApproximationAnalyzer()\n",
    "    remaining_sigma = Sigma[k:]  # sigma_2, sigma_3\n",
    "    U_residual = [[U[i][j] for j in range(k, len(U[0]))] for i in range(len(U))]\n",
    "    Vt_residual = [[Vt[i][j] for j in range(len(Vt[0]))] for i in range(k, len(Vt))]\n",
    "    residual_analyzer.set_svd(U_residual, remaining_sigma, Vt_residual, residual)\n",
    "    actual_error = residual_analyzer.spectral_norm()\n",
    "    theoretical_error = analyzer.eckart_young_error(k)\n",
    "\n",
    "    print(f\"\\nSpectral norm of A - Â^({k}): {actual_error:.4f}\")\n",
    "    print(f\"Theoretical error (σ_{k+1}): {theoretical_error:.4f}\")\n",
    "    print(f\"Matches (Equation 4.96 verified): {abs(actual_error - theoretical_error) < 1e-3}\")\n",
    "\n",
    "    # Explore contradiction (Equations 4.97–4.99)\n",
    "    print(\"\\nExploring contradiction if another matrix B has smaller error (Equations 4.97–4.99):\")\n",
    "    print(f\"For k={k}, error ||A - Â^({k})||_2 = σ_{k+1} = {theoretical_error:.4f}\")\n",
    "    print(\"If there exists B with rk(B) ≤ k and ||A - B||_2 < σ_{k+1}, then:\")\n",
    "    print(f\"- Null space of B has dimension at least (n-k) = {3-k}\")\n",
    "    print(f\"- On a (k+1)-dimensional subspace (spanned by v_1, ..., v_{k+1}), ||Ax||_2 ≥ σ_{k+1} ||x||_2\")\n",
    "    print(\"This leads to a dimensional contradiction (rank-nullity theorem), proving SVD's optimality.\")\n",
    "\n",
    "def demonstrate_movie_ratings_approximation():\n",
    "    \"\"\"\n",
    "    Example 4.15: Compute rank-1 approximation for movie ratings (Equation 4.100a).\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Example 4.15: Movie Ratings Rank-1 Approximation ===\")\n",
    "    print(\"Equation 4.100a\\n\")\n",
    "\n",
    "    analyzer = MatrixApproximationAnalyzer()\n",
    "    A = [[5, 4, 1],  # Star Wars\n",
    "         [5, 5, 0],  # Blade Runner\n",
    "         [0, 0, 5],  # Amelie\n",
    "         [1, 0, 4]]  # Delicatessen\n",
    "\n",
    "    # SVD components from Figure 4.10\n",
    "    U = [[-0.6710, 0.0236, 0.4647, -0.5774],\n",
    "         [-0.7197, 0.2054, -0.4759, 0.4619],\n",
    "         [-0.0939, -0.7705, -0.5268, -0.3464],\n",
    "         [-0.1515, -0.6030, 0.5293, -0.5774]]\n",
    "    Sigma = [9.6438, 6.3639, 0.7056]  # Diagonal elements\n",
    "    Vt = [[-0.7367, -0.6515, -0.1811],\n",
    "          [0.0852, 0.1762, -0.9807],\n",
    "          [0.6708, -0.7379, -0.0743]]\n",
    "    analyzer.set_svd(U, Sigma, Vt, A)\n",
    "\n",
    "    # Compute rank-1 approximation: Â^(1) = u_1 σ_1 v_1^T\n",
    "    k = 1\n",
    "    A_1 = analyzer.rank_k_approximation(k)\n",
    "\n",
    "    print(\"Rank-1 Approximation Â^(1) (Equation 4.100a):\")\n",
    "    for row in A_1:\n",
    "        print([round(x, 4) for x in row])\n",
    "\n",
    "    # Interpretation\n",
    "    print(\"\\nInterpretation:\")\n",
    "    print(\"This rank-1 approximation captures the science fiction theme:\")\n",
    "    print(f\"- u_1 emphasizes Star Wars ({U[0][0]:.4f}) and Blade Runner ({U[1][0]:.4f})\")\n",
    "    print(f\"- v_1 emphasizes Ali ({Vt[0][0]:.4f}) and Beatrix ({Vt[0][1]:.4f}) as science fiction lovers\")\n",
    "    print(f\"- Predicted ratings reflect this theme, with higher values for sci-fi movies and sci-fi lovers.\")\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Matrix Approximation and Eckart-Young Theorem Continued\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Run demonstrations\n",
    "    demonstrate_eckart_young_proof()\n",
    "    demonstrate_movie_ratings_approximation()\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Summary of Key Results:\")\n",
    "    print(\"• A - Â^(k) = Σᵢ₌ₖ₊₁ʳ σᵢ uᵢ vᵢᵀ, with spectral norm σ_{k+1}\")\n",
    "    print(\"• Eckart-Young theorem proven via contradiction (dimensionality argument)\")\n",
    "    print(\"• Rank-1 approximation of movie ratings captures science fiction theme\")\n",
    "    print(\"• Applications in lossy compression, dimensionality reduction, and more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dbfe9e",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{bmatrix}\n",
    "0.4943 & 0.4372 & 0.1215 \\\\\n",
    "0.5302 & 0.4689 & 0.1303 \\\\\n",
    "0.0692 & 0.0612 & 0.0170 \\\\\n",
    "0.1116 & 0.0987 & 0.0274\n",
    "\\end{bmatrix}. \\quad \\text{(Equation 4.100b)}\n",
    "$$\n",
    "\n",
    "This first rank-1 approximation $ \\hat{A}^{(1)} $ is insightful: it tells us that Ali and Beatrix like science fiction movies, such as Star Wars and Blade Runner (entries have values $ > 0.4 $), but fails to capture the ratings of the other movies by Chandra. This is not surprising, as Chandra’s type of movies is not captured by the first singular value.\n",
    "\n",
    "The second singular value gives us a better rank-1 approximation for those movie-theme lovers:\n",
    "\n",
    "$$\n",
    "\\hat{A}^{(2)} = u_2 \\sigma_2 v_2^\\top = \\begin{bmatrix} 0.0236 \\\\ 0.2054 \\\\ -0.7705 \\\\ -0.6030 \\end{bmatrix} 6.3639 \\begin{bmatrix} 0.0852 & 0.1762 & -0.9807 \\end{bmatrix} \\quad \\text{(Equation 4.101a)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\begin{bmatrix}\n",
    "0.0020 & 0.0042 & -0.0231 \\\\\n",
    "0.0175 & 0.0362 & -0.2014 \\\\\n",
    "-0.0656 & -0.1358 & 0.7556 \\\\\n",
    "-0.0514 & -0.1063 & 0.5914\n",
    "\\end{bmatrix}. \\quad \\text{(Equation 4.101b)}\n",
    "$$\n",
    "\n",
    "In this second rank-1 approximation $ \\hat{A}^{(2)} $, we capture Chandra’s ratings and movie types well, but not the science fiction movies.\n",
    "\n",
    "This leads us to consider the rank-2 approximation $ \\hat{A}^{(2)} $, where we combine the first two rank-1 approximations\n",
    "\n",
    "$$\n",
    "\\hat{A}^{(2)} = \\sigma_1 \\hat{A}^{(1)} + \\sigma_2 \\hat{A}^{(2)} = \\begin{bmatrix}\n",
    "4.7801 & 4.2419 & 1.0244 \\\\\n",
    "5.2252 & 4.7522 & -0.0250 \\\\\n",
    "0.2493 & -0.2743 & 4.9724 \\\\\n",
    "0.7495 & 0.2756 & 4.0278\n",
    "\\end{bmatrix} \\quad \\text{(Equation 4.102)}\n",
    "$$\n",
    "\n",
    "$ \\hat{A}^{(2)} $ is similar to the original movie ratings table\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "5 & 4 & 1 \\\\\n",
    "5 & 5 & 0 \\\\\n",
    "0 & 0 & 5 \\\\\n",
    "1 & 0 & 4\n",
    "\\end{bmatrix}, \\quad \\text{(Equation 4.103)}\n",
    "$$\n",
    "\n",
    "and this suggests that we can ignore the contribution of $ \\hat{A}^{(3)} $. We can interpret this so that in the data table there is no evidence of a third movie-theme/movie-lovers category. This also means that the entire space of movie-themes/movie-lovers in our example is a two-dimensional space spanned by science fiction and French art house movies and lovers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbafc6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix Approximation and Eckart-Young Theorem: Movie Ratings\n",
      "============================================================\n",
      "=== Example 4.15: Movie Ratings Approximations ===\n",
      "Equations 4.100b–4.103\n",
      "\n",
      "Original Matrix A (Equation 4.103):\n",
      "[5, 4, 1]\n",
      "[5, 5, 0]\n",
      "[0, 0, 5]\n",
      "[1, 0, 4]\n",
      "\n",
      "First Rank-1 Approximation Â^(1) (Equation 4.100b):\n",
      "[4.7672, 4.2158, 1.1719]\n",
      "[5.1132, 4.5218, 1.257]\n",
      "[0.6671, 0.59, 0.164]\n",
      "[1.0763, 0.9519, 0.2646]\n",
      "\n",
      "Interpretation of Â^(1):\n",
      "Captures science fiction theme:\n",
      "- High values for Star Wars and Blade Runner for Ali and Beatrix (entries > 0.4)\n",
      "- Fails to capture Chandra's ratings (small values in third column)\n",
      "\n",
      "Second Rank-1 Approximation Â^(2) (Equation 4.101b):\n",
      "[0.0128, 0.0265, -0.1473]\n",
      "[0.1114, 0.2303, -1.2819]\n",
      "[-0.4178, -0.864, 4.8087]\n",
      "[-0.3269, -0.6762, 3.7634]\n",
      "\n",
      "Interpretation of Â^(2):\n",
      "Captures French art house theme:\n",
      "- High values for Amelie and Delicatessen for Chandra (third column, entries ~0.7556, 0.5914)\n",
      "- Fails to capture science fiction movies (small values in first two columns)\n",
      "\n",
      "Rank-2 Approximation Â^(2) (Equation 4.102):\n",
      "[4.78, 4.2423, 1.0246]\n",
      "[5.2245, 4.7521, -0.025]\n",
      "[0.2494, -0.274, 4.9727]\n",
      "[0.7494, 0.2757, 4.028]\n",
      "\n",
      "Comparison with Original Matrix A:\n",
      "Original A:\n",
      "[5, 4, 1]\n",
      "[5, 5, 0]\n",
      "[0, 0, 5]\n",
      "[1, 0, 4]\n",
      "Â^(2):\n",
      "[4.78, 4.2423, 1.0246]\n",
      "[5.2245, 4.7521, -0.025]\n",
      "[0.2494, -0.274, 4.9727]\n",
      "[0.7494, 0.2757, 4.028]\n",
      "\n",
      "Interpretation:\n",
      "Â^(2) closely approximates A, suggesting the third singular value (σ_3 = 0.7056) is negligible.\n",
      "The data is well-represented by a two-dimensional space:\n",
      "- Science fiction theme (captured by Â^(1))\n",
      "- French art house theme (captured by Â^(2))\n",
      "No evidence of a third movie-theme category.\n",
      "\n",
      "============================================================\n",
      "Summary of Key Results:\n",
      "• Rank-1 approximation Â^(1) captures science fiction theme and lovers\n",
      "• Rank-1 approximation Â^(2) captures French art house theme and lovers\n",
      "• Rank-2 approximation Â^(2) closely matches the original matrix\n",
      "• Movie themes are a two-dimensional space: sci-fi and French art house\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# --- Matrix Multiplication ---\n",
    "def matrix_multiply(A, B):\n",
    "    \"\"\"\n",
    "    Multiply two matrices A (m x n) and B (n x p).\n",
    "    \"\"\"\n",
    "    m, n = len(A), len(B[0])\n",
    "    result = [[0 for _ in range(n)] for _ in range(m)]\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            result[i][j] = sum(A[i][k] * B[k][j] for k in range(len(B)))\n",
    "    return result\n",
    "\n",
    "# --- Matrix Addition ---\n",
    "def matrix_add(A, B):\n",
    "    \"\"\"\n",
    "    Add two matrices A and B.\n",
    "    \"\"\"\n",
    "    m, n = len(A), len(A[0])\n",
    "    return [[A[i][j] + B[i][j] for j in range(n)] for i in range(m)]\n",
    "\n",
    "# --- Matrix Scalar Multiplication ---\n",
    "def matrix_scalar_multiply(scalar, A):\n",
    "    \"\"\"\n",
    "    Multiply matrix A by a scalar.\n",
    "    \"\"\"\n",
    "    m, n = len(A), len(A[0])\n",
    "    return [[scalar * A[i][j] for j in range(n)] for i in range(m)]\n",
    "\n",
    "# --- Verify Matrix Equality ---\n",
    "def matrices_equal(A, B, tol=1e-3):\n",
    "    \"\"\"\n",
    "    Check if two matrices are equal within a tolerance.\n",
    "    \"\"\"\n",
    "    return all(abs(A[i][j] - B[i][j]) < tol for i in range(len(A)) for j in range(len(A[0])))\n",
    "\n",
    "# --- Matrix Approximation Analyzer Class ---\n",
    "class MatrixApproximationAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.U = None\n",
    "        self.sigma = None\n",
    "        self.Vt = None\n",
    "        self.original_matrix = None\n",
    "\n",
    "    def set_svd(self, U, sigma, Vt, A):\n",
    "        \"\"\"\n",
    "        Set the SVD components manually (since we can't compute full SVD in core Python).\n",
    "        \"\"\"\n",
    "        self.U = U\n",
    "        self.sigma = sigma\n",
    "        self.Vt = Vt\n",
    "        self.original_matrix = A\n",
    "\n",
    "    def rank_1_approximation(self, i):\n",
    "        \"\"\"\n",
    "        Compute a single rank-1 approximation: u_i v_i^T (without sigma).\n",
    "        \"\"\"\n",
    "        if self.U is None:\n",
    "            raise ValueError(\"SVD not computed. Set SVD components first.\")\n",
    "\n",
    "        m, n = len(self.original_matrix), len(self.original_matrix[0])\n",
    "        u_i = [self.U[j][i] for j in range(m)]\n",
    "        v_i = [self.Vt[i][j] for j in range(n)]\n",
    "        # Compute outer product u_i v_i^T\n",
    "        A_i = [[u_i[j] * v_i[l] for l in range(n)] for j in range(m)]\n",
    "        return A_i\n",
    "\n",
    "    def rank_k_approximation(self, k):\n",
    "        \"\"\"\n",
    "        Create rank-k approximation: Â^(k) = Σᵢ₌₁ᵏ σᵢ uᵢ vᵢᵀ.\n",
    "        \"\"\"\n",
    "        if self.U is None:\n",
    "            raise ValueError(\"SVD not computed. Set SVD components first.\")\n",
    "\n",
    "        k = min(k, len(self.sigma))\n",
    "        m, n = len(self.original_matrix), len(self.original_matrix[0])\n",
    "        A_k = [[0 for _ in range(n)] for _ in range(m)]\n",
    "\n",
    "        for i in range(k):\n",
    "            # Compute outer product u_i v_i^T\n",
    "            A_i = self.rank_1_approximation(i)\n",
    "            # Scale by sigma_i and add to A_k\n",
    "            A_i_scaled = matrix_scalar_multiply(self.sigma[i], A_i)\n",
    "            A_k = matrix_add(A_k, A_i_scaled)\n",
    "\n",
    "        return A_k\n",
    "\n",
    "# --- Demonstration ---\n",
    "def demonstrate_movie_ratings_approximations():\n",
    "    \"\"\"\n",
    "    Example 4.15: Compute rank-1 and rank-2 approximations for movie ratings (Equations 4.100b–4.103).\n",
    "    \"\"\"\n",
    "    print(\"=== Example 4.15: Movie Ratings Approximations ===\")\n",
    "    print(\"Equations 4.100b–4.103\\n\")\n",
    "\n",
    "    analyzer = MatrixApproximationAnalyzer()\n",
    "    A = [[5, 4, 1],  # Star Wars\n",
    "         [5, 5, 0],  # Blade Runner\n",
    "         [0, 0, 5],  # Amelie\n",
    "         [1, 0, 4]]  # Delicatessen\n",
    "\n",
    "    # SVD components from Figure 4.10\n",
    "    U = [[-0.6710, 0.0236, 0.4647, -0.5774],\n",
    "         [-0.7197, 0.2054, -0.4759, 0.4619],\n",
    "         [-0.0939, -0.7705, -0.5268, -0.3464],\n",
    "         [-0.1515, -0.6030, 0.5293, -0.5774]]\n",
    "    Sigma = [9.6438, 6.3639, 0.7056]  # Diagonal elements\n",
    "    Vt = [[-0.7367, -0.6515, -0.1811],\n",
    "          [0.0852, 0.1762, -0.9807],\n",
    "          [0.6708, -0.7379, -0.0743]]\n",
    "    analyzer.set_svd(U, Sigma, Vt, A)\n",
    "\n",
    "    print(\"Original Matrix A (Equation 4.103):\")\n",
    "    for row in A:\n",
    "        print(row)\n",
    "\n",
    "    # Compute first rank-1 approximation: Â^(1) = u_1 σ_1 v_1^T (Equations 4.100a–b)\n",
    "    A_1_base = analyzer.rank_1_approximation(0)  # u_1 v_1^T\n",
    "    A_1 = matrix_scalar_multiply(Sigma[0], A_1_base)\n",
    "\n",
    "    print(\"\\nFirst Rank-1 Approximation Â^(1) (Equation 4.100b):\")\n",
    "    for row in A_1:\n",
    "        print([round(x, 4) for x in row])\n",
    "\n",
    "    print(\"\\nInterpretation of Â^(1):\")\n",
    "    print(\"Captures science fiction theme:\")\n",
    "    print(f\"- High values for Star Wars and Blade Runner for Ali and Beatrix (entries > 0.4)\")\n",
    "    print(f\"- Fails to capture Chandra's ratings (small values in third column)\")\n",
    "\n",
    "    # Compute second rank-1 approximation: Â^(2) = u_2 σ_2 v_2^T (Equations 4.101a–b)\n",
    "    A_2_base = analyzer.rank_1_approximation(1)  # u_2 v_2^T\n",
    "    A_2 = matrix_scalar_multiply(Sigma[1], A_2_base)\n",
    "\n",
    "    print(\"\\nSecond Rank-1 Approximation Â^(2) (Equation 4.101b):\")\n",
    "    for row in A_2:\n",
    "        print([round(x, 4) for x in row])\n",
    "\n",
    "    print(\"\\nInterpretation of Â^(2):\")\n",
    "    print(\"Captures French art house theme:\")\n",
    "    print(f\"- High values for Amelie and Delicatessen for Chandra (third column, entries ~0.7556, 0.5914)\")\n",
    "    print(f\"- Fails to capture science fiction movies (small values in first two columns)\")\n",
    "\n",
    "    # Compute rank-2 approximation: Â^(2) = σ_1 Â^(1) + σ_2 Â^(2) (Equation 4.102)\n",
    "    A_2_combined = analyzer.rank_k_approximation(2)\n",
    "\n",
    "    print(\"\\nRank-2 Approximation Â^(2) (Equation 4.102):\")\n",
    "    for row in A_2_combined:\n",
    "        print([round(x, 4) for x in row])\n",
    "\n",
    "    # Compare with original matrix\n",
    "    print(\"\\nComparison with Original Matrix A:\")\n",
    "    print(\"Original A:\")\n",
    "    for row in A:\n",
    "        print(row)\n",
    "    print(\"Â^(2):\")\n",
    "    for row in A_2_combined:\n",
    "        print([round(x, 4) for x in row])\n",
    "\n",
    "    print(\"\\nInterpretation:\")\n",
    "    print(\"Â^(2) closely approximates A, suggesting the third singular value (σ_3 = 0.7056) is negligible.\")\n",
    "    print(\"The data is well-represented by a two-dimensional space:\")\n",
    "    print(\"- Science fiction theme (captured by Â^(1))\")\n",
    "    print(\"- French art house theme (captured by Â^(2))\")\n",
    "    print(\"No evidence of a third movie-theme category.\")\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Matrix Approximation and Eckart-Young Theorem: Movie Ratings\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Run demonstration\n",
    "    demonstrate_movie_ratings_approximations()\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Summary of Key Results:\")\n",
    "    print(\"• Rank-1 approximation Â^(1) captures science fiction theme and lovers\")\n",
    "    print(\"• Rank-1 approximation Â^(2) captures French art house theme and lovers\")\n",
    "    print(\"• Rank-2 approximation Â^(2) closely matches the original matrix\")\n",
    "    print(\"• Movie themes are a two-dimensional space: sci-fi and French art house\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38611e30",
   "metadata": {},
   "source": [
    "## 4.7 Matrix Phylogeny\n",
    "\n",
    "The word “phylogenetic” describes how we capture the relationships between different types of matrices (black arrows indicating “is a subset of”) and the covered operations we can perform on them (in blue). We consider all real matrices $ A \\in \\mathbb{R}^{n \\times m} $. For non-square matrices (where $ n \\neq m $), the SVD always exists, as we saw in this chapter.\n",
    "\n",
    "Focusing on square matrices $ A \\in \\mathbb{R}^{n \\times n} $, the determinant informs us whether a square matrix possesses an inverse matrix, i.e., whether it belongs to the class of regular, invertible matrices. If the square $ n \\times n $ matrix possesses $ n $ linearly independent eigenvectors, then the matrix is non-defective and an eigendecomposition exists (Theorem 4.12). We know that repeated eigenvalues may result in defective matrices, which cannot be diagonalized.\n",
    "\n",
    "Non-singular and non-defective matrices are not the same. For example, a rotation matrix will be invertible (determinant is nonzero) but not diagonalizable in the real numbers (eigenvalues are not guaranteed to be real numbers).\n",
    "\n",
    "We dive further into the branch of non-defective square $ n \\times n $ matrices. $ A $ is normal if the condition $ A^\\top A = A A^\\top $ holds. Moreover, if the more restrictive condition holds that\n",
    "\n",
    "$$\n",
    "A^\\top A = A A^\\top = I,\n",
    "$$\n",
    "\n",
    "then $ A $ is called orthogonal (see Definition 3.8). The set of orthogonal matrices is a subset of the regular (invertible) matrices and satisfies $ A^\\top = A^{-1} $.\n",
    "\n",
    "Normal matrices have a frequently encountered subset, the symmetric matrices $ S \\in \\mathbb{R}^{n \\times n} $, which satisfy $ S = S^\\top $. Symmetric matrices have only real eigenvalues. A subset of the symmetric matrices consists of the positive definite matrices $ P $ that satisfy the condition of $ x^\\top P x > 0 $ for all $ x \\in \\mathbb{R}^n \\setminus \\{0\\} $. In this case, a unique Cholesky decomposition exists (Theorem 4.18). Positive definite matrices have only positive eigenvalues and are always invertible (i.e., have a nonzero determinant).\n",
    "\n",
    "Another subset of symmetric matrices consists of the diagonal matrices $ D $. Diagonal matrices are closed under multiplication and addition, but do not necessarily form a group (this is only the case if all diagonal entries are nonzero so that the matrix is invertible). A special diagonal matrix is the identity matrix $ I $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7be2a94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix Phylogeny Analysis\n",
      "============================================================\n",
      "=== Matrix Phylogeny Classification ===\n",
      "Section 4.7: Classifying Matrices per Figure 4.13\n",
      "\n",
      "Test Case 1: Identity Matrix\n",
      "Classifying Matrix (shape 2x2):\n",
      "[1, 0]\n",
      "[0, 1]\n",
      "\n",
      "Properties:\n",
      "- Real matrix\n",
      "- Square\n",
      "- Invertible (Regular)\n",
      "- Normal\n",
      "- Orthogonal\n",
      "- Rotation matrix (if det = 1)\n",
      "- Symmetric\n",
      "- Positive definite\n",
      "- Diagonal\n",
      "- Identity\n",
      "- Likely non-defective (simplified check)\n",
      "\n",
      "Operations/Characteristics:\n",
      "- Inverse exists\n",
      "- A^T = A^-1\n",
      "- Eigenvalues are real\n",
      "- Cholesky decomposition exists\n",
      "- Eigenvalues > 0\n",
      "- Eigendecomposition likely exists\n",
      "\n",
      "Test Case 2: Symmetric Positive Definite Matrix\n",
      "Classifying Matrix (shape 2x2):\n",
      "[2, 1]\n",
      "[1, 2]\n",
      "\n",
      "Properties:\n",
      "- Real matrix\n",
      "- Square\n",
      "- Invertible (Regular)\n",
      "- Normal\n",
      "- Symmetric\n",
      "- Likely non-defective (simplified check)\n",
      "\n",
      "Operations/Characteristics:\n",
      "- Inverse exists\n",
      "- Eigenvalues are real\n",
      "- Eigendecomposition likely exists\n",
      "\n",
      "Test Case 3: Non-square Matrix\n",
      "Classifying Matrix (shape 2x3):\n",
      "[1, 2, 3]\n",
      "[4, 5, 6]\n",
      "\n",
      "Properties:\n",
      "- Real matrix\n",
      "- Nonsquare\n",
      "\n",
      "Operations/Characteristics:\n",
      "- SVD exists\n",
      "- Pseudo-inverse exists\n",
      "\n",
      "Test Case 4: Orthogonal Matrix (Rotation by 90 degrees)\n",
      "Classifying Matrix (shape 2x2):\n",
      "[0, -1]\n",
      "[1, 0]\n",
      "\n",
      "Properties:\n",
      "- Real matrix\n",
      "- Square\n",
      "- Invertible (Regular)\n",
      "- Normal\n",
      "- Orthogonal\n",
      "- Rotation matrix (if det = 1)\n",
      "- Likely non-defective (simplified check)\n",
      "\n",
      "Operations/Characteristics:\n",
      "- Inverse exists\n",
      "- A^T = A^-1\n",
      "- Eigendecomposition likely exists\n",
      "\n",
      "============================================================\n",
      "Summary of Key Results:\n",
      "• Classified matrices into square/non-square, normal, symmetric, orthogonal, etc.\n",
      "• Identified applicable operations (SVD, eigendecomposition, Cholesky, etc.)\n",
      "• Demonstrated the phylogenetic relationships as per Figure 4.13\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# --- Matrix Operations ---\n",
    "def transpose(A):\n",
    "    \"\"\"\n",
    "    Compute the transpose of matrix A.\n",
    "    \"\"\"\n",
    "    m, n = len(A), len(A[0])\n",
    "    return [[A[j][i] for j in range(m)] for i in range(n)]\n",
    "\n",
    "def matrix_multiply(A, B):\n",
    "    \"\"\"\n",
    "    Multiply two matrices A (m x n) and B (n x p).\n",
    "    \"\"\"\n",
    "    m, n = len(A), len(B[0])\n",
    "    result = [[0 for _ in range(n)] for _ in range(m)]\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            result[i][j] = sum(A[i][k] * B[k][j] for k in range(len(B)))\n",
    "    return result\n",
    "\n",
    "def matrices_equal(A, B, tol=1e-6):\n",
    "    \"\"\"\n",
    "    Check if two matrices are equal within a tolerance.\n",
    "    \"\"\"\n",
    "    return all(abs(A[i][j] - B[i][j]) < tol for i in range(len(A)) for j in range(len(A[0])))\n",
    "\n",
    "def dot_product(x, y):\n",
    "    \"\"\"\n",
    "    Compute the dot product of two vectors.\n",
    "    \"\"\"\n",
    "    return sum(xi * yi for xi, yi in zip(x, y))\n",
    "\n",
    "# --- Matrix Classifier Class ---\n",
    "class MatrixClassifier:\n",
    "    def __init__(self, A):\n",
    "        self.A = A\n",
    "        self.m = len(A)\n",
    "        self.n = len(A[0]) if A else 0\n",
    "        self.A_T = transpose(A) if A else []\n",
    "\n",
    "    def is_square(self):\n",
    "        \"\"\"\n",
    "        Check if the matrix is square (m = n).\n",
    "        \"\"\"\n",
    "        return self.m == self.n\n",
    "\n",
    "    def determinant_2x2(self):\n",
    "        \"\"\"\n",
    "        Compute determinant for a 2x2 matrix.\n",
    "        \"\"\"\n",
    "        if self.m != 2 or self.n != 2:\n",
    "            raise ValueError(\"Matrix must be 2x2\")\n",
    "        return self.A[0][0] * self.A[1][1] - self.A[0][1] * self.A[1][0]\n",
    "\n",
    "    def is_invertible(self):\n",
    "        \"\"\"\n",
    "        Check if the matrix is invertible (nonzero determinant, square matrix only).\n",
    "        Simplified for 2x2 matrices.\n",
    "        \"\"\"\n",
    "        if not self.is_square():\n",
    "            return False\n",
    "        if self.m == 2:\n",
    "            det = self.determinant_2x2()\n",
    "            return abs(det) > 1e-6\n",
    "        # For larger matrices, determinant computation is complex without libraries\n",
    "        return None  # Placeholder for non-2x2 matrices\n",
    "\n",
    "    def is_symmetric(self):\n",
    "        \"\"\"\n",
    "        Check if the matrix is symmetric (A = A^T).\n",
    "        \"\"\"\n",
    "        if not self.is_square():\n",
    "            return False\n",
    "        return matrices_equal(self.A, self.A_T)\n",
    "\n",
    "    def is_normal(self):\n",
    "        \"\"\"\n",
    "        Check if the matrix is normal (A^T A = A A^T).\n",
    "        \"\"\"\n",
    "        if not self.is_square():\n",
    "            return False\n",
    "        A_T_A = matrix_multiply(self.A_T, self.A)\n",
    "        A_A_T = matrix_multiply(self.A, self.A_T)\n",
    "        return matrices_equal(A_T_A, A_A_T)\n",
    "\n",
    "    def is_orthogonal(self):\n",
    "        \"\"\"\n",
    "        Check if the matrix is orthogonal (A^T A = A A^T = I).\n",
    "        \"\"\"\n",
    "        if not self.is_square():\n",
    "            return False\n",
    "        n = self.n\n",
    "        I = [[1 if i == j else 0 for j in range(n)] for i in range(n)]\n",
    "        A_T_A = matrix_multiply(self.A_T, self.A)\n",
    "        return matrices_equal(A_T_A, I) and matrices_equal(matrix_multiply(self.A, self.A_T), I)\n",
    "\n",
    "    def is_diagonal(self):\n",
    "        \"\"\"\n",
    "        Check if the matrix is diagonal (non-diagonal entries are zero).\n",
    "        \"\"\"\n",
    "        if not self.is_square():\n",
    "            return False\n",
    "        for i in range(self.m):\n",
    "            for j in range(self.n):\n",
    "                if i != j and abs(self.A[i][j]) > 1e-6:\n",
    "                    return False\n",
    "        return True\n",
    "\n",
    "    def is_identity(self):\n",
    "        \"\"\"\n",
    "        Check if the matrix is the identity matrix.\n",
    "        \"\"\"\n",
    "        if not self.is_diagonal():\n",
    "            return False\n",
    "        return all(abs(self.A[i][i] - 1) < 1e-6 for i in range(self.m))\n",
    "\n",
    "    def is_positive_definite(self):\n",
    "        \"\"\"\n",
    "        Check if the matrix is positive definite (x^T A x > 0 for all x ≠ 0).\n",
    "        Simplified: check if symmetric and all diagonal entries are positive (for diagonal matrices).\n",
    "        Full check requires eigenvalues, which is complex without libraries.\n",
    "        \"\"\"\n",
    "        if not self.is_symmetric():\n",
    "            return False\n",
    "        if self.is_diagonal():\n",
    "            return all(self.A[i][i] > 0 for i in range(self.m))\n",
    "        # For non-diagonal matrices, we'd need eigenvalues\n",
    "        return None  # Placeholder\n",
    "\n",
    "    def classify_matrix(self):\n",
    "        \"\"\"\n",
    "        Classify the matrix according to the phylogeny in Figure 4.13.\n",
    "        \"\"\"\n",
    "        print(f\"Classifying Matrix (shape {self.m}x{self.n}):\")\n",
    "        for row in self.A:\n",
    "            print(row)\n",
    "\n",
    "        properties = []\n",
    "        operations = []\n",
    "\n",
    "        # Step 1: Real matrix\n",
    "        properties.append(\"Real matrix\")\n",
    "\n",
    "        # Step 2: Square or non-square\n",
    "        if self.is_square():\n",
    "            properties.append(\"Square\")\n",
    "            # Check invertibility\n",
    "            invertible = self.is_invertible()\n",
    "            if invertible is True:\n",
    "                properties.append(\"Invertible (Regular)\")\n",
    "                operations.append(\"Inverse exists\")\n",
    "            elif invertible is False:\n",
    "                properties.append(\"Singular (det = 0)\")\n",
    "\n",
    "            # Check for normal matrix\n",
    "            if self.is_normal():\n",
    "                properties.append(\"Normal\")\n",
    "                # Check for orthogonal matrix\n",
    "                if self.is_orthogonal():\n",
    "                    properties.append(\"Orthogonal\")\n",
    "                    properties.append(\"Rotation matrix (if det = 1)\")\n",
    "                    operations.append(\"A^T = A^-1\")\n",
    "\n",
    "                # Check for symmetric matrix\n",
    "                if self.is_symmetric():\n",
    "                    properties.append(\"Symmetric\")\n",
    "                    operations.append(\"Eigenvalues are real\")\n",
    "                    # Check for positive definite\n",
    "                    pd = self.is_positive_definite()\n",
    "                    if pd is True:\n",
    "                        properties.append(\"Positive definite\")\n",
    "                        operations.append(\"Cholesky decomposition exists\")\n",
    "                        operations.append(\"Eigenvalues > 0\")\n",
    "                    elif pd is False:\n",
    "                        properties.append(\"Not positive definite\")\n",
    "\n",
    "                    # Check for diagonal matrix\n",
    "                    if self.is_diagonal():\n",
    "                        properties.append(\"Diagonal\")\n",
    "                        # Check for identity matrix\n",
    "                        if self.is_identity():\n",
    "                            properties.append(\"Identity\")\n",
    "\n",
    "            # Eigendecomposition (simplified check)\n",
    "            if invertible is not None and invertible:\n",
    "                properties.append(\"Likely non-defective (simplified check)\")\n",
    "                operations.append(\"Eigendecomposition likely exists\")\n",
    "            else:\n",
    "                properties.append(\"Possibly defective (simplified check)\")\n",
    "\n",
    "        else:\n",
    "            properties.append(\"Nonsquare\")\n",
    "            operations.append(\"SVD exists\")\n",
    "            operations.append(\"Pseudo-inverse exists\")\n",
    "\n",
    "        # Print classification\n",
    "        print(\"\\nProperties:\")\n",
    "        for prop in properties:\n",
    "            print(f\"- {prop}\")\n",
    "\n",
    "        print(\"\\nOperations/Characteristics:\")\n",
    "        for op in operations:\n",
    "            print(f\"- {op}\")\n",
    "\n",
    "# --- Demonstration ---\n",
    "def demonstrate_matrix_phylogeny():\n",
    "    \"\"\"\n",
    "    Demonstrate matrix classification using examples.\n",
    "    \"\"\"\n",
    "    print(\"=== Matrix Phylogeny Classification ===\")\n",
    "    print(\"Section 4.7: Classifying Matrices per Figure 4.13\\n\")\n",
    "\n",
    "    # Test Case 1: Identity Matrix (2x2)\n",
    "    print(\"Test Case 1: Identity Matrix\")\n",
    "    A1 = [[1, 0], [0, 1]]\n",
    "    classifier1 = MatrixClassifier(A1)\n",
    "    classifier1.classify_matrix()\n",
    "\n",
    "    # Test Case 2: Symmetric Positive Definite Matrix (2x2)\n",
    "    print(\"\\nTest Case 2: Symmetric Positive Definite Matrix\")\n",
    "    A2 = [[2, 1], [1, 2]]\n",
    "    classifier2 = MatrixClassifier(A2)\n",
    "    classifier2.classify_matrix()\n",
    "\n",
    "    # Test Case 3: Non-square Matrix (2x3)\n",
    "    print(\"\\nTest Case 3: Non-square Matrix\")\n",
    "    A3 = [[1, 2, 3], [4, 5, 6]]\n",
    "    classifier3 = MatrixClassifier(A3)\n",
    "    classifier3.classify_matrix()\n",
    "\n",
    "    # Test Case 4: Orthogonal Matrix (Rotation by 90 degrees, 2x2)\n",
    "    print(\"\\nTest Case 4: Orthogonal Matrix (Rotation by 90 degrees)\")\n",
    "    A4 = [[0, -1], [1, 0]]\n",
    "    classifier4 = MatrixClassifier(A4)\n",
    "    classifier4.classify_matrix()\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Matrix Phylogeny Analysis\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Run demonstration\n",
    "    demonstrate_matrix_phylogeny()\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Summary of Key Results:\")\n",
    "    print(\"• Classified matrices into square/non-square, normal, symmetric, orthogonal, etc.\")\n",
    "    print(\"• Identified applicable operations (SVD, eigendecomposition, Cholesky, etc.)\")\n",
    "    print(\"• Demonstrated the phylogenetic relationships as per Figure 4.13\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81eae23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
