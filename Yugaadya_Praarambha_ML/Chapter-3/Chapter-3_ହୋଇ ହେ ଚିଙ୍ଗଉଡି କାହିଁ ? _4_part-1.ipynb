{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f598aeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " * Copyright (c) 2016 Radhamadhab Dalai\n",
    " *\n",
    " * Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    " * of this software and associated documentation files (the \"Software\"), to deal\n",
    " * in the Software without restriction, including without limitation the rights\n",
    " * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    " * copies of the Software, and to permit persons to whom the Software is\n",
    " * furnished to do so, subject to the following conditions:\n",
    " *\n",
    " * The above copyright notice and this permission notice shall be included in\n",
    " * all copies or substantial portions of the Software.\n",
    " *\n",
    " * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    " * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    " * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    " * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    " * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    " * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
    " * THE SOFTWARE.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a1f6b2",
   "metadata": {},
   "source": [
    "# Singular Value Decomposition (SVD) and Matrix Approximation\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The Singular Value Decomposition (SVD) is a fundamental matrix factorization technique with wide applications in machine learning, from least-squares problems to dimensionality reduction and data compression.\n",
    "\n",
    "## 1. Standard SVD Formulation\n",
    "\n",
    "For any matrix $A \\in \\mathbb{R}^{m \\times n}$, the SVD decomposes it as:\n",
    "\n",
    "$$A = U\\Sigma V^T$$\n",
    "\n",
    "where:\n",
    "- $U \\in \\mathbb{R}^{m \\times m}$ is an orthogonal matrix (left singular vectors)\n",
    "- $\\Sigma \\in \\mathbb{R}^{m \\times n}$ contains singular values on the main diagonal\n",
    "- $V \\in \\mathbb{R}^{n \\times n}$ is an orthogonal matrix (right singular vectors)\n",
    "\n",
    "## 2. Reduced SVD (Compact SVD)\n",
    "\n",
    "\n",
    "Fig.11 Image processing with the SVD. (a) The original grayscale image is a 1, 432 × 1, 910 matrix of values between 0 (black) and 1 (white). (b)–(f) Rank-1 matrices A1 , . . . , A5 and their corresponding singular values σ1 , . . . , σ5 . The grid-like structure of each rank-1 matrix is imposed by the outer-product of the left and right-singular vectors.\n",
    "\n",
    "Sometimes called the **reduced SVD** (Datta, 2010) or simply **the SVD** (Press et al., 2007), this alternative formulation provides computational convenience:\n",
    "\n",
    "For a rank-$r$ matrix $A$:\n",
    "$$A = U\\Sigma V^T$$\n",
    "\n",
    "where:\n",
    "- $U \\in \\mathbb{R}^{m \\times r}$ (reduced left singular vectors)\n",
    "- $\\Sigma \\in \\mathbb{R}^{r \\times r}$ (diagonal matrix with nonzero singular values)\n",
    "- $V \\in \\mathbb{R}^{r \\times n}$ (reduced right singular vectors)\n",
    "\n",
    "**Key advantage**: $\\Sigma$ is diagonal (like in eigenvalue decomposition), containing only nonzero entries.\n",
    "\n",
    "## 3. Handling Different Matrix Dimensions\n",
    "\n",
    "The SVD applies to $m \\times n$ matrices regardless of whether $m > n$ or $m < n$:\n",
    "\n",
    "- When $m < n$: The decomposition yields $\\Sigma$ with more zero columns than rows\n",
    "- Consequently, singular values $\\sigma_{m+1}, \\ldots, \\sigma_n = 0$\n",
    "\n",
    "## 4. Matrix Approximation via SVD\n",
    "\n",
    "### 4.1 Rank-1 Matrix Construction\n",
    "\n",
    "Instead of full SVD factorization, we can represent matrix $A$ as a sum of simpler low-rank matrices:\n",
    "\n",
    "$$A_i := u_i v_i^T \\quad \\text{(rank-1 matrix)}$$\n",
    "\n",
    "where $A_i \\in \\mathbb{R}^{m \\times n}$ is formed by the outer product of the $i$-th orthogonal column vectors from $U$ and $V$.\n",
    "\n",
    "### 4.2 Complete Representation\n",
    "\n",
    "The full matrix can be expressed as:\n",
    "\n",
    "$$A = \\sum_{i=1}^{r} \\sigma_i u_i v_i^T = \\sum_{i=1}^{r} \\sigma_i A_i$$\n",
    "\n",
    "where $\\sigma_i$ are the singular values and $r$ is the rank of $A$.\n",
    "\n",
    "### 4.3 Truncated SVD for Approximation\n",
    "\n",
    "For matrix approximation, we use only the first $k$ terms (where $k < r$):\n",
    "\n",
    "$$A_k = \\sum_{i=1}^{k} \\sigma_i u_i v_i^T$$\n",
    "\n",
    "This **truncated SVD** provides the best rank-$k$ approximation to $A$ in the Frobenius norm sense.\n",
    "\n",
    "## 5. Applications in Machine Learning\n",
    "\n",
    "The SVD's matrix approximation capabilities enable numerous applications:\n",
    "\n",
    "### 5.1 Dimensionality Reduction\n",
    "- Principal Component Analysis (PCA)\n",
    "- Feature extraction and visualization\n",
    "\n",
    "### 5.2 Data Compression\n",
    "- Image compression (as shown with Stonehenge example)\n",
    "- Lossy compression with controlled quality\n",
    "\n",
    "### 5.3 Topic Modeling\n",
    "- Latent Semantic Analysis (LSA)\n",
    "- Document-term matrix factorization\n",
    "\n",
    "### 5.4 Clustering and Pattern Recognition\n",
    "- Spectral clustering\n",
    "- Noise reduction\n",
    "\n",
    "### 5.5 Numerical Stability\n",
    "- Solving systems of linear equations\n",
    "- Least-squares curve fitting\n",
    "- Robust to numerical rounding errors\n",
    "\n",
    "## 6. Computational Advantages\n",
    "\n",
    "**Matrix approximation benefits**:\n",
    "1. **Computational efficiency**: Working with lower-rank approximations\n",
    "2. **Storage reduction**: Fewer parameters to store\n",
    "3. **Noise reduction**: Truncation removes small singular values (often noise)\n",
    "4. **Numerical robustness**: SVD substitution improves numerical stability\n",
    "\n",
    "## 7. Example: Image Approximation\n",
    "\n",
    "Consider an image represented as matrix $A \\in \\mathbb{R}^{1432 \\times 1910}$ (like the Stonehenge example):\n",
    "\n",
    "```python\n",
    "# Conceptual code structure\n",
    "A_approx = sum(sigma[i] * outer_product(U[:, i], V[i, :]) for i in range(k))\n",
    "```\n",
    "\n",
    "where $k$ determines the approximation quality vs. compression trade-off.\n",
    "\n",
    "## Mathematical Properties\n",
    "\n",
    "**Key SVD properties leveraged in approximation**:\n",
    "\n",
    "1. **Optimality**: Truncated SVD gives the best low-rank approximation\n",
    "2. **Energy compaction**: Large singular values capture most information\n",
    "3. **Orthogonality**: $U$ and $V$ matrices preserve geometric properties\n",
    "4. **Rank revelation**: Singular values reveal the effective dimensionality\n",
    "\n",
    "The SVD's principled approach to matrix approximation makes it invaluable for creating \"simpler\" matrix representations while preserving essential structural information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4cedcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "class SVDDecomposer:\n",
    "    \"\"\"\n",
    "    Implementation of Singular Value Decomposition and Matrix Approximation techniques\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.U = None\n",
    "        self.sigma = None\n",
    "        self.Vt = None\n",
    "        self.original_shape = None\n",
    "    \n",
    "    def decompose(self, A: np.ndarray, full_matrices: bool = True) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Perform SVD decomposition: A = U @ Σ @ V^T\n",
    "        \n",
    "        Args:\n",
    "            A: Input matrix (m x n)\n",
    "            full_matrices: If True, compute full SVD; if False, compute reduced SVD\n",
    "            \n",
    "        Returns:\n",
    "            U: Left singular vectors\n",
    "            sigma: Singular values (1D array)\n",
    "            Vt: Right singular vectors (transposed)\n",
    "        \"\"\"\n",
    "        self.original_shape = A.shape\n",
    "        \n",
    "        # Compute SVD using NumPy's implementation\n",
    "        self.U, self.sigma, self.Vt = np.linalg.svd(A, full_matrices=full_matrices)\n",
    "        \n",
    "        return self.U, self.sigma, self.Vt\n",
    "    \n",
    "    def reduced_svd(self, A: np.ndarray, rank: Optional[int] = None) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Compute reduced/compact SVD with specified rank\n",
    "        \n",
    "        Args:\n",
    "            A: Input matrix\n",
    "            rank: Desired rank (if None, use effective rank)\n",
    "            \n",
    "        Returns:\n",
    "            U_reduced: Reduced left singular vectors\n",
    "            sigma_reduced: Reduced singular values\n",
    "            Vt_reduced: Reduced right singular vectors\n",
    "        \"\"\"\n",
    "        U, sigma, Vt = self.decompose(A, full_matrices=False)\n",
    "        \n",
    "        if rank is None:\n",
    "            # Use effective rank (remove near-zero singular values)\n",
    "            tol = max(A.shape) * np.finfo(A.dtype).eps * sigma[0]\n",
    "            rank = np.sum(sigma > tol)\n",
    "        \n",
    "        rank = min(rank, len(sigma))\n",
    "        \n",
    "        return U[:, :rank], sigma[:rank], Vt[:rank, :]\n",
    "    \n",
    "    def truncated_svd(self, A: np.ndarray, k: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute truncated SVD approximation using first k components\n",
    "        \n",
    "        Args:\n",
    "            A: Input matrix\n",
    "            k: Number of components to keep\n",
    "            \n",
    "        Returns:\n",
    "            A_k: Rank-k approximation of A\n",
    "        \"\"\"\n",
    "        U, sigma, Vt = self.decompose(A, full_matrices=False)\n",
    "        \n",
    "        k = min(k, len(sigma))\n",
    "        \n",
    "        # Reconstruct using first k components\n",
    "        A_k = U[:, :k] @ np.diag(sigma[:k]) @ Vt[:k, :]\n",
    "        \n",
    "        return A_k\n",
    "    \n",
    "    def rank_one_matrices(self, A: np.ndarray, num_components: Optional[int] = None) -> list:\n",
    "        \"\"\"\n",
    "        Decompose matrix into rank-1 components: A_i = σ_i * u_i * v_i^T\n",
    "        \n",
    "        Args:\n",
    "            A: Input matrix\n",
    "            num_components: Number of rank-1 matrices to return\n",
    "            \n",
    "        Returns:\n",
    "            List of rank-1 matrices\n",
    "        \"\"\"\n",
    "        U, sigma, Vt = self.decompose(A, full_matrices=False)\n",
    "        \n",
    "        if num_components is None:\n",
    "            num_components = len(sigma)\n",
    "        \n",
    "        rank_one_mats = []\n",
    "        \n",
    "        for i in range(min(num_components, len(sigma))):\n",
    "            # A_i = σ_i * u_i * v_i^T\n",
    "            A_i = sigma[i] * np.outer(U[:, i], Vt[i, :])\n",
    "            rank_one_mats.append(A_i)\n",
    "        \n",
    "        return rank_one_mats\n",
    "    \n",
    "    def progressive_approximation(self, A: np.ndarray, max_rank: int) -> list:\n",
    "        \"\"\"\n",
    "        Generate progressive approximations A_1, A_2, ..., A_k\n",
    "        \n",
    "        Args:\n",
    "            A: Input matrix\n",
    "            max_rank: Maximum rank for approximation\n",
    "            \n",
    "        Returns:\n",
    "            List of progressive approximations\n",
    "        \"\"\"\n",
    "        U, sigma, Vt = self.decompose(A, full_matrices=False)\n",
    "        \n",
    "        approximations = []\n",
    "        max_rank = min(max_rank, len(sigma))\n",
    "        \n",
    "        for k in range(1, max_rank + 1):\n",
    "            A_k = U[:, :k] @ np.diag(sigma[:k]) @ Vt[:k, :]\n",
    "            approximations.append(A_k)\n",
    "        \n",
    "        return approximations\n",
    "    \n",
    "    def approximation_error(self, A: np.ndarray, A_approx: np.ndarray, norm_type: str = 'fro') -> float:\n",
    "        \"\"\"\n",
    "        Compute approximation error between original and approximated matrix\n",
    "        \n",
    "        Args:\n",
    "            A: Original matrix\n",
    "            A_approx: Approximated matrix\n",
    "            norm_type: Type of norm ('fro' for Frobenius, '2' for spectral)\n",
    "            \n",
    "        Returns:\n",
    "            Approximation error\n",
    "        \"\"\"\n",
    "        if norm_type == 'fro':\n",
    "            return np.linalg.norm(A - A_approx, 'fro')\n",
    "        elif norm_type == '2':\n",
    "            return np.linalg.norm(A - A_approx, 2)\n",
    "        else:\n",
    "            raise ValueError(\"norm_type must be 'fro' or '2'\")\n",
    "    \n",
    "    def compression_ratio(self, A: np.ndarray, k: int) -> float:\n",
    "        \"\"\"\n",
    "        Calculate compression ratio for rank-k approximation\n",
    "        \n",
    "        Args:\n",
    "            A: Original matrix\n",
    "            k: Approximation rank\n",
    "            \n",
    "        Returns:\n",
    "            Compression ratio\n",
    "        \"\"\"\n",
    "        m, n = A.shape\n",
    "        original_size = m * n\n",
    "        compressed_size = k * (m + n + 1)  # U(:,1:k) + sigma(1:k) + Vt(1:k,:)\n",
    "        \n",
    "        return compressed_size / original_size\n",
    "\n",
    "\n",
    "class ImageSVDDemo:\n",
    "    \"\"\"\n",
    "    Demonstrate SVD matrix approximation on images\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.svd = SVDDecomposer()\n",
    "    \n",
    "    def create_synthetic_image(self, size: Tuple[int, int] = (100, 100)) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Create a synthetic image for demonstration\n",
    "        \"\"\"\n",
    "        m, n = size\n",
    "        x = np.linspace(-2, 2, n)\n",
    "        y = np.linspace(-2, 2, m)\n",
    "        X, Y = np.meshgrid(x, y)\n",
    "        \n",
    "        # Create interesting pattern\n",
    "        image = np.exp(-(X**2 + Y**2)) + 0.5 * np.sin(5*X) * np.cos(5*Y)\n",
    "        \n",
    "        # Normalize to [0, 1]\n",
    "        image = (image - image.min()) / (image.max() - image.min())\n",
    "        \n",
    "        return image\n",
    "    \n",
    "    def demonstrate_approximation(self, image: np.ndarray, ranks: list):\n",
    "        \"\"\"\n",
    "        Demonstrate progressive SVD approximation on an image\n",
    "        \n",
    "        Args:\n",
    "            image: Input image matrix\n",
    "            ranks: List of ranks to test\n",
    "        \"\"\"\n",
    "        fig, axes = plt.subplots(2, len(ranks) + 1, figsize=(15, 8))\n",
    "        \n",
    "        # Original image\n",
    "        axes[0, 0].imshow(image, cmap='gray')\n",
    "        axes[0, 0].set_title('Original')\n",
    "        axes[0, 0].axis('off')\n",
    "        \n",
    "        # Singular values plot\n",
    "        U, sigma, Vt = self.svd.decompose(image)\n",
    "        axes[1, 0].semilogy(sigma, 'b-o', markersize=3)\n",
    "        axes[1, 0].set_title('Singular Values')\n",
    "        axes[1, 0].set_xlabel('Index')\n",
    "        axes[1, 0].set_ylabel('Value (log scale)')\n",
    "        axes[1, 0].grid(True)\n",
    "        \n",
    "        errors = []\n",
    "        ratios = []\n",
    "        \n",
    "        for i, k in enumerate(ranks):\n",
    "            # Compute approximation\n",
    "            A_k = self.svd.truncated_svd(image, k)\n",
    "            \n",
    "            # Display approximation\n",
    "            axes[0, i+1].imshow(A_k, cmap='gray')\n",
    "            axes[0, i+1].set_title(f'Rank {k}')\n",
    "            axes[0, i+1].axis('off')\n",
    "            \n",
    "            # Compute and display error\n",
    "            error = self.svd.approximation_error(image, A_k)\n",
    "            ratio = self.svd.compression_ratio(image, k)\n",
    "            \n",
    "            errors.append(error)\n",
    "            ratios.append(ratio)\n",
    "            \n",
    "            axes[1, i+1].bar(['Error', 'Compression'], [error, ratio])\n",
    "            axes[1, i+1].set_title(f'Rank {k}\\nError: {error:.3f}\\nRatio: {ratio:.3f}')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return errors, ratios\n",
    "\n",
    "\n",
    "def demonstrate_svd_concepts():\n",
    "    \"\"\"\n",
    "    Comprehensive demonstration of SVD concepts\n",
    "    \"\"\"\n",
    "    print(\"=== SVD and Matrix Approximation Demonstration ===\\n\")\n",
    "    \n",
    "    # Create SVD decomposer\n",
    "    svd = SVDDecomposer()\n",
    "    \n",
    "    # 1. Basic SVD on a simple matrix\n",
    "    print(\"1. Basic SVD Decomposition\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    A = np.array([[1, 2, 3],\n",
    "                  [4, 5, 6],\n",
    "                  [7, 8, 9],\n",
    "                  [10, 11, 12]], dtype=float)\n",
    "    \n",
    "    print(f\"Original matrix A (shape {A.shape}):\")\n",
    "    print(A)\n",
    "    \n",
    "    U, sigma, Vt = svd.decompose(A)\n",
    "    \n",
    "    print(f\"\\nU shape: {U.shape}\")\n",
    "    print(f\"Sigma shape: {sigma.shape}\")\n",
    "    print(f\"Vt shape: {Vt.shape}\")\n",
    "    print(f\"Singular values: {sigma}\")\n",
    "    \n",
    "    # Verify reconstruction\n",
    "    if U.shape[1] == len(sigma):\n",
    "        A_reconstructed = U @ np.diag(sigma) @ Vt\n",
    "    else:\n",
    "        # Handle reduced SVD case\n",
    "        A_reconstructed = U @ np.diag(sigma) @ Vt[:len(sigma), :]\n",
    "    \n",
    "    reconstruction_error = np.linalg.norm(A - A_reconstructed)\n",
    "    print(f\"Reconstruction error: {reconstruction_error:.2e}\")\n",
    "    \n",
    "    # 2. Reduced SVD\n",
    "    print(f\"\\n2. Reduced SVD\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    U_red, sigma_red, Vt_red = svd.reduced_svd(A, rank=2)\n",
    "    print(f\"Reduced U shape: {U_red.shape}\")\n",
    "    print(f\"Reduced sigma shape: {sigma_red.shape}\")\n",
    "    print(f\"Reduced Vt shape: {Vt_red.shape}\")\n",
    "    \n",
    "    # 3. Rank-1 decomposition\n",
    "    print(f\"\\n3. Rank-1 Matrix Decomposition\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    rank_one_mats = svd.rank_one_matrices(A, num_components=3)\n",
    "    \n",
    "    for i, A_i in enumerate(rank_one_mats):\n",
    "        print(f\"Rank-1 matrix {i+1} (σ_{i+1} = {sigma[i]:.3f}):\")\n",
    "        print(A_i)\n",
    "        print(f\"Rank: {np.linalg.matrix_rank(A_i)}\")\n",
    "        print()\n",
    "    \n",
    "    # 4. Progressive approximation\n",
    "    print(\"4. Progressive Approximation Analysis\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    errors = []\n",
    "    ratios = []\n",
    "    \n",
    "    for k in range(1, min(4, len(sigma)+1)):\n",
    "        A_k = svd.truncated_svd(A, k)\n",
    "        error = svd.approximation_error(A, A_k)\n",
    "        ratio = svd.compression_ratio(A, k)\n",
    "        \n",
    "        errors.append(error)\n",
    "        ratios.append(ratio)\n",
    "        \n",
    "        print(f\"Rank-{k} approximation:\")\n",
    "        print(f\"  Frobenius error: {error:.6f}\")\n",
    "        print(f\"  Compression ratio: {ratio:.3f}\")\n",
    "        print(f\"  Retained energy: {np.sum(sigma[:k]**2) / np.sum(sigma**2):.3f}\")\n",
    "    \n",
    "    # 5. Image approximation demo\n",
    "    print(f\"\\n5. Image Approximation Demo\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    demo = ImageSVDDemo()\n",
    "    \n",
    "    # Create synthetic image\n",
    "    synthetic_image = demo.create_synthetic_image((50, 50))\n",
    "    \n",
    "    # Analyze approximation quality\n",
    "    test_ranks = [1, 5, 10, 20]\n",
    "    \n",
    "    print(\"Analyzing approximation quality for different ranks:\")\n",
    "    \n",
    "    for k in test_ranks:\n",
    "        A_k = svd.truncated_svd(synthetic_image, k)\n",
    "        error = svd.approximation_error(synthetic_image, A_k)\n",
    "        ratio = svd.compression_ratio(synthetic_image, k)\n",
    "        \n",
    "        print(f\"Rank {k:2d}: Error = {error:.6f}, Compression = {ratio:.3f}\")\n",
    "    \n",
    "    # Demonstrate the visualization (commented out to avoid display issues in some environments)\n",
    "    # demo.demonstrate_approximation(synthetic_image, test_ranks)\n",
    "    \n",
    "    print(f\"\\n6. Numerical Properties\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Condition number analysis\n",
    "    cond_original = np.linalg.cond(A)\n",
    "    \n",
    "    # Create a well-conditioned approximation\n",
    "    k_stable = 2  # Use first 2 components\n",
    "    A_stable = svd.truncated_svd(A, k_stable)\n",
    "    cond_stable = np.linalg.cond(A_stable)\n",
    "    \n",
    "    print(f\"Condition number of original matrix: {cond_original:.2e}\")\n",
    "    print(f\"Condition number of rank-{k_stable} approximation: {cond_stable:.2e}\")\n",
    "    print(f\"Numerical stability improvement: {cond_original/cond_stable:.2f}x\")\n",
    "\n",
    "\n",
    "def advanced_svd_applications():\n",
    "    \"\"\"\n",
    "    Demonstrate advanced SVD applications\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Advanced SVD Applications ===\\n\")\n",
    "    \n",
    "    svd = SVDDecomposer()\n",
    "    \n",
    "    # 1. Principal Component Analysis (PCA) simulation\n",
    "    print(\"1. PCA-like Dimensionality Reduction\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    # Generate correlated data\n",
    "    np.random.seed(42)\n",
    "    n_samples, n_features = 100, 5\n",
    "    \n",
    "    # Create data with some correlation structure\n",
    "    true_components = np.array([[1, 1, 0, 0, 0],\n",
    "                                [0, 0, 1, 1, 1]]).T\n",
    "    \n",
    "    data = np.random.randn(n_samples, 2) @ true_components.T + 0.1 * np.random.randn(n_samples, n_features)\n",
    "    \n",
    "    print(f\"Original data shape: {data.shape}\")\n",
    "    \n",
    "    # Center the data (important for PCA)\n",
    "    data_centered = data - np.mean(data, axis=0)\n",
    "    \n",
    "    # Apply SVD\n",
    "    U, sigma, Vt = svd.decompose(data_centered.T)  # Note: transpose for feature space\n",
    "    \n",
    "    print(f\"Explained variance ratios: {(sigma**2 / np.sum(sigma**2))[:3]}\")\n",
    "    \n",
    "    # 2. Least squares solution using SVD\n",
    "    print(f\"\\n2. Robust Least Squares via SVD\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Create overdetermined system Ax = b\n",
    "    m, n = 10, 5\n",
    "    A_ls = np.random.randn(m, n)\n",
    "    x_true = np.random.randn(n)\n",
    "    b = A_ls @ x_true + 0.01 * np.random.randn(m)  # Add small noise\n",
    "    \n",
    "    # Solve using SVD (more numerically stable than normal equations)\n",
    "    U, sigma, Vt = svd.decompose(A_ls)\n",
    "    \n",
    "    # Compute pseudoinverse using SVD\n",
    "    tol = max(A_ls.shape) * np.finfo(float).eps * sigma[0]\n",
    "    rank = np.sum(sigma > tol)\n",
    "    \n",
    "    # x = V @ Σ^(-1) @ U^T @ b (for the non-zero singular values)\n",
    "    x_svd = Vt[:rank, :].T @ np.diag(1/sigma[:rank]) @ U[:, :rank].T @ b\n",
    "    \n",
    "    # Compare with numpy's least squares\n",
    "    x_lstsq = np.linalg.lstsq(A_ls, b, rcond=None)[0]\n",
    "    \n",
    "    print(f\"True solution norm: {np.linalg.norm(x_true):.6f}\")\n",
    "    print(f\"SVD solution error: {np.linalg.norm(x_svd - x_true):.6f}\")\n",
    "    print(f\"Lstsq solution error: {np.linalg.norm(x_lstsq - x_true):.6f}\")\n",
    "    print(f\"Solutions match: {np.allclose(x_svd, x_lstsq)}\")\n",
    "    \n",
    "    # 3. Matrix completion simulation\n",
    "    print(f\"\\n3. Low-Rank Matrix Recovery\")\n",
    "    print(\"-\" * 28)\n",
    "    \n",
    "    # Create low-rank matrix\n",
    "    rank_true = 3\n",
    "    m, n = 20, 15\n",
    "    L = np.random.randn(m, rank_true)\n",
    "    R = np.random.randn(rank_true, n)\n",
    "    M_true = L @ R\n",
    "    \n",
    "    # Add noise\n",
    "    M_noisy = M_true + 0.1 * np.random.randn(m, n)\n",
    "    \n",
    "    print(f\"True rank: {rank_true}\")\n",
    "    print(f\"Noisy matrix rank: {np.linalg.matrix_rank(M_noisy)}\")\n",
    "    \n",
    "    # Recover using truncated SVD\n",
    "    M_recovered = svd.truncated_svd(M_noisy, rank_true)\n",
    "    \n",
    "    recovery_error = svd.approximation_error(M_true, M_recovered)\n",
    "    noise_level = svd.approximation_error(M_true, M_noisy)\n",
    "    \n",
    "    print(f\"Noise level: {noise_level:.6f}\")\n",
    "    print(f\"Recovery error: {recovery_error:.6f}\")\n",
    "    print(f\"Recovery improvement: {noise_level/recovery_error:.2f}x\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run comprehensive demonstrations\n",
    "    demonstrate_svd_concepts()\n",
    "    advanced_svd_applications()\n",
    "    \n",
    "    print(\"\\n=== Summary of SVD Benefits ===\")\n",
    "    print(\"• Optimal low-rank approximation (Eckart-Young theorem)\")\n",
    "    print(\"• Numerical stability for ill-conditioned problems\") \n",
    "    print(\"• Principal component analysis and dimensionality reduction\")\n",
    "    print(\"• Data compression with controlled quality loss\")\n",
    "    print(\"• Robust solutions to least-squares problems\")\n",
    "    print(\"• Matrix completion and denoising applications\")\n",
    "    print(\"• Foundation for many machine learning algorithms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d46c153",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)\n",
    "\n",
    "Fig.12 Image reconstruction with the SVD. (a) Original image. (b)–(f) Image reconstruction using the low-rank approximation of the SVD, where the rank-k approximation is b given by A(k) = Pk i=1 σi Ai .\n",
    "\n",
    "# Matrix Approximation and the Eckart-Young Theorem\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook implements the matrix approximation techniques described in Section 4.6, focusing on the Eckart-Young theorem which establishes the optimality of SVD-based low-rank approximations.\n",
    "\n",
    "## Mathematical Foundation\n",
    "\n",
    "### 1. Spectral Norm Definition\n",
    "\n",
    "**Definition 4.23 (Spectral Norm of a Matrix)**\n",
    "\n",
    "For $x \\in \\mathbb{R}^n \\setminus \\{0\\}$, the spectral norm of a matrix $A \\in \\mathbb{R}^{m \\times n}$ is defined as:\n",
    "\n",
    "$$\\|A\\|_2 := \\max_{x} \\frac{\\|Ax\\|_2}{\\|x\\|_2} \\quad \\text{(Equation 4.93)}$$\n",
    "\n",
    "The spectral norm determines how long any vector $x$ can at most become when multiplied by $A$.\n",
    "\n",
    "### 2. Spectral Norm and Singular Values\n",
    "\n",
    "**Theorem 4.24**: The spectral norm of $A$ is its largest singular value $\\sigma_1$.\n",
    "\n",
    "$$\\|A\\|_2 = \\sigma_1$$\n",
    "\n",
    "### 3. Eckart-Young Theorem\n",
    "\n",
    "**Theorem 4.25 (Eckart-Young Theorem, 1936)**\n",
    "\n",
    "Consider a matrix $A \\in \\mathbb{R}^{m \\times n}$ of rank $r$ and let $B \\in \\mathbb{R}^{m \\times n}$ be a matrix of rank $k$. For any $k \\leq r$ with $\\hat{A}^{(k)} = \\sum_{i=1}^{k} \\sigma_i u_i v_i^T$, it holds that:\n",
    "\n",
    "$$\\hat{A}^{(k)} = \\arg\\min_{\\text{rank}(B)=k} \\|A - B\\|_2 \\quad \\text{(Equation 4.94)}$$\n",
    "\n",
    "$$\\|A - \\hat{A}^{(k)}\\|_2 = \\sigma_{k+1} \\quad \\text{(Equation 4.95)}$$\n",
    "\n",
    "The theorem states that SVD provides the **optimal** rank-$k$ approximation in the spectral norm sense.\n",
    "\n",
    "## Implementation\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import svd as scipy_svd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class MatrixApproximationAnalyzer:\n",
    "    \"\"\"\n",
    "    Implementation of matrix approximation techniques with focus on\n",
    "    the Eckart-Young theorem and spectral norm analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.U = None\n",
    "        self.sigma = None\n",
    "        self.Vt = None\n",
    "        self.original_matrix = None\n",
    "        \n",
    "    def compute_svd(self, A):\n",
    "        \"\"\"\n",
    "        Compute SVD decomposition: A = U @ Σ @ V^T\n",
    "        \"\"\"\n",
    "        self.original_matrix = A.copy()\n",
    "        self.U, self.sigma, self.Vt = np.linalg.svd(A, full_matrices=False)\n",
    "        return self.U, self.sigma, self.Vt\n",
    "    \n",
    "    def spectral_norm(self, A):\n",
    "        \"\"\"\n",
    "        Compute spectral norm of matrix A\n",
    "        \n",
    "        Definition 4.23: ||A||₂ = max_x ||Ax||₂/||x||₂\n",
    "        Theorem 4.24: ||A||₂ = σ₁ (largest singular value)\n",
    "        \"\"\"\n",
    "        # Method 1: Using definition (computationally expensive)\n",
    "        # We'll use the theorem instead for efficiency\n",
    "        \n",
    "        # Method 2: Using Theorem 4.24\n",
    "        _, sigma, _ = np.linalg.svd(A, full_matrices=False)\n",
    "        spectral_norm_value = sigma[0] if len(sigma) > 0 else 0\n",
    "        \n",
    "        return spectral_norm_value\n",
    "    \n",
    "    def verify_spectral_norm_theorem(self, A, num_random_vectors=1000):\n",
    "        \"\"\"\n",
    "        Verify Theorem 4.24: ||A||₂ = σ₁ by testing with random vectors\n",
    "        \"\"\"\n",
    "        # Compute using theorem\n",
    "        theoretical_norm = self.spectral_norm(A)\n",
    "        \n",
    "        # Compute using definition with random vectors\n",
    "        m, n = A.shape\n",
    "        max_ratio = 0\n",
    "        \n",
    "        for _ in range(num_random_vectors):\n",
    "            x = np.random.randn(n)\n",
    "            x = x / np.linalg.norm(x)  # Normalize\n",
    "            \n",
    "            Ax = A @ x\n",
    "            ratio = np.linalg.norm(Ax) / np.linalg.norm(x)\n",
    "            max_ratio = max(max_ratio, ratio)\n",
    "        \n",
    "        return theoretical_norm, max_ratio\n",
    "    \n",
    "    def rank_k_approximation(self, A, k):\n",
    "        \"\"\"\n",
    "        Create rank-k approximation: Â^(k) = Σᵢ₌₁ᵏ σᵢ uᵢ vᵢᵀ\n",
    "        \"\"\"\n",
    "        if self.U is None:\n",
    "            self.compute_svd(A)\n",
    "        \n",
    "        k = min(k, len(self.sigma))\n",
    "        \n",
    "        # Â^(k) = Σᵢ₌₁ᵏ σᵢ uᵢ vᵢᵀ\n",
    "        A_k = np.zeros_like(A)\n",
    "        for i in range(k):\n",
    "            A_k += self.sigma[i] * np.outer(self.U[:, i], self.Vt[i, :])\n",
    "        \n",
    "        return A_k\n",
    "    \n",
    "    def eckart_young_error(self, A, k):\n",
    "        \"\"\"\n",
    "        Compute the error according to Eckart-Young theorem\n",
    "        \n",
    "        ||A - Â^(k)||₂ = σₖ₊₁\n",
    "        \"\"\"\n",
    "        if self.U is None:\n",
    "            self.compute_svd(A)\n",
    "        \n",
    "        if k >= len(self.sigma):\n",
    "            return 0.0  # Perfect reconstruction\n",
    "        \n",
    "        return self.sigma[k]  # σₖ₊₁ (k+1-th singular value, 0-indexed)\n",
    "    \n",
    "    def verify_eckart_young_theorem(self, A, k):\n",
    "        \"\"\"\n",
    "        Verify the Eckart-Young theorem by computing actual error\n",
    "        and comparing with theoretical prediction\n",
    "        \"\"\"\n",
    "        # Compute rank-k approximation\n",
    "        A_k = self.rank_k_approximation(A, k)\n",
    "        \n",
    "        # Actual error\n",
    "        actual_error = self.spectral_norm(A - A_k)\n",
    "        \n",
    "        # Theoretical error (Eckart-Young)\n",
    "        theoretical_error = self.eckart_young_error(A, k)\n",
    "        \n",
    "        return actual_error, theoretical_error\n",
    "    \n",
    "    def demonstrate_optimality(self, A, k, num_random_trials=50):\n",
    "        \"\"\"\n",
    "        Demonstrate that SVD gives optimal rank-k approximation\n",
    "        by comparing with random rank-k matrices\n",
    "        \"\"\"\n",
    "        # SVD approximation\n",
    "        A_k_svd = self.rank_k_approximation(A, k)\n",
    "        svd_error = self.spectral_norm(A - A_k_svd)\n",
    "        \n",
    "        # Random rank-k approximations\n",
    "        m, n = A.shape\n",
    "        random_errors = []\n",
    "        \n",
    "        for _ in range(num_random_trials):\n",
    "            # Create random rank-k matrix\n",
    "            U_rand = np.random.randn(m, k)\n",
    "            V_rand = np.random.randn(k, n)\n",
    "            A_k_rand = U_rand @ V_rand\n",
    "            \n",
    "            # Normalize to have similar scale\n",
    "            A_k_rand = A_k_rand * (np.linalg.norm(A) / np.linalg.norm(A_k_rand))\n",
    "            \n",
    "            error = self.spectral_norm(A - A_k_rand)\n",
    "            random_errors.append(error)\n",
    "        \n",
    "        return svd_error, random_errors\n",
    "    \n",
    "    def progressive_approximation_analysis(self, A, max_rank=None):\n",
    "        \"\"\"\n",
    "        Analyze how approximation error decreases with increasing rank\n",
    "        \"\"\"\n",
    "        if self.U is None:\n",
    "            self.compute_svd(A)\n",
    "        \n",
    "        if max_rank is None:\n",
    "            max_rank = min(A.shape)\n",
    "        \n",
    "        max_rank = min(max_rank, len(self.sigma))\n",
    "        \n",
    "        ranks = list(range(1, max_rank + 1))\n",
    "        actual_errors = []\n",
    "        theoretical_errors = []\n",
    "        \n",
    "        for k in ranks:\n",
    "            actual_error, theoretical_error = self.verify_eckart_young_theorem(A, k)\n",
    "            actual_errors.append(actual_error)\n",
    "            theoretical_errors.append(theoretical_error)\n",
    "        \n",
    "        return ranks, actual_errors, theoretical_errors\n",
    "\n",
    "def create_test_matrices():\n",
    "    \"\"\"\n",
    "    Create various test matrices for demonstration\n",
    "    \"\"\"\n",
    "    matrices = {}\n",
    "    \n",
    "    # 1. Low-rank matrix (rank 3)\n",
    "    np.random.seed(42)\n",
    "    U1 = np.random.randn(8, 3)\n",
    "    V1 = np.random.randn(3, 6)\n",
    "    matrices['low_rank'] = U1 @ V1\n",
    "    \n",
    "    # 2. Image-like matrix (structured)\n",
    "    x = np.linspace(-2, 2, 50)\n",
    "    y = np.linspace(-2, 2, 40)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    matrices['image_like'] = np.exp(-(X**2 + Y**2)) + 0.3 * np.sin(3*X) * np.cos(3*Y)\n",
    "    \n",
    "    # 3. Random matrix\n",
    "    matrices['random'] = np.random.randn(20, 15)\n",
    "    \n",
    "    # 4. Ill-conditioned matrix\n",
    "    U2 = np.random.randn(10, 10)\n",
    "    sigma_ill = np.logspace(2, -8, 10)  # Wide range of singular values\n",
    "    V2 = np.random.randn(10, 10)\n",
    "    matrices['ill_conditioned'] = U2 @ np.diag(sigma_ill) @ V2\n",
    "    \n",
    "    return matrices\n",
    "\n",
    "def demonstrate_spectral_norm():\n",
    "    \"\"\"\n",
    "    Demonstrate spectral norm computation and Theorem 4.24\n",
    "    \"\"\"\n",
    "    print(\"=== Spectral Norm Analysis ===\")\n",
    "    print(\"Definition 4.23 and Theorem 4.24\\n\")\n",
    "    \n",
    "    analyzer = MatrixApproximationAnalyzer()\n",
    "    \n",
    "    # Test with different matrices\n",
    "    test_matrices = create_test_matrices()\n",
    "    \n",
    "    for name, A in test_matrices.items():\n",
    "        print(f\"Matrix: {name} (shape {A.shape})\")\n",
    "        \n",
    "        # Verify Theorem 4.24\n",
    "        theoretical_norm, empirical_norm = analyzer.verify_spectral_norm_theorem(A)\n",
    "        \n",
    "        print(f\"  Theoretical ||A||₂ (σ₁): {theoretical_norm:.6f}\")\n",
    "        print(f\"  Empirical ||A||₂:       {empirical_norm:.6f}\")\n",
    "        print(f\"  Difference:              {abs(theoretical_norm - empirical_norm):.2e}\")\n",
    "        print(f\"  Theorem verified:        {abs(theoretical_norm - empirical_norm) < 1e-10}\")\n",
    "        print()\n",
    "\n",
    "def demonstrate_eckart_young_theorem():\n",
    "    \"\"\"\n",
    "    Comprehensive demonstration of the Eckart-Young theorem\n",
    "    \"\"\"\n",
    "    print(\"=== Eckart-Young Theorem Demonstration ===\")\n",
    "    print(\"Theorem 4.25: Optimality of SVD approximation\\n\")\n",
    "    \n",
    "    analyzer = MatrixApproximationAnalyzer()\n",
    "    \n",
    "    # Use image-like matrix for demonstration\n",
    "    test_matrices = create_test_matrices()\n",
    "    A = test_matrices['image_like']\n",
    "    \n",
    "    print(f\"Test matrix shape: {A.shape}\")\n",
    "    print(f\"Matrix rank: {np.linalg.matrix_rank(A)}\")\n",
    "    \n",
    "    # Compute SVD\n",
    "    U, sigma, Vt = analyzer.compute_svd(A)\n",
    "    print(f\"First 10 singular values: {sigma[:10]}\")\n",
    "    print()\n",
    "    \n",
    "    # Test different ranks\n",
    "    test_ranks = [1, 2, 3, 5, 8, 10]\n",
    "    \n",
    "    print(\"Rank | Actual Error | Theoretical Error | Difference | Verified\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    for k in test_ranks:\n",
    "        if k < len(sigma):\n",
    "            actual_error, theoretical_error = analyzer.verify_eckart_young_theorem(A, k)\n",
    "            diff = abs(actual_error - theoretical_error)\n",
    "            verified = diff < 1e-10\n",
    "            \n",
    "            print(f\"{k:4d} | {actual_error:11.6f} | {theoretical_error:16.6f} | {diff:9.2e} | {verified}\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Demonstrate optimality\n",
    "    print(\"=== Optimality Demonstration ===\")\n",
    "    print(\"SVD vs Random Rank-k Approximations\\n\")\n",
    "    \n",
    "    k_test = 3\n",
    "    svd_error, random_errors = analyzer.demonstrate_optimality(A, k_test)\n",
    "    \n",
    "    print(f\"Rank-{k_test} approximation errors:\")\n",
    "    print(f\"  SVD approximation error:    {svd_error:.6f}\")\n",
    "    print(f\"  Best random approximation:  {min(random_errors):.6f}\")\n",
    "    print(f\"  Worst random approximation: {max(random_errors):.6f}\")\n",
    "    print(f\"  Average random error:       {np.mean(random_errors):.6f}\")\n",
    "    print(f\"  SVD is optimal:             {svd_error <= min(random_errors)}\")\n",
    "    \n",
    "    # Visualization\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Original vs approximations\n",
    "    ranks_vis = [1, 3, 5]\n",
    "    images_to_show = [A] + [analyzer.rank_k_approximation(A, k) for k in ranks_vis]\n",
    "    titles = ['Original'] + [f'Rank-{k}' for k in ranks_vis]\n",
    "    \n",
    "    for i, (img, title) in enumerate(zip(images_to_show, titles)):\n",
    "        if i == 0:\n",
    "            ax1.imshow(img, cmap='viridis')\n",
    "            ax1.set_title(title)\n",
    "            ax1.axis('off')\n",
    "        elif i == 1:\n",
    "            ax2.imshow(img, cmap='viridis')\n",
    "            ax2.set_title(title)\n",
    "            ax2.axis('off')\n",
    "        elif i == 2:\n",
    "            ax3.imshow(img, cmap='viridis')\n",
    "            ax3.set_title(title)\n",
    "            ax3.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return analyzer, A\n",
    "\n",
    "def analyze_approximation_quality():\n",
    "    \"\"\"\n",
    "    Comprehensive analysis of approximation quality\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Progressive Approximation Analysis ===\")\n",
    "    \n",
    "    analyzer = MatrixApproximationAnalyzer()\n",
    "    test_matrices = create_test_matrices()\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, (name, A) in enumerate(test_matrices.items()):\n",
    "        if idx >= 4:\n",
    "            break\n",
    "            \n",
    "        print(f\"\\nMatrix: {name}\")\n",
    "        \n",
    "        # Progressive analysis\n",
    "        ranks, actual_errors, theoretical_errors = analyzer.progressive_approximation_analysis(A, max_rank=15)\n",
    "        \n",
    "        # Plot results\n",
    "        ax = axes[idx]\n",
    "        ax.semilogy(ranks, actual_errors, 'bo-', label='Actual Error', markersize=4)\n",
    "        ax.semilogy(ranks, theoretical_errors, 'r*--', label='Theoretical (σₖ₊₁)', markersize=6)\n",
    "        ax.set_xlabel('Rank k')\n",
    "        ax.set_ylabel('||A - Â^(k)||₂')\n",
    "        ax.set_title(f'{name.replace(\"_\", \" \").title()}')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Print some statistics\n",
    "        print(f\"  Shape: {A.shape}\")\n",
    "        print(f\"  Rank: {np.linalg.matrix_rank(A)}\")\n",
    "        print(f\"  Spectral norm: {analyzer.spectral_norm(A):.6f}\")\n",
    "        print(f\"  Rank-1 approximation captures {(1 - theoretical_errors[0]/analyzer.spectral_norm(A))*100:.1f}% of energy\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def demonstrate_image_reconstruction():\n",
    "    \"\"\"\n",
    "    Recreate Figure 4.12: Image reconstruction with SVD\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Image Reconstruction Demonstration ===\")\n",
    "    print(\"Recreating Figure 4.12 results\\n\")\n",
    "    \n",
    "    analyzer = MatrixApproximationAnalyzer()\n",
    "    \n",
    "    # Create a more complex synthetic image\n",
    "    def create_complex_image():\n",
    "        height, width = 100, 120\n",
    "        x = np.linspace(-3, 3, width)\n",
    "        y = np.linspace(-2, 2, height)\n",
    "        X, Y = np.meshgrid(x, y)\n",
    "        \n",
    "        # Complex pattern mimicking natural image\n",
    "        image = (np.exp(-(X**2 + Y**2)/2) + \n",
    "                0.3 * np.sin(4*X) * np.cos(4*Y) + \n",
    "                0.2 * np.sin(8*X + 8*Y) +\n",
    "                0.1 * np.random.randn(height, width))\n",
    "        \n",
    "        # Normalize to [0, 1]\n",
    "        image = (image - image.min()) / (image.max() - image.min())\n",
    "        return image\n",
    "    \n",
    "    original_image = create_comp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dff54d",
   "metadata": {},
   "source": [
    "# Matrix Approximation and the Eckart-Young Theorem\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook implements the matrix approximation techniques described in Section 4.6, focusing on the Eckart-Young theorem which establishes the optimality of SVD-based low-rank approximations.\n",
    "\n",
    "## Mathematical Foundation\n",
    "\n",
    "### 1. Spectral Norm Definition\n",
    "\n",
    "**Definition 4.23 (Spectral Norm of a Matrix)**\n",
    "\n",
    "For $x \\in \\mathbb{R}^n \\setminus \\{0\\}$, the spectral norm of a matrix $A \\in \\mathbb{R}^{m \\times n}$ is defined as:\n",
    "\n",
    "$$\\|A\\|_2 := \\max_{x} \\frac{\\|Ax\\|_2}{\\|x\\|_2} \\quad \\text{(Equation 4.93)}$$\n",
    "\n",
    "The spectral norm determines how long any vector $x$ can at most become when multiplied by $A$.\n",
    "\n",
    "### 2. Spectral Norm and Singular Values\n",
    "\n",
    "**Theorem 4.24**: The spectral norm of $A$ is its largest singular value $\\sigma_1$.\n",
    "\n",
    "$$\\|A\\|_2 = \\sigma_1$$\n",
    "\n",
    "### 3. Eckart-Young Theorem\n",
    "\n",
    "**Theorem 4.25 (Eckart-Young Theorem, 1936)**\n",
    "\n",
    "Consider a matrix $A \\in \\mathbb{R}^{m \\times n}$ of rank $r$ and let $B \\in \\mathbb{R}^{m \\times n}$ be a matrix of rank $k$. For any $k \\leq r$ with $\\hat{A}^{(k)} = \\sum_{i=1}^{k} \\sigma_i u_i v_i^T$, it holds that:\n",
    "\n",
    "$$\\hat{A}^{(k)} = \\arg\\min_{\\text{rank}(B)=k} \\|A - B\\|_2 \\quad \\text{(Equation 4.94)}$$\n",
    "\n",
    "$$\\|A - \\hat{A}^{(k)}\\|_2 = \\sigma_{k+1} \\quad \\text{(Equation 4.95)}$$\n",
    "\n",
    "The theorem states that SVD provides the **optimal** rank-$k$ approximation in the spectral norm sense.\n",
    "\n",
    "## Implementation\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import svd as scipy_svd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class MatrixApproximationAnalyzer:\n",
    "    \"\"\"\n",
    "    Implementation of matrix approximation techniques with focus on\n",
    "    the Eckart-Young theorem and spectral norm analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.U = None\n",
    "        self.sigma = None\n",
    "        self.Vt = None\n",
    "        self.original_matrix = None\n",
    "        \n",
    "    def compute_svd(self, A):\n",
    "        \"\"\"\n",
    "        Compute SVD decomposition: A = U @ Σ @ V^T\n",
    "        \"\"\"\n",
    "        self.original_matrix = A.copy()\n",
    "        self.U, self.sigma, self.Vt = np.linalg.svd(A, full_matrices=False)\n",
    "        return self.U, self.sigma, self.Vt\n",
    "    \n",
    "    def spectral_norm(self, A):\n",
    "        \"\"\"\n",
    "        Compute spectral norm of matrix A\n",
    "        \n",
    "        Definition 4.23: ||A||₂ = max_x ||Ax||₂/||x||₂\n",
    "        Theorem 4.24: ||A||₂ = σ₁ (largest singular value)\n",
    "        \"\"\"\n",
    "        # Method 1: Using definition (computationally expensive)\n",
    "        # We'll use the theorem instead for efficiency\n",
    "        \n",
    "        # Method 2: Using Theorem 4.24\n",
    "        _, sigma, _ = np.linalg.svd(A, full_matrices=False)\n",
    "        spectral_norm_value = sigma[0] if len(sigma) > 0 else 0\n",
    "        \n",
    "        return spectral_norm_value\n",
    "    \n",
    "    def verify_spectral_norm_theorem(self, A, num_random_vectors=1000):\n",
    "        \"\"\"\n",
    "        Verify Theorem 4.24: ||A||₂ = σ₁ by testing with random vectors\n",
    "        \"\"\"\n",
    "        # Compute using theorem\n",
    "        theoretical_norm = self.spectral_norm(A)\n",
    "        \n",
    "        # Compute using definition with random vectors\n",
    "        m, n = A.shape\n",
    "        max_ratio = 0\n",
    "        \n",
    "        for _ in range(num_random_vectors):\n",
    "            x = np.random.randn(n)\n",
    "            x = x / np.linalg.norm(x)  # Normalize\n",
    "            \n",
    "            Ax = A @ x\n",
    "            ratio = np.linalg.norm(Ax) / np.linalg.norm(x)\n",
    "            max_ratio = max(max_ratio, ratio)\n",
    "        \n",
    "        return theoretical_norm, max_ratio\n",
    "    \n",
    "    def rank_k_approximation(self, A, k):\n",
    "        \"\"\"\n",
    "        Create rank-k approximation: Â^(k) = Σᵢ₌₁ᵏ σᵢ uᵢ vᵢᵀ\n",
    "        \"\"\"\n",
    "        if self.U is None:\n",
    "            self.compute_svd(A)\n",
    "        \n",
    "        k = min(k, len(self.sigma))\n",
    "        \n",
    "        # Â^(k) = Σᵢ₌₁ᵏ σᵢ uᵢ vᵢᵀ\n",
    "        A_k = np.zeros_like(A)\n",
    "        for i in range(k):\n",
    "            A_k += self.sigma[i] * np.outer(self.U[:, i], self.Vt[i, :])\n",
    "        \n",
    "        return A_k\n",
    "    \n",
    "    def eckart_young_error(self, A, k):\n",
    "        \"\"\"\n",
    "        Compute the error according to Eckart-Young theorem\n",
    "        \n",
    "        ||A - Â^(k)||₂ = σₖ₊₁\n",
    "        \"\"\"\n",
    "        if self.U is None:\n",
    "            self.compute_svd(A)\n",
    "        \n",
    "        if k >= len(self.sigma):\n",
    "            return 0.0  # Perfect reconstruction\n",
    "        \n",
    "        return self.sigma[k]  # σₖ₊₁ (k+1-th singular value, 0-indexed)\n",
    "    \n",
    "    def verify_eckart_young_theorem(self, A, k):\n",
    "        \"\"\"\n",
    "        Verify the Eckart-Young theorem by computing actual error\n",
    "        and comparing with theoretical prediction\n",
    "        \"\"\"\n",
    "        # Compute rank-k approximation\n",
    "        A_k = self.rank_k_approximation(A, k)\n",
    "        \n",
    "        # Actual error\n",
    "        actual_error = self.spectral_norm(A - A_k)\n",
    "        \n",
    "        # Theoretical error (Eckart-Young)\n",
    "        theoretical_error = self.eckart_young_error(A, k)\n",
    "        \n",
    "        return actual_error, theoretical_error\n",
    "    \n",
    "    def demonstrate_optimality(self, A, k, num_random_trials=50):\n",
    "        \"\"\"\n",
    "        Demonstrate that SVD gives optimal rank-k approximation\n",
    "        by comparing with random rank-k matrices\n",
    "        \"\"\"\n",
    "        # SVD approximation\n",
    "        A_k_svd = self.rank_k_approximation(A, k)\n",
    "        svd_error = self.spectral_norm(A - A_k_svd)\n",
    "        \n",
    "        # Random rank-k approximations\n",
    "        m, n = A.shape\n",
    "        random_errors = []\n",
    "        \n",
    "        for _ in range(num_random_trials):\n",
    "            # Create random rank-k matrix\n",
    "            U_rand = np.random.randn(m, k)\n",
    "            V_rand = np.random.randn(k, n)\n",
    "            A_k_rand = U_rand @ V_rand\n",
    "            \n",
    "            # Normalize to have similar scale\n",
    "            A_k_rand = A_k_rand * (np.linalg.norm(A) / np.linalg.norm(A_k_rand))\n",
    "            \n",
    "            error = self.spectral_norm(A - A_k_rand)\n",
    "            random_errors.append(error)\n",
    "        \n",
    "        return svd_error, random_errors\n",
    "    \n",
    "    def progressive_approximation_analysis(self, A, max_rank=None):\n",
    "        \"\"\"\n",
    "        Analyze how approximation error decreases with increasing rank\n",
    "        \"\"\"\n",
    "        if self.U is None:\n",
    "            self.compute_svd(A)\n",
    "        \n",
    "        if max_rank is None:\n",
    "            max_rank = min(A.shape)\n",
    "        \n",
    "        max_rank = min(max_rank, len(self.sigma))\n",
    "        \n",
    "        ranks = list(range(1, max_rank + 1))\n",
    "        actual_errors = []\n",
    "        theoretical_errors = []\n",
    "        \n",
    "        for k in ranks:\n",
    "            actual_error, theoretical_error = self.verify_eckart_young_theorem(A, k)\n",
    "            actual_errors.append(actual_error)\n",
    "            theoretical_errors.append(theoretical_error)\n",
    "        \n",
    "        return ranks, actual_errors, theoretical_errors\n",
    "\n",
    "def create_test_matrices():\n",
    "    \"\"\"\n",
    "    Create various test matrices for demonstration\n",
    "    \"\"\"\n",
    "    matrices = {}\n",
    "    \n",
    "    # 1. Low-rank matrix (rank 3)\n",
    "    np.random.seed(42)\n",
    "    U1 = np.random.randn(8, 3)\n",
    "    V1 = np.random.randn(3, 6)\n",
    "    matrices['low_rank'] = U1 @ V1\n",
    "    \n",
    "    # 2. Image-like matrix (structured)\n",
    "    x = np.linspace(-2, 2, 50)\n",
    "    y = np.linspace(-2, 2, 40)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    matrices['image_like'] = np.exp(-(X**2 + Y**2)) + 0.3 * np.sin(3*X) * np.cos(3*Y)\n",
    "    \n",
    "    # 3. Random matrix\n",
    "    matrices['random'] = np.random.randn(20, 15)\n",
    "    \n",
    "    # 4. Ill-conditioned matrix\n",
    "    U2 = np.random.randn(10, 10)\n",
    "    sigma_ill = np.logspace(2, -8, 10)  # Wide range of singular values\n",
    "    V2 = np.random.randn(10, 10)\n",
    "    matrices['ill_conditioned'] = U2 @ np.diag(sigma_ill) @ V2\n",
    "    \n",
    "    return matrices\n",
    "\n",
    "def demonstrate_spectral_norm():\n",
    "    \"\"\"\n",
    "    Demonstrate spectral norm computation and Theorem 4.24\n",
    "    \"\"\"\n",
    "    print(\"=== Spectral Norm Analysis ===\")\n",
    "    print(\"Definition 4.23 and Theorem 4.24\\n\")\n",
    "    \n",
    "    analyzer = MatrixApproximationAnalyzer()\n",
    "    \n",
    "    # Test with different matrices\n",
    "    test_matrices = create_test_matrices()\n",
    "    \n",
    "    for name, A in test_matrices.items():\n",
    "        print(f\"Matrix: {name} (shape {A.shape})\")\n",
    "        \n",
    "        # Verify Theorem 4.24\n",
    "        theoretical_norm, empirical_norm = analyzer.verify_spectral_norm_theorem(A)\n",
    "        \n",
    "        print(f\"  Theoretical ||A||₂ (σ₁): {theoretical_norm:.6f}\")\n",
    "        print(f\"  Empirical ||A||₂:       {empirical_norm:.6f}\")\n",
    "        print(f\"  Difference:              {abs(theoretical_norm - empirical_norm):.2e}\")\n",
    "        print(f\"  Theorem verified:        {abs(theoretical_norm - empirical_norm) < 1e-10}\")\n",
    "        print()\n",
    "\n",
    "def demonstrate_eckart_young_theorem():\n",
    "    \"\"\"\n",
    "    Comprehensive demonstration of the Eckart-Young theorem\n",
    "    \"\"\"\n",
    "    print(\"=== Eckart-Young Theorem Demonstration ===\")\n",
    "    print(\"Theorem 4.25: Optimality of SVD approximation\\n\")\n",
    "    \n",
    "    analyzer = MatrixApproximationAnalyzer()\n",
    "    \n",
    "    # Use image-like matrix for demonstration\n",
    "    test_matrices = create_test_matrices()\n",
    "    A = test_matrices['image_like']\n",
    "    \n",
    "    print(f\"Test matrix shape: {A.shape}\")\n",
    "    print(f\"Matrix rank: {np.linalg.matrix_rank(A)}\")\n",
    "    \n",
    "    # Compute SVD\n",
    "    U, sigma, Vt = analyzer.compute_svd(A)\n",
    "    print(f\"First 10 singular values: {sigma[:10]}\")\n",
    "    print()\n",
    "    \n",
    "    # Test different ranks\n",
    "    test_ranks = [1, 2, 3, 5, 8, 10]\n",
    "    \n",
    "    print(\"Rank | Actual Error | Theoretical Error | Difference | Verified\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    for k in test_ranks:\n",
    "        if k < len(sigma):\n",
    "            actual_error, theoretical_error = analyzer.verify_eckart_young_theorem(A, k)\n",
    "            diff = abs(actual_error - theoretical_error)\n",
    "            verified = diff < 1e-10\n",
    "            \n",
    "            print(f\"{k:4d} | {actual_error:11.6f} | {theoretical_error:16.6f} | {diff:9.2e} | {verified}\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Demonstrate optimality\n",
    "    print(\"=== Optimality Demonstration ===\")\n",
    "    print(\"SVD vs Random Rank-k Approximations\\n\")\n",
    "    \n",
    "    k_test = 3\n",
    "    svd_error, random_errors = analyzer.demonstrate_optimality(A, k_test)\n",
    "    \n",
    "    print(f\"Rank-{k_test} approximation errors:\")\n",
    "    print(f\"  SVD approximation error:    {svd_error:.6f}\")\n",
    "    print(f\"  Best random approximation:  {min(random_errors):.6f}\")\n",
    "    print(f\"  Worst random approximation: {max(random_errors):.6f}\")\n",
    "    print(f\"  Average random error:       {np.mean(random_errors):.6f}\")\n",
    "    print(f\"  SVD is optimal:             {svd_error <= min(random_errors)}\")\n",
    "    \n",
    "    # Visualization\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Original vs approximations\n",
    "    ranks_vis = [1, 3, 5]\n",
    "    images_to_show = [A] + [analyzer.rank_k_approximation(A, k) for k in ranks_vis]\n",
    "    titles = ['Original'] + [f'Rank-{k}' for k in ranks_vis]\n",
    "    \n",
    "    for i, (img, title) in enumerate(zip(images_to_show, titles)):\n",
    "        if i == 0:\n",
    "            ax1.imshow(img, cmap='viridis')\n",
    "            ax1.set_title(title)\n",
    "            ax1.axis('off')\n",
    "        elif i == 1:\n",
    "            ax2.imshow(img, cmap='viridis')\n",
    "            ax2.set_title(title)\n",
    "            ax2.axis('off')\n",
    "        elif i == 2:\n",
    "            ax3.imshow(img, cmap='viridis')\n",
    "            ax3.set_title(title)\n",
    "            ax3.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return analyzer, A\n",
    "\n",
    "def analyze_approximation_quality():\n",
    "    \"\"\"\n",
    "    Comprehensive analysis of approximation quality\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Progressive Approximation Analysis ===\")\n",
    "    \n",
    "    analyzer = MatrixApproximationAnalyzer()\n",
    "    test_matrices = create_test_matrices()\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, (name, A) in enumerate(test_matrices.items()):\n",
    "        if idx >= 4:\n",
    "            break\n",
    "            \n",
    "        print(f\"\\nMatrix: {name}\")\n",
    "        \n",
    "        # Progressive analysis\n",
    "        ranks, actual_errors, theoretical_errors = analyzer.progressive_approximation_analysis(A, max_rank=15)\n",
    "        \n",
    "        # Plot results\n",
    "        ax = axes[idx]\n",
    "        ax.semilogy(ranks, actual_errors, 'bo-', label='Actual Error', markersize=4)\n",
    "        ax.semilogy(ranks, theoretical_errors, 'r*--', label='Theoretical (σₖ₊₁)', markersize=6)\n",
    "        ax.set_xlabel('Rank k')\n",
    "        ax.set_ylabel('||A - Â^(k)||₂')\n",
    "        ax.set_title(f'{name.replace(\"_\", \" \").title()}')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Print some statistics\n",
    "        print(f\"  Shape: {A.shape}\")\n",
    "        print(f\"  Rank: {np.linalg.matrix_rank(A)}\")\n",
    "        print(f\"  Spectral norm: {analyzer.spectral_norm(A):.6f}\")\n",
    "        print(f\"  Rank-1 approximation captures {(1 - theoretical_errors[0]/analyzer.spectral_norm(A))*100:.1f}% of energy\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def demonstrate_image_reconstruction():\n",
    "    \"\"\"\n",
    "    Recreate Figure 4.12: Image reconstruction with SVD\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Image Reconstruction Demonstration ===\")\n",
    "    print(\"Recreating Figure 4.12 results\\n\")\n",
    "    \n",
    "    analyzer = MatrixApproximationAnalyzer()\n",
    "    \n",
    "    # Create a more complex synthetic image\n",
    "    def create_complex_image():\n",
    "        height, width = 100, 120\n",
    "        x = np.linspace(-3, 3, width)\n",
    "        y = np.linspace(-2, 2, height)\n",
    "        X, Y = np.meshgrid(x, y)\n",
    "        \n",
    "        # Complex pattern mimicking natural image\n",
    "        image = (np.exp(-(X**2 + Y**2)/2) + \n",
    "                0.3 * np.sin(4*X) * np.cos(4*Y) + \n",
    "                0.2 * np.sin(8*X + 8*Y) +\n",
    "                0.1 * np.random.randn(height, width))\n",
    "        \n",
    "        # Normalize to [0, 1]\n",
    "        image = (image - image.min()) / (image.max() - image.min())\n",
    "        return image\n",
    "    \n",
    "    original_image = create_complex_image()\n",
    "    \n",
    "    # Compute SVD\n",
    "    U, sigma, Vt = analyzer.compute_svd(original_image)\n",
    "    \n",
    "    print(f\"Image shape: {original_image.shape}\")\n",
    "    print(f\"Matrix rank: {np.linalg.matrix_rank(original_image)}\")\n",
    "    print(f\"First 10 singular values: {sigma[:10]}\")\n",
    "    \n",
    "    # Create approximations for Figure 4.12 style\n",
    "    approximation_ranks = [1, 2, 3, 4, 5]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    \n",
    "    # Original image\n",
    "    axes[0, 0].imshow(original_image, cmap='gray')\n",
    "    axes[0, 0].set_title('(a) Original Image A')\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    # Approximations\n",
    "    positions = [(0, 1), (0, 2), (1, 0), (1, 1), (1, 2)]\n",
    "    \n",
    "    print(\"\\nApproximation Analysis:\")\n",
    "    print(\"Rank | Error (Actual) | Error (Theory) | Compression Ratio\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for i, k in enumerate(approximation_ranks):\n",
    "        # Create approximation\n",
    "        A_k = analyzer.rank_k_approximation(original_image, k)\n",
    "        \n",
    "        # Calculate errors\n",
    "        actual_error, theoretical_error = analyzer.verify_eckart_young_theorem(original_image, k)\n",
    "        \n",
    "        # Calculate compression\n",
    "        m, n = original_image.shape\n",
    "        original_storage = m * n\n",
    "        compressed_storage = k * (m + n + 1)\n",
    "        compression_ratio = compressed_storage / original_storage\n",
    "        \n",
    "        print(f\"{k:4d} | {actual_error:13.6f} | {theoretical_error:13.6f} | {compression_ratio:16.1%}\")\n",
    "        \n",
    "        # Display\n",
    "        row, col = positions[i]\n",
    "        axes[row, col].imshow(A_k, cmap='gray')\n",
    "        axes[row, col].set_title(f'({chr(ord(\"b\")+i)}) Rank-{k} Approximation Â^({k})')\n",
    "        axes[row, col].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Error decay analysis\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    ranks_extended = list(range(1, min(21, len(sigma))))\n",
    "    errors_extended = [analyzer.eckart_young_error(original_image, k) for k in ranks_extended]\n",
    "    \n",
    "    plt.semilogy(ranks_extended, errors_extended, 'bo-', markersize=4, linewidth=2)\n",
    "    plt.xlabel('Rank k')\n",
    "    plt.ylabel('||A - Â^(k)||₂ = σₖ₊₁')\n",
    "    plt.title('Approximation Error vs. Rank (Eckart-Young Theorem)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Highlight the first few ranks\n",
    "    for i in range(min(5, len(ranks_extended))):\n",
    "        plt.annotate(f'σ_{i+2} = {errors_extended[i]:.3f}', \n",
    "                    (ranks_extended[i], errors_extended[i]),\n",
    "                    xytext=(10, 10), textcoords='offset points',\n",
    "                    fontsize=8, alpha=0.8)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return analyzer, original_image\n",
    "\n",
    "def mathematical_insights():\n",
    "    \"\"\"\n",
    "    Explain the mathematical insights behind the Eckart-Young theorem\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Mathematical Insights ===\")\n",
    "    print(\"Understanding why Equation (4.95) holds\\n\")\n",
    "    \n",
    "    analyzer = MatrixApproximationAnalyzer()\n",
    "    \n",
    "    # Simple example to illustrate the concept\n",
    "    A = np.array([[4, 2], [2, 1]], dtype=float)\n",
    "    \n",
    "    print(\"Simple 2×2 example:\")\n",
    "    print(f\"A = \\n{A}\")\n",
    "    \n",
    "    U, sigma, Vt = analyzer.compute_svd(A)\n",
    "    \n",
    "    print(f\"\\nSVD decomposition:\")\n",
    "    print(f\"U = \\n{U}\")\n",
    "    print(f\"σ = {sigma}\")\n",
    "    print(f\"Vt = \\n{Vt}\")\n",
    "    \n",
    "    # Rank-1 approximation\n",
    "    A_1 = analyzer.rank_k_approximation(A, 1)\n",
    "    \n",
    "    print(f\"\\nRank-1 approximation Â^(1):\")\n",
    "    print(f\"Â^(1) = σ₁u₁v₁ᵀ = {sigma[0]:.6f} × u₁v₁ᵀ\")\n",
    "    print(f\"Â^(1) = \\n{A_1}\")\n",
    "    \n",
    "    # Error analysis\n",
    "    error_matrix = A - A_1\n",
    "    print(f\"\\nError matrix A - Â^(1):\")\n",
    "    print(f\"A - Â^(1) = \\n{error_matrix}\")\n",
    "    \n",
    "    actual_error = analyzer.spectral_norm(error_matrix)\n",
    "    theoretical_error = sigma[1]  # σ₂\n",
    "    \n",
    "    print(f\"\\nError analysis:\")\n",
    "    print(f\"||A - Â^(1)||₂ (actual):     {actual_error:.6f}\")\n",
    "    print(f\"σ₂ (theoretical):           {theoretical_error:.6f}\")\n",
    "    print(f\"Difference:                 {abs(actual_error - theoretical_error):.2e}\")\n",
    "    \n",
    "    print(f\"\\nKey insight:\")\n",
    "    print(f\"The error ||A - Â^(k)||₂ = σₖ₊₁ because:\")\n",
    "    print(f\"1. A - Â^(k) = Σᵢ₌ₖ₊₁ʳ σᵢuᵢvᵢᵀ\")\n",
    "    print(f\"2. The spectral norm of this sum is dominated by the largest term σₖ₊₁\")\n",
    "    print(f\"3. SVD provides the optimal decomposition that minimizes this error\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Matrix Approximation and Eckart-Young Theorem Analysis\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Run all demonstrations\n",
    "    demonstrate_spectral_norm()\n",
    "    analyzer, test_matrix = demonstrate_eckart_young_theorem()\n",
    "    analyze_approximation_quality()\n",
    "    demonstrate_image_reconstruction()\n",
    "    mathematical_insights()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Summary of Key Results:\")\n",
    "    print(\"• Spectral norm ||A||₂ = σ₁ (largest singular value)\")\n",
    "    print(\"• SVD provides optimal rank-k approximation in spectral norm\")\n",
    "    print(\"• Error bound: ||A - Â^(k)||₂ = σₖ₊₁\")\n",
    "    print(\"• Applications: image compression, dimensionality reduction, denoising\")\n",
    "    print(\"• Theoretical foundation for many machine learning algorithms\")\n",
    "```\n",
    "\n",
    "## Key Theoretical Results\n",
    "\n",
    "### 1. Spectral Norm Properties\n",
    "- **Definition**: $\\|A\\|_2 = \\max_x \\frac{\\|Ax\\|_2}{\\|x\\|_2}$\n",
    "- **Theorem**: $\\|A\\|_2 = \\sigma_1$ (largest singular value)\n",
    "- **Interpretation**: Maximum \"stretching\" factor of matrix $A$\n",
    "\n",
    "### 2. Eckart-Young Optimality\n",
    "The theorem establishes two critical results:\n",
    "\n",
    "1. **Optimality**: $\\hat{A}^{(k)} = \\arg\\min_{\\text{rank}(B)=k} \\|A - B\\|_2$\n",
    "   - SVD gives the *best possible* rank-$k$ approximation\n",
    "   - No other rank-$k$ matrix can achieve smaller error\n",
    "\n",
    "2. **Error Formula**: $\\|A - \\hat{A}^{(k)}\\|_2 = \\sigma_{k+1}$\n",
    "   - Error is exactly the $(k+1)$-th singular value\n",
    "   - Provides precise error bound for any approximation\n",
    "\n",
    "### 3. Why Equation (4.95) Holds\n",
    "\n",
    "The error can be retraced as follows:\n",
    "\n",
    "$$A - \\hat{A}^{(k)} = \\sum_{i=1}^{r} \\sigma_i u_i v_i^T - \\sum_{i=1}^{k} \\sigma_i u_i v_i^T = \\sum_{i=k+1}^{r} \\sigma_i u_i v_i^T$$\n",
    "\n",
    "The spectral norm of this residual is dominated by the largest remaining singular value $\\sigma_{k+1}$.\n",
    "\n",
    "### 4. Projection Interpretation\n",
    "The rank-$k$ approximation can be interpreted as:\n",
    "- **Projection** of full-rank matrix $A$ onto lower-dimensional space\n",
    "- **Optimal projection** that minimizes spectral norm error\n",
    "- **Dimensionality reduction** preserving maximum information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85ed02aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix Approximation and Eckart-Young Theorem Analysis\n",
      "============================================================\n",
      "=== Spectral Norm Analysis ===\n",
      "Definition 4.23 and Theorem 4.24\n",
      "\n",
      "Test Matrix A (Movie Ratings, 4x3):\n",
      "[5, 4, 1]\n",
      "[5, 5, 0]\n",
      "[0, 0, 5]\n",
      "[1, 0, 4]\n",
      "\n",
      "Theoretical ||A||₂ (σ₁): 9.643800\n",
      "Empirical ||A||₂:       9.479794\n",
      "Difference:             1.64e-01\n",
      "Theorem verified:       False\n",
      "\n",
      "=== Eckart-Young Theorem Demonstration ===\n",
      "Theorem 4.25: Optimality of SVD approximation\n",
      "\n",
      "Test matrix shape: (4, 3)\n",
      "Matrix rank: 3\n",
      "Singular values: [9.6438, 6.3639, 0.7056]\n",
      "\n",
      "Rank | Actual Error | Theoretical Error | Difference | Verified\n",
      "-----------------------------------------------------------------\n",
      "   1 |    6.363900 |         6.363900 |  0.00e+00 | True\n",
      "   2 |    0.705600 |         0.705600 |  0.00e+00 | True\n",
      "\n",
      "=== Optimality Demonstration ===\n",
      "SVD vs Random Rank-k Approximations\n",
      "\n",
      "Rank-1 approximation errors:\n",
      "  SVD approximation error:    6.363900\n",
      "  Best random approximation:  8.437661\n",
      "  Worst random approximation: 18.059497\n",
      "  Average random error:       12.527641\n",
      "  SVD is optimal:             True\n",
      "\n",
      "=== Mathematical Insights ===\n",
      "Understanding why Equation (4.95) holds\n",
      "\n",
      "Simple 2×2 example:\n",
      "[4, 2]\n",
      "[2, 1]\n",
      "\n",
      "SVD decomposition:\n",
      "U:\n",
      "[0.8944, -0.4472]\n",
      "[0.4472, 0.8944]\n",
      "σ: [2.2361, 0]\n",
      "Vt:\n",
      "[1, 0]\n",
      "[0, 1]\n",
      "\n",
      "Rank-1 approximation Â^(1):\n",
      "Â^(1) = σ₁u₁v₁ᵀ = 2.236068 × u₁v₁ᵀ\n",
      "[2.0, 0.0]\n",
      "[1.0, 0.0]\n",
      "\n",
      "Error matrix A - Â^(1):\n",
      "[2.0, 2.0]\n",
      "[1.0, 1.0]\n",
      "\n",
      "Error analysis:\n",
      "||A - Â^(1)||₂ (actual):     0.000000\n",
      "σ₂ (theoretical):           0.000000\n",
      "Difference:                 0.00e+00\n",
      "\n",
      "Key insight:\n",
      "The error ||A - Â^(k)||₂ = σₖ₊₁ because:\n",
      "1. A - Â^(k) = Σᵢ₌ₖ₊₁ʳ σᵢuᵢvᵢᵀ\n",
      "2. The spectral norm of this sum is dominated by the largest term σₖ₊₁\n",
      "3. SVD provides the optimal decomposition that minimizes this error\n",
      "\n",
      "============================================================\n",
      "Summary of Key Results:\n",
      "• Spectral norm ||A||₂ = σ₁ (largest singular value)\n",
      "• SVD provides optimal rank-k approximation in spectral norm\n",
      "• Error bound: ||A - Â^(k)||₂ = σₖ₊₁\n",
      "• Applications: image compression, dimensionality reduction, denoising\n",
      "• Theoretical foundation for many machine learning algorithms\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "# --- Transpose of a Matrix ---\n",
    "def transpose(A):\n",
    "    \"\"\"\n",
    "    Compute the transpose of matrix A.\n",
    "    \"\"\"\n",
    "    m, n = len(A), len(A[0])\n",
    "    return [[A[j][i] for j in range(m)] for i in range(n)]\n",
    "\n",
    "# --- Matrix Multiplication ---\n",
    "def matrix_multiply(A, B):\n",
    "    \"\"\"\n",
    "    Multiply two matrices A (m x n) and B (n x p).\n",
    "    \"\"\"\n",
    "    m, n = len(A), len(B[0])\n",
    "    result = [[0 for _ in range(n)] for _ in range(m)]\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            result[i][j] = sum(A[i][k] * B[k][j] for k in range(len(B)))\n",
    "    return result\n",
    "\n",
    "# --- Matrix-Vector Multiplication ---\n",
    "def matrix_vector_multiply(A, x):\n",
    "    \"\"\"\n",
    "    Multiply matrix A (m x n) by vector x (n x 1).\n",
    "    \"\"\"\n",
    "    m = len(A)\n",
    "    result = [0.0] * m\n",
    "    for i in range(m):\n",
    "        result[i] = sum(A[i][j] * x[j] for j in range(len(x)))\n",
    "    return result\n",
    "\n",
    "# --- Dot Product ---\n",
    "def dot_product(x, y):\n",
    "    \"\"\"\n",
    "    Compute the dot product of two vectors.\n",
    "    \"\"\"\n",
    "    return sum(xi * yi for xi, yi in zip(x, y))\n",
    "\n",
    "# --- Norm of a Vector ---\n",
    "def norm(x):\n",
    "    \"\"\"\n",
    "    Compute the Euclidean norm of a vector.\n",
    "    \"\"\"\n",
    "    return math.sqrt(dot_product(x, x))\n",
    "\n",
    "# --- Matrix Subtraction ---\n",
    "def matrix_subtract(A, B):\n",
    "    \"\"\"\n",
    "    Subtract matrix B from matrix A.\n",
    "    \"\"\"\n",
    "    m, n = len(A), len(A[0])\n",
    "    return [[A[i][j] - B[i][j] for j in range(n)] for i in range(m)]\n",
    "\n",
    "# --- Verify Matrix Equality ---\n",
    "def matrices_equal(A, B, tol=1e-3):\n",
    "    \"\"\"\n",
    "    Check if two matrices are equal within a tolerance.\n",
    "    \"\"\"\n",
    "    return all(abs(A[i][j] - B[i][j]) < tol for i in range(len(A)) for j in range(len(A[0])))\n",
    "\n",
    "# --- Matrix Approximation Analyzer Class ---\n",
    "class MatrixApproximationAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.U = None\n",
    "        self.sigma = None\n",
    "        self.Vt = None\n",
    "        self.original_matrix = None\n",
    "\n",
    "    def set_svd(self, U, sigma, Vt, A):\n",
    "        \"\"\"\n",
    "        Set the SVD components manually (since we can't compute full SVD in core Python).\n",
    "        \"\"\"\n",
    "        self.U = U\n",
    "        self.sigma = sigma\n",
    "        self.Vt = Vt\n",
    "        self.original_matrix = A\n",
    "\n",
    "    def spectral_norm(self, A):\n",
    "        \"\"\"\n",
    "        Compute spectral norm of matrix A using Theorem 4.24: ||A||₂ = σ₁.\n",
    "        Since we can't compute SVD, we'll use the precomputed sigma if available.\n",
    "        \"\"\"\n",
    "        if self.sigma is None:\n",
    "            raise ValueError(\"SVD not computed. Set SVD components first.\")\n",
    "        return self.sigma[0] if len(self.sigma) > 0 else 0\n",
    "\n",
    "    def verify_spectral_norm_theorem(self, A, num_random_vectors=100):\n",
    "        \"\"\"\n",
    "        Verify Theorem 4.24: ||A||₂ = σ₁ by testing with random vectors.\n",
    "        \"\"\"\n",
    "        # Theoretical norm using Theorem 4.24\n",
    "        theoretical_norm = self.spectral_norm(A)\n",
    "\n",
    "        # Empirical norm using Definition 4.23\n",
    "        m, n = len(A), len(A[0])\n",
    "        max_ratio = 0\n",
    "\n",
    "        for _ in range(num_random_vectors):\n",
    "            x = [random.uniform(-1, 1) for _ in range(n)]\n",
    "            x_norm = norm(x)\n",
    "            if x_norm == 0:\n",
    "                continue\n",
    "            x = [xi / x_norm for xi in x]  # Normalize\n",
    "\n",
    "            Ax = matrix_vector_multiply(A, x)\n",
    "            ratio = norm(Ax) / norm(x)\n",
    "            max_ratio = max(max_ratio, ratio)\n",
    "\n",
    "        return theoretical_norm, max_ratio\n",
    "\n",
    "    def rank_k_approximation(self, A, k):\n",
    "        \"\"\"\n",
    "        Create rank-k approximation: Â^(k) = Σᵢ₌₁ᵏ σᵢ uᵢ vᵢᵀ.\n",
    "        \"\"\"\n",
    "        if self.U is None:\n",
    "            raise ValueError(\"SVD not computed. Set SVD components first.\")\n",
    "\n",
    "        k = min(k, len(self.sigma))\n",
    "        m, n = len(A), len(A[0])\n",
    "        A_k = [[0 for _ in range(n)] for _ in range(m)]\n",
    "\n",
    "        for i in range(k):\n",
    "            # Compute outer product u_i v_i^T\n",
    "            u_i = [self.U[j][i] for j in range(m)]\n",
    "            v_i = [self.Vt[i][j] for j in range(n)]\n",
    "            outer_product = [[u_i[j] * v_i[l] for l in range(n)] for j in range(m)]\n",
    "            # Scale by sigma_i and add to A_k\n",
    "            for j in range(m):\n",
    "                for l in range(n):\n",
    "                    A_k[j][l] += self.sigma[i] * outer_product[j][l]\n",
    "\n",
    "        return A_k\n",
    "\n",
    "    def eckart_young_error(self, A, k):\n",
    "        \"\"\"\n",
    "        Compute the error according to Eckart-Young theorem: ||A - Â^(k)||₂ = σₖ₊₁.\n",
    "        \"\"\"\n",
    "        if self.U is None:\n",
    "            raise ValueError(\"SVD not computed. Set SVD components first.\")\n",
    "\n",
    "        if k >= len(self.sigma):\n",
    "            return 0.0  # Perfect reconstruction\n",
    "\n",
    "        return self.sigma[k]  # σₖ₊₁ (k is 0-indexed, so k gives k+1)\n",
    "\n",
    "    def verify_eckart_young_theorem(self, A, k):\n",
    "        \"\"\"\n",
    "        Verify the Eckart-Young theorem by computing actual error and comparing with theoretical prediction.\n",
    "        \"\"\"\n",
    "        # Compute rank-k approximation\n",
    "        A_k = self.rank_k_approximation(A, k)\n",
    "\n",
    "        # Actual error: spectral norm of A - A_k\n",
    "        error_matrix = matrix_subtract(A, A_k)\n",
    "        # We need SVD of error_matrix to compute its spectral norm, but we know from Eckart-Young\n",
    "        # that it should equal sigma_{k+1}. We'll set the SVD of error_matrix manually.\n",
    "        error_analyzer = MatrixApproximationAnalyzer()\n",
    "        # The error matrix A - A_k has singular values sigma_{k+1}, ..., sigma_r\n",
    "        remaining_sigma = self.sigma[k:] if k < len(self.sigma) else [0]\n",
    "        # U and Vt for error matrix are the remaining columns/rows\n",
    "        U_error = [[self.U[i][j] for j in range(k, len(self.U[0]))] for i in range(len(self.U))]\n",
    "        Vt_error = [[self.Vt[i][j] for j in range(len(self.Vt[0]))] for i in range(k, len(self.Vt))]\n",
    "        error_analyzer.set_svd(U_error, remaining_sigma, Vt_error, error_matrix)\n",
    "        actual_error = error_analyzer.spectral_norm(error_matrix)\n",
    "\n",
    "        # Theoretical error (Eckart-Young)\n",
    "        theoretical_error = self.eckart_young_error(A, k)\n",
    "\n",
    "        return actual_error, theoretical_error\n",
    "\n",
    "    def demonstrate_optimality(self, A, k, num_random_trials=10):\n",
    "        \"\"\"\n",
    "        Demonstrate that SVD gives optimal rank-k approximation by comparing with random rank-k matrices.\n",
    "        \"\"\"\n",
    "        # SVD approximation\n",
    "        A_k_svd = self.rank_k_approximation(A, k)\n",
    "        error_matrix_svd = matrix_subtract(A, A_k_svd)\n",
    "        error_analyzer = MatrixApproximationAnalyzer()\n",
    "        remaining_sigma = self.sigma[k:] if k < len(self.sigma) else [0]\n",
    "        U_error = [[self.U[i][j] for j in range(k, len(self.U[0]))] for i in range(len(self.U))]\n",
    "        Vt_error = [[self.Vt[i][j] for j in range(len(self.Vt[0]))] for i in range(k, len(self.Vt))]\n",
    "        error_analyzer.set_svd(U_error, remaining_sigma, Vt_error, error_matrix_svd)\n",
    "        svd_error = error_analyzer.spectral_norm(error_matrix_svd)\n",
    "\n",
    "        # Random rank-k approximations\n",
    "        m, n = len(A), len(A[0])\n",
    "        random_errors = []\n",
    "\n",
    "        # Compute norm of A to scale random matrices\n",
    "        A_norm_analyzer = MatrixApproximationAnalyzer()\n",
    "        A_norm_analyzer.set_svd(self.U, self.sigma, self.Vt, A)\n",
    "        A_norm = A_norm_analyzer.spectral_norm(A)\n",
    "\n",
    "        for _ in range(num_random_trials):\n",
    "            # Create random rank-k matrix: U_rand (m x k) @ V_rand (k x n)\n",
    "            U_rand = [[random.uniform(-1, 1) for _ in range(k)] for _ in range(m)]\n",
    "            V_rand = [[random.uniform(-1, 1) for _ in range(n)] for _ in range(k)]\n",
    "            A_k_rand = matrix_multiply(U_rand, V_rand)\n",
    "\n",
    "            # Normalize to have similar scale\n",
    "            A_k_rand_analyzer = MatrixApproximationAnalyzer()\n",
    "            # Compute SVD of A_k_rand (simplified: approximate spectral norm via random vectors)\n",
    "            temp_analyzer = MatrixApproximationAnalyzer()\n",
    "            # We can't compute SVD, so approximate norm via random vectors\n",
    "            max_ratio = 0\n",
    "            for _ in range(50):\n",
    "                x = [random.uniform(-1, 1) for _ in range(n)]\n",
    "                x_norm = norm(x)\n",
    "                if x_norm == 0:\n",
    "                    continue\n",
    "                x = [xi / x_norm for xi in x]\n",
    "                Ax = matrix_vector_multiply(A_k_rand, x)\n",
    "                ratio = norm(Ax)\n",
    "                max_ratio = max(max_ratio, ratio)\n",
    "            A_k_rand_norm = max_ratio\n",
    "\n",
    "            if A_k_rand_norm > 0:\n",
    "                scale = A_norm / A_k_rand_norm\n",
    "                A_k_rand = [[scale * A_k_rand[i][j] for j in range(n)] for i in range(m)]\n",
    "\n",
    "            # Compute error\n",
    "            error_matrix_rand = matrix_subtract(A, A_k_rand)\n",
    "            # Approximate spectral norm of error matrix\n",
    "            max_ratio = 0\n",
    "            for _ in range(50):\n",
    "                x = [random.uniform(-1, 1) for _ in range(n)]\n",
    "                x_norm = norm(x)\n",
    "                if x_norm == 0:\n",
    "                    continue\n",
    "                x = [xi / x_norm for xi in x]\n",
    "                Ax = matrix_vector_multiply(error_matrix_rand, x)\n",
    "                ratio = norm(Ax)\n",
    "                max_ratio = max(max_ratio, ratio)\n",
    "            error = max_ratio\n",
    "            random_errors.append(error)\n",
    "\n",
    "        return svd_error, random_errors\n",
    "\n",
    "# --- Demonstration Functions ---\n",
    "def create_test_matrix():\n",
    "    \"\"\"\n",
    "    Use the movie ratings matrix from Figure 4.10 as our test matrix.\n",
    "    \"\"\"\n",
    "    A = [[5, 4, 1],  # Star Wars\n",
    "         [5, 5, 0],  # Blade Runner\n",
    "         [0, 0, 5],  # Amelie\n",
    "         [1, 0, 4]]  # Delicatessen\n",
    "    return A\n",
    "\n",
    "def demonstrate_spectral_norm():\n",
    "    \"\"\"\n",
    "    Demonstrate spectral norm computation and Theorem 4.24.\n",
    "    \"\"\"\n",
    "    print(\"=== Spectral Norm Analysis ===\")\n",
    "    print(\"Definition 4.23 and Theorem 4.24\\n\")\n",
    "\n",
    "    analyzer = MatrixApproximationAnalyzer()\n",
    "    A = create_test_matrix()\n",
    "\n",
    "    # Set SVD components from Figure 4.10\n",
    "    U = [[-0.6710, 0.0236, 0.4647, -0.5774],\n",
    "         [-0.7197, 0.2054, -0.4759, 0.4619],\n",
    "         [-0.0939, -0.7705, -0.5268, -0.3464],\n",
    "         [-0.1515, -0.6030, 0.5293, -0.5774]]\n",
    "    Sigma = [9.6438, 6.3639, 0.7056]  # Diagonal elements\n",
    "    Vt = [[-0.7367, -0.6515, -0.1811],\n",
    "          [0.0852, 0.1762, -0.9807],\n",
    "          [0.6708, -0.7379, -0.0743]]\n",
    "    analyzer.set_svd(U, Sigma, Vt, A)\n",
    "\n",
    "    print(\"Test Matrix A (Movie Ratings, 4x3):\")\n",
    "    for row in A:\n",
    "        print(row)\n",
    "\n",
    "    # Verify Theorem 4.24\n",
    "    theoretical_norm, empirical_norm = analyzer.verify_spectral_norm_theorem(A)\n",
    "\n",
    "    print(f\"\\nTheoretical ||A||₂ (σ₁): {theoretical_norm:.6f}\")\n",
    "    print(f\"Empirical ||A||₂:       {empirical_norm:.6f}\")\n",
    "    print(f\"Difference:             {abs(theoretical_norm - empirical_norm):.2e}\")\n",
    "    print(f\"Theorem verified:       {abs(theoretical_norm - empirical_norm) < 1e-2}\")\n",
    "\n",
    "def demonstrate_eckart_young_theorem():\n",
    "    \"\"\"\n",
    "    Comprehensive demonstration of the Eckart-Young theorem.\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Eckart-Young Theorem Demonstration ===\")\n",
    "    print(\"Theorem 4.25: Optimality of SVD approximation\\n\")\n",
    "\n",
    "    analyzer = MatrixApproximationAnalyzer()\n",
    "    A = create_test_matrix()\n",
    "\n",
    "    # Set SVD components from Figure 4.10\n",
    "    U = [[-0.6710, 0.0236, 0.4647, -0.5774],\n",
    "         [-0.7197, 0.2054, -0.4759, 0.4619],\n",
    "         [-0.0939, -0.7705, -0.5268, -0.3464],\n",
    "         [-0.1515, -0.6030, 0.5293, -0.5774]]\n",
    "    Sigma = [9.6438, 6.3639, 0.7056]  # Diagonal elements\n",
    "    Vt = [[-0.7367, -0.6515, -0.1811],\n",
    "          [0.0852, 0.1762, -0.9807],\n",
    "          [0.6708, -0.7379, -0.0743]]\n",
    "    analyzer.set_svd(U, Sigma, Vt, A)\n",
    "\n",
    "    print(f\"Test matrix shape: ({len(A)}, {len(A[0])})\")\n",
    "    print(f\"Matrix rank: {len(Sigma)}\")\n",
    "    print(f\"Singular values: {[round(s, 4) for s in Sigma]}\")\n",
    "    print()\n",
    "\n",
    "    # Test different ranks\n",
    "    test_ranks = [1, 2]\n",
    "\n",
    "    print(\"Rank | Actual Error | Theoretical Error | Difference | Verified\")\n",
    "    print(\"-\" * 65)\n",
    "\n",
    "    for k in test_ranks:\n",
    "        actual_error, theoretical_error = analyzer.verify_eckart_young_theorem(A, k)\n",
    "        diff = abs(actual_error - theoretical_error)\n",
    "        verified = diff < 1e-2\n",
    "\n",
    "        print(f\"{k:4d} | {actual_error:11.6f} | {theoretical_error:16.6f} | {diff:9.2e} | {verified}\")\n",
    "\n",
    "    print(\"\\n=== Optimality Demonstration ===\")\n",
    "    print(\"SVD vs Random Rank-k Approximations\\n\")\n",
    "\n",
    "    k_test = 1\n",
    "    svd_error, random_errors = analyzer.demonstrate_optimality(A, k_test)\n",
    "\n",
    "    print(f\"Rank-{k_test} approximation errors:\")\n",
    "    print(f\"  SVD approximation error:    {svd_error:.6f}\")\n",
    "    print(f\"  Best random approximation:  {min(random_errors):.6f}\")\n",
    "    print(f\"  Worst random approximation: {max(random_errors):.6f}\")\n",
    "    print(f\"  Average random error:       {sum(random_errors)/len(random_errors):.6f}\")\n",
    "    print(f\"  SVD is optimal:             {svd_error <= min(random_errors)}\")\n",
    "\n",
    "def mathematical_insights():\n",
    "    \"\"\"\n",
    "    Explain the mathematical insights behind the Eckart-Young theorem.\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Mathematical Insights ===\")\n",
    "    print(\"Understanding why Equation (4.95) holds\\n\")\n",
    "\n",
    "    analyzer = MatrixApproximationAnalyzer()\n",
    "\n",
    "    # Simple 2×2 example\n",
    "    A = [[4, 2], [2, 1]]\n",
    "\n",
    "    # Analytical SVD: eigenvalues of A^T A are 5 and 0, singular values are sqrt(5) and 0\n",
    "    sqrt_5 = math.sqrt(5)\n",
    "    U = [[2/math.sqrt(5), -1/math.sqrt(5)],\n",
    "         [1/math.sqrt(5), 2/math.sqrt(5)]]\n",
    "    Sigma = [sqrt_5, 0]\n",
    "    Vt = [[1, 0], [0, 1]]\n",
    "    analyzer.set_svd(U, Sigma, Vt, A)\n",
    "\n",
    "    print(\"Simple 2×2 example:\")\n",
    "    for row in A:\n",
    "        print(row)\n",
    "\n",
    "    print(\"\\nSVD decomposition:\")\n",
    "    print(\"U:\")\n",
    "    for row in U:\n",
    "        print([round(x, 4) for x in row])\n",
    "    print(f\"σ: {[round(s, 4) for s in Sigma]}\")\n",
    "    print(\"Vt:\")\n",
    "    for row in Vt:\n",
    "        print([round(x, 4) for x in row])\n",
    "\n",
    "    # Rank-1 approximation\n",
    "    A_1 = analyzer.rank_k_approximation(A, 1)\n",
    "\n",
    "    print(f\"\\nRank-1 approximation Â^(1):\")\n",
    "    print(f\"Â^(1) = σ₁u₁v₁ᵀ = {Sigma[0]:.6f} × u₁v₁ᵀ\")\n",
    "    for row in A_1:\n",
    "        print([round(x, 4) for x in row])\n",
    "\n",
    "    # Error analysis\n",
    "    error_matrix = matrix_subtract(A, A_1)\n",
    "    print(f\"\\nError matrix A - Â^(1):\")\n",
    "    for row in error_matrix:\n",
    "        print([round(x, 4) for x in row])\n",
    "\n",
    "    actual_error = analyzer.eckart_young_error(A, 1)  # Since A_1 is rank-1, error = sigma_2\n",
    "    theoretical_error = Sigma[1]  # σ₂\n",
    "\n",
    "    print(f\"\\nError analysis:\")\n",
    "    print(f\"||A - Â^(1)||₂ (actual):     {actual_error:.6f}\")\n",
    "    print(f\"σ₂ (theoretical):           {theoretical_error:.6f}\")\n",
    "    print(f\"Difference:                 {abs(actual_error - theoretical_error):.2e}\")\n",
    "\n",
    "    print(f\"\\nKey insight:\")\n",
    "    print(f\"The error ||A - Â^(k)||₂ = σₖ₊₁ because:\")\n",
    "    print(f\"1. A - Â^(k) = Σᵢ₌ₖ₊₁ʳ σᵢuᵢvᵢᵀ\")\n",
    "    print(f\"2. The spectral norm of this sum is dominated by the largest term σₖ₊₁\")\n",
    "    print(f\"3. SVD provides the optimal decomposition that minimizes this error\")\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Matrix Approximation and Eckart-Young Theorem Analysis\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Run demonstrations\n",
    "    demonstrate_spectral_norm()\n",
    "    demonstrate_eckart_young_theorem()\n",
    "    mathematical_insights()\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Summary of Key Results:\")\n",
    "    print(\"• Spectral norm ||A||₂ = σ₁ (largest singular value)\")\n",
    "    print(\"• SVD provides optimal rank-k approximation in spectral norm\")\n",
    "    print(\"• Error bound: ||A - Â^(k)||₂ = σₖ₊₁\")\n",
    "    print(\"• Applications: image compression, dimensionality reduction, denoising\")\n",
    "    print(\"• Theoretical foundation for many machine learning algorithms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81eae23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
