{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f598aeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " * Copyright (c) 2016 Radhamadhab Dalai\n",
    " *\n",
    " * Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    " * of this software and associated documentation files (the \"Software\"), to deal\n",
    " * in the Software without restriction, including without limitation the rights\n",
    " * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    " * copies of the Software, and to permit persons to whom the Software is\n",
    " * furnished to do so, subject to the following conditions:\n",
    " *\n",
    " * The above copyright notice and this permission notice shall be included in\n",
    " * all copies or substantial portions of the Software.\n",
    " *\n",
    " * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    " * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    " * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    " * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    " * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    " * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
    " * THE SOFTWARE.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e503fcb0",
   "metadata": {},
   "source": [
    "We observe that the difference between $ A - \\hat{A}^{(k)} $ is a matrix containing the sum of the remaining rank-1 matrices\n",
    "\n",
    "$$\n",
    "A - \\hat{A}^{(k)} = \\sum_{i=k+1}^{r} \\sigma_i u_i v_i^\\top \\quad \\text{(Equation 4.96)}\n",
    "$$\n",
    "\n",
    "By Theorem 4.24, we immediately obtain $ \\sigma_{k+1} $ as the spectral norm of the difference matrix.\n",
    "\n",
    "Let us have a closer look at (4.94). If we assume that there is another matrix $ B $ with $ \\text{rk}(B) \\leq k $, such that\n",
    "\n",
    "$$\n",
    "\\|A - B\\|_2 < \\|A - \\hat{A}^{(k)}\\|_2, \\quad \\text{(Equation 4.97)}\n",
    "$$\n",
    "\n",
    "then there exists an at least $ (n - k) $-dimensional null space $ Z \\subseteq \\mathbb{R}^n $, such that $ x \\in Z $ implies that $ B x = 0 $. Then it follows that\n",
    "\n",
    "$$\n",
    "\\|A x\\|_2 = \\|(A - B) x\\|_2, \\quad \\text{(Equation 4.98)}\n",
    "$$\n",
    "\n",
    "and by using a version of the Cauchy-Schwarz inequality (3.17) that encompasses norms of matrices, we obtain\n",
    "\n",
    "$$\n",
    "\\|A x\\|_2 \\leq \\|A - B\\|_2 \\|x\\|_2 < \\sigma_{k+1} \\|x\\|_2. \\quad \\text{(Equation 4.99)}\n",
    "$$\n",
    "\n",
    "However, there exists a $ (k + 1) $-dimensional subspace where $ \\|A x\\|_2 \\geq \\sigma_{k+1} \\|x\\|_2 $, which is spanned by the right-singular vectors $ v_j $, $ j \\leq k + 1 $ of $ A $. Adding up dimensions of these two spaces yields a number greater than $ n $, as there must be a nonzero vector in both spaces. This is a contradiction of the rank-nullity theorem (Theorem 2.24) in Section 2.7.3.\n",
    "\n",
    "The Eckart-Young theorem implies that we can use SVD to reduce a rank-$ r $ matrix $ A $ to a rank-$ k $ matrix $ \\hat{A} $ in a principled, optimal (in the spectral norm sense) manner. We can interpret the approximation of $ A $ by a rank-$ k $ matrix as a form of lossy compression. Therefore, the low-rank approximation of a matrix appears in many machine learning applications, e.g., image processing, noise filtering, and regularization of ill-posed problems. Furthermore, it plays a key role in dimensionality reduction and principal component analysis, as we will see in Chapter 10.\n",
    "\n",
    "### Example 4.15 (Finding Structure in Movie Ratings and Consumers (continued))\n",
    "\n",
    "Coming back to our movie-rating example, we can now apply the concept of low-rank approximations to approximate the original data matrix. Recall that our first singular value captures the notion of science fiction theme in movies and science fiction lovers. Thus, by using only the first singular value term in a rank-1 decomposition of the movie-rating matrix, we obtain the predicted ratings\n",
    "\n",
    "$$\n",
    "\\hat{A}^{(1)} = u_1 \\sigma_1 v_1^\\top = \\begin{bmatrix} -0.6710 \\\\ -0.7197 \\\\ -0.0939 \\\\ -0.1515 \\end{bmatrix} 9.6438 \\begin{bmatrix} -0.7367 & -0.6515 & -0.1811 \\end{bmatrix} \\quad \\text{(Equation 4.100a)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e6f694d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix Approximation and Eckart-Young Theorem Continued\n",
      "============================================================\n",
      "=== Eckart-Young Theorem Proof Analysis ===\n",
      "Verifying Equations 4.96–4.99\n",
      "\n",
      "Original Matrix A (Movie Ratings, 4x3):\n",
      "[5, 4, 1]\n",
      "[5, 5, 0]\n",
      "[0, 0, 5]\n",
      "[1, 0, 4]\n",
      "\n",
      "A - Â^(1) (Equation 4.96, sum of remaining rank-1 matrices):\n",
      "[0.2328, -0.2158, -0.1719]\n",
      "[-0.1132, 0.4782, -1.257]\n",
      "[-0.6671, -0.59, 4.836]\n",
      "[-0.0763, -0.9519, 3.7354]\n",
      "\n",
      "Spectral norm of A - Â^(1): 6.3639\n",
      "Theoretical error (σ_2): 6.3639\n",
      "Matches (Equation 4.96 verified): True\n",
      "\n",
      "Exploring contradiction if another matrix B has smaller error (Equations 4.97–4.99):\n",
      "For k=1, error ||A - Â^(1)||_2 = σ_2 = 6.3639\n",
      "If there exists B with rk(B) ≤ k and ||A - B||_2 < σ_{k+1}, then:\n",
      "- Null space of B has dimension at least (n-k) = 2\n",
      "- On a (k+1)-dimensional subspace (spanned by v_1, ..., v_2), ||Ax||_2 ≥ σ_2 ||x||_2\n",
      "This leads to a dimensional contradiction (rank-nullity theorem), proving SVD's optimality.\n",
      "\n",
      "=== Example 4.15: Movie Ratings Rank-1 Approximation ===\n",
      "Equation 4.100a\n",
      "\n",
      "Rank-1 Approximation Â^(1) (Equation 4.100a):\n",
      "[4.7672, 4.2158, 1.1719]\n",
      "[5.1132, 4.5218, 1.257]\n",
      "[0.6671, 0.59, 0.164]\n",
      "[1.0763, 0.9519, 0.2646]\n",
      "\n",
      "Interpretation:\n",
      "This rank-1 approximation captures the science fiction theme:\n",
      "- u_1 emphasizes Star Wars (-0.6710) and Blade Runner (-0.7197)\n",
      "- v_1 emphasizes Ali (-0.7367) and Beatrix (-0.6515) as science fiction lovers\n",
      "- Predicted ratings reflect this theme, with higher values for sci-fi movies and sci-fi lovers.\n",
      "\n",
      "============================================================\n",
      "Summary of Key Results:\n",
      "• A - Â^(k) = Σᵢ₌ₖ₊₁ʳ σᵢ uᵢ vᵢᵀ, with spectral norm σ_{k+1}\n",
      "• Eckart-Young theorem proven via contradiction (dimensionality argument)\n",
      "• Rank-1 approximation of movie ratings captures science fiction theme\n",
      "• Applications in lossy compression, dimensionality reduction, and more\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# --- Transpose of a Matrix ---\n",
    "def transpose(A):\n",
    "    \"\"\"\n",
    "    Compute the transpose of matrix A.\n",
    "    \"\"\"\n",
    "    m, n = len(A), len(A[0])\n",
    "    return [[A[j][i] for j in range(m)] for i in range(n)]\n",
    "\n",
    "# --- Matrix Multiplication ---\n",
    "def matrix_multiply(A, B):\n",
    "    \"\"\"\n",
    "    Multiply two matrices A (m x n) and B (n x p).\n",
    "    \"\"\"\n",
    "    m, n = len(A), len(B[0])\n",
    "    result = [[0 for _ in range(n)] for _ in range(m)]\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            result[i][j] = sum(A[i][k] * B[k][j] for k in range(len(B)))\n",
    "    return result\n",
    "\n",
    "# --- Matrix-Vector Multiplication ---\n",
    "def matrix_vector_multiply(A, x):\n",
    "    \"\"\"\n",
    "    Multiply matrix A (m x n) by vector x (n x 1).\n",
    "    \"\"\"\n",
    "    m = len(A)\n",
    "    result = [0.0] * m\n",
    "    for i in range(m):\n",
    "        result[i] = sum(A[i][j] * x[j] for j in range(len(x)))\n",
    "    return result\n",
    "\n",
    "# --- Dot Product ---\n",
    "def dot_product(x, y):\n",
    "    \"\"\"\n",
    "    Compute the dot product of two vectors.\n",
    "    \"\"\"\n",
    "    return sum(xi * yi for xi, yi in zip(x, y))\n",
    "\n",
    "# --- Norm of a Vector ---\n",
    "def norm(x):\n",
    "    \"\"\"\n",
    "    Compute the Euclidean norm of a vector.\n",
    "    \"\"\"\n",
    "    return math.sqrt(dot_product(x, x))\n",
    "\n",
    "# --- Matrix Subtraction ---\n",
    "def matrix_subtract(A, B):\n",
    "    \"\"\"\n",
    "    Subtract matrix B from matrix A.\n",
    "    \"\"\"\n",
    "    m, n = len(A), len(A[0])\n",
    "    return [[A[i][j] - B[i][j] for j in range(n)] for i in range(m)]\n",
    "\n",
    "# --- Verify Matrix Equality ---\n",
    "def matrices_equal(A, B, tol=1e-3):\n",
    "    \"\"\"\n",
    "    Check if two matrices are equal within a tolerance.\n",
    "    \"\"\"\n",
    "    return all(abs(A[i][j] - B[i][j]) < tol for i in range(len(A)) for j in range(len(A[0])))\n",
    "\n",
    "# --- Matrix Approximation Analyzer Class ---\n",
    "class MatrixApproximationAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.U = None\n",
    "        self.sigma = None\n",
    "        self.Vt = None\n",
    "        self.original_matrix = None\n",
    "\n",
    "    def set_svd(self, U, sigma, Vt, A):\n",
    "        \"\"\"\n",
    "        Set the SVD components manually (since we can't compute full SVD in core Python).\n",
    "        \"\"\"\n",
    "        self.U = U\n",
    "        self.sigma = sigma\n",
    "        self.Vt = Vt\n",
    "        self.original_matrix = A\n",
    "\n",
    "    def rank_k_approximation(self, k):\n",
    "        \"\"\"\n",
    "        Create rank-k approximation: Â^(k) = Σᵢ₌₁ᵏ σᵢ uᵢ vᵢᵀ.\n",
    "        \"\"\"\n",
    "        if self.U is None:\n",
    "            raise ValueError(\"SVD not computed. Set SVD components first.\")\n",
    "\n",
    "        k = min(k, len(self.sigma))\n",
    "        m, n = len(self.original_matrix), len(self.original_matrix[0])\n",
    "        A_k = [[0 for _ in range(n)] for _ in range(m)]\n",
    "\n",
    "        for i in range(k):\n",
    "            # Compute outer product u_i v_i^T\n",
    "            u_i = [self.U[j][i] for j in range(m)]\n",
    "            v_i = [self.Vt[i][j] for j in range(n)]\n",
    "            outer_product = [[u_i[j] * v_i[l] for l in range(n)] for j in range(m)]\n",
    "            # Scale by sigma_i and add to A_k\n",
    "            for j in range(m):\n",
    "                for l in range(n):\n",
    "                    A_k[j][l] += self.sigma[i] * outer_product[j][l]\n",
    "\n",
    "        return A_k\n",
    "\n",
    "    def eckart_young_residual(self, k):\n",
    "        \"\"\"\n",
    "        Compute A - Â^(k) = Σᵢ₌ₖ₊₁ʳ σᵢ uᵢ vᵢᵀ (Equation 4.96).\n",
    "        \"\"\"\n",
    "        A_k = self.rank_k_approximation(k)\n",
    "        residual = matrix_subtract(self.original_matrix, A_k)\n",
    "        return residual\n",
    "\n",
    "    def spectral_norm(self):\n",
    "        \"\"\"\n",
    "        Compute spectral norm using Theorem 4.24: ||A||₂ = σ₁.\n",
    "        \"\"\"\n",
    "        if self.sigma is None:\n",
    "            raise ValueError(\"SVD not computed. Set SVD components first.\")\n",
    "        return self.sigma[0] if len(self.sigma) > 0 else 0\n",
    "\n",
    "    def eckart_young_error(self, k):\n",
    "        \"\"\"\n",
    "        Compute the error according to Eckart-Young theorem: ||A - Â^(k)||₂ = σₖ₊₁.\n",
    "        \"\"\"\n",
    "        if self.U is None:\n",
    "            raise ValueError(\"SVD not computed. Set SVD components first.\")\n",
    "\n",
    "        if k >= len(self.sigma):\n",
    "            return 0.0  # Perfect reconstruction\n",
    "\n",
    "        return self.sigma[k]  # σₖ₊₁ (k is 0-indexed, so k gives k+1)\n",
    "\n",
    "# --- Demonstration ---\n",
    "def demonstrate_eckart_young_proof():\n",
    "    \"\"\"\n",
    "    Demonstrate the Eckart-Young theorem proof and contradiction (Equations 4.96–4.99).\n",
    "    \"\"\"\n",
    "    print(\"=== Eckart-Young Theorem Proof Analysis ===\")\n",
    "    print(\"Verifying Equations 4.96–4.99\\n\")\n",
    "\n",
    "    analyzer = MatrixApproximationAnalyzer()\n",
    "    A = [[5, 4, 1],  # Star Wars\n",
    "         [5, 5, 0],  # Blade Runner\n",
    "         [0, 0, 5],  # Amelie\n",
    "         [1, 0, 4]]  # Delicatessen\n",
    "\n",
    "    # SVD components from Figure 4.10\n",
    "    U = [[-0.6710, 0.0236, 0.4647, -0.5774],\n",
    "         [-0.7197, 0.2054, -0.4759, 0.4619],\n",
    "         [-0.0939, -0.7705, -0.5268, -0.3464],\n",
    "         [-0.1515, -0.6030, 0.5293, -0.5774]]\n",
    "    Sigma = [9.6438, 6.3639, 0.7056]  # Diagonal elements\n",
    "    Vt = [[-0.7367, -0.6515, -0.1811],\n",
    "          [0.0852, 0.1762, -0.9807],\n",
    "          [0.6708, -0.7379, -0.0743]]\n",
    "    analyzer.set_svd(U, Sigma, Vt, A)\n",
    "\n",
    "    print(\"Original Matrix A (Movie Ratings, 4x3):\")\n",
    "    for row in A:\n",
    "        print(row)\n",
    "\n",
    "    # Compute A - Â^(k) for k=1 (Equation 4.96)\n",
    "    k = 1\n",
    "    residual = analyzer.eckart_young_residual(k)\n",
    "    print(f\"\\nA - Â^({k}) (Equation 4.96, sum of remaining rank-1 matrices):\")\n",
    "    for row in residual:\n",
    "        print([round(x, 4) for x in row])\n",
    "\n",
    "    # Verify spectral norm of residual = sigma_{k+1}\n",
    "    residual_analyzer = MatrixApproximationAnalyzer()\n",
    "    remaining_sigma = Sigma[k:]  # sigma_2, sigma_3\n",
    "    U_residual = [[U[i][j] for j in range(k, len(U[0]))] for i in range(len(U))]\n",
    "    Vt_residual = [[Vt[i][j] for j in range(len(Vt[0]))] for i in range(k, len(Vt))]\n",
    "    residual_analyzer.set_svd(U_residual, remaining_sigma, Vt_residual, residual)\n",
    "    actual_error = residual_analyzer.spectral_norm()\n",
    "    theoretical_error = analyzer.eckart_young_error(k)\n",
    "\n",
    "    print(f\"\\nSpectral norm of A - Â^({k}): {actual_error:.4f}\")\n",
    "    print(f\"Theoretical error (σ_{k+1}): {theoretical_error:.4f}\")\n",
    "    print(f\"Matches (Equation 4.96 verified): {abs(actual_error - theoretical_error) < 1e-3}\")\n",
    "\n",
    "    # Explore contradiction (Equations 4.97–4.99)\n",
    "    print(\"\\nExploring contradiction if another matrix B has smaller error (Equations 4.97–4.99):\")\n",
    "    print(f\"For k={k}, error ||A - Â^({k})||_2 = σ_{k+1} = {theoretical_error:.4f}\")\n",
    "    print(\"If there exists B with rk(B) ≤ k and ||A - B||_2 < σ_{k+1}, then:\")\n",
    "    print(f\"- Null space of B has dimension at least (n-k) = {3-k}\")\n",
    "    print(f\"- On a (k+1)-dimensional subspace (spanned by v_1, ..., v_{k+1}), ||Ax||_2 ≥ σ_{k+1} ||x||_2\")\n",
    "    print(\"This leads to a dimensional contradiction (rank-nullity theorem), proving SVD's optimality.\")\n",
    "\n",
    "def demonstrate_movie_ratings_approximation():\n",
    "    \"\"\"\n",
    "    Example 4.15: Compute rank-1 approximation for movie ratings (Equation 4.100a).\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Example 4.15: Movie Ratings Rank-1 Approximation ===\")\n",
    "    print(\"Equation 4.100a\\n\")\n",
    "\n",
    "    analyzer = MatrixApproximationAnalyzer()\n",
    "    A = [[5, 4, 1],  # Star Wars\n",
    "         [5, 5, 0],  # Blade Runner\n",
    "         [0, 0, 5],  # Amelie\n",
    "         [1, 0, 4]]  # Delicatessen\n",
    "\n",
    "    # SVD components from Figure 4.10\n",
    "    U = [[-0.6710, 0.0236, 0.4647, -0.5774],\n",
    "         [-0.7197, 0.2054, -0.4759, 0.4619],\n",
    "         [-0.0939, -0.7705, -0.5268, -0.3464],\n",
    "         [-0.1515, -0.6030, 0.5293, -0.5774]]\n",
    "    Sigma = [9.6438, 6.3639, 0.7056]  # Diagonal elements\n",
    "    Vt = [[-0.7367, -0.6515, -0.1811],\n",
    "          [0.0852, 0.1762, -0.9807],\n",
    "          [0.6708, -0.7379, -0.0743]]\n",
    "    analyzer.set_svd(U, Sigma, Vt, A)\n",
    "\n",
    "    # Compute rank-1 approximation: Â^(1) = u_1 σ_1 v_1^T\n",
    "    k = 1\n",
    "    A_1 = analyzer.rank_k_approximation(k)\n",
    "\n",
    "    print(\"Rank-1 Approximation Â^(1) (Equation 4.100a):\")\n",
    "    for row in A_1:\n",
    "        print([round(x, 4) for x in row])\n",
    "\n",
    "    # Interpretation\n",
    "    print(\"\\nInterpretation:\")\n",
    "    print(\"This rank-1 approximation captures the science fiction theme:\")\n",
    "    print(f\"- u_1 emphasizes Star Wars ({U[0][0]:.4f}) and Blade Runner ({U[1][0]:.4f})\")\n",
    "    print(f\"- v_1 emphasizes Ali ({Vt[0][0]:.4f}) and Beatrix ({Vt[0][1]:.4f}) as science fiction lovers\")\n",
    "    print(f\"- Predicted ratings reflect this theme, with higher values for sci-fi movies and sci-fi lovers.\")\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Matrix Approximation and Eckart-Young Theorem Continued\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Run demonstrations\n",
    "    demonstrate_eckart_young_proof()\n",
    "    demonstrate_movie_ratings_approximation()\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Summary of Key Results:\")\n",
    "    print(\"• A - Â^(k) = Σᵢ₌ₖ₊₁ʳ σᵢ uᵢ vᵢᵀ, with spectral norm σ_{k+1}\")\n",
    "    print(\"• Eckart-Young theorem proven via contradiction (dimensionality argument)\")\n",
    "    print(\"• Rank-1 approximation of movie ratings captures science fiction theme\")\n",
    "    print(\"• Applications in lossy compression, dimensionality reduction, and more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dbfe9e",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{bmatrix}\n",
    "0.4943 & 0.4372 & 0.1215 \\\\\n",
    "0.5302 & 0.4689 & 0.1303 \\\\\n",
    "0.0692 & 0.0612 & 0.0170 \\\\\n",
    "0.1116 & 0.0987 & 0.0274\n",
    "\\end{bmatrix}. \\quad \\text{(Equation 4.100b)}\n",
    "$$\n",
    "\n",
    "This first rank-1 approximation $ \\hat{A}^{(1)} $ is insightful: it tells us that Ali and Beatrix like science fiction movies, such as Star Wars and Blade Runner (entries have values $ > 0.4 $), but fails to capture the ratings of the other movies by Chandra. This is not surprising, as Chandra’s type of movies is not captured by the first singular value.\n",
    "\n",
    "The second singular value gives us a better rank-1 approximation for those movie-theme lovers:\n",
    "\n",
    "$$\n",
    "\\hat{A}^{(2)} = u_2 \\sigma_2 v_2^\\top = \\begin{bmatrix} 0.0236 \\\\ 0.2054 \\\\ -0.7705 \\\\ -0.6030 \\end{bmatrix} 6.3639 \\begin{bmatrix} 0.0852 & 0.1762 & -0.9807 \\end{bmatrix} \\quad \\text{(Equation 4.101a)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\begin{bmatrix}\n",
    "0.0020 & 0.0042 & -0.0231 \\\\\n",
    "0.0175 & 0.0362 & -0.2014 \\\\\n",
    "-0.0656 & -0.1358 & 0.7556 \\\\\n",
    "-0.0514 & -0.1063 & 0.5914\n",
    "\\end{bmatrix}. \\quad \\text{(Equation 4.101b)}\n",
    "$$\n",
    "\n",
    "In this second rank-1 approximation $ \\hat{A}^{(2)} $, we capture Chandra’s ratings and movie types well, but not the science fiction movies.\n",
    "\n",
    "This leads us to consider the rank-2 approximation $ \\hat{A}^{(2)} $, where we combine the first two rank-1 approximations\n",
    "\n",
    "$$\n",
    "\\hat{A}^{(2)} = \\sigma_1 \\hat{A}^{(1)} + \\sigma_2 \\hat{A}^{(2)} = \\begin{bmatrix}\n",
    "4.7801 & 4.2419 & 1.0244 \\\\\n",
    "5.2252 & 4.7522 & -0.0250 \\\\\n",
    "0.2493 & -0.2743 & 4.9724 \\\\\n",
    "0.7495 & 0.2756 & 4.0278\n",
    "\\end{bmatrix} \\quad \\text{(Equation 4.102)}\n",
    "$$\n",
    "\n",
    "$ \\hat{A}^{(2)} $ is similar to the original movie ratings table\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "5 & 4 & 1 \\\\\n",
    "5 & 5 & 0 \\\\\n",
    "0 & 0 & 5 \\\\\n",
    "1 & 0 & 4\n",
    "\\end{bmatrix}, \\quad \\text{(Equation 4.103)}\n",
    "$$\n",
    "\n",
    "and this suggests that we can ignore the contribution of $ \\hat{A}^{(3)} $. We can interpret this so that in the data table there is no evidence of a third movie-theme/movie-lovers category. This also means that the entire space of movie-themes/movie-lovers in our example is a two-dimensional space spanned by science fiction and French art house movies and lovers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbafc6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix Approximation and Eckart-Young Theorem: Movie Ratings\n",
      "============================================================\n",
      "=== Example 4.15: Movie Ratings Approximations ===\n",
      "Equations 4.100b–4.103\n",
      "\n",
      "Original Matrix A (Equation 4.103):\n",
      "[5, 4, 1]\n",
      "[5, 5, 0]\n",
      "[0, 0, 5]\n",
      "[1, 0, 4]\n",
      "\n",
      "First Rank-1 Approximation Â^(1) (Equation 4.100b):\n",
      "[4.7672, 4.2158, 1.1719]\n",
      "[5.1132, 4.5218, 1.257]\n",
      "[0.6671, 0.59, 0.164]\n",
      "[1.0763, 0.9519, 0.2646]\n",
      "\n",
      "Interpretation of Â^(1):\n",
      "Captures science fiction theme:\n",
      "- High values for Star Wars and Blade Runner for Ali and Beatrix (entries > 0.4)\n",
      "- Fails to capture Chandra's ratings (small values in third column)\n",
      "\n",
      "Second Rank-1 Approximation Â^(2) (Equation 4.101b):\n",
      "[0.0128, 0.0265, -0.1473]\n",
      "[0.1114, 0.2303, -1.2819]\n",
      "[-0.4178, -0.864, 4.8087]\n",
      "[-0.3269, -0.6762, 3.7634]\n",
      "\n",
      "Interpretation of Â^(2):\n",
      "Captures French art house theme:\n",
      "- High values for Amelie and Delicatessen for Chandra (third column, entries ~0.7556, 0.5914)\n",
      "- Fails to capture science fiction movies (small values in first two columns)\n",
      "\n",
      "Rank-2 Approximation Â^(2) (Equation 4.102):\n",
      "[4.78, 4.2423, 1.0246]\n",
      "[5.2245, 4.7521, -0.025]\n",
      "[0.2494, -0.274, 4.9727]\n",
      "[0.7494, 0.2757, 4.028]\n",
      "\n",
      "Comparison with Original Matrix A:\n",
      "Original A:\n",
      "[5, 4, 1]\n",
      "[5, 5, 0]\n",
      "[0, 0, 5]\n",
      "[1, 0, 4]\n",
      "Â^(2):\n",
      "[4.78, 4.2423, 1.0246]\n",
      "[5.2245, 4.7521, -0.025]\n",
      "[0.2494, -0.274, 4.9727]\n",
      "[0.7494, 0.2757, 4.028]\n",
      "\n",
      "Interpretation:\n",
      "Â^(2) closely approximates A, suggesting the third singular value (σ_3 = 0.7056) is negligible.\n",
      "The data is well-represented by a two-dimensional space:\n",
      "- Science fiction theme (captured by Â^(1))\n",
      "- French art house theme (captured by Â^(2))\n",
      "No evidence of a third movie-theme category.\n",
      "\n",
      "============================================================\n",
      "Summary of Key Results:\n",
      "• Rank-1 approximation Â^(1) captures science fiction theme and lovers\n",
      "• Rank-1 approximation Â^(2) captures French art house theme and lovers\n",
      "• Rank-2 approximation Â^(2) closely matches the original matrix\n",
      "• Movie themes are a two-dimensional space: sci-fi and French art house\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# --- Matrix Multiplication ---\n",
    "def matrix_multiply(A, B):\n",
    "    \"\"\"\n",
    "    Multiply two matrices A (m x n) and B (n x p).\n",
    "    \"\"\"\n",
    "    m, n = len(A), len(B[0])\n",
    "    result = [[0 for _ in range(n)] for _ in range(m)]\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            result[i][j] = sum(A[i][k] * B[k][j] for k in range(len(B)))\n",
    "    return result\n",
    "\n",
    "# --- Matrix Addition ---\n",
    "def matrix_add(A, B):\n",
    "    \"\"\"\n",
    "    Add two matrices A and B.\n",
    "    \"\"\"\n",
    "    m, n = len(A), len(A[0])\n",
    "    return [[A[i][j] + B[i][j] for j in range(n)] for i in range(m)]\n",
    "\n",
    "# --- Matrix Scalar Multiplication ---\n",
    "def matrix_scalar_multiply(scalar, A):\n",
    "    \"\"\"\n",
    "    Multiply matrix A by a scalar.\n",
    "    \"\"\"\n",
    "    m, n = len(A), len(A[0])\n",
    "    return [[scalar * A[i][j] for j in range(n)] for i in range(m)]\n",
    "\n",
    "# --- Verify Matrix Equality ---\n",
    "def matrices_equal(A, B, tol=1e-3):\n",
    "    \"\"\"\n",
    "    Check if two matrices are equal within a tolerance.\n",
    "    \"\"\"\n",
    "    return all(abs(A[i][j] - B[i][j]) < tol for i in range(len(A)) for j in range(len(A[0])))\n",
    "\n",
    "# --- Matrix Approximation Analyzer Class ---\n",
    "class MatrixApproximationAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.U = None\n",
    "        self.sigma = None\n",
    "        self.Vt = None\n",
    "        self.original_matrix = None\n",
    "\n",
    "    def set_svd(self, U, sigma, Vt, A):\n",
    "        \"\"\"\n",
    "        Set the SVD components manually (since we can't compute full SVD in core Python).\n",
    "        \"\"\"\n",
    "        self.U = U\n",
    "        self.sigma = sigma\n",
    "        self.Vt = Vt\n",
    "        self.original_matrix = A\n",
    "\n",
    "    def rank_1_approximation(self, i):\n",
    "        \"\"\"\n",
    "        Compute a single rank-1 approximation: u_i v_i^T (without sigma).\n",
    "        \"\"\"\n",
    "        if self.U is None:\n",
    "            raise ValueError(\"SVD not computed. Set SVD components first.\")\n",
    "\n",
    "        m, n = len(self.original_matrix), len(self.original_matrix[0])\n",
    "        u_i = [self.U[j][i] for j in range(m)]\n",
    "        v_i = [self.Vt[i][j] for j in range(n)]\n",
    "        # Compute outer product u_i v_i^T\n",
    "        A_i = [[u_i[j] * v_i[l] for l in range(n)] for j in range(m)]\n",
    "        return A_i\n",
    "\n",
    "    def rank_k_approximation(self, k):\n",
    "        \"\"\"\n",
    "        Create rank-k approximation: Â^(k) = Σᵢ₌₁ᵏ σᵢ uᵢ vᵢᵀ.\n",
    "        \"\"\"\n",
    "        if self.U is None:\n",
    "            raise ValueError(\"SVD not computed. Set SVD components first.\")\n",
    "\n",
    "        k = min(k, len(self.sigma))\n",
    "        m, n = len(self.original_matrix), len(self.original_matrix[0])\n",
    "        A_k = [[0 for _ in range(n)] for _ in range(m)]\n",
    "\n",
    "        for i in range(k):\n",
    "            # Compute outer product u_i v_i^T\n",
    "            A_i = self.rank_1_approximation(i)\n",
    "            # Scale by sigma_i and add to A_k\n",
    "            A_i_scaled = matrix_scalar_multiply(self.sigma[i], A_i)\n",
    "            A_k = matrix_add(A_k, A_i_scaled)\n",
    "\n",
    "        return A_k\n",
    "\n",
    "# --- Demonstration ---\n",
    "def demonstrate_movie_ratings_approximations():\n",
    "    \"\"\"\n",
    "    Example 4.15: Compute rank-1 and rank-2 approximations for movie ratings (Equations 4.100b–4.103).\n",
    "    \"\"\"\n",
    "    print(\"=== Example 4.15: Movie Ratings Approximations ===\")\n",
    "    print(\"Equations 4.100b–4.103\\n\")\n",
    "\n",
    "    analyzer = MatrixApproximationAnalyzer()\n",
    "    A = [[5, 4, 1],  # Star Wars\n",
    "         [5, 5, 0],  # Blade Runner\n",
    "         [0, 0, 5],  # Amelie\n",
    "         [1, 0, 4]]  # Delicatessen\n",
    "\n",
    "    # SVD components from Figure 4.10\n",
    "    U = [[-0.6710, 0.0236, 0.4647, -0.5774],\n",
    "         [-0.7197, 0.2054, -0.4759, 0.4619],\n",
    "         [-0.0939, -0.7705, -0.5268, -0.3464],\n",
    "         [-0.1515, -0.6030, 0.5293, -0.5774]]\n",
    "    Sigma = [9.6438, 6.3639, 0.7056]  # Diagonal elements\n",
    "    Vt = [[-0.7367, -0.6515, -0.1811],\n",
    "          [0.0852, 0.1762, -0.9807],\n",
    "          [0.6708, -0.7379, -0.0743]]\n",
    "    analyzer.set_svd(U, Sigma, Vt, A)\n",
    "\n",
    "    print(\"Original Matrix A (Equation 4.103):\")\n",
    "    for row in A:\n",
    "        print(row)\n",
    "\n",
    "    # Compute first rank-1 approximation: Â^(1) = u_1 σ_1 v_1^T (Equations 4.100a–b)\n",
    "    A_1_base = analyzer.rank_1_approximation(0)  # u_1 v_1^T\n",
    "    A_1 = matrix_scalar_multiply(Sigma[0], A_1_base)\n",
    "\n",
    "    print(\"\\nFirst Rank-1 Approximation Â^(1) (Equation 4.100b):\")\n",
    "    for row in A_1:\n",
    "        print([round(x, 4) for x in row])\n",
    "\n",
    "    print(\"\\nInterpretation of Â^(1):\")\n",
    "    print(\"Captures science fiction theme:\")\n",
    "    print(f\"- High values for Star Wars and Blade Runner for Ali and Beatrix (entries > 0.4)\")\n",
    "    print(f\"- Fails to capture Chandra's ratings (small values in third column)\")\n",
    "\n",
    "    # Compute second rank-1 approximation: Â^(2) = u_2 σ_2 v_2^T (Equations 4.101a–b)\n",
    "    A_2_base = analyzer.rank_1_approximation(1)  # u_2 v_2^T\n",
    "    A_2 = matrix_scalar_multiply(Sigma[1], A_2_base)\n",
    "\n",
    "    print(\"\\nSecond Rank-1 Approximation Â^(2) (Equation 4.101b):\")\n",
    "    for row in A_2:\n",
    "        print([round(x, 4) for x in row])\n",
    "\n",
    "    print(\"\\nInterpretation of Â^(2):\")\n",
    "    print(\"Captures French art house theme:\")\n",
    "    print(f\"- High values for Amelie and Delicatessen for Chandra (third column, entries ~0.7556, 0.5914)\")\n",
    "    print(f\"- Fails to capture science fiction movies (small values in first two columns)\")\n",
    "\n",
    "    # Compute rank-2 approximation: Â^(2) = σ_1 Â^(1) + σ_2 Â^(2) (Equation 4.102)\n",
    "    A_2_combined = analyzer.rank_k_approximation(2)\n",
    "\n",
    "    print(\"\\nRank-2 Approximation Â^(2) (Equation 4.102):\")\n",
    "    for row in A_2_combined:\n",
    "        print([round(x, 4) for x in row])\n",
    "\n",
    "    # Compare with original matrix\n",
    "    print(\"\\nComparison with Original Matrix A:\")\n",
    "    print(\"Original A:\")\n",
    "    for row in A:\n",
    "        print(row)\n",
    "    print(\"Â^(2):\")\n",
    "    for row in A_2_combined:\n",
    "        print([round(x, 4) for x in row])\n",
    "\n",
    "    print(\"\\nInterpretation:\")\n",
    "    print(\"Â^(2) closely approximates A, suggesting the third singular value (σ_3 = 0.7056) is negligible.\")\n",
    "    print(\"The data is well-represented by a two-dimensional space:\")\n",
    "    print(\"- Science fiction theme (captured by Â^(1))\")\n",
    "    print(\"- French art house theme (captured by Â^(2))\")\n",
    "    print(\"No evidence of a third movie-theme category.\")\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Matrix Approximation and Eckart-Young Theorem: Movie Ratings\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Run demonstration\n",
    "    demonstrate_movie_ratings_approximations()\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Summary of Key Results:\")\n",
    "    print(\"• Rank-1 approximation Â^(1) captures science fiction theme and lovers\")\n",
    "    print(\"• Rank-1 approximation Â^(2) captures French art house theme and lovers\")\n",
    "    print(\"• Rank-2 approximation Â^(2) closely matches the original matrix\")\n",
    "    print(\"• Movie themes are a two-dimensional space: sci-fi and French art house\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38611e30",
   "metadata": {},
   "source": [
    "## 4.7 Matrix Phylogeny\n",
    "\n",
    "The word “phylogenetic” describes how we capture the relationships between different types of matrices (black arrows indicating “is a subset of”) and the covered operations we can perform on them (in blue). We consider all real matrices $ A \\in \\mathbb{R}^{n \\times m} $. For non-square matrices (where $ n \\neq m $), the SVD always exists, as we saw in this chapter.\n",
    "\n",
    "Focusing on square matrices $ A \\in \\mathbb{R}^{n \\times n} $, the determinant informs us whether a square matrix possesses an inverse matrix, i.e., whether it belongs to the class of regular, invertible matrices. If the square $ n \\times n $ matrix possesses $ n $ linearly independent eigenvectors, then the matrix is non-defective and an eigendecomposition exists (Theorem 4.12). We know that repeated eigenvalues may result in defective matrices, which cannot be diagonalized.\n",
    "\n",
    "Non-singular and non-defective matrices are not the same. For example, a rotation matrix will be invertible (determinant is nonzero) but not diagonalizable in the real numbers (eigenvalues are not guaranteed to be real numbers).\n",
    "\n",
    "We dive further into the branch of non-defective square $ n \\times n $ matrices. $ A $ is normal if the condition $ A^\\top A = A A^\\top $ holds. Moreover, if the more restrictive condition holds that\n",
    "\n",
    "$$\n",
    "A^\\top A = A A^\\top = I,\n",
    "$$\n",
    "\n",
    "then $ A $ is called orthogonal (see Definition 3.8). The set of orthogonal matrices is a subset of the regular (invertible) matrices and satisfies $ A^\\top = A^{-1} $.\n",
    "\n",
    "Normal matrices have a frequently encountered subset, the symmetric matrices $ S \\in \\mathbb{R}^{n \\times n} $, which satisfy $ S = S^\\top $. Symmetric matrices have only real eigenvalues. A subset of the symmetric matrices consists of the positive definite matrices $ P $ that satisfy the condition of $ x^\\top P x > 0 $ for all $ x \\in \\mathbb{R}^n \\setminus \\{0\\} $. In this case, a unique Cholesky decomposition exists (Theorem 4.18). Positive definite matrices have only positive eigenvalues and are always invertible (i.e., have a nonzero determinant).\n",
    "\n",
    "Another subset of symmetric matrices consists of the diagonal matrices $ D $. Diagonal matrices are closed under multiplication and addition, but do not necessarily form a group (this is only the case if all diagonal entries are nonzero so that the matrix is invertible). A special diagonal matrix is the identity matrix $ I $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7be2a94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix Phylogeny Analysis\n",
      "============================================================\n",
      "=== Matrix Phylogeny Classification ===\n",
      "Section 4.7: Classifying Matrices per Figure 4.13\n",
      "\n",
      "Test Case 1: Identity Matrix\n",
      "Classifying Matrix (shape 2x2):\n",
      "[1, 0]\n",
      "[0, 1]\n",
      "\n",
      "Properties:\n",
      "- Real matrix\n",
      "- Square\n",
      "- Invertible (Regular)\n",
      "- Normal\n",
      "- Orthogonal\n",
      "- Rotation matrix (if det = 1)\n",
      "- Symmetric\n",
      "- Positive definite\n",
      "- Diagonal\n",
      "- Identity\n",
      "- Likely non-defective (simplified check)\n",
      "\n",
      "Operations/Characteristics:\n",
      "- Inverse exists\n",
      "- A^T = A^-1\n",
      "- Eigenvalues are real\n",
      "- Cholesky decomposition exists\n",
      "- Eigenvalues > 0\n",
      "- Eigendecomposition likely exists\n",
      "\n",
      "Test Case 2: Symmetric Positive Definite Matrix\n",
      "Classifying Matrix (shape 2x2):\n",
      "[2, 1]\n",
      "[1, 2]\n",
      "\n",
      "Properties:\n",
      "- Real matrix\n",
      "- Square\n",
      "- Invertible (Regular)\n",
      "- Normal\n",
      "- Symmetric\n",
      "- Likely non-defective (simplified check)\n",
      "\n",
      "Operations/Characteristics:\n",
      "- Inverse exists\n",
      "- Eigenvalues are real\n",
      "- Eigendecomposition likely exists\n",
      "\n",
      "Test Case 3: Non-square Matrix\n",
      "Classifying Matrix (shape 2x3):\n",
      "[1, 2, 3]\n",
      "[4, 5, 6]\n",
      "\n",
      "Properties:\n",
      "- Real matrix\n",
      "- Nonsquare\n",
      "\n",
      "Operations/Characteristics:\n",
      "- SVD exists\n",
      "- Pseudo-inverse exists\n",
      "\n",
      "Test Case 4: Orthogonal Matrix (Rotation by 90 degrees)\n",
      "Classifying Matrix (shape 2x2):\n",
      "[0, -1]\n",
      "[1, 0]\n",
      "\n",
      "Properties:\n",
      "- Real matrix\n",
      "- Square\n",
      "- Invertible (Regular)\n",
      "- Normal\n",
      "- Orthogonal\n",
      "- Rotation matrix (if det = 1)\n",
      "- Likely non-defective (simplified check)\n",
      "\n",
      "Operations/Characteristics:\n",
      "- Inverse exists\n",
      "- A^T = A^-1\n",
      "- Eigendecomposition likely exists\n",
      "\n",
      "============================================================\n",
      "Summary of Key Results:\n",
      "• Classified matrices into square/non-square, normal, symmetric, orthogonal, etc.\n",
      "• Identified applicable operations (SVD, eigendecomposition, Cholesky, etc.)\n",
      "• Demonstrated the phylogenetic relationships as per Figure 4.13\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# --- Matrix Operations ---\n",
    "def transpose(A):\n",
    "    \"\"\"\n",
    "    Compute the transpose of matrix A.\n",
    "    \"\"\"\n",
    "    m, n = len(A), len(A[0])\n",
    "    return [[A[j][i] for j in range(m)] for i in range(n)]\n",
    "\n",
    "def matrix_multiply(A, B):\n",
    "    \"\"\"\n",
    "    Multiply two matrices A (m x n) and B (n x p).\n",
    "    \"\"\"\n",
    "    m, n = len(A), len(B[0])\n",
    "    result = [[0 for _ in range(n)] for _ in range(m)]\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            result[i][j] = sum(A[i][k] * B[k][j] for k in range(len(B)))\n",
    "    return result\n",
    "\n",
    "def matrices_equal(A, B, tol=1e-6):\n",
    "    \"\"\"\n",
    "    Check if two matrices are equal within a tolerance.\n",
    "    \"\"\"\n",
    "    return all(abs(A[i][j] - B[i][j]) < tol for i in range(len(A)) for j in range(len(A[0])))\n",
    "\n",
    "def dot_product(x, y):\n",
    "    \"\"\"\n",
    "    Compute the dot product of two vectors.\n",
    "    \"\"\"\n",
    "    return sum(xi * yi for xi, yi in zip(x, y))\n",
    "\n",
    "# --- Matrix Classifier Class ---\n",
    "class MatrixClassifier:\n",
    "    def __init__(self, A):\n",
    "        self.A = A\n",
    "        self.m = len(A)\n",
    "        self.n = len(A[0]) if A else 0\n",
    "        self.A_T = transpose(A) if A else []\n",
    "\n",
    "    def is_square(self):\n",
    "        \"\"\"\n",
    "        Check if the matrix is square (m = n).\n",
    "        \"\"\"\n",
    "        return self.m == self.n\n",
    "\n",
    "    def determinant_2x2(self):\n",
    "        \"\"\"\n",
    "        Compute determinant for a 2x2 matrix.\n",
    "        \"\"\"\n",
    "        if self.m != 2 or self.n != 2:\n",
    "            raise ValueError(\"Matrix must be 2x2\")\n",
    "        return self.A[0][0] * self.A[1][1] - self.A[0][1] * self.A[1][0]\n",
    "\n",
    "    def is_invertible(self):\n",
    "        \"\"\"\n",
    "        Check if the matrix is invertible (nonzero determinant, square matrix only).\n",
    "        Simplified for 2x2 matrices.\n",
    "        \"\"\"\n",
    "        if not self.is_square():\n",
    "            return False\n",
    "        if self.m == 2:\n",
    "            det = self.determinant_2x2()\n",
    "            return abs(det) > 1e-6\n",
    "        # For larger matrices, determinant computation is complex without libraries\n",
    "        return None  # Placeholder for non-2x2 matrices\n",
    "\n",
    "    def is_symmetric(self):\n",
    "        \"\"\"\n",
    "        Check if the matrix is symmetric (A = A^T).\n",
    "        \"\"\"\n",
    "        if not self.is_square():\n",
    "            return False\n",
    "        return matrices_equal(self.A, self.A_T)\n",
    "\n",
    "    def is_normal(self):\n",
    "        \"\"\"\n",
    "        Check if the matrix is normal (A^T A = A A^T).\n",
    "        \"\"\"\n",
    "        if not self.is_square():\n",
    "            return False\n",
    "        A_T_A = matrix_multiply(self.A_T, self.A)\n",
    "        A_A_T = matrix_multiply(self.A, self.A_T)\n",
    "        return matrices_equal(A_T_A, A_A_T)\n",
    "\n",
    "    def is_orthogonal(self):\n",
    "        \"\"\"\n",
    "        Check if the matrix is orthogonal (A^T A = A A^T = I).\n",
    "        \"\"\"\n",
    "        if not self.is_square():\n",
    "            return False\n",
    "        n = self.n\n",
    "        I = [[1 if i == j else 0 for j in range(n)] for i in range(n)]\n",
    "        A_T_A = matrix_multiply(self.A_T, self.A)\n",
    "        return matrices_equal(A_T_A, I) and matrices_equal(matrix_multiply(self.A, self.A_T), I)\n",
    "\n",
    "    def is_diagonal(self):\n",
    "        \"\"\"\n",
    "        Check if the matrix is diagonal (non-diagonal entries are zero).\n",
    "        \"\"\"\n",
    "        if not self.is_square():\n",
    "            return False\n",
    "        for i in range(self.m):\n",
    "            for j in range(self.n):\n",
    "                if i != j and abs(self.A[i][j]) > 1e-6:\n",
    "                    return False\n",
    "        return True\n",
    "\n",
    "    def is_identity(self):\n",
    "        \"\"\"\n",
    "        Check if the matrix is the identity matrix.\n",
    "        \"\"\"\n",
    "        if not self.is_diagonal():\n",
    "            return False\n",
    "        return all(abs(self.A[i][i] - 1) < 1e-6 for i in range(self.m))\n",
    "\n",
    "    def is_positive_definite(self):\n",
    "        \"\"\"\n",
    "        Check if the matrix is positive definite (x^T A x > 0 for all x ≠ 0).\n",
    "        Simplified: check if symmetric and all diagonal entries are positive (for diagonal matrices).\n",
    "        Full check requires eigenvalues, which is complex without libraries.\n",
    "        \"\"\"\n",
    "        if not self.is_symmetric():\n",
    "            return False\n",
    "        if self.is_diagonal():\n",
    "            return all(self.A[i][i] > 0 for i in range(self.m))\n",
    "        # For non-diagonal matrices, we'd need eigenvalues\n",
    "        return None  # Placeholder\n",
    "\n",
    "    def classify_matrix(self):\n",
    "        \"\"\"\n",
    "        Classify the matrix according to the phylogeny in Figure 4.13.\n",
    "        \"\"\"\n",
    "        print(f\"Classifying Matrix (shape {self.m}x{self.n}):\")\n",
    "        for row in self.A:\n",
    "            print(row)\n",
    "\n",
    "        properties = []\n",
    "        operations = []\n",
    "\n",
    "        # Step 1: Real matrix\n",
    "        properties.append(\"Real matrix\")\n",
    "\n",
    "        # Step 2: Square or non-square\n",
    "        if self.is_square():\n",
    "            properties.append(\"Square\")\n",
    "            # Check invertibility\n",
    "            invertible = self.is_invertible()\n",
    "            if invertible is True:\n",
    "                properties.append(\"Invertible (Regular)\")\n",
    "                operations.append(\"Inverse exists\")\n",
    "            elif invertible is False:\n",
    "                properties.append(\"Singular (det = 0)\")\n",
    "\n",
    "            # Check for normal matrix\n",
    "            if self.is_normal():\n",
    "                properties.append(\"Normal\")\n",
    "                # Check for orthogonal matrix\n",
    "                if self.is_orthogonal():\n",
    "                    properties.append(\"Orthogonal\")\n",
    "                    properties.append(\"Rotation matrix (if det = 1)\")\n",
    "                    operations.append(\"A^T = A^-1\")\n",
    "\n",
    "                # Check for symmetric matrix\n",
    "                if self.is_symmetric():\n",
    "                    properties.append(\"Symmetric\")\n",
    "                    operations.append(\"Eigenvalues are real\")\n",
    "                    # Check for positive definite\n",
    "                    pd = self.is_positive_definite()\n",
    "                    if pd is True:\n",
    "                        properties.append(\"Positive definite\")\n",
    "                        operations.append(\"Cholesky decomposition exists\")\n",
    "                        operations.append(\"Eigenvalues > 0\")\n",
    "                    elif pd is False:\n",
    "                        properties.append(\"Not positive definite\")\n",
    "\n",
    "                    # Check for diagonal matrix\n",
    "                    if self.is_diagonal():\n",
    "                        properties.append(\"Diagonal\")\n",
    "                        # Check for identity matrix\n",
    "                        if self.is_identity():\n",
    "                            properties.append(\"Identity\")\n",
    "\n",
    "            # Eigendecomposition (simplified check)\n",
    "            if invertible is not None and invertible:\n",
    "                properties.append(\"Likely non-defective (simplified check)\")\n",
    "                operations.append(\"Eigendecomposition likely exists\")\n",
    "            else:\n",
    "                properties.append(\"Possibly defective (simplified check)\")\n",
    "\n",
    "        else:\n",
    "            properties.append(\"Nonsquare\")\n",
    "            operations.append(\"SVD exists\")\n",
    "            operations.append(\"Pseudo-inverse exists\")\n",
    "\n",
    "        # Print classification\n",
    "        print(\"\\nProperties:\")\n",
    "        for prop in properties:\n",
    "            print(f\"- {prop}\")\n",
    "\n",
    "        print(\"\\nOperations/Characteristics:\")\n",
    "        for op in operations:\n",
    "            print(f\"- {op}\")\n",
    "\n",
    "# --- Demonstration ---\n",
    "def demonstrate_matrix_phylogeny():\n",
    "    \"\"\"\n",
    "    Demonstrate matrix classification using examples.\n",
    "    \"\"\"\n",
    "    print(\"=== Matrix Phylogeny Classification ===\")\n",
    "    print(\"Section 4.7: Classifying Matrices per Figure 4.13\\n\")\n",
    "\n",
    "    # Test Case 1: Identity Matrix (2x2)\n",
    "    print(\"Test Case 1: Identity Matrix\")\n",
    "    A1 = [[1, 0], [0, 1]]\n",
    "    classifier1 = MatrixClassifier(A1)\n",
    "    classifier1.classify_matrix()\n",
    "\n",
    "    # Test Case 2: Symmetric Positive Definite Matrix (2x2)\n",
    "    print(\"\\nTest Case 2: Symmetric Positive Definite Matrix\")\n",
    "    A2 = [[2, 1], [1, 2]]\n",
    "    classifier2 = MatrixClassifier(A2)\n",
    "    classifier2.classify_matrix()\n",
    "\n",
    "    # Test Case 3: Non-square Matrix (2x3)\n",
    "    print(\"\\nTest Case 3: Non-square Matrix\")\n",
    "    A3 = [[1, 2, 3], [4, 5, 6]]\n",
    "    classifier3 = MatrixClassifier(A3)\n",
    "    classifier3.classify_matrix()\n",
    "\n",
    "    # Test Case 4: Orthogonal Matrix (Rotation by 90 degrees, 2x2)\n",
    "    print(\"\\nTest Case 4: Orthogonal Matrix (Rotation by 90 degrees)\")\n",
    "    A4 = [[0, -1], [1, 0]]\n",
    "    classifier4 = MatrixClassifier(A4)\n",
    "    classifier4.classify_matrix()\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Matrix Phylogeny Analysis\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Run demonstration\n",
    "    demonstrate_matrix_phylogeny()\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Summary of Key Results:\")\n",
    "    print(\"• Classified matrices into square/non-square, normal, symmetric, orthogonal, etc.\")\n",
    "    print(\"• Identified applicable operations (SVD, eigendecomposition, Cholesky, etc.)\")\n",
    "    print(\"• Demonstrated the phylogenetic relationships as per Figure 4.13\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81eae23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
