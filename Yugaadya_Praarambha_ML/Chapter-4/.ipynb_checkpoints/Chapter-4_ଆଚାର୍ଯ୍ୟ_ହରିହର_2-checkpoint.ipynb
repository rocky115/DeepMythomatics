{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858e461d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " * Copyright (c) 2016 Radhamadhab Dalai\n",
    " *\n",
    " * Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    " * of this software and associated documentation files (the \"Software\"), to deal\n",
    " * in the Software without restriction, including without limitation the rights\n",
    " * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    " * copies of the Software, and to permit persons to whom the Software is\n",
    " * furnished to do so, subject to the following conditions:\n",
    " *\n",
    " * The above copyright notice and this permission notice shall be included in\n",
    " * all copies or substantial portions of the Software.\n",
    " *\n",
    " * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    " * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    " * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    " * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    " * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    " * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
    " * THE SOFTWARE.\n",
    "'''"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOYAAAA/CAIAAAB7OSMLAAAHl0lEQVR4Ae3cX2jUSBgA8Lydd+CDB11SRXy5e/DOLki7Lx7cnbYPnqdQ5KQNPlS5F49innZ9WSgEKRZd0Lpo6ZNQjLRia5UNhdaz/on1UI/2olJqlSCCPaLbbo+hSrO7c7Sj6TbtbneTbDKTThCcmSTDN9/8mp1ukzCQbjQDRGWAISpaGizNAKRkKQLCMkDJEjZhNFxKlhogLAOULGETRsOlZKkBwjJAyRI2YSWHm/5Q8il4n0DJ4j0/NqNL/g0nO+G/f8L5/2z2hM/plCw+c+F0JMjrZOeCWh/BpWSdhoJJfyavPoJLyWJCzNEw8nn1BVxK1lErOHS2plfC4VKyOChzLobivRILl5J1jovnPVnwSiBcStZzaA4FYMcrUXApWYfELO9G0zR1+abr+vJDHK054pUQuJSsLToAAEVRRFHkeZ7jOJZl6+rquM9bfHH7XFv43zhAFEVFUVKpFMMwgiDYAu2sV+zhUrIlk9U0TZIknudZluU4ThRFWZZVVS2+I03TFEWJx+N79+5lFrctW7aMjo4W38PSkWXyijFcSnZp9guXFEURBIFlWZ7nJUnSNK3w8cXs7ejo2LBhA8Mwu3btOnToUDGnLDum3F6xhEvJLjOwsqKqqiAIwWBQEARFUVYe4FmLa14xg0vJrk5O13VJkoLBIM/zeElF8brvFRu4lKyZLAAgHo8Hg0FRFAEA5t041D30igFcSnbJIAAArQEkSbL1K/xSl2Uo4eDVU7iU7IIqXddFUURX1uKVSZI0NDQEIcxms4ODg+jETCZz586d4jsp7UisvHoEl5KFsiyzLCuKYklX1rGxsSNHjgiCACHs6+t78+aNge/BgwdPnz41qo4V8PTqOtx1TRYAgL7nL3XNOjExcfTo0YaGBkSzvb3d5PLMmTOmFrtVzL26CHf9kkVfCFj+NqCpqenWrVsI4qlTpyCEmUzm0aNHqOXkyZN2jeaeT4pXV+CuR7K6rnMcx/N8SSuBXEIQwp07d7579w41trW1QQjT6fT9+/dRS2trq+l461XivJYZ7rojq6oqy7KyLFs3tPjrWlVVldHD2bNnjTIqxGIxU4vFKrleywYXC7KKorAsW+qC0gICVVWDwaDNv7U+fPjw0qVLJ06cMAK4evXqhw9Lz16PjIw8fvzY2GutoChKoOLr776p5PaH4i2NSqLl01OHBgVyCupwK1e/e/fPP1lLheksLMjW1tYyDNPYGO/shHb+mca2atXOYgB1+Pz5866urlQqZfSfTqeHh4dRNZPJJBIJY5flAgAgUFGBbpphGObH0LckktX+inH7Q2gUFRUV9pMPIQZvPgQAoCFt3rzDjtfOTss8MD1R07RAIGCoFfgD4J/zZMFt+LXGiF8URUcS7f1VVpIkY1SxmGZHrSMZwaoTpHbTpk07dnyPskQW3N9/+wGF7dQlFourbDweN8i2tCiUrOlnBgCAPk9lWQ4GqwiCK/AHGIaprq6ur6+XJMk0LstV76+yEEJVVRmGsXmJ9d/CYNVJJQUu8lpXV+vI+jU3Fa6SffbsWWceWQzD2Lm+onNzB+bvMuZwy+fV7YXB27dva2pqVsVEya6alsKNeMItq1e3yXZ3d+/bt+/atWu5N5GgWaFkC+tEe9+/f9/b25tIJLLZrHE8VnALeNV1vb+/P5FIJJNJI3gLBVcXBseOHevt7b137x7HcQCA0dHR+fl5FDQlu+bkzc7OVldXJ5PJaDSaSqVevHiRewoOcAt4hRAePHjw7t27V65cGRkZefXq1evXr3PjL77sKtlQKJRKpWKxWCQSuX379vT0tLG0pWTXnLPr1683Njaiw+bm5k6fPr3yFA/hFvY6MzNTWVmJPhySyeTY2FhbW1vuZ8XKseRrcZVsOBzu7+9vampCN5QMDg5OTk6iyCjZfDNktE9NTR0+fHhoaKirq2tmZqajo8PYZSq4D7ewV3QXPMdxAwMDPT094+Pjs7Ozly9fNoVdZNVVsuj+fxRZX19fNBodGBhAVUq2mAnLZrMfP36EEM7NzUUikcIvT3AN7ppejaGh4AEAzc3NgiAQcJU1Ql9ZoGRX5sSRlnLDLd6rI8Nx+xuDAkFTsgWSY39XmeC675WStY+BpB6cheuJV0qWJHBOxeoIXK+8UrJOMSCvHztwPfRKyZJHzdmILcD11isl6ywAUnsrHq7nXilZUpGVI+414eLg1Q9kz58HFy/q6+3mw3KQRX3mg4uJVz+Q3bhx4emoPXv4SER2/G7i8snAvGcT3OgfvzAMU477tS3kwe0/2OYL0XiWxk4hEAi48GR5viH4r92A+9WXX2DiFa+rrB2sn56IWnypm//oeDsiWZbHx8fx+QTD5SpreVba29t5npdluiqwnELCTiSGbDKZvHDhgs0XvRA2OTTc1TJADFkI4fbt23Pf0bLacGib/zNADNmpqamtW7fevHnT/uuu/D+rvh4hMWS7u7uPHz+u6/q2bdvS6fTLly99PS90cHkzQAzZcDh848aNJ0+ehEKh6enplS/OzjtEusNfGSCGbE9Pz7lz55qbm9HCoMCTT/6aIDoacwaIIYueG0NPC2maFg6HbT4Ob84ErROSgf8B3Ku5qK3pWlcAAAAASUVORK5CYII="
    }
   },
   "cell_type": "markdown",
   "id": "cd9f4967",
   "metadata": {},
   "source": [
    "# Partial Derivatives and the Jacobian\n",
    "\n",
    "The partial derivative of a vector-valued function $ f : \\mathbb{R}^n \\to \\mathbb{R}^m $ with respect to $ x_i \\in \\mathbb{R} $ (where $ i = 1, \\ldots, n $) is given as the vector:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x_i} = \\lim_{h \\to 0} \\frac{f_1(x_1, \\ldots, x_{i-1}, x_i + h, x_{i+1}, \\ldots, x_n) - f_1(x)}{h}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\vdots\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x_i} = \\lim_{h \\to 0} \\frac{f_m(x_1, \\ldots, x_{i-1}, x_i + h, x_{i+1}, \\ldots, x_n) - f_m(x)}{h}\n",
    "$$\n",
    "\n",
    "From the definition of the gradient, we know that the gradient of $ f $ with respect to a vector is the row vector of the partial derivatives. Therefore, we can express the gradient of $ f : \\mathbb{R}^n \\to \\mathbb{R}^m $ as:\n",
    "\n",
    "$$\n",
    "\\frac{df(x)}{dx} = \\begin{bmatrix}\n",
    "\\frac{\\partial f_1(x)}{\\partial x_1} & \\cdots & \\frac{\\partial f_1(x)}{\\partial x_n} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial f_m(x)}{\\partial x_1} & \\cdots & \\frac{\\partial f_m(x)}{\\partial x_n}\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{m \\times n}\n",
    "$$\n",
    "\n",
    "## Definition  (Jacobian)\n",
    "\n",
    "The collection of all first-order partial derivatives of a vector-valued function $ f : \\mathbb{R}^n \\to \\mathbb{R}^m $ is called the Jacobian. The Jacobian $ J $ is an $ m \\times n $ matrix, defined as:\n",
    "\n",
    "$$\n",
    "J = \\nabla_x f = \\begin{bmatrix}\n",
    "\\frac{\\partial f_1(x)}{\\partial x_1} & \\cdots & \\frac{\\partial f_1(x)}{\\partial x_n} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial f_m(x)}{\\partial x_1} & \\cdots & \\frac{\\partial f_m(x)}{\\partial x_n}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "As a special case, for a function $ P f : \\mathbb{R}^n \\to \\mathbb{R}^1 $ (e.g., $ f(x) = \\sum_{i=1}^n x_i $), the Jacobian is a row vector (matrix of dimension $ 1 \\times n $).\n",
    "\n",
    "## Remark\n",
    "\n",
    "In this context, we use the numerator layout of the derivative, i.e., the derivative $ \\frac{df}{dx} $ of $ f \\in \\mathbb{R}^m $ with respect to $ x \\in \\mathbb{R}^n $ is an $ m \\times n $ matrix, where the elements of $ f $ define the rows and the elements of $ x $ define the columns of the corresponding Jacobian.\n",
    "\n",
    "# Determinant of the Jacobian and Area Scaling\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "Fig.5 illustrates that the determinant of the Jacobian of $ f $ can be used to compute the magnifier between the blue and orange areas. There exists also a denominator layout, which is the transpose of the numerator layout. In this book, we will use the numerator layout.\n",
    "\n",
    "We will see how the Jacobian is used in the change-of-variable method for probability distributions in Section 6.7. The amount of scaling due to the transformation of a variable is provided by the determinant. \n",
    "\n",
    "In Section 4.1, we saw that the determinant can be used to compute the area of a parallelogram. If we are given two vectors \n",
    "\n",
    "$$\n",
    "b_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}, \\quad b_2 = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "as the sides of the unit square (blue; see Figure 5.5), the area of this square is \n",
    "\n",
    "$$\n",
    "\\text{det} \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} = 1.\n",
    "$$\n",
    "\n",
    "If we take a parallelogram with the sides \n",
    "\n",
    "$$\n",
    "c_1 = \\begin{bmatrix} -2 \\\\ 1 \\end{bmatrix}, \\quad c_2 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\n",
    "$$ \n",
    "\n",
    "(orange in Figure 5.5), its area is given as the absolute value of the determinant:\n",
    "\n",
    "$$\n",
    "\\text{det} \\begin{bmatrix} -2 & 1 \\\\ 1 & 1 \\end{bmatrix} = |-3| = 3,\n",
    "$$ \n",
    "\n",
    "i.e., the area of this is exactly three times the area of the unit square. We can find this scaling factor by finding a mapping that transforms the unit square into the other square. \n",
    "\n",
    "In linear algebra terms, we effectively perform a variable transformation from $ (b_1, b_2) $ to $ (c_1, c_2) $. In our case, the mapping is linear, and the absolute value of the determinant of this mapping gives us exactly the scaling factor we are looking for. \n",
    "\n",
    "## Approach 1\n",
    "\n",
    "To get started with the linear algebra approach, we identify both $ \\{b_1, b_2\\} $ and $ \\{c_1, c_2\\} $ as bases of $ \\mathbb{R}^2 $. What we effectively perform is a change of basis from $ (b_1, b_2) $ to $ (c_1, c_2) $, and we are looking for the transformation matrix that implements the basis change. \n",
    "\n",
    "Using results from Section 2.7.2, we identify the desired basis change matrix as \n",
    "\n",
    "$$\n",
    "J = \\begin{bmatrix} -2 & 1 \\\\ 1 & 1 \\end{bmatrix},\n",
    "$$ \n",
    "\n",
    "such that $ J b_1 = c_1 $ and $ J b_2 = c_2 $. The absolute value of the determinant of $ J $ gives us the scaling factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "410c0b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area of the unit square: 1\n",
      "Area of the parallelogram: 3\n"
     ]
    }
   ],
   "source": [
    "def compute_determinant(matrix):\n",
    "    \"\"\"Compute the determinant of a 2x2 matrix.\"\"\"\n",
    "    return matrix[0][0] * matrix[1][1] - matrix[0][1] * matrix[1][0]\n",
    "\n",
    "def main():\n",
    "    # Define the unit square vectors\n",
    "    b1 = [[1, 0], [0, 1]]  # Matrix for unit square\n",
    "\n",
    "    # Define the parallelogram vectors\n",
    "    c1 = [[-2, 1], [1, 1]]  # Matrix for parallelogram\n",
    "\n",
    "    # Compute the determinant for the unit square\n",
    "    unit_square_det = compute_determinant(b1)\n",
    "    print(f\"Area of the unit square: {unit_square_det}\")  # Should be 1\n",
    "\n",
    "    # Compute the determinant for the parallelogram\n",
    "    parallelogram_det = compute_determinant(c1)\n",
    "    print(f\"Area of the parallelogram: {abs(parallelogram_det)}\")  # Should be 3\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eedf4756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacobian Matrix:\n",
      "[2.00001000001393, 4.000010000027032]\n",
      "[2.0000000000131024, 1.0000000000065512]\n",
      "Determinant of the Jacobian: -6.000010000079442\n"
     ]
    }
   ],
   "source": [
    "def compute_determinant(matrix):\n",
    "    \"\"\"Compute the determinant of an n x n matrix using recursion.\"\"\"\n",
    "    if len(matrix) == 2:\n",
    "        return matrix[0][0] * matrix[1][1] - matrix[0][1] * matrix[1][0]\n",
    "\n",
    "    determinant = 0\n",
    "    for c in range(len(matrix)):\n",
    "        # Create a submatrix by excluding the first row and current column\n",
    "        submatrix = [row[:c] + row[c + 1:] for row in matrix[1:]]\n",
    "        determinant += ((-1) ** c) * matrix[0][c] * compute_determinant(submatrix)\n",
    "    return determinant\n",
    "\n",
    "def compute_jacobian(func, variables):\n",
    "    \"\"\"Compute the Jacobian matrix of a function.\"\"\"\n",
    "    n = len(variables)\n",
    "    jacobian = [[0] * n for _ in range(n)]\n",
    "    \n",
    "    h = 1e-5  # Small value for numerical differentiation\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            # Create a perturbed variable vector\n",
    "            x_plus_h = variables[:]\n",
    "            x_plus_h[j] += h\n",
    "            \n",
    "            # Numerical partial derivative\n",
    "            f_plus_h = func(*x_plus_h)\n",
    "            f_original = func(*variables)\n",
    "            jacobian[i][j] = (f_plus_h[i] - f_original[i]) / h\n",
    "    \n",
    "    return jacobian\n",
    "\n",
    "def example_function(x, y):\n",
    "    \"\"\"Example function for testing: f(x, y) = [x^2 + y^2, x * y]\"\"\"\n",
    "    return [x**2 + y**2, x * y]\n",
    "\n",
    "def main():\n",
    "    # Define the variables\n",
    "    variables = [1.0, 2.0]  # Example values for x and y\n",
    "    \n",
    "    # Compute the Jacobian\n",
    "    jacobian = compute_jacobian(example_function, variables)\n",
    "    print(\"Jacobian Matrix:\")\n",
    "    for row in jacobian:\n",
    "        print(row)\n",
    "    \n",
    "    # Compute the determinant of the Jacobian\n",
    "    jacobian_det = compute_determinant(jacobian)\n",
    "    print(f\"Determinant of the Jacobian: {jacobian_det}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFYAAABFCAIAAABfdoaVAAAEcElEQVR4Ae2bT0vrQBDAQ09+hH4bP4dfoCcvXvTiTUSPUg+CB0GwWi/+PVURRatgCv0TL01VAg9ReGJpbRQSm8yjXRn37aZJ3NfUvHZDkNntzO7OL7OzmywqMPKXMvIEQCKQCEAikAigLwg+Pj7+65z6r+nQbDQmxscHT0HhromJCbEn8a8IAOD34+PgKSiKAn/fP4kgDIVms3l9ff3w8KDrutizYqxihyCQQrlczufzOzs7hUKBcUasGEcEgRTS6XS9XrcsS8xnxiqmCHpRcF13dXV1dnZW0zTDMBhnxIrxRdCLgmVZrut6hgCX2pVkMhnIhbcaGxsLtPJU6MOKwLf7rTWCf56dmqBLzMqz1eDOPM0CK8NTEHNGzMpz2IIInp6e8vk8AFiWdXNzQzdt2zapCUlBzBkxK3qcKAsiWF5eVlUVAE5OThzHweaIcHl5aZpmr7zAKIs5I2bFdE2KIggMw0ilUs/PzwCwv78PAI7jlEql+/v7UqlEQiOXy5EOAmNBzBkxq74haDabS0tLANBut7PZLABommbb9vT0NO58NjY2sD9/CslkkknvYVYEMSscEi2IRMH5+fnZ2RkA2La9vb1NmqtUKru7u7jsZTIZuht/CrTm4GURBOl0utFokLGSiZDJZNbW1g4PDyuVComOvb09xpnYUvg2Al3XV1ZW0L2joyPXdW3bpnc+qqq+vLygDgqPv34xMT/gIo6EFr6N4PT09Pb2FptotVqapmGR5ELMCHQ9kTtpLGbXoAckEYBEIBHAyCDg8zzudviJ4KOMeZPX4WuwC7QKKUSSDrt+Kt0v9J9/0XMUcHydmsxfd1idICvswl+QCKI5UOs+RhkFHbhfN8Y2ChicnZqgkPbW6W1lGIaqqq7r4kY2m81eXV157lmHcyK8vb0dHx9Xq9WpqSkAqNfrqVTq7u4OudNCJAh83mT5KPBRxoHyOolEglkUcEVwHGdmZqbdbtdqtcnJSdLI3NwctsYIkSBg+qCLPAL6177Iuq7Pz88DwObm5sXFBXmpX1xc7NX4ECJ4f39fWFioVqtbW1vE7XK5fHBwMEIIAMA0Tcx8tVptfX291WqNFgLa20KhUCwW6RpGHsKJwHgYWIwEAZOrFeXrjIxPhz7KOHpeh6/BFQGtQgpRIaD3Rd33w8+OPBH0d2tkmiYPCGt4LkOIgDhZLBZzudzr6yvvM1MznAj4DTLjNl0cTgT0Bpk56aKdJ/IQImA2yPxJF0MhKgSYfoiQSCQ+kXMf0RlNRVFQGcfK6/A1aMVvkJmTLmz2c0hMuS/Fbtr/elMe8IrAbJCZky7ewaii4AcXRWaDzJx0jQoC3k+fmuGMAh+H+Z8kgsg+nzIZG9O15wa5lzI+MUbBs4hdoFVIQUZBZFHwsytCyOdP1GQUyCjohuu3oiaU8s/uDkMNkVKKZCLwn/19PumEUeZ1fM4RKO9CiZEgCNVzbJQkgmjSYWwecKiByCiQURDRohgq/mKjJCeCnAhyIvTnP9djM6kFB/IHDWz0N1pNoicAAAAASUVORK5CYII="
    }
   },
   "cell_type": "markdown",
   "id": "e2d14e48",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)\n",
    "\n",
    "# Jacobian and Scaling Factors\n",
    "\n",
    "The determinant of $ J $, which yields the scaling factor we are looking for, is given as \n",
    "\n",
    "$$\n",
    "| \\text{det}(J) | = 3,\n",
    "$$ \n",
    "\n",
    "i.e., the area of the square spanned by $ (c_1, c_2) $ is three times greater than the area spanned by $ (b_1, b_2) $.\n",
    "\n",
    "## Approach 2\n",
    "\n",
    "The linear algebra approach works for linear transformations; for nonlinear transformations (which become relevant in Section 6.7), we follow a more general approach using partial derivatives. For this approach, we consider a function \n",
    "\n",
    "$$\n",
    "f : \\mathbb{R}^2 \\to \\mathbb{R}^2\n",
    "$$ \n",
    "\n",
    "that performs a variable transformation. In our example, $ f $ maps the coordinate representation of any vector $ x \\in \\mathbb{R}^2 $ with respect to $ (b_1, b_2) $ onto the coordinate representation $ y \\in \\mathbb{R}^2 $ with respect to $ (c_1, c_2) $. We want to identify the mapping so that we can compute how an area (or volume) changes when it is being transformed by $ f $.\n",
    "\n",
    "For this, we need to find out how $ f(x) $ changes if we modify $ x $ a bit. This question is exactly answered by the Jacobian matrix \n",
    "\n",
    "$$\n",
    "\\frac{df}{dx} \\in \\mathbb{R}^{2 \\times 2}.\n",
    "$$ \n",
    "\n",
    "Since we can write \n",
    "\n",
    "$$\n",
    "y_1 = -2x_1 + x_2 \\quad (5.63)\n",
    "$$ \n",
    "\n",
    "$$\n",
    "y_2 = x_1 + x_2 \\quad (5.64)\n",
    "$$ \n",
    "\n",
    "we obtain the functional relationship between $ x $ and $ y $, which allows us to get the partial derivatives:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y_1}{\\partial x_1} = -2, \\quad \\frac{\\partial y_1}{\\partial x_2} = 1,\n",
    "$$ \n",
    "\n",
    "$$\n",
    "\\frac{\\partial y_2}{\\partial x_1} = 1, \\quad \\frac{\\partial y_2}{\\partial x_2} = 1. \\quad (5.65)\n",
    "$$ \n",
    "\n",
    "We can then compose the Jacobian as \n",
    "\n",
    "$$\n",
    "J = \\begin{bmatrix}\n",
    "\\frac{\\partial y_1}{\\partial x_1} & \\frac{\\partial y_1}{\\partial x_2} \\\\\n",
    "\\frac{\\partial y_2}{\\partial x_1} & \\frac{\\partial y_2}{\\partial x_2}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "-2 & 1 \\\\\n",
    "1 & 1\n",
    "\\end{bmatrix} \\quad (5.66)\n",
    "$$ \n",
    "\n",
    "The Jacobian represents the coordinate transformation we are looking for. It is exact if the coordinate transformation is linear (as in our case), and (5.66) recovers exactly the basis change matrix in (5.62). If the coordinate transformation is nonlinear, the Jacobian approximates this nonlinear transformation locally with a linear one.\n",
    "\n",
    "The absolute value of the Jacobian determinant \n",
    "\n",
    "$$\n",
    "| \\text{det}(J) | \n",
    "$$ \n",
    "\n",
    "is the factor by which areas or volumes are scaled when coordinates are transformed. Our case yields \n",
    "\n",
    "$$\n",
    "| \\text{det}(J) | = 3.\n",
    "$$ \n",
    "\n",
    "The Jacobian determinant and variable transformations will become relevant in Section 6.7 when we transform random variables and probability distributions. These transformations are extremely relevant in machine learning in the context of training deep neural networks using the reparametrization trick, also called infinite perturbation analysis.\n",
    "\n",
    "In this chapter, we encountered derivatives of functions. Figure 5.6 summarizes the dimensions of those derivatives. If \n",
    "\n",
    "$$\n",
    "f : \\mathbb{R} \\to \\mathbb{R} \n",
    "$$ \n",
    "\n",
    "the gradient is simply a scalar (top-left entry). For \n",
    "\n",
    "$$\n",
    "f : \\mathbb{R}^D \\to \\mathbb{R} \n",
    "$$ \n",
    "\n",
    "the gradient is a $ 1 \\times D $ row vector (top-right entry). For \n",
    "\n",
    "$$\n",
    "f : \\mathbb{R} \\to \\mathbb{R}^E \n",
    "$$ \n",
    "\n",
    "the gradient is an $ E \\times 1 $ column vector, and for \n",
    "\n",
    "$$\n",
    "f : \\mathbb{R}^D \\to \\mathbb{R}^E \n",
    "$$ \n",
    "\n",
    "the gradient is an $ E \\times D $ matrix.\n",
    "\n",
    "# Example  (Gradient of a Vector-Valued Function)\n",
    "\n",
    "We are given \n",
    "\n",
    "$$\n",
    "f(x) = Ax, \\quad f(x) \\in \\mathbb{R}^M, \\quad A \\in \\mathbb{R}^{M \\times N}, \\quad x \\in \\mathbb{R}^N.\n",
    "$$ \n",
    "\n",
    "To compute the gradient $ \\frac{df}{dx} $, we first determine the dimension of $ \\frac{df}{dx} $: Since $ f : \\mathbb{R}^N \\to \\mathbb{R}^M $, it follows that $ \\frac{df}{dx} \\in \\mathbb{R}^{M \\times N} $.\n",
    "\n",
    "Second, to compute the gradient, we determine the partial derivatives of $ f $ with respect to every $ x_j $:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f_i}{\\partial x_j} = A_{ij} \\quad \\Rightarrow \\quad \\sum_{j=1}^{N} \\frac{\\partial f_i}{\\partial x_j} = A_{ij} \\quad (5.67)\n",
    "$$ \n",
    "\n",
    "We collect the partial derivatives in the Jacobian and obtain the gradient:\n",
    "\n",
    "$$\n",
    "\\frac{df}{dx} = \\begin{bmatrix}\n",
    "\\frac{\\partial f_1}{\\partial x_1} & \\cdots & \\frac{\\partial f_1}{\\partial x_N} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial f_M}{\\partial x_1} & \\cdots & \\frac{\\partial f_M}{\\partial x_N}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "A_{11} & \\cdots & A_{1N} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "A_{M1} & \\cdots & A_{MN}\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{M \\times N} \\quad (5.68)\n",
    "$$ \n",
    "\n",
    "# Example  (Chain Rule)\n",
    "\n",
    "Consider the function \n",
    "\n",
    "$$\n",
    "h : \\mathbb{R} \\to \\mathbb{R}, \\quad h(t) = (f \\circ g)(t)\n",
    "$$ \n",
    "\n",
    "with \n",
    "\n",
    "$$\n",
    "f : \\mathbb{R}^2 \\to \\mathbb{R} \\quad (5.69)\n",
    "$$ \n",
    "\n",
    "$$\n",
    "g : \\mathbb{R} \\to \\mathbb{R}^2 \\quad (5.70)\n",
    "$$ \n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "f(x) = \\exp(x_1 x_2^2) \\quad (5.71)\n",
    "$$ \n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "g(t) = \\begin{bmatrix}\n",
    "t \\cos t \\\\\n",
    "t \\sin t\n",
    "\\end{bmatrix} \\quad (5.72)\n",
    "$$ \n",
    "\n",
    "We compute the gradient of $ h $ with respect to $ t $. Since $ f : \\mathbb{R}^2 \\to \\mathbb{R} $ and $ g : \\mathbb{R} \\to \\mathbb{R}^2 $, we note that \n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x} \\in \\mathbb{R}^{1 \\times 2}, \\quad \\frac{\\partial g}{\\partial t} \\in \\mathbb{R}^{2 \\times 1} \\quad (5.73)\n",
    "$$ \n",
    "\n",
    "The desired gradient is computed by applying the chain rule:\n",
    "\n",
    "$$\n",
    "\\frac{dh}{dt} = \\frac{\\partial f}{\\partial x} \\frac{\\partial g}{\\partial t} \\quad (5.74a)\n",
    "$$ \n",
    "\n",
    "Calculating this gives:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial f}{\\partial x_1} \\\\\n",
    "\\frac{\\partial f}{\\partial x_2}\n",
    "\\end{bmatrix} = \\left( \\exp(x_1 x_2) x_2^2 \\quad \\exp(x_1 x_2) x_1 x_2^2 \\right)\n",
    "$$ \n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "x_1 = t \\cos t, \\quad x_2 = t \\sin t \\quad (5.72)\n",
    "$$ \n",
    "\n",
    "Thus, we have:\n",
    "\n",
    "$$\n",
    "\\frac{dh}{dt} = \\exp(x_1 x_2) x_2^2 \\left( \\cos t - t \\sin t \\right) + 2 x_1 x_2 \\left( \\sin t + t \\cos t \\right) \\quad (5.74c)\n",
    "$$\n",
    "\n",
    "# Example  (Gradient of a Least-Squares Loss in a Linear Model)\n",
    "\n",
    "Let us consider the linear model \n",
    "\n",
    "$$\n",
    "y = \\Phi \\theta, \\quad (5.75)\n",
    "$$ \n",
    "\n",
    "where $ \\theta \\in \\mathbb{R}^D $ is a parameter vector, $ \\Phi \\in \\mathbb{R}^{N \\times D} $ are input features, and $ y \\in \\mathbb{R}^N $ are the corresponding observations. We define the functions \n",
    "\n",
    "$$\n",
    "L(e) := \\|e\\|_2, \\quad (5.76)\n",
    "$$ \n",
    "\n",
    "$$\n",
    "e(\\theta) := y - \\Phi \\theta. \\quad (5.77)\n",
    "$$ \n",
    "\n",
    "We seek \n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\theta}, \n",
    "$$ \n",
    "\n",
    "and we will use the chain rule for this purpose. $ L $ is called a least-squares loss function. Before we start our calculation, we determine the dimensionality of the gradient as \n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\theta} \\in \\mathbb{R}^{1 \\times D}. \\quad (5.78)\n",
    "$$ \n",
    "\n",
    "The chain rule allows us to compute the gradient as \n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\theta} = \\frac{\\partial L}{\\partial e} \\frac{\\partial e}{\\partial \\theta}, \\quad (5.79)\n",
    "$$ \n",
    "\n",
    "where the $ d $-th element is given by \n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\theta}[1, d] = \\sum_{n=1}^{N} \\frac{\\partial L}{\\partial e} \\frac{\\partial e}{\\partial \\theta}. \\quad (5.80)\n",
    "$$ \n",
    "\n",
    "We know that \n",
    "\n",
    "$$\n",
    "\\|e\\|_2 = e^\\top e \\quad (see \\, Section \\, 3.2)\n",
    "$$ \n",
    "\n",
    "and determine \n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial e} = 2e^\\top \\in \\mathbb{R}^{1 \\times N}. \\quad (5.81)\n",
    "$$ \n",
    "\n",
    "Furthermore, we obtain \n",
    "\n",
    "$$\n",
    "\\frac{\\partial e}{\\partial \\theta} = -\\Phi \\in \\mathbb{R}^{N \\times D}. \\quad (5.82)\n",
    "$$ \n",
    "\n",
    "Thus, our desired derivative is \n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\theta} = -2e^\\top \\Phi = -2(y^\\top - \\theta^\\top \\Phi^\\top) \\quad | \\{z\\} \\, \\Phi \\in \\mathbb{R}^{1 \\times D}. \\quad (5.83)\n",
    "$$ \n",
    "\n",
    "**Remark**: We would have obtained the same result without using the chain rule by immediately looking at the function \n",
    "\n",
    "$$\n",
    "L^2(\\theta) := \\|y - \\Phi \\theta\\|^2 = (y - \\Phi \\theta)^\\top (y - \\Phi \\theta). \\quad (5.84)\n",
    "$$ \n",
    "\n",
    "This approach is still practical for simple functions like $ L^2 $ but becomes impractical for deep function compositions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e81e8c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least-squares loss: 27.5\n",
      "Gradient of the least-squares loss: [18.0, 56.0]\n"
     ]
    }
   ],
   "source": [
    "def least_squares_loss(y, Phi, theta):\n",
    "    \"\"\"Compute the least-squares loss L(e) = ||e||^2.\"\"\"\n",
    "    e = subtract_lists(y, dot_product(Phi, theta))  # Compute the residuals\n",
    "    return sum(x**2 for x in e)  # L(e) = ||e||^2\n",
    "\n",
    "def gradient_least_squares_loss(y, Phi, theta):\n",
    "    \"\"\"Compute the gradient of the least-squares loss with respect to theta.\"\"\"\n",
    "    e = subtract_lists(y, dot_product(Phi, theta))  # Compute the residuals\n",
    "    grad = [-2 * sum(e[i] * Phi[i][j] for i in range(len(e))) for j in range(len(theta))]\n",
    "    return grad\n",
    "\n",
    "def dot_product(A, B):\n",
    "    \"\"\"Compute the dot product of matrix A and vector B.\"\"\"\n",
    "    return [sum(A[i][j] * B[j] for j in range(len(B))) for i in range(len(A))]\n",
    "\n",
    "def subtract_lists(a, b):\n",
    "    \"\"\"Subtract two lists element-wise.\"\"\"\n",
    "    return [a[i] - b[i] for i in range(len(a))]\n",
    "\n",
    "def main():\n",
    "    # Example input\n",
    "    y = [1.0, 2.0, 3.0]  # Observations\n",
    "    Phi = [               # Input features\n",
    "        [1.0, 2.0],\n",
    "        [1.0, 3.0],\n",
    "        [1.0, 4.0]\n",
    "    ]\n",
    "    theta = [0.5, 1.5]  # Parameter vector\n",
    "\n",
    "    # Compute the least-squares loss\n",
    "    loss = least_squares_loss(y, Phi, theta)\n",
    "    print(f\"Least-squares loss: {loss}\")\n",
    "\n",
    "    # Compute the gradient\n",
    "    gradient = gradient_least_squares_loss(y, Phi, theta)\n",
    "    print(f\"Gradient of the least-squares loss: {gradient}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "08e0f5ba",
   "metadata": {},
   "source": [
    "### Gradient Computation of a Matrix with Respect to a Vector\n",
    "\n",
    "Let $ A \\in \\mathbb{R}^{4 \\times 2} $ and $ x \\in \\mathbb{R}^{3} $. We want to compute the gradient of $ A $ with respect to $ x $:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial A}{\\partial x} \\in \\mathbb{R}^{4 \\times 2 \\times 3}\n",
    "$$\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "Fig.7 Visualization of gradient computation of a matrix with respect to a vector. We are interested in computing the gradient of A ∈ R4×2 with respect to a vector x ∈ R3 . We know that gradient dA dx ∈ R4×2×3 . We follow two equivalent approaches to arrive there: (a) collating partial derivatives into a Jacobian tensor; (b) flattening of the matrix into a vector, computing the Jacobian matrix, re-shaping into a Jacobian tensor.\n",
    "\n",
    "## Approach 1: Collating Partial Derivatives into a Jacobian Tensor\n",
    "We compute the partial derivatives:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial A}{\\partial x_1}, \\quad \\frac{\\partial A}{\\partial x_2}, \\quad \\frac{\\partial A}{\\partial x_3}\n",
    "$$\n",
    "\n",
    "Each of these is a $ 4 \\times 2 $ matrix, and we collate them into a $ 4 \\times 2 \\times 3 $ tensor.\n",
    "\n",
    "## Approach 2: Flattening and Computing the Jacobian Matrix\n",
    "We reshape $ A \\in \\mathbb{R}^{4 \\times 2} $ into a vector $ Ã \\in \\mathbb{R}^{8} $. Then, we compute the gradient:\n",
    "\n",
    "$$\n",
    "\\frac{d Ã}{dx} \\in \\mathbb{R}^{8 \\times 3}\n",
    "$$\n",
    "\n",
    "The gradient tensor is obtained by reshaping this gradient into the original dimensions.\n",
    "\n",
    "## Gradients of Matrices\n",
    "In many situations, we need to compute gradients of matrices with respect to vectors or other matrices, resulting in a multidimensional tensor. This tensor can be thought of as a multidimensional array.\n",
    "\n",
    "# Gradient Computation of Matrices\n",
    "\n",
    "## Collecting Partial Derivatives\n",
    "For example, if we compute the gradient of an $ m \\times n $ matrix $ A $ with respect to a $ p \\times q $ matrix $ B $, the resulting Jacobian would be $ (m \\times n) \\times (p \\times q) $, i.e., a four-dimensional tensor $ J $, whose entries are given as:\n",
    "\n",
    "$$\n",
    "J_{ijkl} = \\frac{\\partial A_{ij}}{\\partial B_{kl}}\n",
    "$$\n",
    "\n",
    "Since matrices represent linear mappings, we can exploit the fact that there is a vector-space isomorphism (linear, invertible mapping) between the space $ \\mathbb{R}^{m \\times n} $ of $ m \\times n $ matrices and the space $ \\mathbb{R}^{mn} $ of $ mn $ vectors. Therefore, we can re-shape our matrices into vectors of lengths $ mn $ and $pq $, respectively.\n",
    "\n",
    "The gradient using these $ mn $ vectors results in a Jacobian of size $ mn \\times pq $.\n",
    "\n",
    "Figure 5.7 visualizes both approaches.\n",
    "\n",
    "## Practical Applications of Reshaping\n",
    "In practical applications, it is often desirable to reshape the matrix into a vector and continue working with this Jacobian matrix. The chain rule boils down to simple matrix multiplication, whereas in the case of a Jacobian tensor, we will need to pay more attention to what dimensions we need to sum out.\n",
    "\n",
    "## Example: Gradient of Vectors with Respect to Matrices\n",
    "\n",
    "Let us consider the following example:\n",
    "\n",
    "$$\n",
    "f = A x, \\quad f \\in \\mathbb{R}^{M}, \\quad A \\in \\mathbb{R}^{M \\times N}, \\quad x \\in \\mathbb{R}^{N}\n",
    "$$\n",
    "\n",
    "where we seek the gradient $ \\frac{df}{dA} $. \n",
    "\n",
    "### Determining the Dimension of the Gradient\n",
    "By definition, the gradient is the collection of the partial derivatives:\n",
    "\n",
    "$$\n",
    "\\frac{df}{dA} \\in \\mathbb{R}^{M \\times (M \\times N)}\n",
    "$$\n",
    "\n",
    "Written explicitly:\n",
    "\n",
    "$$\n",
    "\\frac{df}{dA} =\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial f_1}{\\partial A} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial f_M}{\\partial A}\n",
    "\\end{bmatrix}\n",
    "\\in \\mathbb{R}^{1 \\times (M \\times N)}\n",
    "$$\n",
    "\n",
    "### Computing Partial Derivatives\n",
    "To compute the partial derivatives, it will be helpful to explicitly write out the matrix-vector multiplication:\n",
    "\n",
    "$$\n",
    "f_i = \\sum_{j=1}^{N} A_{ij} x_j, \\quad i = 1, \\dots, M\n",
    "$$\n",
    "\n",
    "The partial derivatives are then given as:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f_i}{\\partial A_{iq}} = x_q\n",
    "$$\n",
    "\n",
    "This allows us to compute the partial derivatives of $ f_i $ with respect to a row of $ A $, which is given as:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f_i}{\\partial A_{i,:}} = x^\\top \\in \\mathbb{R}^{1 \\times 1 \\times N}\n",
    "$$\n",
    "\n",
    "# Useful Identities for Computing Gradients\n",
    "\n",
    "We define the partial derivative tensor entries as follows:\n",
    "\n",
    "$$\n",
    "\\partial_{pqij} =\n",
    "\\begin{cases}\n",
    "R_{iq}, & \\text{if } j = p, p \\neq q \\\\\n",
    "R_{ip}, & \\text{if } j = q, p \\neq q \\\\\n",
    "2R_{iq}, & \\text{if } j = p, p = q \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "From Equation (5.94), the desired gradient has the dimension \\( (N \\times N) \\times (M \\times N) \\), and every single entry of this tensor is given by \\( \\partial_{pqij} \\) above, where \\( p, q, j = 1, \\dots, N \\) and \\( i = 1, \\dots, M \\).\n",
    "\n",
    "## Common Gradient Identities in Machine Learning\n",
    "\n",
    "We list some useful gradients frequently required in a machine learning context (Petersen and Pedersen, 2012):\n",
    "\n",
    "1. **Transpose of a Function's Gradient**:\n",
    "   $$\n",
    "   \\frac{\\partial}{\\partial X} f(X)^\\top = \\left( \\frac{\\partial f(X)}{\\partial X} \\right)^\\top\n",
    "   $$\n",
    "\n",
    "2. **Trace Gradient**:\n",
    "   $$\n",
    "   \\frac{\\partial}{\\partial X} \\text{tr}(f(X)) = \\text{tr} \\left( \\frac{\\partial f(X)}{\\partial X} \\right)\n",
    "   $$\n",
    "\n",
    "3. **Determinant Gradient**:\n",
    "   $$\n",
    "   \\frac{\\partial}{\\partial X} \\det(f(X)) = \\det(f(X)) \\text{tr} \\left( f(X)^{-1} \\frac{\\partial f(X)}{\\partial X} \\right)\n",
    "   $$\n",
    "\n",
    "4. **Inverse Function Gradient**:\n",
    "   $$\n",
    "   \\frac{\\partial}{\\partial X} f(X)^{-1} = -f(X)^{-1} \\frac{\\partial f(X)}{\\partial X} f(X)^{-1}\n",
    "   $$\n",
    "\n",
    "5. **Gradient of Quadratic Forms**:\n",
    "   $$\n",
    "   \\frac{\\partial a^\\top X^{-1} b}{\\partial X} = -(X^{-1})^\\top a b^\\top (X^{-1})^\\top\n",
    "   $$\n",
    "\n",
    "6. **Vector Gradients**:\n",
    "   $$\n",
    "   \\frac{\\partial x^\\top a}{\\partial x} = a^\\top\n",
    "   $$\n",
    "\n",
    "   $$\n",
    "   \\frac{\\partial a^\\top x}{\\partial x} = a^\\top\n",
    "   $$\n",
    "\n",
    "7. **Matrix Gradients**:\n",
    "   $$\n",
    "   \\frac{\\partial a^\\top Xb}{\\partial X} = a b^\\top\n",
    "   $$\n",
    "\n",
    "   $$\n",
    "   \\frac{\\partial x^\\top Bx}{\\partial x} = x^\\top (B + B^\\top)\n",
    "   $$\n",
    "\n",
    "8. **Gradient of Quadratic Form with Weight Matrix**:\n",
    "   $$\n",
    "   \\frac{\\partial (x - As)^\\top W (x - As)}{\\partial s} = -2(x - As)^\\top W A \\quad \\text{for symmetric } W\n",
    "   $$\n",
    "\n",
    "## Remark on Higher-Dimensional Tensors\n",
    "\n",
    "While matrices allow us to compute traces and transposes, derivatives can extend to higher-dimensional tensors where traditional definitions do not apply. For example, the **trace** of a tensor of shape $ D \\times D \\times E \\times F $ results in an $ E \\times F $ matrix. This is a special case of a **tensor contraction**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d66e0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient ∂A[0,0]/∂B:\n",
      "[18, 10, 11]\n",
      "[12, 0, 0]\n",
      "\n",
      "Gradient ∂A[0,1]/∂B:\n",
      "[0, 10, 0]\n",
      "[12, 26, 14]\n",
      "\n",
      "Gradient ∂A[1,0]/∂B:\n",
      "[18, 10, 11]\n",
      "[12, 0, 0]\n",
      "\n",
      "Gradient ∂A[1,1]/∂B:\n",
      "[0, 10, 0]\n",
      "[12, 26, 14]\n",
      "\n",
      "Gradient ∂A[2,0]/∂B:\n",
      "[18, 10, 11]\n",
      "[12, 0, 0]\n",
      "\n",
      "Gradient ∂A[2,1]/∂B:\n",
      "[0, 10, 0]\n",
      "[12, 26, 14]\n",
      "\n",
      "Gradient ∂A[3,0]/∂B:\n",
      "[18, 10, 11]\n",
      "[12, 0, 0]\n",
      "\n",
      "Gradient ∂A[3,1]/∂B:\n",
      "[0, 10, 0]\n",
      "[12, 26, 14]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to compute the gradient of matrix A w.r.t. matrix B\n",
    "def compute_gradient(A, B):\n",
    "    \"\"\" Computes the gradient ∂A / ∂B without using any libraries \"\"\"\n",
    "    m, n = len(A), len(A[0])   # A is m × n\n",
    "    p, q = len(B), len(B[0])   # B is p × q\n",
    "\n",
    "    # Initialize a 4D tensor for storing gradients\n",
    "    J = [[[[0 for _ in range(q)] for _ in range(p)] for _ in range(n)] for _ in range(m)]\n",
    "\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            for p_idx in range(p):\n",
    "                for q_idx in range(q):\n",
    "                    # Example derivative logic (user should replace with actual derivative formulas)\n",
    "                    if j == p_idx and p_idx != q_idx:\n",
    "                        J[i][j][p_idx][q_idx] = B[p_idx][q_idx]\n",
    "                    elif j == q_idx and p_idx != q_idx:\n",
    "                        J[i][j][p_idx][q_idx] = B[p_idx][j]\n",
    "                    elif j == p_idx and p_idx == q_idx:\n",
    "                        J[i][j][p_idx][q_idx] = 2 * B[p_idx][q_idx]\n",
    "                    else:\n",
    "                        J[i][j][p_idx][q_idx] = 0\n",
    "\n",
    "    return J\n",
    "\n",
    "# Example matrices A and B\n",
    "A = [[1, 2], [3, 4], [5, 6], [7, 8]]  # A is 4 × 2\n",
    "B = [[9, 10, 11], [12, 13, 14]]  # B is 2 × 3\n",
    "\n",
    "# Compute gradient\n",
    "gradient_tensor = compute_gradient(A, B)\n",
    "\n",
    "# Display result\n",
    "for i in range(len(gradient_tensor)):\n",
    "    for j in range(len(gradient_tensor[0])):\n",
    "        print(f\"Gradient ∂A[{i},{j}]/∂B:\")\n",
    "        for row in gradient_tensor[i][j]:\n",
    "            print(row)\n",
    "        print()\n"
   ]
  },
  {
   "attachments": {
    "image-2.png": {
     "image/png": "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"
    },
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "b5aaf544",
   "metadata": {},
   "source": [
    "## Transposing a Tensor\n",
    "\n",
    "In tensor computations, the term **\"transpose\"** refers to swapping the **first two dimensions** of a multi-dimensional array. Specifically, in equations (5.99) through (5.102), we require tensor-related operations when working with multivariate functions $ f(\\cdot) $ and computing derivatives with respect to matrices, particularly when avoiding vectorization (as discussed in Section 5.4).\n",
    "\n",
    "---\n",
    "\n",
    "## Backpropagation and Automatic Differentiation\n",
    "\n",
    "In many machine learning applications, we determine optimal **model parameters** by performing **gradient descent** (see Section 7.1). This process relies on computing the **gradient** of the learning objective with respect to the parameters of the model.\n",
    "\n",
    "For a given **objective function**, we obtain the gradient using **calculus** and the **chain rule** (Section 5.2.2). We've already seen an example in Section 5.3 when analyzing the gradient of a squared loss function in **linear regression**.\n",
    "\n",
    "---\n",
    "\n",
    "## Example Function for Gradient Calculation\n",
    "\n",
    "Consider the function:\n",
    "\n",
    "$$\n",
    "f(x) = x^2 + \\exp(x^2) + \\cos(x^2) + \\exp(x^2)\n",
    "$$\n",
    "\n",
    "Using **the chain rule**, and noting that differentiation is linear, we compute:\n",
    "\n",
    "$$\n",
    "\\frac{df}{dx} = 2x + 2x \\exp(x^2) - \\sin(x^2) + \\exp(x^2) + 2x + 2x \\exp(x^2)\n",
    "$$\n",
    "\n",
    "Rearranging:\n",
    "\n",
    "$$\n",
    "\\frac{df}{dx} = 2x \\left( 1 - \\sin(x^2) + \\exp(x^2) \\right) \\left( 1 + \\exp(x^2) \\right)\n",
    "$$\n",
    "\n",
    "In practice, computing such a gradient explicitly can be **impractical**, as it often results in **lengthy expressions**. If not optimized properly, the gradient calculation can become **more computationally expensive** than evaluating the original function, leading to unnecessary overhead.\n",
    "\n",
    "---\n",
    "\n",
    "## Backpropagation Algorithm\n",
    "\n",
    "For deep neural networks, the **backpropagation algorithm** (Kelley, 1960; Bryson, 1961; Dreyfus, 1962; Rumelhart et al., 1986) provides an efficient way to compute the **gradient of an error function** with respect to model parameters.\n",
    "\n",
    "## Gradients in Deep Networks\n",
    "\n",
    "One area where the **chain rule** is extensively used is **deep learning**, where a function $ y $ is computed using multiple function compositions:\n",
    "\n",
    "$$\n",
    "y = (f_K \\circ f_{K-1} \\circ \\cdots \\circ f_1)(x) = f_K(f_{K-1}(\\cdots(f_1(x))\\cdots))\n",
    "$$\n",
    "\n",
    "Here:\n",
    "- $ x $ represents **input data** (e.g., images).\n",
    "- $ y $ represents **output observations** (e.g., class labels).\n",
    "- Each function $ f_i $ has its own **parameters**.\n",
    "\n",
    "Backpropagation efficiently computes **gradients** layer-by-layer using the **chain rule**, allowing neural networks to learn effectively.\n",
    "\n",
    "# Forward Pass in a Multi-Layer Neural Network\n",
    "\n",
    "In neural networks with multiple layers, we define the layer function as:\n",
    "\n",
    "$$\n",
    "f_i(x_{i-1}) = \\sigma(A_{i-1} x_{i-1} + b_{i-1})\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ x_{i-1} $ is the output from layer $ i-1 $,\n",
    "- $ \\sigma $ is an activation function (e.g., **logistic sigmoid**, **tanh**, or **ReLU**).\n",
    "\n",
    "To train these models, we require the gradient of a loss function $ L $ with respect to the model parameters $ A_j, b_j $, for $ j = 1, \\dots, K $. We also compute the gradient of $ L $ with respect to the inputs of each layer.\n",
    "\n",
    "## Network Structure\n",
    "\n",
    "Given inputs $ x $ and observations $ y $, the network structure is defined as:\n",
    "\n",
    "$$\n",
    "f^0 := x\n",
    "$$\n",
    "\n",
    "$$\n",
    "f^i := \\sigma_i(A_{i-1} f^{i-1} + b_{i-1}), \\quad i = 1, \\dots, K\n",
    "$$\n",
    "\n",
    "We aim to optimize the parameters $ A_j, b_j $ such that the squared loss function is minimized:\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\| y - f^K(\\theta, x) \\|^2\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\n",
    "\\theta = \\{ A_0, b_0, \\dots, A_{K-1}, b_{K-1} \\}\n",
    "$$\n",
    "\n",
    "---\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "![image-2.png](attachment:image-2.png)\n",
    "\n",
    "## Computing Gradients Using the Chain Rule\n",
    "\n",
    "The chain rule allows us to compute the partial derivatives of $ L $ with respect to the parameters $ \\theta_j = \\{A_j, b_j\\} $:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\theta_{K-1}} = \\frac{\\partial L}{\\partial f^K} \\frac{\\partial f^K}{\\partial \\theta_{K-1}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\theta_{K-2}} = \\frac{\\partial L}{\\partial f^K} \\frac{\\partial f^K}{\\partial f^{K-1}} \\frac{\\partial f^{K-1}}{\\partial \\theta_{K-2}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\theta_{K-3}} = \\frac{\\partial L}{\\partial f^K} \\frac{\\partial f^K}{\\partial f^{K-1}} \\frac{\\partial f^{K-1}}{\\partial f^{K-2}} \\frac{\\partial f^{K-2}}{\\partial \\theta_{K-3}}\n",
    "$$\n",
    "\n",
    "More generally:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\theta_i} = \\frac{\\partial L}{\\partial f^K} \\frac{\\partial f^K}{\\partial f^{K-1}} \\cdots \\frac{\\partial f^{i+1}}{\\partial \\theta_i}\n",
    "$$\n",
    "\n",
    "### Understanding the Terms:\n",
    "- **Orange terms**: Partial derivatives of a layer output with respect to its inputs.\n",
    "- **Blue terms**: Partial derivatives of a layer output with respect to its parameters.\n",
    "\n",
    "If $ \\frac{\\partial L}{\\partial \\theta_{i+1}} $ has already been computed, we can **reuse most computations** to efficiently obtain $ \\frac{\\partial L}{\\partial \\theta_i} $, reducing unnecessary overhead.\n",
    "\n",
    "---\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "For a more **in-depth discussion** on neural network gradients and backpropagation, refer to:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae3fe165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output before training: [0.7889176625306139]\n",
      "Output after training: [0.995164913282539]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Define activation functions and their derivatives\n",
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid activation function\"\"\"\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    \"\"\"Derivative of the Sigmoid function\"\"\"\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Define a simple Neural Network class\n",
    "class SimpleNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        \"\"\"Initialize network weights and biases manually\"\"\"\n",
    "        # Weights between input and hidden layer\n",
    "        self.W1 = [[0.5, 0.2], [0.3, 0.7]]  # Example values (2x2 matrix)\n",
    "        self.b1 = [0.1, 0.2]\n",
    "\n",
    "        # Weights between hidden and output layer\n",
    "        self.W2 = [[0.6, 0.9]]  # Example values (1x2 matrix)\n",
    "        self.b2 = [0.3]\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass computation\"\"\"\n",
    "        # Input to hidden layer\n",
    "        self.z1 = [sum(x[i] * self.W1[i][j] for i in range(len(x))) + self.b1[j] for j in range(len(self.b1))]\n",
    "        self.a1 = [sigmoid(v) for v in self.z1]  # Apply activation function\n",
    "        \n",
    "        # Hidden to output layer\n",
    "        self.z2 = [sum(self.a1[i] * self.W2[0][i] for i in range(len(self.a1))) + self.b2[0]]\n",
    "        self.a2 = [sigmoid(v) for v in self.z2]  # Output activation\n",
    "        \n",
    "        return self.a2\n",
    "\n",
    "    def backward(self, x, y, learning_rate=0.1):\n",
    "        \"\"\"Backward pass using manual backpropagation\"\"\"\n",
    "        # Compute loss gradient at output layer\n",
    "        error_output = [(self.a2[i] - y[i]) for i in range(len(y))]\n",
    "        d_output = [error_output[i] * sigmoid_derivative(self.a2[i]) for i in range(len(y))]\n",
    "\n",
    "        # Compute loss gradient at hidden layer\n",
    "        error_hidden = [sum(d_output[i] * self.W2[i][j] for i in range(len(d_output))) for j in range(len(self.a1))]\n",
    "        d_hidden = [error_hidden[j] * sigmoid_derivative(self.a1[j]) for j in range(len(self.a1))]\n",
    "\n",
    "        # Update weights and biases (Gradient Descent)\n",
    "        for i in range(len(self.W2[0])):\n",
    "            self.W2[0][i] -= learning_rate * d_output[0] * self.a1[i]\n",
    "        self.b2[0] -= learning_rate * d_output[0]\n",
    "\n",
    "        for i in range(len(self.W1)):\n",
    "            for j in range(len(self.W1[i])):\n",
    "                self.W1[i][j] -= learning_rate * d_hidden[j] * x[i]\n",
    "            self.b1[i] -= learning_rate * d_hidden[i]\n",
    "\n",
    "# Example usage:\n",
    "nn = SimpleNN(input_size=2, hidden_size=2, output_size=1)\n",
    "\n",
    "# Sample input and expected output\n",
    "x = [0.5, 0.8]\n",
    "y = [1]\n",
    "\n",
    "# Forward pass\n",
    "output = nn.forward(x)\n",
    "print(\"Output before training:\", output)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1000):\n",
    "    nn.backward(x, y, learning_rate=0.05)\n",
    "\n",
    "# Forward pass after training\n",
    "output = nn.forward(x)\n",
    "print(\"Output after training:\", output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7de7518",
   "metadata": {},
   "source": [
    "# Automatic Differentiation in Machine Learning\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Automatic Differentiation (AD) is a technique used in numerical analysis to compute **exact gradients** of functions using intermediate variables and the **chain rule**. Unlike symbolic differentiation, which explicitly derives equations, AD **evaluates gradients numerically**, allowing efficient computations up to machine precision.\n",
    "\n",
    "AD can be classified into:\n",
    "- **Forward Mode AD**: Computes derivatives as data flows **from inputs to outputs**.\n",
    "- **Reverse Mode AD**: Computes derivatives **backward** (propagating gradients **from outputs to inputs**). This is the foundation of **backpropagation** in neural networks.\n",
    "\n",
    "---\n",
    "\n",
    "## Computing Gradients Using AD\n",
    "\n",
    "### Example Function:\n",
    "\n",
    "$$\n",
    "f(x) = x^2 + \\exp(x^2) + \\cos(x^2) + \\exp(x^2)\n",
    "$$\n",
    "\n",
    "Using intermediate variables:\n",
    "\n",
    "$$\n",
    "a = x^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "b = \\exp(a)\n",
    "$$\n",
    "\n",
    "$$\n",
    "c = a + b\n",
    "$$\n",
    "\n",
    "$$\n",
    "d = \\sqrt{c}\n",
    "$$\n",
    "\n",
    "$$\n",
    "e = \\cos(c)\n",
    "$$\n",
    "\n",
    "$$\n",
    "f = d + e\n",
    "$$\n",
    "\n",
    "This structured computation minimizes redundant calculations and mirrors **computation graphs** used in deep learning frameworks.\n",
    "\n",
    "---\n",
    "\n",
    "## Derivatives Using Chain Rule\n",
    "\n",
    "Applying the **chain rule**, we compute:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial a}{\\partial x} = 2x\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial b}{\\partial a} = \\exp(a)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial c}{\\partial a} = 1, \\quad \\frac{\\partial c}{\\partial b} = 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial d}{\\partial c} = \\frac{1}{2\\sqrt{c}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial e}{\\partial c} = -\\sin(c)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial d} = 1, \\quad \\frac{\\partial f}{\\partial e} = 1\n",
    "$$\n",
    "\n",
    "Using **reverse mode AD**, we compute:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x} =\n",
    "\\frac{\\partial f}{\\partial d} \\frac{\\partial d}{\\partial c} \\frac{\\partial c}{\\partial a} \\frac{\\partial a}{\\partial x} +\n",
    "\\frac{\\partial f}{\\partial e} \\frac{\\partial e}{\\partial c} \\frac{\\partial c}{\\partial a} \\frac{\\partial a}{\\partial x}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Why Reverse Mode AD in Deep Learning?\n",
    "\n",
    "In **neural networks**, the input dimensionality is typically much higher than the output dimensionality. Reverse mode AD efficiently computes **gradients layer by layer**, reducing computational cost compared to forward mode AD.\n",
    "\n",
    "---\n",
    "# Automatic Differentiation in Machine Learning\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Automatic Differentiation (AD) is a technique used in numerical analysis to compute **exact gradients** of functions using intermediate variables and the **chain rule**. Unlike symbolic differentiation, which explicitly derives equations, AD **evaluates gradients numerically**, allowing efficient computations up to machine precision.\n",
    "\n",
    "AD can be classified into:\n",
    "- **Forward Mode AD**: Computes derivatives as data flows **from inputs to outputs**.\n",
    "- **Reverse Mode AD**: Computes derivatives **backward** (propagating gradients **from outputs to inputs**). This is the foundation of **backpropagation** in neural networks.\n",
    "\n",
    "---\n",
    "\n",
    "## Computing Gradients Using AD\n",
    "\n",
    "### Example Function:\n",
    "\n",
    "$$\n",
    "f(x) = x^2 + \\exp(x^2) + \\cos(x^2) + \\exp(x^2)\n",
    "$$\n",
    "\n",
    "Using intermediate variables:\n",
    "\n",
    "$$\n",
    "a = x^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "b = \\exp(a)\n",
    "$$\n",
    "\n",
    "$$\n",
    "c = a + b\n",
    "$$\n",
    "\n",
    "$$\n",
    "d = \\sqrt{c}\n",
    "$$\n",
    "\n",
    "$$\n",
    "e = \\cos(c)\n",
    "$$\n",
    "\n",
    "$$\n",
    "f = d + e\n",
    "$$\n",
    "\n",
    "This structured computation minimizes redundant calculations and mirrors **computation graphs** used in deep learning frameworks.\n",
    "\n",
    "---\n",
    "\n",
    "## Derivatives Using Chain Rule\n",
    "\n",
    "Applying the **chain rule**, we compute:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial a}{\\partial x} = 2x\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial b}{\\partial a} = \\exp(a)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial c}{\\partial a} = 1, \\quad \\frac{\\partial c}{\\partial b} = 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial d}{\\partial c} = \\frac{1}{2\\sqrt{c}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial e}{\\partial c} = -\\sin(c)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial d} = 1, \\quad \\frac{\\partial f}{\\partial e} = 1\n",
    "$$\n",
    "\n",
    "Using **reverse mode AD**, we compute:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x} =\n",
    "\\frac{\\partial f}{\\partial d} \\frac{\\partial d}{\\partial c} \\frac{\\partial c}{\\partial a} \\frac{\\partial a}{\\partial x} +\n",
    "\\frac{\\partial f}{\\partial e} \\frac{\\partial e}{\\partial c} \\frac{\\partial c}{\\partial a} \\frac{\\partial a}{\\partial x}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Why Reverse Mode AD in Deep Learning?\n",
    "\n",
    "In **neural networks**, the input dimensionality is typically much higher than the output dimensionality. Reverse mode AD efficiently computes **gradients layer by layer**, reducing computational cost compared to forward mode AD.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96038755",
   "metadata": {},
   "source": [
    "## Automatic Differentiation and Higher-Order Derivatives\n",
    "\n",
    "### Computation Graphs and Chain Rule\n",
    "\n",
    "Given a function expressed as a **computation graph**, we define:\n",
    "\n",
    "- $ g_i(\\cdot) $ as elementary functions.\n",
    "- $ x_{\\text{Pa}(x_i)} $ as the parent nodes of the variable $ x_i $ in the graph.\n",
    "\n",
    "Using the **chain rule**, we compute derivatives **step-by-step**.\n",
    "\n",
    "### Forward Propagation:\n",
    "\n",
    "$$\n",
    "x_i = g_i(x_{\\text{Pa}(x_i)}), \\quad i = d+1, \\dots, D\n",
    "$$\n",
    "\n",
    "### Backpropagation (Gradient Computation):\n",
    "\n",
    "By definition:\n",
    "\n",
    "$$\n",
    "f = x_D \\quad \\Rightarrow \\quad \\frac{\\partial f}{\\partial x_D} = 1\n",
    "$$\n",
    "\n",
    "For other variables $ x_i $, we apply the **chain rule**:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x_i} = \\sum_{j: x_i \\in \\text{Pa}(x_j)} \\frac{\\partial f}{\\partial x_j} \\frac{\\partial x_j}{\\partial x_i}\n",
    "$$\n",
    "\n",
    "where $ \\text{Pa}(x_j) $ represents the **parent nodes** of $ x_j $ in the computation graph.\n",
    "\n",
    "Automatic differentiation efficiently computes gradients when functions are expressed as computation graphs with **differentiable elementary functions**.\n",
    "\n",
    "---\n",
    "\n",
    "# Higher-Order Derivatives\n",
    "\n",
    "### **Notation for Higher-Order Partial Derivatives**\n",
    "Consider a function $ f: \\mathbb{R}^2 \\to \\mathbb{R} $ with variables $ x $ and $ y $:\n",
    "\n",
    "- $ \\frac{\\partial^2 f}{\\partial x^2} $: Second-order derivative of $ f $ with respect to $ x $.\n",
    "- $ \\frac{\\partial^n f}{\\partial x^n} $: $ n $-th order derivative of $ f $ with respect to $ x $.\n",
    "- $ \\frac{\\partial^2 f}{\\partial y \\partial x} $: First differentiating by $ x $, then by $ y $.\n",
    "- $ \\frac{\\partial^2 f}{\\partial x \\partial y} $: First differentiating by $ y $, then by $ x $.\n",
    "\n",
    "### **The Hessian Matrix**\n",
    "The Hessian is the matrix containing **all second-order partial derivatives**, defined as:\n",
    "\n",
    "$$\n",
    "H(f) =\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial^2 f}{\\partial x^2} & \\frac{\\partial^2 f}{\\partial x \\partial y} \\\\\n",
    "\\frac{\\partial^2 f}{\\partial y \\partial x} & \\frac{\\partial^2 f}{\\partial y^2}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The Hessian plays a critical role in **optimization algorithms**, such as **Newton's Method**, which requires second-order derivatives for curvature analysis.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Automatic Differentiation Matters?**\n",
    "Automatic differentiation enables efficient computation of **both first-order gradients and second-order Hessians**, allowing rapid optimization in machine learning applications.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f80ec8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hessian Matrix:\n",
      "[2.0000001654807416, 0.9999978622943216]\n",
      "[0.9999978622943216, 2.0000001654807416]\n"
     ]
    }
   ],
   "source": [
    "# Function to compute the derivative of f(x, y) using finite differences\n",
    "def derivative(f, x, y, var='x', h=1e-5):\n",
    "    \"\"\" Compute first derivative using central finite difference method \"\"\"\n",
    "    if var == 'x':\n",
    "        return (f(x + h, y) - f(x - h, y)) / (2 * h)\n",
    "    elif var == 'y':\n",
    "        return (f(x, y + h) - f(x, y - h)) / (2 * h)\n",
    "\n",
    "# Function to compute the Hessian matrix\n",
    "def hessian(f, x, y, h=1e-5):\n",
    "    \"\"\" Compute Hessian matrix using finite differences \"\"\"\n",
    "    dxx = (f(x + h, y) - 2 * f(x, y) + f(x - h, y)) / (h ** 2)\n",
    "    dyy = (f(x, y + h) - 2 * f(x, y) + f(x, y - h)) / (h ** 2)\n",
    "    dxy = (f(x + h, y + h) - f(x + h, y - h) - f(x - h, y + h) + f(x - h, y - h)) / (4 * h ** 2)\n",
    "\n",
    "    return [[dxx, dxy], [dxy, dyy]]\n",
    "\n",
    "# Example function: f(x, y) = x**2 + y**2 + xy\n",
    "def example_function(x, y):\n",
    "    return x**2 + y**2 + x*y\n",
    "\n",
    "# Compute Hessian at (1, 2)\n",
    "hessian_matrix = hessian(example_function, 1, 2)\n",
    "\n",
    "# Display result\n",
    "print(\"Hessian Matrix:\")\n",
    "for row in hessian_matrix:\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca59eec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
