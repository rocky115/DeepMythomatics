{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c0b8145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.4681524318815702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/radha/anaconda3/envs/cv37/lib/python3.7/site-packages/ipykernel_launcher.py:5: RuntimeWarning: overflow encountered in exp\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Loss: 0.18758802808359074\n",
      "Epoch 200, Loss: 0.17770087723250533\n",
      "Epoch 300, Loss: 0.17636825992241711\n",
      "Epoch 400, Loss: 0.15753630605875954\n",
      "Epoch 500, Loss: 0.14626931173766103\n",
      "Epoch 600, Loss: 0.13753374779123764\n",
      "Epoch 700, Loss: 0.13749546064546003\n",
      "Epoch 800, Loss: 0.13746613754177991\n",
      "Epoch 900, Loss: 0.13742716125599438\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (64,100) and (12,1) not aligned: 100 (dim 1) != 12 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6970/3317289092.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;31m# Predict Odia text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m \u001b[0mpredicted_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msanskrit_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0modia_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minv_odia_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Predicted Odia Text:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_6970/3317289092.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(rnn, text, src_vocab, tgt_vocab, inv_tgt_vocab)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0mx_encoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_hot_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_encoded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0my_pred_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_6970/3317289092.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mxt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                 \u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (64,100) and (12,1) not aligned: 100 (dim 1) != 12 (dim 0)"
     ]
    }
   ],
   "source": [
    "######################################################################################\n",
    "'''Copyright (c) 2004- 2024 , Prof. Radhamadhab Dalai Odisha, India\n",
    "Author's email address :  rmdi115@gmail.com'''\n",
    "###################################################################################\n",
    "import numpy as np\n",
    "\n",
    "# Activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Derivative of activation function\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "class RNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Weight matrices\n",
    "        self.U = np.random.randn(hidden_size, input_size)\n",
    "        self.W = np.random.randn(hidden_size, hidden_size)\n",
    "        self.V = np.random.randn(output_size, hidden_size)\n",
    "        \n",
    "        # Biases\n",
    "        self.b_hidden = np.zeros((hidden_size, 1))\n",
    "        self.b_output = np.zeros((output_size, 1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        T = x.shape[1]\n",
    "        h = np.zeros((self.hidden_size, T))\n",
    "        y = np.zeros((self.output_size, T))\n",
    "        \n",
    "        for t in range(T):\n",
    "            xt = x[:, t].reshape(-1, 1)\n",
    "            if t == 0:\n",
    "                h[:, t] = sigmoid(np.dot(self.U, xt) + self.b_hidden).flatten()\n",
    "            else:\n",
    "                h[:, t] = sigmoid(np.dot(self.U, xt) + np.dot(self.W, h[:, t-1].reshape(-1, 1)) + self.b_hidden).flatten()\n",
    "            \n",
    "            y[:, t] = sigmoid(np.dot(self.V, h[:, t].reshape(-1, 1)) + self.b_output).flatten()\n",
    "        \n",
    "        return h, y\n",
    "    \n",
    "    def backward(self, x, y_true, h, y_pred, lr):\n",
    "        T = x.shape[1]\n",
    "        dU = np.zeros_like(self.U)\n",
    "        dW = np.zeros_like(self.W)\n",
    "        dV = np.zeros_like(self.V)\n",
    "        db_hidden = np.zeros_like(self.b_hidden)\n",
    "        db_output = np.zeros_like(self.b_output)\n",
    "        \n",
    "        delta_hidden_next = np.zeros((self.hidden_size, 1))\n",
    "        \n",
    "        for t in range(T-1, -1, -1):\n",
    "            yt = y_true[:, t].reshape(-1, 1)\n",
    "            yp = y_pred[:, t].reshape(-1, 1)\n",
    "            delta_output = (yp - yt) * sigmoid_derivative(yp)\n",
    "            dV += np.dot(delta_output, h[:, t].reshape(1, -1))\n",
    "            db_output += delta_output\n",
    "            \n",
    "            delta_hidden = np.dot(self.V.T, delta_output) * sigmoid_derivative(h[:, t].reshape(-1, 1))\n",
    "            if t > 0:\n",
    "                delta_hidden += np.dot(self.W.T, delta_hidden_next)\n",
    "            \n",
    "            xt = x[:, t].reshape(-1, 1)\n",
    "            dU += np.dot(delta_hidden, xt.T)\n",
    "            db_hidden += delta_hidden\n",
    "            \n",
    "            if t > 0:\n",
    "                dW += np.dot(delta_hidden, h[:, t-1].reshape(1, -1))\n",
    "            \n",
    "            delta_hidden_next = delta_hidden\n",
    "        \n",
    "        # Update weights and biases\n",
    "        self.U -= lr * dU\n",
    "        self.W -= lr * dW\n",
    "        self.V -= lr * dV\n",
    "        self.b_hidden -= lr * db_hidden\n",
    "        self.b_output -= lr * db_output\n",
    "\n",
    "# Example usage\n",
    "input_size = 100  # Assuming input matrix size is 100xT\n",
    "hidden_size = 64\n",
    "output_size = 100\n",
    "\n",
    "# Create an RNN instance\n",
    "rnn = RNN(input_size, hidden_size, output_size)\n",
    "\n",
    "# Generate random input matrix (assuming it contains Sanskrit characters)\n",
    "x = np.random.randn(input_size, 10)  # Assuming 10 time steps\n",
    "\n",
    "# Forward pass\n",
    "h, y_pred = rnn.forward(x)\n",
    "\n",
    "# Backward pass (assuming y_pred is the target output)\n",
    "lr = 0.01\n",
    "rnn.backward(x, y_pred, h, y_pred, lr)\n",
    "\n",
    "# Character mappings (for illustration)\n",
    "sanskrit_to_odia = {\n",
    "    'अ': 'ଅ', 'आ': 'ଆ', 'इ': 'ଇ', 'ई': 'ଈ', 'उ': 'ଉ', \n",
    "    'ऊ': 'ଊ', 'ऋ': 'ଋ', 'ए': 'ଏ', 'ऐ': 'ଐ', 'ओ': 'ଓ', 'औ': 'ଔ'\n",
    "}\n",
    "\n",
    "# Convert characters to integer representation\n",
    "def char_to_int(char, vocab):\n",
    "    return vocab[char] if char in vocab else vocab['UNK']\n",
    "\n",
    "def int_to_char(index, inv_vocab):\n",
    "    return inv_vocab[index]\n",
    "\n",
    "# Build vocabularies\n",
    "sanskrit_chars = sorted(list(sanskrit_to_odia.keys()))\n",
    "odia_chars = sorted(list(set(sanskrit_to_odia.values())))\n",
    "\n",
    "sanskrit_vocab = {char: idx for idx, char in enumerate(sanskrit_chars)}\n",
    "odia_vocab = {char: idx for idx, char in enumerate(odia_chars)}\n",
    "\n",
    "# Add a special 'UNK' token for unknown characters\n",
    "sanskrit_vocab['UNK'] = len(sanskrit_vocab)\n",
    "odia_vocab['UNK'] = len(odia_vocab)\n",
    "\n",
    "inv_odia_vocab = {idx: char for char, idx in odia_vocab.items()}\n",
    "\n",
    "# Prepare data\n",
    "def prepare_data(text, mapping, src_vocab, tgt_vocab):\n",
    "    input_data = []\n",
    "    target_data = []\n",
    "    for char in text:\n",
    "        input_char = char_to_int(char, src_vocab)\n",
    "        input_data.append(input_char)\n",
    "        \n",
    "        if char in mapping:\n",
    "            target_char = mapping[char]\n",
    "        else:\n",
    "            target_char = 'UNK'\n",
    "        target_data.append(char_to_int(target_char, tgt_vocab))\n",
    "    \n",
    "    return np.array(input_data).reshape(-1, 1), np.array(target_data).reshape(-1, 1)\n",
    "\n",
    "# Sample input text in Sanskrit\n",
    "sanskrit_text = \"अआइईउऊऋएऐओऔ\"\n",
    "\n",
    "# Prepare training data\n",
    "input_data, target_data = prepare_data(sanskrit_text, sanskrit_to_odia, sanskrit_vocab, odia_vocab)\n",
    "\n",
    "# One-hot encoding for the input and target data\n",
    "def one_hot_encode(data, vocab_size):\n",
    "    one_hot = np.zeros((data.size, vocab_size))\n",
    "    one_hot[np.arange(data.size), data.flatten()] = 1\n",
    "    return one_hot\n",
    "\n",
    "# Training\n",
    "lr = 0.01\n",
    "num_epochs = 1000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # One-hot encode input and target data\n",
    "    x_encoded = one_hot_encode(input_data, input_size)\n",
    "    y_encoded = one_hot_encode(target_data, output_size)\n",
    "    \n",
    "    # Forward pass\n",
    "    h, y_pred = rnn.forward(x_encoded.T)\n",
    "    \n",
    "    # Backward pass\n",
    "    rnn.backward(x_encoded.T, y_encoded.T, h, y_pred, lr)\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        loss = np.mean((y_pred - y_encoded.T) ** 2)\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "# Inference\n",
    "def predict(rnn, text, src_vocab, tgt_vocab, inv_tgt_vocab):\n",
    "    input_data, _ = prepare_data(text, {}, src_vocab, tgt_vocab)\n",
    "    x_encoded = one_hot_encode(input_data, len(src_vocab))\n",
    "    \n",
    "    _, y_pred = rnn.forward(x_encoded.T)\n",
    "    \n",
    "    y_pred_indices = np.argmax(y_pred, axis=0)\n",
    "    output_text = ''.join([int_to_char(idx, inv_tgt_vocab) for idx in y_pred_indices])\n",
    "    \n",
    "    return output_text\n",
    "\n",
    "# Sample input text in Sanskrit for prediction\n",
    "sample_text = \"अआइईउऊऋएऐओऔ\"\n",
    "\n",
    "# Predict Odia text\n",
    "predicted_text = predict(rnn, sample_text, sanskrit_vocab, odia_vocab, inv_odia_vocab)\n",
    "print(\"Predicted Odia Text:\", predicted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c68156f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.6088008579235998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/radha/anaconda3/envs/cv37/lib/python3.7/site-packages/ipykernel_launcher.py:5: RuntimeWarning: overflow encountered in exp\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Loss: 0.3516792618456343\n",
      "Epoch 200, Loss: 0.28307130440126715\n",
      "Epoch 300, Loss: 0.21461898024752077\n",
      "Epoch 400, Loss: 0.21455239094185952\n",
      "Epoch 500, Loss: 0.21449701802055263\n",
      "Epoch 600, Loss: 0.21444115099812724\n",
      "Epoch 700, Loss: 0.21438266757522778\n",
      "Epoch 800, Loss: 0.21431990304529225\n",
      "Epoch 900, Loss: 0.21425115980838186\n",
      "Predicted Odia Text: ଋଋଏଋଋଋଋଋଋଋଋ\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Derivative of activation function\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "class RNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Weight matrices\n",
    "        self.U = np.random.randn(hidden_size, input_size)\n",
    "        self.W = np.random.randn(hidden_size, hidden_size)\n",
    "        self.V = np.random.randn(output_size, hidden_size)\n",
    "        \n",
    "        # Biases\n",
    "        self.b_hidden = np.zeros((hidden_size, 1))\n",
    "        self.b_output = np.zeros((output_size, 1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        T = x.shape[1]\n",
    "        h = np.zeros((self.hidden_size, T))\n",
    "        y = np.zeros((self.output_size, T))\n",
    "        \n",
    "        for t in range(T):\n",
    "            xt = x[:, t].reshape(-1, 1)\n",
    "            if t == 0:\n",
    "                h[:, t] = sigmoid(np.dot(self.U, xt) + self.b_hidden).flatten()\n",
    "            else:\n",
    "                h[:, t] = sigmoid(np.dot(self.U, xt) + np.dot(self.W, h[:, t-1].reshape(-1, 1)) + self.b_hidden).flatten()\n",
    "            \n",
    "            y[:, t] = sigmoid(np.dot(self.V, h[:, t].reshape(-1, 1)) + self.b_output).flatten()\n",
    "        \n",
    "        return h, y\n",
    "    \n",
    "    def backward(self, x, y_true, y_pred, h, lr):\n",
    "        T = x.shape[1]\n",
    "        dU = np.zeros_like(self.U)\n",
    "        dW = np.zeros_like(self.W)\n",
    "        dV = np.zeros_like(self.V)\n",
    "        db_hidden = np.zeros_like(self.b_hidden)\n",
    "        db_output = np.zeros_like(self.b_output)\n",
    "        \n",
    "        delta_hidden_next = np.zeros((self.hidden_size, 1))\n",
    "        \n",
    "        for t in range(T-1, -1, -1):\n",
    "            yt = y_true[:, t].reshape(-1, 1)\n",
    "            yp = y_pred[:, t].reshape(-1, 1)\n",
    "            delta_output = (yp - yt) * sigmoid_derivative(yp)\n",
    "            dV += np.dot(delta_output, h[:, t].reshape(1, -1))\n",
    "            db_output += delta_output\n",
    "            \n",
    "            delta_hidden = np.dot(self.V.T, delta_output) * sigmoid_derivative(h[:, t].reshape(-1, 1))\n",
    "            if t > 0:\n",
    "                delta_hidden += np.dot(self.W.T, delta_hidden_next)\n",
    "            \n",
    "            xt = x[:, t].reshape(-1, 1)\n",
    "            dU += np.dot(delta_hidden, xt.T)\n",
    "            db_hidden += delta_hidden\n",
    "            \n",
    "            if t > 0:\n",
    "                dW += np.dot(delta_hidden, h[:, t-1].reshape(1, -1))\n",
    "            \n",
    "            delta_hidden_next = delta_hidden\n",
    "        \n",
    "        # Update weights and biases\n",
    "        self.U -= lr * dU\n",
    "        self.W -= lr * dW\n",
    "        self.V -= lr * dV\n",
    "        self.b_hidden -= lr * db_hidden\n",
    "        self.b_output -= lr * db_output\n",
    "\n",
    "# Example usage\n",
    "input_size = 12  # Assuming 12 unique Sanskrit characters\n",
    "hidden_size = 64\n",
    "output_size = 12  # Assuming 12 unique Odia characters\n",
    "\n",
    "# Create an RNN instance\n",
    "rnn = RNN(input_size, hidden_size, output_size)\n",
    "\n",
    "# Character mappings (for illustration)\n",
    "sanskrit_to_odia = {\n",
    "    'अ': 'ଅ', 'आ': 'ଆ', 'इ': 'ଇ', 'ई': 'ଈ', 'उ': 'ଉ', \n",
    "    'ऊ': 'ଊ', 'ऋ': 'ଋ', 'ए': 'ଏ', 'ऐ': 'ଐ', 'ओ': 'ଓ', 'औ': 'ଔ'\n",
    "}\n",
    "\n",
    "# Convert characters to integer representation\n",
    "def char_to_int(char, vocab):\n",
    "    return vocab[char] if char in vocab else vocab['UNK']\n",
    "\n",
    "def int_to_char(index, inv_vocab):\n",
    "    return inv_vocab[index]\n",
    "\n",
    "# Build vocabularies\n",
    "sanskrit_chars = sorted(list(sanskrit_to_odia.keys()))\n",
    "odia_chars = sorted(list(set(sanskrit_to_odia.values())))\n",
    "\n",
    "sanskrit_vocab = {char: idx for idx, char in enumerate(sanskrit_chars)}\n",
    "odia_vocab = {char: idx for idx, char in enumerate(odia_chars)}\n",
    "\n",
    "# Add a special 'UNK' token for unknown characters\n",
    "sanskrit_vocab['UNK'] = len(sanskrit_vocab)\n",
    "odia_vocab['UNK'] = len(odia_vocab)\n",
    "\n",
    "inv_odia_vocab = {idx: char for char, idx in odia_vocab.items()}\n",
    "\n",
    "# Prepare data\n",
    "def prepare_data(text, mapping, src_vocab, tgt_vocab):\n",
    "    input_data = []\n",
    "    target_data = []\n",
    "    for char in text:\n",
    "        input_char = char_to_int(char, src_vocab)\n",
    "        input_data.append(input_char)\n",
    "        \n",
    "        if char in mapping:\n",
    "            target_char = mapping[char]\n",
    "        else:\n",
    "            target_char = 'UNK'\n",
    "        target_data.append(char_to_int(target_char, tgt_vocab))\n",
    "    \n",
    "    return np.array(input_data).reshape(-1, 1), np.array(target_data).reshape(-1, 1)\n",
    "\n",
    "# Sample input text in Sanskrit\n",
    "sanskrit_text = \"अआइईउऊऋएऐओऔ\"\n",
    "\n",
    "# Prepare training data\n",
    "input_data, target_data = prepare_data(sanskrit_text, sanskrit_to_odia, sanskrit_vocab, odia_vocab)\n",
    "\n",
    "# One-hot encoding for the input and target data\n",
    "def one_hot_encode(data, vocab_size):\n",
    "    one_hot = np.zeros((data.size, vocab_size))\n",
    "    one_hot[np.arange(data.size), data.flatten()] = 1\n",
    "    return one_hot\n",
    "\n",
    "# Training\n",
    "lr = 0.01\n",
    "num_epochs = 1000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # One-hot encode input and target data\n",
    "    x_encoded = one_hot_encode(input_data, input_size)\n",
    "    y_encoded = one_hot_encode(target_data, output_size)\n",
    "    \n",
    "    # Forward pass\n",
    "    h, y_pred = rnn.forward(x_encoded.T)\n",
    "    \n",
    "    # Backward pass\n",
    "    rnn.backward(x_encoded.T, y_encoded.T, y_pred, h, lr)\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        loss = np.mean((y_pred - y_encoded.T) ** 2)\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "# Inference\n",
    "def predict(rnn, text, src_vocab, tgt_vocab, inv_tgt_vocab):\n",
    "    input_data, _ = prepare_data(text, {}, src_vocab, tgt_vocab)\n",
    "    x_encoded = one_hot_encode(input_data, len(src_vocab))\n",
    "    \n",
    "    _, y_pred = rnn.forward(x_encoded.T)\n",
    "    \n",
    "    y_pred_indices = np.argmax(y_pred, axis=0)\n",
    "    output_text = ''.join([int_to_char(idx, inv_tgt_vocab) for idx in y_pred_indices])\n",
    "    \n",
    "    return output_text\n",
    "\n",
    "# Sample input text in Sanskrit for prediction\n",
    "sample_text = \"अआइईउऊऋएऐओऔ\"\n",
    "\n",
    "# Predict Odia text\n",
    "predicted_text = predict(rnn, sample_text, sanskrit_vocab, odia_vocab, inv_odia_vocab)\n",
    "print(\"Predicted Odia Text:\", predicted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e40f961e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.35199852570175844\n",
      "Epoch 100, Loss: 0.1477246497675128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/radha/anaconda3/envs/cv37/lib/python3.7/site-packages/ipykernel_launcher.py:5: RuntimeWarning: overflow encountered in exp\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200, Loss: 0.14748312401594962\n",
      "Epoch 300, Loss: 0.1474274050494749\n",
      "Epoch 400, Loss: 0.14739803739380355\n",
      "Epoch 500, Loss: 0.14737796245611307\n",
      "Epoch 600, Loss: 0.14736223148259667\n",
      "Epoch 700, Loss: 0.14734887960350132\n",
      "Epoch 800, Loss: 0.147336975393266\n",
      "Epoch 900, Loss: 0.14732602047762164\n",
      "Epoch 1000, Loss: 0.14731572275999827\n",
      "Epoch 1100, Loss: 0.14730589791008714\n",
      "Epoch 1200, Loss: 0.14729642189946487\n",
      "Epoch 1300, Loss: 0.14728720620188854\n",
      "Epoch 1400, Loss: 0.14727818394086534\n",
      "Epoch 1500, Loss: 0.14726930168568542\n",
      "Epoch 1600, Loss: 0.1472605143145074\n",
      "Epoch 1700, Loss: 0.1472517816039789\n",
      "Epoch 1800, Loss: 0.1472430658068355\n",
      "Epoch 1900, Loss: 0.147234329784445\n",
      "Epoch 2000, Loss: 0.14722553542007946\n",
      "Epoch 2100, Loss: 0.14721664211956337\n",
      "Epoch 2200, Loss: 0.14720760524111828\n",
      "Epoch 2300, Loss: 0.14719837429989374\n",
      "Epoch 2400, Loss: 0.147188890768692\n",
      "Epoch 2500, Loss: 0.1471790852406456\n",
      "Epoch 2600, Loss: 0.14716887361908013\n",
      "Epoch 2700, Loss: 0.14715815182797043\n",
      "Epoch 2800, Loss: 0.14714678824267713\n",
      "Epoch 2900, Loss: 0.1471346125279024\n",
      "Epoch 3000, Loss: 0.14712139864591126\n",
      "Epoch 3100, Loss: 0.14710683806712618\n",
      "Epoch 3200, Loss: 0.14709049582115774\n",
      "Epoch 3300, Loss: 0.1470717350135084\n",
      "Epoch 3400, Loss: 0.14704958005002325\n",
      "Epoch 3500, Loss: 0.14702245270750977\n",
      "Epoch 3600, Loss: 0.1469876239730259\n",
      "Epoch 3700, Loss: 0.146939977326535\n",
      "Epoch 3800, Loss: 0.14686899427352978\n",
      "Epoch 3900, Loss: 0.14675154074879643\n",
      "Epoch 4000, Loss: 0.14654756310305175\n",
      "Epoch 4100, Loss: 0.14632261879636999\n",
      "Epoch 4200, Loss: 0.1462542967944398\n",
      "Epoch 4300, Loss: 0.146217259359333\n",
      "Epoch 4400, Loss: 0.14616797360492592\n",
      "Epoch 4500, Loss: 0.14609656260773685\n",
      "Epoch 4600, Loss: 0.14599015083792857\n",
      "Epoch 4700, Loss: 0.14585160332565136\n",
      "Epoch 4800, Loss: 0.14574698478411882\n",
      "Epoch 4900, Loss: 0.1456973437324686\n",
      "Epoch 5000, Loss: 0.14564885530220367\n",
      "Epoch 5100, Loss: 0.1455750416870589\n",
      "Epoch 5200, Loss: 0.14542926852736957\n",
      "Epoch 5300, Loss: 0.14488949405880894\n",
      "Epoch 5400, Loss: 0.07703599033651831\n",
      "Epoch 5500, Loss: 0.0768619438568607\n",
      "Epoch 5600, Loss: 0.0768050764570815\n",
      "Epoch 5700, Loss: 0.07671598757781245\n",
      "Epoch 5800, Loss: 0.0765592863315481\n",
      "Epoch 5900, Loss: 0.07632631717531316\n",
      "Epoch 6000, Loss: 0.07621573455840706\n",
      "Epoch 6100, Loss: 0.07620148464156365\n",
      "Epoch 6200, Loss: 0.07619268924982664\n",
      "Epoch 6300, Loss: 0.07618409617564592\n",
      "Epoch 6400, Loss: 0.07617562340754473\n",
      "Epoch 6500, Loss: 0.07616726112780874\n",
      "Epoch 6600, Loss: 0.07615900016899627\n",
      "Epoch 6700, Loss: 0.0761508312226289\n",
      "Epoch 6800, Loss: 0.07614274479823796\n",
      "Epoch 6900, Loss: 0.07613473118571999\n",
      "Epoch 7000, Loss: 0.07612678041277848\n",
      "Epoch 7100, Loss: 0.07611888219675769\n",
      "Epoch 7200, Loss: 0.07611102589012882\n",
      "Epoch 7300, Loss: 0.07610320041874001\n",
      "Epoch 7400, Loss: 0.07609539421176557\n",
      "Epoch 7500, Loss: 0.07608759512207797\n",
      "Epoch 7600, Loss: 0.07607979033551406\n",
      "Epoch 7700, Loss: 0.0760719662671992\n",
      "Epoch 7800, Loss: 0.07606410844272314\n",
      "Epoch 7900, Loss: 0.07605620136150668\n",
      "Epoch 8000, Loss: 0.07604822833914557\n",
      "Epoch 8100, Loss: 0.07604017132483175\n",
      "Epoch 8200, Loss: 0.07603201068910928\n",
      "Epoch 8300, Loss: 0.07602372497617037\n",
      "Epoch 8400, Loss: 0.07601529061358459\n",
      "Epoch 8500, Loss: 0.07600668157070813\n",
      "Epoch 8600, Loss: 0.07599786895494237\n",
      "Epoch 8700, Loss: 0.07598882053237645\n",
      "Epoch 8800, Loss: 0.07597950015599403\n",
      "Epoch 8900, Loss: 0.07596986708032318\n",
      "Epoch 9000, Loss: 0.0759598751358714\n",
      "Epoch 9100, Loss: 0.07594947172951698\n",
      "Epoch 9200, Loss: 0.07593859662769152\n",
      "Epoch 9300, Loss: 0.07592718046696827\n",
      "Epoch 9400, Loss: 0.07591514292059047\n",
      "Epoch 9500, Loss: 0.07590239042820852\n",
      "Epoch 9600, Loss: 0.07588881336783536\n",
      "Epoch 9700, Loss: 0.07587428251133084\n",
      "Epoch 9800, Loss: 0.07585864455425935\n",
      "Epoch 9900, Loss: 0.0758417164432826\n",
      "Predicted Odia Text: ଇଈଈଈଈଈଈଈଈଈଈ\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Derivative of activation function\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "class RNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Weight matrices\n",
    "        self.U = np.random.randn(hidden_size, input_size)\n",
    "        self.W = np.random.randn(hidden_size, hidden_size)\n",
    "        self.V = np.random.randn(output_size, hidden_size)\n",
    "        \n",
    "        # Biases\n",
    "        self.b_hidden = np.zeros((hidden_size, 1))\n",
    "        self.b_output = np.zeros((output_size, 1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        T = x.shape[1]\n",
    "        h = np.zeros((self.hidden_size, T))\n",
    "        y = np.zeros((self.output_size, T))\n",
    "        \n",
    "        for t in range(T):\n",
    "            xt = x[:, t].reshape(-1, 1)\n",
    "            if t == 0:\n",
    "                h[:, t] = sigmoid(np.dot(self.U, xt) + self.b_hidden).flatten()\n",
    "            else:\n",
    "                h[:, t] = sigmoid(np.dot(self.U, xt) + np.dot(self.W, h[:, t-1].reshape(-1, 1)) + self.b_hidden).flatten()\n",
    "            \n",
    "            y[:, t] = sigmoid(np.dot(self.V, h[:, t].reshape(-1, 1)) + self.b_output).flatten()\n",
    "        \n",
    "        return h, y\n",
    "    \n",
    "    def backward(self, x, y_true, y_pred, h, lr):\n",
    "        T = x.shape[1]\n",
    "        dU = np.zeros_like(self.U)\n",
    "        dW = np.zeros_like(self.W)\n",
    "        dV = np.zeros_like(self.V)\n",
    "        db_hidden = np.zeros_like(self.b_hidden)\n",
    "        db_output = np.zeros_like(self.b_output)\n",
    "        \n",
    "        delta_hidden_next = np.zeros((self.hidden_size, 1))\n",
    "        \n",
    "        for t in range(T-1, -1, -1):\n",
    "            yt = y_true[:, t].reshape(-1, 1)\n",
    "            yp = y_pred[:, t].reshape(-1, 1)\n",
    "            delta_output = (yp - yt) * sigmoid_derivative(yp)\n",
    "            dV += np.dot(delta_output, h[:, t].reshape(1, -1))\n",
    "            db_output += delta_output\n",
    "            \n",
    "            delta_hidden = np.dot(self.V.T, delta_output) * sigmoid_derivative(h[:, t].reshape(-1, 1))\n",
    "            if t > 0:\n",
    "                delta_hidden += np.dot(self.W.T, delta_hidden_next)\n",
    "            \n",
    "            xt = x[:, t].reshape(-1, 1)\n",
    "            dU += np.dot(delta_hidden, xt.T)\n",
    "            db_hidden += delta_hidden\n",
    "            \n",
    "            if t > 0:\n",
    "                dW += np.dot(delta_hidden, h[:, t-1].reshape(1, -1))\n",
    "            \n",
    "            delta_hidden_next = delta_hidden\n",
    "        \n",
    "        # Update weights and biases\n",
    "        self.U -= lr * dU\n",
    "        self.W -= lr * dW\n",
    "        self.V -= lr * dV\n",
    "        self.b_hidden -= lr * db_hidden\n",
    "        self.b_output -= lr * db_output\n",
    "\n",
    "# Example usage\n",
    "input_size = 12  # Assuming 12 unique Sanskrit characters\n",
    "hidden_size = 64\n",
    "output_size = 12  # Assuming 12 unique Odia characters\n",
    "\n",
    "# Create an RNN instance\n",
    "rnn = RNN(input_size, hidden_size, output_size)\n",
    "\n",
    "# Character mappings (for illustration)\n",
    "sanskrit_to_odia = {\n",
    "    'अ': 'ଅ', 'आ': 'ଆ', 'इ': 'ଇ', 'ई': 'ଈ', 'उ': 'ଉ', \n",
    "    'ऊ': 'ଊ', 'ऋ': 'ଋ', 'ए': 'ଏ', 'ऐ': 'ଐ', 'ओ': 'ଓ', 'औ': 'ଔ'\n",
    "}\n",
    "\n",
    "# Convert characters to integer representation\n",
    "def char_to_int(char, vocab):\n",
    "    return vocab[char] if char in vocab else vocab['UNK']\n",
    "\n",
    "def int_to_char(index, inv_vocab):\n",
    "    return inv_vocab[index]\n",
    "\n",
    "# Build vocabularies\n",
    "sanskrit_chars = sorted(list(sanskrit_to_odia.keys()))\n",
    "odia_chars = sorted(list(set(sanskrit_to_odia.values())))\n",
    "\n",
    "sanskrit_vocab = {char: idx for idx, char in enumerate(sanskrit_chars)}\n",
    "odia_vocab = {char: idx for idx, char in enumerate(odia_chars)}\n",
    "\n",
    "# Add a special 'UNK' token for unknown characters\n",
    "sanskrit_vocab['UNK'] = len(sanskrit_vocab)\n",
    "odia_vocab['UNK'] = len(odia_vocab)\n",
    "\n",
    "inv_odia_vocab = {idx: char for char, idx in odia_vocab.items()}\n",
    "\n",
    "# Prepare data\n",
    "def prepare_data(text, mapping, src_vocab, tgt_vocab):\n",
    "    input_data = []\n",
    "    target_data = []\n",
    "    for char in text:\n",
    "        input_char = char_to_int(char, src_vocab)\n",
    "        input_data.append(input_char)\n",
    "        \n",
    "        if char in mapping:\n",
    "            target_char = mapping[char]\n",
    "        else:\n",
    "            target_char = 'UNK'\n",
    "        target_data.append(char_to_int(target_char, tgt_vocab))\n",
    "    \n",
    "    return np.array(input_data).reshape(-1, 1), np.array(target_data).reshape(-1, 1)\n",
    "\n",
    "# Sample input text in Sanskrit\n",
    "sanskrit_text = \"अआइईउऊऋएऐओऔ\"\n",
    "\n",
    "# Prepare training data\n",
    "input_data, target_data = prepare_data(sanskrit_text, sanskrit_to_odia, sanskrit_vocab, odia_vocab)\n",
    "\n",
    "# One-hot encoding for the input and target data\n",
    "def one_hot_encode(data, vocab_size):\n",
    "    one_hot = np.zeros((data.size, vocab_size))\n",
    "    one_hot[np.arange(data.size), data.flatten()] = 1\n",
    "    return one_hot\n",
    "\n",
    "# Training\n",
    "lr = 0.01\n",
    "num_epochs = 10000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # One-hot encode input and target data\n",
    "    x_encoded = one_hot_encode(input_data, input_size)\n",
    "    y_encoded = one_hot_encode(target_data, output_size)\n",
    "    \n",
    "    # Forward pass\n",
    "    h, y_pred = rnn.forward(x_encoded.T)\n",
    "    \n",
    "    # Backward pass\n",
    "    rnn.backward(x_encoded.T, y_encoded.T, y_pred, h, lr)\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        loss = np.mean((y_pred - y_encoded.T) ** 2)\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "# Inference\n",
    "def predict(rnn, text, src_vocab, tgt_vocab, inv_tgt_vocab):\n",
    "    input_data, _ = prepare_data(text, {}, src_vocab, tgt_vocab)\n",
    "    x_encoded = one_hot_encode(input_data, len(src_vocab))\n",
    "    \n",
    "    _, y_pred = rnn.forward(x_encoded.T)\n",
    "    \n",
    "    y_pred_indices = np.argmax(y_pred, axis=0)\n",
    "    output_text = ''.join([int_to_char(idx, inv_tgt_vocab) for idx in y_pred_indices])\n",
    "    \n",
    "    return output_text\n",
    "\n",
    "# Sample input text in Sanskrit for prediction\n",
    "sample_text = \"अआइईउऊऋएऐओऔ\"\n",
    "\n",
    "# Predict Odia text\n",
    "predicted_text = predict(rnn, sample_text, sanskrit_vocab, odia_vocab, inv_odia_vocab)\n",
    "print(\"Predicted Odia Text:\", predicted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e75073a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try to improve further"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
