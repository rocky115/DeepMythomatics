{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0867ab7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " * Copyright (c) 2004 Radhamadhab Dalai\n",
    " *\n",
    " * Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    " * of this software and associated documentation files (the \"Software\"), to deal\n",
    " * in the Software without restriction, including without limitation the rights\n",
    " * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    " * copies of the Software, and to permit persons to whom the Software is\n",
    " * furnished to do so, subject to the following conditions:\n",
    " *\n",
    " * The above copyright notice and this permission notice shall be included in\n",
    " * all copies or substantial portions of the Software.\n",
    " *\n",
    " * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    " * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    " * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    " * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    " * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    " * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
    " * THE SOFTWARE.\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9236170a",
   "metadata": {},
   "source": [
    "## Estimating and Comparing Classifiers\n",
    "\n",
    "There are at least two reasons for wanting to know the generalization rate of a classifier on a given problem. One is to see if the classifier performs well enough to be useful; another is to compare its performance with that of a competing design. Estimating the final generalization performance invariably requires making assumptions about the classifier or the problem or both, and can fail if the assumptions are not valid. We should stress, then, that all the following methods are heuristic. Indeed, if there were a foolproof method for choosing which of two classifiers would generalize better on an arbitrary new problem, we could incorporate such a method into the learning and violate the **No Free Lunch Theorem**. Occasionally, our assumptions are explicit (as in parametric models), but more often than not they are implicit and difficult to identify or relate to the final estimation (as in empirical methods).\n",
    "\n",
    "## Parametric Models\n",
    "\n",
    "One approach to estimating the generalization rate is to compute it from the assumed parametric model. For example, in the two-class multivariate normal case, we might estimate the probability of error using the **Bhattacharyya** or **Chernoff bounds**, substituting estimates of the means and the covariance matrix for the unknown parameters. However, there are three problems with this approach:\n",
    "\n",
    "1. **Overly Optimistic Error Estimates**: Such an error estimate is often overly optimistic; characteristics that make the training samples peculiar or unrepresentative will not be revealed.\n",
    "2. **Suspecting the Model**: We should always suspect the validity of an assumed parametric model; a performance evaluation based on the same model cannot be believed unless the evaluation is unfavorable.\n",
    "3. **Difficulty in Non-Parametric Models**: In more general situations where the distributions are not simple, it is very difficult to compute the error rate exactly, even if the probabilistic structure is known completely.\n",
    "\n",
    "![image-2.png](attachment:image-2.png)\n",
    "\n",
    "Fig.9: In cross validation, the data set D is split into two parts. The ﬁrst (e.g., 90% of the patterns) is used as a standard training set for setting free parameters in the classiﬁer model; the other (e.g., 10%) is the validation set and is meant to represent the full generalization task. For most problems, the training error decreases monotonically during training, as shown in black. Typically, the error on the validation set decreases, but then increases, an indication that the classiﬁer may be overﬁtting the training data. In cross validation, training or parameter adjustment is stopped at the ﬁrst minimum of the validation error.\n",
    "\n",
    "##  Cross Validation\n",
    "\n",
    "In cross validation, we randomly split the set of labeled training samples $D$ into two parts: one is used as the traditional training set for adjusting model parameters in the classifier. The other set — the **validation set** — is used to estimate the generalization validation error. Since our ultimate goal is low generalization error, we train the classifier until we reach a minimum of this validation error, as shown in the figure below.\n",
    "\n",
    "**Fig.9**: Cross-validation process. The dataset $D$ is split into two parts: the training set (used to adjust model parameters) and the validation set (used to estimate generalization error).\n",
    "\n",
    "                    Training Set (90%)\n",
    "                        |\n",
    "                        v\n",
    "               +---------------------+\n",
    "               |     Model Training  |\n",
    "               +---------------------+\n",
    "                        |\n",
    "                        v\n",
    "            Validation Set (10%)\n",
    "                        |\n",
    "                        v\n",
    "               +---------------------+\n",
    "               | Estimate Validation |\n",
    "               |        Error        |\n",
    "               +---------------------+\n",
    "\n",
    "It is essential that the validation (or test) set not include points used for training the parameters in the classifier — a methodological error known as “testing on the training set.”\n",
    "\n",
    "Cross validation can be applied to virtually every classification method. For example:\n",
    "- In **neural networks** with a fixed topology, the amount of training corresponds to the number of epochs or presentations of the training set.\n",
    "- In **k-nearest neighbor classifiers**, the optimal value of $k$ can be set by cross-validation.\n",
    "\n",
    "### Heuristic for Validation Set Proportion\n",
    "\n",
    "There are several heuristics for choosing the portion $\\gamma$ of $D$ to be used as a validation set, where $0 < \\gamma < 1$. Typically, a smaller portion of the data is used as the validation set, $ \\gamma < 0.5 $, because the validation set is used to set a global property of the classifier. A traditional default is to split the data with $\\gamma = 0.1$.\n",
    "\n",
    "A simple generalization of the above method is **m-fold cross validation**. Here, the training set is randomly divided into $m$ disjoint sets of equal size $n/m$, where $n$ is the total number of patterns in $D$. The classifier is trained $m$ times, each time with a different set held out as a validation set. The estimated performance is the mean of these $m$ errors.\n",
    "\n",
    "In the limit where $m = n$, the method is effectively the **leave-one-out** approach.\n",
    "\n",
    "Cross-validation is heuristic and doesn't always work on every problem. In fact, there are problems where **anti-cross validation** (halting on the adjustment of parameters when the validation error reaches the first local maximum) might be effective.\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "Fig.10: The 95% conﬁdence intervals for a given estimated error probability p̂ can be derived from a binomial distribution of Eq. 38. For each value of p̂, the true probability has a 95% chance of lying between the curves marked by the number of test samples n\u0004 . The larger the number of test samples, the more precise the estimate of the true probability and hence the smaller the 95% conﬁdence interval.\n",
    "\n",
    "\n",
    "### Estimation of Generalization Error\n",
    "\n",
    "Once we train a classifier using cross-validation, the validation error gives an estimate of the accuracy of the final classifier on the unknown test set. If the true but unknown error rate of the classifier is $p$, and if $k$ of the $n$ independent, randomly drawn test samples are misclassified, then $k$ has the **binomial distribution**:\n",
    "\n",
    "$$\n",
    "P(k) = \\binom{n}{k} p^k (1 - p)^{n-k}\n",
    "$$\n",
    "\n",
    "Thus, the fraction of test samples misclassified is exactly the maximum likelihood estimate for \\(p\\):\n",
    "\n",
    "$$\n",
    "\\hat{p} = \\frac{k}{n}\n",
    "$$\n",
    "\n",
    "The properties of this estimate for the parameter $p$ of a binomial distribution are well known. **Figure 9.10** shows 95% confidence intervals as a function of $\\hat{p}$ and $n$.\n",
    "\n",
    "### 95% Confidence Intervals for the Error Rate\n",
    "\n",
    "For a given value of $\\hat{p}$, the true value of $p$ has a 95% chance of lying between the curves marked by the number of test samples $n$. The larger the number of test samples, the more precise the estimate of the true probability, and hence the smaller the 95% confidence interval.\n",
    "\n",
    "**Fig.10**: The 95% confidence intervals for a given estimated error probability $\\hat{p}$, derived from a binomial distribution. The larger the number of test samples, the narrower the confidence interval.\n",
    "\n",
    "                0.8 ─┐                                  .\n",
    "                     │                               .\n",
    "                     │                            .\n",
    "                     │                        .\n",
    "                     │                    .\n",
    "                0.7 ─┤                .\n",
    "                     │            .\n",
    "                     │        .\n",
    "                0.6 ─┤    .\n",
    "                     │ .\n",
    "                     └──────────────────────────────────\n",
    "                       0.1    0.2    0.3    0.4    0.5\n",
    "                                 p̂ (estimated error rate)\n",
    "\n",
    "The larger the test set $n$, the more confident we can be in the estimate of the true error rate.\n",
    "\n",
    "![image-3.png](attachment:image-3.png)\n",
    "\n",
    "Fig.10: The 95% conﬁdence intervals for a given estimated error probability p̂ can be derived from a binomial distribution of Eq. 38. For each value of p̂, the true probability has a 95% chance of lying between the curves marked by the number of test samples n\u0004 . The larger the number of test samples, the more precise the estimate of the true probability and hence the smaller the 95% conﬁdence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5027d739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Cross-Validation Error: 0.5700\n",
      "95% Confidence Interval for Error Rate: (0.0412, 0.1588)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Sample dataset (replace this with your actual data)\n",
    "X = [[random.random(), random.random()] for _ in range(100)]  # 100 samples with 2 features each\n",
    "y = [random.choice([0, 1]) for _ in range(100)]  # Binary labels\n",
    "\n",
    "# Function to split data into k-folds\n",
    "def k_fold_split(X, y, k=5):\n",
    "    data = list(zip(X, y))\n",
    "    random.shuffle(data)  # Shuffle the dataset\n",
    "    fold_size = len(data) // k\n",
    "    folds = [data[i * fold_size:(i + 1) * fold_size] for i in range(k)]\n",
    "    return folds\n",
    "\n",
    "# Function for a simple classifier: thresholding based on the sum of features\n",
    "def simple_classifier(x):\n",
    "    return 1 if sum(x) > 1 else 0\n",
    "\n",
    "# Cross-validation function\n",
    "def cross_validation(X, y, k=5):\n",
    "    folds = k_fold_split(X, y, k)\n",
    "    errors = []\n",
    "\n",
    "    for i in range(k):\n",
    "        # Split into training and validation sets\n",
    "        train_data = [item for j, fold in enumerate(folds) if j != i for item in fold]\n",
    "        val_data = folds[i]\n",
    "\n",
    "        # Train a simple model (dummy classifier in this case)\n",
    "        predictions = [simple_classifier(x) for x, _ in val_data]\n",
    "        true_labels = [label for _, label in val_data]\n",
    "\n",
    "        # Calculate the error rate\n",
    "        error = sum(p != t for p, t in zip(predictions, true_labels)) / len(val_data)\n",
    "        errors.append(error)\n",
    "\n",
    "    avg_error = sum(errors) / k\n",
    "    return avg_error\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "k = 5\n",
    "avg_error = cross_validation(X, y, k)\n",
    "print(f\"Average Cross-Validation Error: {avg_error:.4f}\")\n",
    "\n",
    "# Binomial distribution error estimation (95% confidence intervals)\n",
    "def binomial_confidence_interval(k, n, p_hat):\n",
    "    # Binomial confidence intervals using the normal approximation\n",
    "    z = 1.96  # 95% confidence\n",
    "    standard_error = (p_hat * (1 - p_hat) / n) ** 0.5\n",
    "    lower_bound = p_hat - z * standard_error\n",
    "    upper_bound = p_hat + z * standard_error\n",
    "    return lower_bound, upper_bound\n",
    "\n",
    "# Example usage for 100 test samples and 0.1 error rate (hypothetical)\n",
    "k = 10  # Number of errors\n",
    "n = 100  # Number of test samples\n",
    "p_hat = k / n  # Estimated error rate\n",
    "\n",
    "lower, upper = binomial_confidence_interval(k, n, p_hat)\n",
    "print(f\"95% Confidence Interval for Error Rate: ({lower:.4f}, {upper:.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c086a14b",
   "metadata": {},
   "source": [
    "## Jackknife and Bootstrap Estimation of Classification Accuracy\n",
    "\n",
    "### Jackknife Method\n",
    "\n",
    "The jackknife is closely related to cross-validation but works by training the classifier $ n $ separate times, each time deleting one different training point from the dataset. The accuracy for each of these classifiers is computed, and the jackknife estimate of the accuracy is the mean of these leave-one-out accuracies.\n",
    "\n",
    "Formally, for a dataset $ D $ with $ n $ training points:\n",
    "\n",
    "1. Train the classifier $ n $ times, each time leaving out one data point.\n",
    "2. For each leave-one-out classifier, evaluate its performance on the deleted point.\n",
    "3. The jackknife estimate of the accuracy is the average of these leave-one-out accuracies.\n",
    "\n",
    "This method is particularly useful because each classifier is quite similar to the one being tested, differing only by one data point. However, the computational cost can be high, especially for large datasets.\n",
    "\n",
    "Additionally, the jackknife estimate of the variance of the accuracies can be calculated, allowing us to determine the statistical significance of differences between classifiers. For example, if classifier $ C_1 $ has an accuracy of 80% and $ C_2 $ has 85%, we can assess whether this difference is statistically significant.\n",
    "\n",
    "#### Hypothesis Testing\n",
    "\n",
    "To compare two classifiers $ C_1 $ and $ C_2 $, we calculate the jackknife estimate of their variances and use hypothesis testing to check if the difference in their accuracies is significant.\n",
    "\n",
    "### Bootstrap Method\n",
    "\n",
    "The bootstrap method involves training $ B $ classifiers on different bootstrap samples (random samples drawn with replacement) and then testing the classifiers on other bootstrap data. The bootstrap estimate of classification accuracy is simply the mean of the accuracies from these classifiers.\n",
    "\n",
    "Though useful, the bootstrap estimation can be computationally expensive, and in practice, it often doesn't provide substantial improvements over other methods like cross-validation.\n",
    "\n",
    "### Maximum-Likelihood Model Comparison\n",
    "\n",
    "Maximum-likelihood model comparison, or ML-II, is a generalization of maximum-likelihood estimation for choosing between candidate models based on their ability to explain the training data. The goal is to select the model that best explains the observed data.\n",
    "\n",
    "Given a model with unknown parameter vector $ \\theta $, we maximize the likelihood of the training data $ p(D|\\hat{\\theta}) $, where $ D $ represents the training data. The selection of the best model is done using the posterior probability of each hypothesis $ h_i $, calculated via Bayes' rule:\n",
    "\n",
    "$$\n",
    "P(h_i | D) \\propto P(D | h_i) P(h_i)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $ P(D | h_i) $ is the likelihood (evidence) of the data given the model.\n",
    "- $ P(h_i) $ is the prior probability of the model.\n",
    "- $ P(h_i | D) $ is the posterior probability of the model given the data.\n",
    "\n",
    "In many cases, the prior $ P(h_i) $ is neglected, and the model selection is based solely on the likelihood $ P(D | h_i) $. The model with the highest likelihood is chosen as the best model for the data.\n",
    "\n",
    "### Model Comparison Example\n",
    "\n",
    "Consider three candidate models $ h_1 $, $ h_2 $, and $ h_3 $ of varying complexity. If the observed data is $ D_0 $, then the model $ h_2 $ would be chosen if it maximizes the likelihood $ P(D_0 | h_2) $, as shown in the figure below:\n",
    "\n",
    "$$\n",
    "P(D | h_1), P(D | h_2), P(D | h_3)\n",
    "$$\n",
    "\n",
    "- Model $ h_1 $: Most expressive, can fit a wide range of data sets.\n",
    "- Model $ h_3 $: Most restrictive, less flexible.\n",
    "- Model $ h_2 $: Best matches the observed data $ D_0 $.\n",
    "\n",
    "Thus, maximum-likelihood model selection suggests choosing $ h_2 $ as it explains the data $ D_0 $ better than the other models.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "- **Jackknife** provides a good estimate of classification accuracy, especially in small datasets, by leaving out one point at a time.\n",
    "- **Bootstrap** involves resampling the data and can estimate the model accuracy but can be computationally expensive.\n",
    "- **Maximum-likelihood model comparison** selects the model that best explains the training data, based on its likelihood.\n",
    "\n",
    "These methods are useful for understanding the performance and reliability of classifiers, and they can be applied for model comparison, accuracy estimation, and hypothesis testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7212ae22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jackknife Estimated Accuracy: 0.55\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Sample Data Generation\n",
    "X, y = np.random.rand(100, 2), np.random.randint(0, 2, 100)\n",
    "\n",
    "# Jackknife function\n",
    "def jackknife_estimate(X, y, model, metric=accuracy_score):\n",
    "    n = len(X)\n",
    "    accuracies = []\n",
    "    \n",
    "    # Loop through each data point\n",
    "    for i in range(n):\n",
    "        # Create training set excluding the i-th sample\n",
    "        X_train = np.delete(X, i, axis=0)\n",
    "        y_train = np.delete(y, i)\n",
    "        \n",
    "        # Create test set with just the i-th sample\n",
    "        X_test = X[i].reshape(1, -1)\n",
    "        y_test = y[i]\n",
    "        \n",
    "        # Train model on the modified training set\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict on the single test point\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate the accuracy and store it\n",
    "        acc = metric([y_test], y_pred)\n",
    "        accuracies.append(acc)\n",
    "    \n",
    "    # Return the mean of accuracies (Jackknife estimate)\n",
    "    return np.mean(accuracies)\n",
    "\n",
    "# Initialize model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Perform Jackknife estimation\n",
    "jackknife_accuracy = jackknife_estimate(X, y, model)\n",
    "print(f\"Jackknife Estimated Accuracy: {jackknife_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8814c3b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap Estimated Accuracy: 0.597\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Bootstrap function\n",
    "def bootstrap_estimate(X, y, model, n_iterations=100, metric=accuracy_score):\n",
    "    accuracies = []\n",
    "    \n",
    "    # Perform Bootstrap resampling\n",
    "    for _ in range(n_iterations):\n",
    "        # Sample the data with replacement\n",
    "        X_resampled, y_resampled = resample(X, y, n_samples=len(X), random_state=None)\n",
    "        \n",
    "        # Train the model on the bootstrap sample\n",
    "        model.fit(X_resampled, y_resampled)\n",
    "        \n",
    "        # Test the model on the original data\n",
    "        y_pred = model.predict(X)\n",
    "        \n",
    "        # Calculate the accuracy and store it\n",
    "        acc = metric(y, y_pred)\n",
    "        accuracies.append(acc)\n",
    "    \n",
    "    # Return the mean of the accuracies (Bootstrap estimate)\n",
    "    return np.mean(accuracies)\n",
    "\n",
    "# Perform Bootstrap estimation\n",
    "bootstrap_accuracy = bootstrap_estimate(X, y, model)\n",
    "print(f\"Bootstrap Estimated Accuracy: {bootstrap_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70586fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jackknife Estimated Accuracy: 0.54\n",
      "Bootstrap Estimated Accuracy: 0.5583\n",
      "Model Comparison (Likelihood):\n",
      "Logistic Regression: 0.59\n",
      "Random Forest: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import resample\n",
    "import numpy as np\n",
    "\n",
    "# Sample Data Generation\n",
    "X, y = np.random.rand(100, 2), np.random.randint(0, 2, 100)\n",
    "\n",
    "# Jackknife function\n",
    "def jackknife_estimate(X, y, model, metric=accuracy_score):\n",
    "    n = len(X)\n",
    "    accuracies = []\n",
    "    \n",
    "    # Loop through each data point\n",
    "    for i in range(n):\n",
    "        # Create training set excluding the i-th sample\n",
    "        X_train = np.delete(X, i, axis=0)\n",
    "        y_train = np.delete(y, i)\n",
    "        \n",
    "        # Create test set with just the i-th sample\n",
    "        X_test = X[i].reshape(1, -1)\n",
    "        y_test = y[i]\n",
    "        \n",
    "        # Train model on the modified training set\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict on the single test point\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate the accuracy and store it\n",
    "        acc = metric([y_test], y_pred)\n",
    "        accuracies.append(acc)\n",
    "    \n",
    "    # Return the mean of accuracies (Jackknife estimate)\n",
    "    return np.mean(accuracies)\n",
    "\n",
    "# Bootstrap function\n",
    "def bootstrap_estimate(X, y, model, n_iterations=100, metric=accuracy_score):\n",
    "    accuracies = []\n",
    "    \n",
    "    # Perform Bootstrap resampling\n",
    "    for _ in range(n_iterations):\n",
    "        # Sample the data with replacement\n",
    "        X_resampled, y_resampled = resample(X, y, n_samples=len(X), random_state=None)\n",
    "        \n",
    "        # Train the model on the bootstrap sample\n",
    "        model.fit(X_resampled, y_resampled)\n",
    "        \n",
    "        # Test the model on the original data\n",
    "        y_pred = model.predict(X)\n",
    "        \n",
    "        # Calculate the accuracy and store it\n",
    "        acc = metric(y, y_pred)\n",
    "        accuracies.append(acc)\n",
    "    \n",
    "    # Return the mean of the accuracies (Bootstrap estimate)\n",
    "    return np.mean(accuracies)\n",
    "\n",
    "# Define multiple classifiers for comparison\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100)\n",
    "}\n",
    "\n",
    "# Likelihood estimation function (using accuracy for simplicity)\n",
    "def model_comparison(X, y, models):\n",
    "    model_likelihoods = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        model.fit(X, y)\n",
    "        y_pred = model.predict(X)\n",
    "        accuracy = accuracy_score(y, y_pred)\n",
    "        \n",
    "        # In a real-world scenario, you might calculate log-likelihood instead of accuracy\n",
    "        model_likelihoods[name] = accuracy\n",
    "    \n",
    "    return model_likelihoods\n",
    "\n",
    "# Perform Jackknife estimation\n",
    "model = LogisticRegression()\n",
    "jackknife_accuracy = jackknife_estimate(X, y, model)\n",
    "print(f\"Jackknife Estimated Accuracy: {jackknife_accuracy}\")\n",
    "\n",
    "# Perform Bootstrap estimation\n",
    "bootstrap_accuracy = bootstrap_estimate(X, y, model)\n",
    "print(f\"Bootstrap Estimated Accuracy: {bootstrap_accuracy}\")\n",
    "\n",
    "# Perform model comparison\n",
    "model_likelihoods = model_comparison(X, y, models)\n",
    "print(\"Model Comparison (Likelihood):\")\n",
    "for model, likelihood in model_likelihoods.items():\n",
    "    print(f\"{model}: {likelihood}\")\n"
   ]
  },
  {
   "attachments": {
    "image-2.png": {
     "image/png": "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"
    },
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "5e96eb49",
   "metadata": {},
   "source": [
    "##  Bayesian Model Comparison\n",
    "\n",
    "Bayesian model comparison uses the full information over priors when computing posterior probabilities in Eq. (40). In particular, the evidence for a particular hypothesis is an integral:\n",
    "\n",
    "$$ P(D | h_i) = \\int p(D | \\theta, h_i) p(\\theta | D, h_i) \\, d\\theta, $$\n",
    "\n",
    "where, as before, $\\theta$ describes the parameters in the candidate model. It is common for the posterior $p(\\theta | D, h_i)$ to be peaked at $\\hat{\\theta}$, and thus the evidence integral can often be approximated as:\n",
    "\n",
    "$$ P(D | h_i) \\approx p(D | \\hat{\\theta}, h_i) p(\\hat{\\theta} | h_i) \\Delta \\theta. $$\n",
    "\n",
    "### Occam Factor and Model Complexity\n",
    "\n",
    "Before the data arrive, model $h_i$ has some broad range of model parameters, denoted by $\\Delta_0 \\theta$ and shown in Fig. 9.13. After the data arrive, a smaller range is commensurate or compatible with $D$, denoted $\\Delta \\theta$. The **Occam factor** in Eq. (42) is the ratio of two volumes in parameter space:\n",
    "\n",
    "$$ \\text{Occam factor} = \\frac{\\Delta \\theta}{\\Delta_0 \\theta}, $$\n",
    "\n",
    "where:\n",
    "- $\\Delta_0 \\theta$: The prior volume, accessible to the model without regard to $D$.\n",
    "- $\\Delta \\theta$: The volume commensurate with the data $D$.\n",
    "\n",
    "The Occam factor has magnitude less than 1.0 and simply measures the fractional decrease in the volume of the model’s parameter space due to the presence of training data. The more the training data, the smaller the range of parameters that are commensurate with it, and thus the greater this collapse in the parameter space and the larger the Occam factor.\n",
    "\n",
    "$$ p(\\theta | D, h_i) \\Delta \\theta \\quad \\text{vs.} \\quad p(\\theta | h_i) \\Delta_0 \\theta $$\n",
    "\n",
    "In practice, the Occam factor can be calculated fairly easily if the evidence is approximated as a $k$-dimensional Gaussian, centered on the maximum-likelihood value $\\hat{\\theta}$.\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "Fig.12: The evidence (i.e., probability of generating diﬀerent data sets given a model) is shown for three models of diﬀerent expressive power or complexity. Model h1 is the most expressive, since with diﬀerent values of its parameters the model can ﬁt a wide range of data sets. Model h3 is the most restrictive of the three. If the actual data observed is D0 , then maximum-likelihood model selection states that we should choose h2 , which has the highest evidence. Model h2 “matches” this particular data set better than do the other two models, and should be selected.\n",
    "\n",
    "\n",
    "### Approximate Evidence\n",
    "\n",
    "In the case where the posterior can be assumed to be a Gaussian, the evidence can be calculated directly, yielding:\n",
    "\n",
    "$$ P(D | h_i) \\approx p(D | \\hat{\\theta}, h_i) p(\\hat{\\theta} | h_i) \\frac{(2\\pi)^k}{|H|^{1/2}}, $$\n",
    "\n",
    "where $H$ is the Hessian matrix defined as:\n",
    "\n",
    "$$ H = \\frac{\\partial^2 \\ln p(\\theta | D, h_i)}{\\partial \\theta^2}, $$\n",
    "\n",
    "and $k$ is the number of parameters in the model. The Hessian matrix measures how \"peaked\" the posterior is around the value $\\hat{\\theta}$.\n",
    "\n",
    "### Degeneracies and Scaling\n",
    "\n",
    "There may be a problem due to degeneracies in a model — several parameters could be relabeled and leave the classiﬁcation rule (and hence the likelihood) unchanged. The resulting degeneracy leads, in essence, to an “overcounting” which alters the eﬀective volume in parameter space.\n",
    "\n",
    "For such cases, we must multiply the right-hand side of Eq. (42) by the degeneracy of $\\hat{\\theta}$ in order to scale the Occam factor and obtain the proper estimate of the evidence.\n",
    "\n",
    "![image-2.png](attachment:image-2.png)\n",
    "\n",
    "Fig.13: In the absence of training data, a particular model h has available a large range of possible values of its parameters, denoted ∆0 θ. In the presence of a particular training set D, a smaller range is available. The Occam factor, ∆θ/∆0 θ, measures the fractional decrease in the volume of the model’s parameter space due to the presence of training data D. In practice, the Occam factor can be calculated fairly easily if the evidence is approximated as a k-dimensional Gaussian, centered on the maximum-likelihood value θ̂.\n",
    "\n",
    "### Bayesian Model Selection and the No Free Lunch Theorem\n",
    "\n",
    "There seems to be a fundamental contradiction between two deep ideas in statistical pattern recognition:\n",
    "\n",
    "1. The **No Free Lunch Theorem** states that in the absence of prior information about the problem, there is no reason to prefer one classification algorithm over another.\n",
    "2. Bayesian model selection seems to show how to reliably choose the better of two algorithms.\n",
    "\n",
    "Consider two “composite” algorithms — algorithm A and algorithm B — each of which employs two others (algorithm 1 and algorithm 2). For any problem:\n",
    "- Algorithm A uses Bayesian model selection and applies the “better” of algorithm 1 and algorithm 2.\n",
    "- Algorithm B uses anti-Bayesian model selection and applies the “worse” of algorithm 1 and algorithm 2.\n",
    "\n",
    "It appears that algorithm A will reliably outperform algorithm B throughout the full class of problems — in contradiction with the No Free Lunch Theorem.\n",
    "\n",
    "#### Resolution of the Contradiction\n",
    "\n",
    "In Bayesian model selection, we ignore the prior over the space of models, $H$, effectively assuming it is uniform. This assumption therefore does not take into account how those models correspond to underlying target functions, i.e., mappings from input to category labels. Accordingly, Bayesian model selection usually corresponds to a **non-uniform prior** over target functions. \n",
    "\n",
    "In fact, the non-uniform prior varies depending on the choice of model. Therefore, Bayesian model selection usually applies a non-uniform prior that seems to match many important real-world problems.\n",
    "\n",
    "The **No Free Lunch Theorem** allows that for some particular non-uniform prior, there may be a learning algorithm that gives better-than-chance or even optimal results. This shows that Bayesian model selection, despite appearing to contradict the No Free Lunch Theorem, is consistent with many real-world learning scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb6922c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Evidence for Model 1 (Linear): -149.7927566773839\n",
      "Log Evidence for Model 2 (Quadratic): -149.01673960682518\n",
      "Model 2 (Quadratic) is preferred based on Bayesian evidence.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.linalg import det, inv\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "x = np.linspace(0, 10, n_samples)\n",
    "y = 2 * x + 1 + np.random.normal(scale=2, size=n_samples)  # Linear relationship with noise\n",
    "\n",
    "x = x[:, np.newaxis]  # Reshape for model compatibility\n",
    "\n",
    "# Define two models for comparison\n",
    "def fit_model(x, y, degree):\n",
    "    \"\"\"Fit a polynomial model of a given degree.\"\"\"\n",
    "    model = Pipeline([\n",
    "        (\"poly_features\", PolynomialFeatures(degree=degree, include_bias=False)),\n",
    "        (\"regression\", LinearRegression())\n",
    "    ])\n",
    "    model.fit(x, y)\n",
    "    return model\n",
    "\n",
    "# Evidence approximation (Gaussian assumption)\n",
    "def compute_evidence(model, x, y):\n",
    "    \"\"\"Compute the evidence using Gaussian approximation.\"\"\"\n",
    "    y_pred = model.predict(x)\n",
    "    residuals = y - y_pred\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    n_params = len(model.named_steps[\"regression\"].coef_) + 1  # Coefficients + intercept\n",
    "\n",
    "    # Approximation of the Hessian determinant (using variance of residuals)\n",
    "    hessian_det = (1 / mse) ** n_params\n",
    "\n",
    "    # Compute log-evidence\n",
    "    log_evidence = -0.5 * len(y) * np.log(2 * np.pi * mse) - 0.5 * np.log(hessian_det)\n",
    "    return log_evidence\n",
    "\n",
    "# Fit two models\n",
    "model1 = fit_model(x, y, degree=1)  # Linear model\n",
    "model2 = fit_model(x, y, degree=2)  # Quadratic model\n",
    "\n",
    "# Compute evidences\n",
    "log_evidence1 = compute_evidence(model1, x, y)\n",
    "log_evidence2 = compute_evidence(model2, x, y)\n",
    "\n",
    "# Compare models\n",
    "print(f\"Log Evidence for Model 1 (Linear): {log_evidence1}\")\n",
    "print(f\"Log Evidence for Model 2 (Quadratic): {log_evidence2}\")\n",
    "\n",
    "if log_evidence1 > log_evidence2:\n",
    "    print(\"Model 1 (Linear) is preferred based on Bayesian evidence.\")\n",
    "else:\n",
    "    print(\"Model 2 (Quadratic) is preferred based on Bayesian evidence.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72e0b5b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'math' has no attribute 'random'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12616/1601123733.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;31m# Main comparison\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintercept\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;31m# Fit models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_12616/1601123733.py\u001b[0m in \u001b[0;36mgenerate_data\u001b[0;34m(n_samples, slope, intercept, noise_level)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintercept\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mslope\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mxi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mintercept\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnoise_level\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mxi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_12616/1601123733.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintercept\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mslope\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mxi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mintercept\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnoise_level\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mxi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'math' has no attribute 'random'"
     ]
    }
   ],
   "source": [
    "# Generate synthetic data\n",
    "import math\n",
    "\n",
    "def generate_data(n_samples, slope, intercept, noise_level):\n",
    "    x = [i for i in range(n_samples)]\n",
    "    y = [slope * xi + intercept + noise_level * (2 * (math.random() - 0.5)) for xi in x]\n",
    "    return x, y\n",
    "\n",
    "# Polynomial model\n",
    "def fit_polynomial(x, y, degree):\n",
    "    # Solve for polynomial coefficients using normal equations\n",
    "    X = [[xi**d for d in range(degree + 1)] for xi in x]\n",
    "    X_T = transpose(X)\n",
    "    XTX = matmul(X_T, X)\n",
    "    XTy = matmul(X_T, y)\n",
    "    coeffs = solve(XTX, XTy)\n",
    "    return coeffs\n",
    "\n",
    "def predict_polynomial(coeffs, x):\n",
    "    return sum(c * (x**i) for i, c in enumerate(coeffs))\n",
    "\n",
    "# Matrix operations\n",
    "def transpose(matrix):\n",
    "    return list(map(list, zip(*matrix)))\n",
    "\n",
    "def matmul(A, B):\n",
    "    return [[sum(a * b for a, b in zip(A_row, B_col)) for B_col in zip(*B)] for A_row in A]\n",
    "\n",
    "def solve(A, b):\n",
    "    # Gaussian elimination\n",
    "    n = len(b)\n",
    "    for i in range(n):\n",
    "        max_row = max(range(i, n), key=lambda r: abs(A[r][i]))\n",
    "        A[i], A[max_row] = A[max_row], A[i]\n",
    "        b[i], b[max_row] = b[max_row], b[i]\n",
    "        for j in range(i + 1, n):\n",
    "            ratio = A[j][i] / A[i][i]\n",
    "            for k in range(i, n):\n",
    "                A[j][k] -= ratio * A[i][k]\n",
    "            b[j] -= ratio * b[i]\n",
    "    x = [0] * n\n",
    "    for i in range(n - 1, -1, -1):\n",
    "        x[i] = (b[i] - sum(A[i][j] * x[j] for j in range(i + 1, n))) / A[i][i]\n",
    "    return x\n",
    "\n",
    "# Bayesian evidence calculation\n",
    "def compute_log_evidence(x, y, coeffs):\n",
    "    predictions = [predict_polynomial(coeffs, xi) for xi in x]\n",
    "    residuals = [yi - pred for yi, pred in zip(y, predictions)]\n",
    "    mse = sum(r**2 for r in residuals) / len(y)\n",
    "    n_params = len(coeffs)\n",
    "    \n",
    "    # Compute Gaussian approximation of evidence\n",
    "    hessian_det = (1 / mse) ** n_params\n",
    "    log_evidence = -0.5 * len(y) * math.log(2 * math.pi * mse) - 0.5 * math.log(hessian_det)\n",
    "    return log_evidence\n",
    "\n",
    "# Main comparison\n",
    "n_samples = 100\n",
    "x, y = generate_data(n_samples, slope=2, intercept=1, noise_level=2)\n",
    "\n",
    "# Fit models\n",
    "coeffs1 = fit_polynomial(x, y, degree=1)\n",
    "coeffs2 = fit_polynomial(x, y, degree=2)\n",
    "\n",
    "# Compute evidences\n",
    "log_evidence1 = compute_log_evidence(x, y, coeffs1)\n",
    "log_evidence2 = compute_log_evidence(x, y, coeffs2)\n",
    "\n",
    "# Compare models\n",
    "print(f\"Log Evidence for Model 1 (Linear): {log_evidence1}\")\n",
    "print(f\"Log Evidence for Model 2 (Quadratic): {log_evidence2}\")\n",
    "\n",
    "if log_evidence1 > log_evidence2:\n",
    "    print(\"Model 1 (Linear) is preferred based on Bayesian evidence.\")\n",
    "else:\n",
    "    print(\"Model 2 (Quadratic) is preferred based on Bayesian evidence.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5371cfbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Evidence for Model 1 (Linear): -96.07438815735479\n",
      "Log Evidence for Model 2 (Quadratic): -94.46296967242567\n",
      "Model 2 (Quadratic) is preferred based on Bayesian evidence.\n"
     ]
    }
   ],
   "source": [
    "# Generate synthetic data\n",
    "import random\n",
    "\n",
    "def generate_data(n_samples, slope, intercept, noise_level):\n",
    "    x = [i for i in range(n_samples)]\n",
    "    y = [slope * xi + intercept + noise_level * (2 * (random.random() - 0.5)) for xi in x]\n",
    "    return x, y\n",
    "\n",
    "# Polynomial model\n",
    "def fit_polynomial(x, y, degree):\n",
    "    # Construct the design matrix\n",
    "    X = [[xi**d for d in range(degree + 1)] for xi in x]\n",
    "    X_T = transpose(X)\n",
    "    XTX = matmul(X_T, X)\n",
    "    \n",
    "    # Convert y to a column vector\n",
    "    y_column = [[yi] for yi in y]\n",
    "    XTy = matmul(X_T, y_column)\n",
    "    \n",
    "    # Solve for polynomial coefficients\n",
    "    coeffs_column = solve(XTX, XTy)\n",
    "    # Flatten the coefficients column vector\n",
    "    coeffs = [c[0] for c in coeffs_column]\n",
    "    return coeffs\n",
    "\n",
    "def predict_polynomial(coeffs, x):\n",
    "    return sum(c * (x**i) for i, c in enumerate(coeffs))\n",
    "\n",
    "# Matrix operations\n",
    "def transpose(matrix):\n",
    "    return list(map(list, zip(*matrix)))\n",
    "\n",
    "def matmul(A, B):\n",
    "    return [[sum(a * b for a, b in zip(A_row, B_col)) for B_col in zip(*B)] for A_row in A]\n",
    "\n",
    "def solve(A, b):\n",
    "    \"\"\"Solves the linear system Ax = b using Gaussian elimination.\"\"\"\n",
    "    n = len(A)\n",
    "    \n",
    "    # Flatten b if it's a column vector\n",
    "    if isinstance(b[0], list):\n",
    "        b = [row[0] for row in b]\n",
    "    \n",
    "    for i in range(n):\n",
    "        # Make the diagonal element 1 and scale the row\n",
    "        diag = A[i][i]\n",
    "        for j in range(i, n):\n",
    "            A[i][j] /= diag\n",
    "        b[i] /= diag\n",
    "        \n",
    "        # Make the elements below the pivot in column i zero\n",
    "        for j in range(i + 1, n):\n",
    "            ratio = A[j][i]\n",
    "            for k in range(i, n):\n",
    "                A[j][k] -= ratio * A[i][k]\n",
    "            b[j] -= ratio * b[i]\n",
    "    \n",
    "    # Back substitution\n",
    "    x = [0] * n\n",
    "    for i in range(n - 1, -1, -1):\n",
    "        x[i] = b[i]\n",
    "        for j in range(i + 1, n):\n",
    "            x[i] -= A[i][j] * x[j]\n",
    "    \n",
    "    return [[xi] for xi in x]  # Return result as a column vector\n",
    "\n",
    "\n",
    "# Bayesian evidence calculation\n",
    "def compute_log_evidence(x, y, coeffs):\n",
    "    predictions = [predict_polynomial(coeffs, xi) for xi in x]\n",
    "    residuals = [yi - pred for yi, pred in zip(y, predictions)]\n",
    "    mse = sum(r**2 for r in residuals) / len(y)\n",
    "    n_params = len(coeffs)\n",
    "    \n",
    "    # Compute Gaussian approximation of evidence\n",
    "    hessian_det = (1 / mse) ** n_params\n",
    "    log_evidence = -0.5 * len(y) * math.log(2 * math.pi * mse) - 0.5 * math.log(hessian_det)\n",
    "    return log_evidence\n",
    "\n",
    "# Main comparison\n",
    "n_samples = 100\n",
    "x, y = generate_data(n_samples, slope=2, intercept=1, noise_level=2)\n",
    "\n",
    "# Fit models\n",
    "coeffs1 = fit_polynomial(x, y, degree=1)\n",
    "coeffs2 = fit_polynomial(x, y, degree=2)\n",
    "\n",
    "# Compute evidences\n",
    "log_evidence1 = compute_log_evidence(x, y, coeffs1)\n",
    "log_evidence2 = compute_log_evidence(x, y, coeffs2)\n",
    "\n",
    "# Compare models\n",
    "print(f\"Log Evidence for Model 1 (Linear): {log_evidence1}\")\n",
    "print(f\"Log Evidence for Model 2 (Quadratic): {log_evidence2}\")\n",
    "\n",
    "if log_evidence1 > log_evidence2:\n",
    "    print(\"Model 1 (Linear) is preferred based on Bayesian evidence.\")\n",
    "else:\n",
    "    print(\"Model 2 (Quadratic) is preferred based on Bayesian evidence.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38a1387d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Evidence for Model 1 (Linear): -99.43789980253715\n",
      "Log Evidence for Model 2 (Quadratic): -99.35820045210853\n",
      "Model 2 (Quadratic) is preferred based on Bayesian evidence.\n"
     ]
    }
   ],
   "source": [
    "def plot_graph(x, y, coeffs1, coeffs2):\n",
    "    \"\"\"Plots the data points and the fitted models.\"\"\"\n",
    "    import turtle\n",
    "    \n",
    "    # Set up the drawing area\n",
    "    screen = turtle.Screen()\n",
    "    screen.setup(width=800, height=600)\n",
    "    screen.setworldcoordinates(-10, -10, 110, 250)  # Adjust axes as per data\n",
    "    turtle.speed(0)\n",
    "\n",
    "    # Plot data points\n",
    "    turtle.penup()\n",
    "    turtle.color(\"black\")\n",
    "    for xi, yi in zip(x, y):\n",
    "        turtle.goto(xi, yi)\n",
    "        turtle.dot(3, \"black\")  # Data points as small black dots\n",
    "    \n",
    "    # Plot linear model\n",
    "    turtle.color(\"blue\")\n",
    "    turtle.penup()\n",
    "    turtle.goto(x[0], coeffs1[0] + coeffs1[1] * x[0])\n",
    "    turtle.pendown()\n",
    "    for xi in x:\n",
    "        yi = coeffs1[0] + coeffs1[1] * xi\n",
    "        turtle.goto(xi, yi)\n",
    "    \n",
    "    # Plot quadratic model\n",
    "    turtle.color(\"red\")\n",
    "    turtle.penup()\n",
    "    turtle.goto(x[0], coeffs2[0] + coeffs2[1] * x[0] + coeffs2[2] * x[0]**2)\n",
    "    turtle.pendown()\n",
    "    for xi in x:\n",
    "        yi = coeffs2[0] + coeffs2[1] * xi + coeffs2[2] * xi**2\n",
    "        turtle.goto(xi, yi)\n",
    "\n",
    "    # Keep the window open until clicked\n",
    "    turtle.done()\n",
    "\n",
    "# Generate data\n",
    "n_samples = 100\n",
    "x, y = generate_data(n_samples, slope=2, intercept=1, noise_level=2)\n",
    "\n",
    "# Fit models\n",
    "coeffs1 = fit_polynomial(x, y, degree=1)  # Linear model\n",
    "coeffs2 = fit_polynomial(x, y, degree=2)  # Quadratic model\n",
    "\n",
    "# Compute Bayesian evidences\n",
    "log_evidence1 = compute_log_evidence(x, y, coeffs1)\n",
    "log_evidence2 = compute_log_evidence(x, y, coeffs2)\n",
    "\n",
    "# Model comparison\n",
    "print(f\"Log Evidence for Model 1 (Linear): {log_evidence1}\")\n",
    "print(f\"Log Evidence for Model 2 (Quadratic): {log_evidence2}\")\n",
    "\n",
    "if log_evidence1 > log_evidence2:\n",
    "    print(\"Model 1 (Linear) is preferred based on Bayesian evidence.\")\n",
    "else:\n",
    "    print(\"Model 2 (Quadratic) is preferred based on Bayesian evidence.\")\n",
    "\n",
    "# Plot the graphs\n",
    "plot_graph(x, y, coeffs1, coeffs2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296dfb56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
