{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edc1fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " * Copyright (c) 2018 Radhamadhab Dalai\n",
    " *\n",
    " * Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    " * of this software and associated documentation files (the \"Software\"), to deal\n",
    " * in the Software without restriction, including without limitation the rights\n",
    " * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    " * copies of the Software, and to permit persons to whom the Software is\n",
    " * furnished to do so, subject to the following conditions:\n",
    " *\n",
    " * The above copyright notice and this permission notice shall be included in\n",
    " * all copies or substantial portions of the Software.\n",
    " *\n",
    " * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    " * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    " * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    " * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    " * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    " * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
    " * THE SOFTWARE.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb1d450",
   "metadata": {},
   "source": [
    "#Semi-Supervised and Graph Regression## Semi-Supervised and Graph Regression Representer Theorem\n",
    "\n",
    "### Representer Theorem for Semi-Supervised Learning\n",
    "\n",
    "The representer theorem for supervised learning (Theorem 8.4) can be extended to semi-supervised learning and graph signals.\n",
    "\n",
    "**Theorem 8.5 (Representer Theorem for Semi-Supervised Learning)**\n",
    "\n",
    "Given a set of $ l $ labeled examples $\\{(x_i, y_i)\\}_{i=1}^l$, a set of $ u $ unlabeled examples $\\{x_j\\}_{j=l+1}^{l+u}$, and the graph Laplacian $ L $, the minimizer of the semi-supervised/graph optimization problem\n",
    "\n",
    "$$\n",
    "f^* = \\arg \\min_f \\left\\{ \\frac{1}{2} \\sum_{i=1}^l V(x_i, y_i, f) + \\frac{\\gamma_A}{2} \\|f\\|_H^2 + \\frac{(u-l)^2}{2} f^T L f \\right\\}\n",
    "$$\n",
    "\n",
    "admits an expansion\n",
    "\n",
    "$$\n",
    "f^*(x) = \\sum_{i=1}^{l+u} \\alpha_i K(x, x_i),\n",
    "$$\n",
    "\n",
    "where  K(x, $x_i$)  denotes the kernel function, and $ \\alpha_i $ are the coefficients to be determined.\n",
    "\n",
    "Note that when the graph Laplacian L  is the identity matrix I , Theorem 8.5 reduces to the representer theorem for semi-supervised learning. Further, if $ u = 0 $, then Theorem 8.5 reduces to Theorem 8.4 for supervised learning.\n",
    "\n",
    "### Loss Representation\n",
    "\n",
    "The loss function can be represented in terms of $ \\alpha $ as follows:\n",
    "\n",
    "$$\n",
    "L(\\alpha) = \\frac{1}{2} \\left\\{ (y - J K \\alpha)^T (y - J K \\alpha) + \\gamma_A \\alpha^T K \\alpha + \\frac{(u-l)^2}{2} \\alpha^T L \\alpha \\right\\},\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- K  is the $$(l + u) \\times (l + u)$$ Gram matrix with entries $$ K_{ij} = K(x_i, x_j) $$,\n",
    "- $$ y = [y_1, \\dots, y_l, 0, \\dots, 0]^T $$ is the (l + u)-dimensional label vector,\n",
    "- J is the matrix that selects labeled examples (essentially a matrix that maps the predictions to the labeled examples).\n",
    "\n",
    "This formulation shows that the semi-supervised learning problem can be solved by finding the optimal solution for $ \\alpha $ that minimizes the given loss function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f25e27",
   "metadata": {},
   "source": [
    "## Semi-Supervised and Graph Regression Representer Theorem\n",
    "\n",
    "### Representer Theorem for Semi-Supervised Learning\n",
    "\n",
    "The representer theorem for supervised learning (Theorem 8.4) can be extended to semi-supervised learning and graph signals.\n",
    "\n",
    "**Theorem 8.5 (Representer Theorem for Semi-Supervised Learning)**\n",
    "\n",
    "Given a set of \\( l \\) labeled examples \\(\\{(x_i, y_i)\\}_{i=1}^l\\), a set of \\( u \\) unlabeled examples \\(\\{x_j\\}_{j=l+1}^{l+u}\\), and the graph Laplacian \\( L \\), the minimizer of the semi-supervised/graph optimization problem\n",
    "\n",
    "\\[\n",
    "f^* = \\arg \\min_f \\left\\{ \\frac{1}{2} \\sum_{i=1}^l V(x_i, y_i, f) + \\frac{\\gamma_A}{2} \\|f\\|_H^2 + \\frac{(u-l)^2}{2} f^T L f \\right\\}\n",
    "\\]\n",
    "\n",
    "admits an expansion\n",
    "\n",
    "\\[\n",
    "f^*(x) = \\sum_{i=1}^{l+u} \\alpha_i K(x, x_i),\n",
    "\\]\n",
    "\n",
    "where \\( K(x, x_i) \\) denotes the kernel function, and \\( \\alpha_i \\) are the coefficients to be determined.\n",
    "\n",
    "Note that when the graph Laplacian \\( L \\) is the identity matrix \\( I \\), Theorem 8.5 reduces to the representer theorem for semi-supervised learning. Further, if \\( u = 0 \\), then Theorem 8.5 reduces to Theorem 8.4 for supervised learning.\n",
    "\n",
    "### Loss Representation\n",
    "\n",
    "The loss function can be represented in terms of \\( \\alpha \\) as follows:\n",
    "\n",
    "\\[\n",
    "L(\\alpha) = \\frac{1}{2} \\left\\{ (y - J K \\alpha)^T (y - J K \\alpha) + \\gamma_A \\alpha^T K \\alpha + \\frac{(u-l)^2}{2} \\alpha^T L \\alpha \\right\\},\n",
    "\\]\n",
    "\n",
    "where:\n",
    "\n",
    "- \\( K \\) is the \\((l + u) \\times (l + u)\\) Gram matrix with entries \\( K_{ij} = K(x_i, x_j) \\),\n",
    "- \\( y = [y_1, \\dots, y_l, 0, \\dots, 0]^T \\) is the \\((l + u)\\)-dimensional label vector,\n",
    "- \\( J \\) is the matrix that selects labeled examples (essentially a matrix that maps the predictions to the labeled examples).\n",
    "\n",
    "This formulation shows that the semi-supervised learning problem can be solved by finding the optimal solution for \\( \\alpha \\) that minimizes the given loss function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723a4bac",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03130739",
   "metadata": {},
   "source": [
    "## Laplacian Regularized Least Squares (LapRLS)\n",
    "\n",
    "### Diagonal Matrix \\( \\text{Diag}(1, \\dots, 1, 0, \\dots, 0) \\)\n",
    "Let \\( \\text{Diag}(1, \\dots, 1, 0, \\dots, 0) \\) be an \\( (l + u) \\times (l + u) \\) diagonal matrix with the first \\( l \\) diagonal entries as 1 and the remaining entries as 0.\n",
    "\n",
    "### First-Order Optimization Condition\n",
    "\n",
    "The first-order optimization condition can be derived from the loss function by setting the gradient with respect to \\( \\alpha \\) to zero:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L(\\alpha)}{\\partial \\alpha} = 0 \\Rightarrow - (J K)^T (y - J K \\alpha) + \\gamma_A K^T K \\alpha + \\frac{\\gamma_l}{(u - l)^2} K^T L K \\alpha = 0.\n",
    "$$\n",
    "\n",
    "Given that $ K^T J^T = K^T $, $ J^T J = J $, and $ J y - J J K \\alpha = J y - J K \\alpha $, the optimization condition simplifies to:\n",
    "\n",
    "$$\n",
    "\\alpha^* = \\left( J K + \\gamma_A l I + \\frac{\\gamma_l l}{(u - l)^2} L K \\right)^{-1} J y.\n",
    "$$\n",
    "\n",
    "This solution is known as the Laplacian Regularized Least Squares (LapRLS) solution.\n",
    "\n",
    "### LapRLS for Graph Supervised Regression and Classification\n",
    "\n",
    "LapRLS can also be applied to graph supervised regression and classification. In this scenario, the number \\( u \\) of unlabeled samples is zero, which simplifies the matrix \\( J \\) to the identity matrix \\( I \\). Consequently, the optimization solution simplifies to:\n",
    "\n",
    "$$\n",
    "\\alpha^* = \\left( K + \\gamma_A l I + \\frac{\\gamma_l l}{(u - l)^2} L K \\right)^{-1} y.\n",
    "$$\n",
    "\n",
    "This is the LapRLS solution for supervised regression and classification.\n",
    "\n",
    "### Connection to Regularized Least Squares (RLS)\n",
    "\n",
    "LapRLS contains the regularized least squares (RLS) as a special case. The RLS algorithm for non-graph signals is a fully supervised method where the optimization problem is:\n",
    "\n",
    "$$\n",
    "f^* = \\arg \\min_f \\left\\{ \\frac{1}{2} \\sum_{i=1}^l (y_i - f(x_i))^2 + \\frac{\\gamma_A}{2} \\|f\\|_K^2 \\right\\}.\n",
    "$$\n",
    "\n",
    "By the classical representer theorem, the solution is given by:\n",
    "\n",
    "$$\n",
    "f^*(x) = \\sum_{i=1}^l \\alpha_i^* K(x, x_i).\n",
    "$$\n",
    "\n",
    "Substituting this into the optimization problem yields:\n",
    "\n",
    "$$\n",
    "\\alpha^* = \\arg \\min_\\alpha \\left\\{ \\frac{1}{2} (y - K \\alpha)^T (y - K \\alpha) + \\frac{\\gamma_A}{2} \\alpha^T K \\alpha \\right\\}.\n",
    "$$\n",
    "\n",
    "From the first-order optimization condition \\( \\frac{\\partial V(\\alpha)}{\\partial \\alpha} = 0 \\), we obtain the solution:\n",
    "\n",
    "$$\n",
    "\\alpha^* = \\left( K + \\gamma_A l I \\right)^{-1} y.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4ee3c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.linalg import inv\n",
    "\n",
    "class LapRLS:\n",
    "    def __init__(self, gamma_A=1.0, gamma_l=1.0, kernel='rbf', sigma=1.0):\n",
    "        \"\"\"\n",
    "        Initialize the LapRLS model.\n",
    "        :param gamma_A: Regularization parameter for the function norm.\n",
    "        :param gamma_l: Regularization parameter for the Laplacian term.\n",
    "        :param kernel: Kernel type ('linear', 'poly', 'rbf').\n",
    "        :param sigma: Parameter for the RBF kernel.\n",
    "        \"\"\"\n",
    "        self.gamma_A = gamma_A\n",
    "        self.gamma_l = gamma_l\n",
    "        self.kernel_type = kernel\n",
    "        self.sigma = sigma\n",
    "        self.alpha = None\n",
    "\n",
    "    def _kernel(self, X, Y=None):\n",
    "        \"\"\"\n",
    "        Compute the kernel matrix.\n",
    "        :param X: Input data.\n",
    "        :param Y: Second input data (optional, for non-square kernel matrices).\n",
    "        :return: Kernel matrix.\n",
    "        \"\"\"\n",
    "        if Y is None:\n",
    "            Y = X\n",
    "\n",
    "        if self.kernel_type == 'linear':\n",
    "            return np.dot(X, Y.T)\n",
    "        elif self.kernel_type == 'poly':\n",
    "            return (np.dot(X, Y.T) + 1) ** 3\n",
    "        elif self.kernel_type == 'rbf':\n",
    "            pairwise_sq_dists = squareform(pdist(X, 'sqeuclidean'))\n",
    "            K = np.exp(-pairwise_sq_dists / (2 * self.sigma ** 2))\n",
    "            return K\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported kernel type.\")\n",
    "\n",
    "    def _laplacian_matrix(self, X):\n",
    "        \"\"\"\n",
    "        Compute the graph Laplacian matrix.\n",
    "        :param X: Input data.\n",
    "        :return: Laplacian matrix.\n",
    "        \"\"\"\n",
    "        pairwise_sq_dists = squareform(pdist(X, 'sqeuclidean'))\n",
    "        W = np.exp(-pairwise_sq_dists / (2 * self.sigma ** 2))\n",
    "        D = np.diag(W.sum(axis=1))\n",
    "        L = D - W\n",
    "        return L\n",
    "\n",
    "    def fit(self, X_labeled, y_labeled, X_unlabeled):\n",
    "        \"\"\"\n",
    "        Fit the LapRLS model using labeled and unlabeled data.\n",
    "        :param X_labeled: Labeled input data.\n",
    "        :param y_labeled: Labels corresponding to the labeled data.\n",
    "        :param X_unlabeled: Unlabeled input data.\n",
    "        \"\"\"\n",
    "        X = np.vstack((X_labeled, X_unlabeled))\n",
    "        l, u = X_labeled.shape[0], X_unlabeled.shape[0]\n",
    "\n",
    "        # Compute the kernel matrix\n",
    "        K = self._kernel(X)\n",
    "\n",
    "        # Compute the Laplacian matrix\n",
    "        L = self._laplacian_matrix(X)\n",
    "\n",
    "        # Create diagonal matrix J\n",
    "        J = np.zeros((l + u, l + u))\n",
    "        J[:l, :l] = np.eye(l)\n",
    "\n",
    "        # Solve for alpha\n",
    "        A = J @ K + self.gamma_A * l * np.eye(l + u) + self.gamma_l * l * L @ K\n",
    "        self.alpha = inv(A) @ J @ y_labeled\n",
    "\n",
    "    def predict(self, X_test, X_train=None):\n",
    "        \"\"\"\n",
    "        Make predictions using the fitted model.\n",
    "        :param X_test: Test input data.\n",
    "        :param X_train: Training input data.\n",
    "        :return: Predicted labels.\n",
    "        \"\"\"\n",
    "        if X_train is None:\n",
    "            X_train = self.X\n",
    "\n",
    "        K_test = self._kernel(X_test, X_train)\n",
    "        return K_test @ self.alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4365bf7a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16334/1243637321.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Create and fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mlaprls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLapRLS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgamma_A\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma_l\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rbf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mlaprls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_labeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_labeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_unlabeled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Predict on new data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_16334/965432333.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_labeled, y_labeled, X_unlabeled)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;31m# Solve for alpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJ\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mK\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma_A\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma_l\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mL\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mJ\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0my_labeled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 5)"
     ]
    }
   ],
   "source": [
    "# Example data (labeled and unlabeled)\n",
    "X_labeled = np.array([[1, 2], [2, 3], [3, 4]])\n",
    "y_labeled = np.array([1, -1, 1])\n",
    "X_unlabeled = np.array([[4, 5], [5, 6]])\n",
    "\n",
    "# Create and fit the model\n",
    "laprls = LapRLS(gamma_A=0.1, gamma_l=0.1, kernel='rbf', sigma=1.0)\n",
    "laprls.fit(X_labeled, y_labeled, X_unlabeled)\n",
    "\n",
    "# Predict on new data\n",
    "X_test = np.array([[2, 3], [3, 4], [6, 7]])\n",
    "predictions = laprls.predict(X_test, X_labeled)\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d7722c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
