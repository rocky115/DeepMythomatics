{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d35ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " * Copyright (c) 2018 Radhamadhab Dalai\n",
    " *\n",
    " * Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    " * of this software and associated documentation files (the \"Software\"), to deal\n",
    " * in the Software without restriction, including without limitation the rights\n",
    " * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    " * copies of the Software, and to permit persons to whom the Software is\n",
    " * furnished to do so, subject to the following conditions:\n",
    " *\n",
    " * The above copyright notice and this permission notice shall be included in\n",
    " * all copies or substantial portions of the Software.\n",
    " *\n",
    " * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    " * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    " * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    " * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    " * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    " * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
    " * THE SOFTWARE.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7380e5",
   "metadata": {},
   "source": [
    "# SVM-Recursive Feature Elimination (SVM-RFE)\n",
    "\n",
    "The decision function for SVM is defined as a linear discriminant function:\n",
    "\n",
    "$$ D(x) = \\langle w, x \\rangle + b, \\tag{8.4.58} $$\n",
    "\n",
    "where \\( w \\) is the weight vector and \\( b \\) is the bias term.\n",
    "\n",
    "The optimal hyperplane \\( (w, b) \\) is constructed such that:\n",
    "\n",
    "$$ w^\\top_{\\text{opt}} x + b_{\\text{opt}} = 0, \\tag{8.4.59} $$\n",
    "\n",
    "which separates a set of training data \\( (x_1, y_1), \\ldots, (x_n, y_n) \\). Vectors \\( x_i \\) such that:\n",
    "\n",
    "$$ y_i (w^\\top x_i + b) = 1 $$\n",
    "\n",
    "are termed **support vectors**. The constrained optimization problem for finding the optimal weight vector \\( w \\) is formulated as:\n",
    "\n",
    "$$ \\min_{w,b} f(w,b) = \\frac{1}{2} \\|w\\|_2^2, \\tag{8.4.60} $$\n",
    "\n",
    "subject to:\n",
    "\n",
    "$$ y_i (x_i^\\top w + b) \\geq 1, \\quad i = 1, \\ldots, n, \\tag{8.4.61} $$\n",
    "\n",
    "where the inequality constraint ensures that \\( x \\) corresponds to support vectors.\n",
    "\n",
    "### Lagrangian Form\n",
    "\n",
    "The above constrained optimization problem can be written in the Lagrangian form as:\n",
    "\n",
    "$$ \\min_{w,b,\\alpha} L(w, b, \\alpha) = \\frac{1}{2} \\|w\\|_2^2 - \\sum_{i=1}^{n} \\alpha_i \\left[ y_i (x_i^\\top w + b) - 1 \\right], \\tag{8.4.62} $$\n",
    "\n",
    "where \\( \\alpha = [\\alpha_1, \\ldots, \\alpha_n]^\\top \\) is the vector of non-negative Lagrange multipliers \\( \\alpha_i \\geq 0 \\), \\( i = 1, \\ldots, n \\).\n",
    "\n",
    "### Optimization Conditions\n",
    "\n",
    "The optimization conditions for \\( w \\) and \\( b \\) are given by:\n",
    "\n",
    "$$ \\frac{\\partial L(w, b, \\alpha)}{\\partial w} = w - \\sum_{i=1}^{n} \\alpha_i y_i x_i = 0 \\quad \\Rightarrow \\quad w = \\sum_{i=1}^{n} \\alpha_i y_i x_i, \\tag{8.4.63} $$\n",
    "\n",
    "$$ \\frac{\\partial L(w, b, \\alpha)}{\\partial b} = \\sum_{i=1}^{n} \\alpha_i y_i = 0, \\tag{8.4.64} $$\n",
    "\n",
    "Substituting these into Equation (8.4.62) yields:\n",
    "\n",
    "$$ \\min_{\\alpha} J(\\alpha) = \\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_i \\alpha_j y_i y_j x_i^\\top x_j - \\sum_{i=1}^{n} \\alpha_i, $$\n",
    "\n",
    "subject to \\( 0 \\leq \\alpha \\leq C \\mathbf{1} \\) and \\( \\alpha^\\top y = 0 \\). \\( (8.4.65) \\)\n",
    "\n",
    "For nonlinear binary classification, this becomes:\n",
    "\n",
    "$$ \\min_{\\alpha} J(\\alpha) = \\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_i \\alpha_j y_i y_j \\phi(x_i)^\\top \\phi(x_j) - \\sum_{i=1}^{n} \\alpha_i, \\tag{8.4.66} $$\n",
    "\n",
    "subject to the same constraints.\n",
    "\n",
    "### SVM-RFE Algorithm\n",
    "\n",
    "The goal of the SVM-Recursive Feature Elimination (SVM-RFE) algorithm is to find a subset of features that maximizes the performance of the predictor. The algorithm works as follows:\n",
    "\n",
    "$$ \\|\\mathbf{w}\\|_2^2 - \\|\\mathbf{w}^{(i)}\\|_2^2 = 2 \\sum_{j=1}^{d} \\sum_{k=1}^{d} \\alpha_j \\alpha_k y_j y_k \\left[ K(x_j, x_k) - K^{(i)}(x_j, x_k) \\right], \\tag{8.4.67} $$\n",
    "\n",
    "where \\( K^{(i)}(x_j, x_k) = \\langle \\phi(x_j^{(i)}), \\phi(x_k^{(i)}) \\rangle \\) is the Gram matrix of the training data with feature \\( i \\) removed.\n",
    "\n",
    "### Algorithm Steps\n",
    "\n",
    "1. **Input:** Training data \\( X = \\{x_1, \\ldots, x_d\\} \\), class labels \\( Y = \\{y_1, \\ldots, y_d\\} \\), and expected feature number \\( r \\).\n",
    "2. **Initialization:** Index subset of surviving features \\( S = \\{1, \\ldots, d\\} \\).\n",
    "3. **Repeat:**\n",
    "   - Restrict training examples to good feature indices \\( (X, Y) \\).\n",
    "   - Solve Equation (8.4.65) or (8.4.66) for the classifier \\( \\alpha \\).\n",
    "   - Compute the weight vector of dimension \\( m = \\text{length}(S) \\) as:\n",
    "   \n",
    "     $$ w = \\sum_{k=1}^{m} \\alpha_k y_k x_k, $$\n",
    "\n",
    "   - Compute the ranking criterion \\( c_i = (w_i)^2 \\) for all \\( i = 1, \\ldots, m \\).\n",
    "   - Find the feature index with the smallest ranking criterion:\n",
    "   \n",
    "     $$ i = \\arg \\min \\{c_1, \\ldots, c_m\\}, $$\n",
    "\n",
    "   - Eliminate the variable \\( i \\) with the smallest ranking criterion and update \\( X \\leftarrow X \\setminus x_i \\), \\( Y \\leftarrow Y \\setminus y_i \\), and \\( S \\leftarrow S \\setminus i \\).\n",
    "4. **Until:** \\( \\text{length}(S) = r \\).\n",
    "5. **Output:** Feature ranked list \\( X \\).\n",
    "\n",
    "This process iteratively removes the least important features based on their ranking criterion until only the top \\( r \\) features remain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec3bf9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surviving feature indices after SVM-RFE: [0, 2]\n"
     ]
    }
   ],
   "source": [
    "# Simple Python implementation of SVM-RFE without using libraries like NumPy or scikit-learn\n",
    "\n",
    "import random\n",
    "\n",
    "# SVM Decision function\n",
    "def decision_function(w, x, b):\n",
    "    return sum(w[i] * x[i] for i in range(len(w))) + b\n",
    "\n",
    "# Dot product between two vectors\n",
    "def dot_product(v1, v2):\n",
    "    return sum(v1[i] * v2[i] for i in range(len(v1)))\n",
    "\n",
    "# Training SVM (simplified version)\n",
    "def train_svm(X, Y, C=1.0):\n",
    "    n = len(X)\n",
    "    d = len(X[0])\n",
    "    \n",
    "    # Initialize weights and bias\n",
    "    w = [0.0] * d\n",
    "    b = 0.0\n",
    "    alpha = [0.0] * n  # Lagrange multipliers\n",
    "\n",
    "    # Simplified Gradient Descent for optimization (simplified for small datasets)\n",
    "    for epoch in range(100):  # max iterations\n",
    "        for i in range(n):\n",
    "            if Y[i] * decision_function(w, X[i], b) <= 1:\n",
    "                for j in range(d):\n",
    "                    w[j] += C * Y[i] * X[i][j]\n",
    "                b += C * Y[i]\n",
    "    \n",
    "    return w, b\n",
    "\n",
    "# SVM-RFE algorithm\n",
    "def svm_rfe(X, Y, r):\n",
    "    # X is the feature matrix, Y is the labels\n",
    "    n = len(X)\n",
    "    d = len(X[0])\n",
    "\n",
    "    # Feature indices that survive the elimination\n",
    "    S = list(range(d))\n",
    "\n",
    "    while len(S) > r:\n",
    "        # Train SVM\n",
    "        X_reduced = [[X[i][j] for j in S] for i in range(n)]\n",
    "        w, b = train_svm(X_reduced, Y)\n",
    "\n",
    "        # Compute ranking criterion c_i = (w_i)^2\n",
    "        ranking_criteria = [(i, w[idx] ** 2) for idx, i in enumerate(S)]\n",
    "\n",
    "        # Find feature with smallest ranking criterion\n",
    "        i_min = min(ranking_criteria, key=lambda x: x[1])[0]\n",
    "\n",
    "        # Remove the feature with smallest ranking criterion\n",
    "        S.remove(i_min)\n",
    "\n",
    "    return S\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example dataset (small and simple for demo purposes)\n",
    "    X = [\n",
    "        [2.0, 1.0, 3.0],\n",
    "        [1.0, 4.0, 1.0],\n",
    "        [2.0, 3.0, 4.0],\n",
    "        [5.0, 4.0, 2.0],\n",
    "        [6.0, 3.0, 3.0],\n",
    "        [7.0, 2.0, 5.0]\n",
    "    ]\n",
    "    Y = [1, -1, 1, -1, 1, -1]\n",
    "\n",
    "    # Reduce to top 2 features\n",
    "    r = 2\n",
    "    surviving_features = svm_rfe(X, Y, r)\n",
    "\n",
    "    print(\"Surviving feature indices after SVM-RFE:\", surviving_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04681121",
   "metadata": {},
   "source": [
    "# Support Vector Machine Multiclass Classification\n",
    "\n",
    "A multiclass classifier is a function $ H: X \\to Y $ that maps an instance $ x \\in X $ (for example, $ X = \\mathbb{R}^n $) into an element $ y \\in Y $ (for example, $ y \\in \\{1, \\ldots, k\\} $).\n",
    "\n",
    "## Decomposition Methods for Multiclass Classification\n",
    "\n",
    "A popular way to solve a \\(k\\)-class problem is to decompose it into a set of \\(L\\) binary classification problems. Three of the most common decomposition approaches are:\n",
    "\n",
    "- **One-Versus-All (OVA)** (or One-Against-All)\n",
    "- **One-Versus-One (OVO)** (or One-Against-One)\n",
    "\n",
    "### One-Against-All Method\n",
    "\n",
    "In the one-against-all (OVA) approach, we construct \\( L = k \\) binary classifiers $ C_m $,  m = 1, $\\ldots$, k . The \\( i \\)-th SVM is trained with all of the examples in the \\( i \\)-th class as positive labels, and all other examples as negative labels.\n",
    "\n",
    "Let  S = {($x_1$, $y_1$), $\\ldots$, ($x_N$, $y_N$)}  be a set of \\( N \\) training examples, where each $ x_i $ is drawn from a domain $ X \\subseteq \\mathbb{R}^n $ and $ y_i \\in \\{1, $\\ldots$, k\\} $ represents the class of $ x_i $.\n",
    "\n",
    "The optimization problem for the \\( m \\)-th one-against-all classifier is formulated as:\n",
    "\n",
    "$$\n",
    "\\min_{w_m, b_m, \\xi_m} \\frac{1}{2} \\|w_m\\|^2 + C \\sum_{i=1}^{N} \\xi_{m,i}\n",
    "$$\n",
    "subject to:\n",
    "$$\n",
    "w_m^T \\phi(x_i) + b_m \\geq 1 - \\xi_{m,i}, \\quad \\text{if } y_i = m,\n",
    "$$\n",
    "$$\n",
    "w_m^T \\phi(x_i) + b_m \\leq -1 + \\xi_{m,i}, \\quad \\text{if } y_i \\neq m,\n",
    "$$\n",
    "$$\n",
    "\\xi_{m,i} \\geq 0, \\quad i = 1, \\ldots, N,\n",
    "$$\n",
    "where \\( w_m \\) is the weight vector for the \\( m \\)-th class, \\( C \\) is the penalty parameter, and \\( \\xi_{m,i} \\) represents the training error for the \\( i \\)-th data point. \n",
    "\n",
    "The decision function is represented as:\n",
    "\n",
    "$$\n",
    "f(x) = \\arg \\max_{m=1,\\ldots,k} \\left( w_m^T \\phi(x) + b_m \\right)\n",
    "$$\n",
    "\n",
    "### One-Against-One Method\n",
    "\n",
    "In the one-against-one (OVO) method, we construct $ L = \\frac{k(k-1)}{2} $ binary classifiers, each distinguishing between two classes. For every pair of classes \\( i \\) and \\( j \\), a binary classifier is trained using the examples from the \\( i \\)-th class as positive and the examples from the \\( j \\)-th class as negative. This results in \\( k(k-1)/2 \\) classifiers.\n",
    "\n",
    "For training data from the \\( i \\)-th and \\( j \\)-th classes, the optimization problem is:\n",
    "\n",
    "$$\n",
    "\\min_{w^{ij}, b^{ij}, \\xi^{ij}} \\frac{1}{2} \\left( w^{ij} \\right)^T w^{ij} + C \\sum_{n=1}^{N} \\xi_n^{ij}\n",
    "$$\n",
    "subject to:\n",
    "$$\n",
    "\\left( w^{ij} \\right)^T \\phi(x_n) + b^{ij} \\geq 1 - \\xi_n^{ij}, \\quad \\text{if } y_n = i,\n",
    "$$\n",
    "$$\n",
    "\\left( w^{ij} \\right)^T \\phi(x_n) + b^{ij} \\leq -1 + \\xi_n^{ij}, \\quad \\text{if } y_n = j,\n",
    "$$\n",
    "$$\n",
    "\\xi_n^{ij} \\geq 0, \\quad n = 1, \\ldots, N.\n",
    "$$\n",
    "\n",
    "After training, the \"Max Wins\" voting strategy is used. Each binary classifier $ C_{ij} $ votes for either the \\( i \\)-th or \\( j \\)-th class. The final class of the new instance is predicted as the class with the most votes.\n",
    "\n",
    "### References\n",
    "- [1] Support Vector Machines, [21], [41], [48].\n",
    "- [2] One-Versus-One Method [13], [25].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6361ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SVM_OVA:\n",
    "    def __init__(self, learning_rate=0.001, lambda_param=0.01, n_iters=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lambda_param = lambda_param\n",
    "        self.n_iters = n_iters\n",
    "        self.classes = None\n",
    "        self.weights = None\n",
    "        self.biases = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # One-Versus-All: Get unique class labels\n",
    "        self.classes = np.unique(y)\n",
    "        n_classes = len(self.classes)\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Initialize weights and biases for each class\n",
    "        self.weights = np.zeros((n_classes, n_features))\n",
    "        self.biases = np.zeros(n_classes)\n",
    "\n",
    "        for idx, c in enumerate(self.classes):\n",
    "            # Convert labels to +1/-1 for binary classification for class c\n",
    "            binary_y = np.where(y == c, 1, -1)\n",
    "            self.weights[idx], self.biases[idx] = self._train(X, binary_y)\n",
    "\n",
    "    def _train(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        # Initialize weights and bias\n",
    "        w = np.zeros(n_features)\n",
    "        b = 0\n",
    "\n",
    "        # Gradient Descent\n",
    "        for _ in range(self.n_iters):\n",
    "            for i in range(n_samples):\n",
    "                condition = y[i] * (np.dot(X[i], w) - b) >= 1\n",
    "                if condition:\n",
    "                    # L2 regularization term\n",
    "                    w -= self.learning_rate * (2 * self.lambda_param * w)\n",
    "                else:\n",
    "                    # Update weights and bias\n",
    "                    w -= self.learning_rate * (2 * self.lambda_param * w - np.dot(X[i], y[i]))\n",
    "                    b -= self.learning_rate * y[i]\n",
    "\n",
    "        return w, b\n",
    "\n",
    "    def predict(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        predictions = np.zeros(n_samples)\n",
    "\n",
    "        # Predict using each classifier\n",
    "        for idx, (w, b) in enumerate(zip(self.weights, self.biases)):\n",
    "            linear_output = np.dot(X, w) + b\n",
    "            predictions += (linear_output >= 0) * (self.classes[idx])\n",
    "\n",
    "        return predictions\n",
    "\n",
    "# Sample Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample dataset (4 samples, 2 features)\n",
    "    X = np.array([[1, 2], [2, 3], [3, 4], [5, 6]])\n",
    "    y = np.array([0, 1, 0, 1])  # Two classes: 0 and 1\n",
    "\n",
    "    # Create and train the model\n",
    "    svm = SVM_OVA(learning_rate=0.001, lambda_param=0.01, n_iters=1000)\n",
    "    svm.fit(X, y)\n",
    "\n",
    "    # Predict on new samples\n",
    "    predictions = svm.predict(X)\n",
    "    print(\"Predictions:\", predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c1c4bf",
   "metadata": {},
   "source": [
    "# DAGSVM Method\n",
    "\n",
    "The Directed Acyclic Graph Support Vector Machine (DAGSVM) method is a variation of multiclass classification. It is called the DAGSVM method because, during the testing phase, it uses a **rooted binary directed acyclic graph** (DAG), while its training phase is the same as the one-against-one method.\n",
    "\n",
    "## Training Phase\n",
    "\n",
    "The training phase is identical to the one-against-one (OAO) method, solving \\( \\frac{k(k-1)}{2} \\) binary SVM classification problems for a \\(k\\)-class problem.\n",
    "\n",
    "## Testing Phase\n",
    "\n",
    "In the testing phase, instead of using a voting strategy like OVO, the DAGSVM method uses a **directed acyclic graph** (DAG). The DAG is rooted, with internal nodes and leaves. Each internal node corresponds to a binary SVM classifier for two classes, and the leaves correspond to the final class labels.\n",
    "\n",
    "For a \\(k\\)-class classification problem, a rooted binary DAG has \\(k\\) leaves and \\( \\frac{k(k-1)}{2} \\) internal nodes. Each node is a binary SVM classifier of two classes, and the graph is structured such that the nodes eliminate one class at a time from the list of possible classes until only one remains.\n",
    "\n",
    "### Definition: Decision Directed Acyclic Graph (DDAG)\n",
    "\n",
    "A **decision directed acyclic graph (DDAG)** is a graph whose edges have orientations and no cycles. It is defined as follows:\n",
    "\n",
    "Given a space \\(X\\) and a set of Boolean functions \\( F = \\{f: X \\to \\{0, 1\\} \\} \\), the decision directed acyclic graphs (DDAGs) on \\(k\\) classes over \\(F\\) are functions that can be implemented using a rooted binary DAG with \\(k\\) leaves labeled by the classes. Each of the \\(L = \\frac{k(k-1)}{2}\\) internal nodes is labeled with an element of \\(F\\).\n",
    "\n",
    "The nodes are arranged in a triangular structure:\n",
    "- The root node is at the top.\n",
    "- The second layer has two nodes.\n",
    "- The third layer has three nodes, and so on until the final layer has \\(k\\) leaves.\n",
    "\n",
    "Each internal node represents a binary SVM decision between two classes. The testing process eliminates classes from the list based on the decision of the nodes, eventually leaving only one class.\n",
    "\n",
    "### DDAG Testing Procedure\n",
    "\n",
    "1. Start with the root node.\n",
    "2. The binary decision at each node eliminates one class from the list.\n",
    "3. Continue evaluating the first and last elements of the updated list until only one class remains.\n",
    "\n",
    "Thus, for a \\(k\\)-class problem, \\(k-1\\) decision nodes are evaluated to determine the final class.\n",
    "\n",
    "### Example 8.1\n",
    "\n",
    "Consider \\(N\\) test samples \\( \\{x_i, y_i\\} \\), \\( i = 1, \\dots, N \\), where \\(x_i \\in \\mathbb{R}^n\\) and \\(y_i \\in \\{1, 2, 3, 4\\}\\). The DDAG method proceeds as follows:\n",
    "\n",
    "- Start with the list of classes \\(\\{1, 2, 3, 4\\}\\).\n",
    "- Evaluate the decision node corresponding to the first and last classes, \\(1\\) vs. \\(4\\).\n",
    "- If class \\(1\\) wins, eliminate class \\(4\\), leaving the list \\(\\{1, 2, 3\\}\\).\n",
    "- Evaluate the decision node \\(1\\) vs. \\(3\\). If class \\(1\\) wins again, eliminate class \\(3\\).\n",
    "- Continue until only one class remains.\n",
    "\n",
    "This decision process is shown below:\n",
    "\n",
    "$$\n",
    "\\text{Decision Directed Acyclic Graph (DDAG) for } k=4:\n",
    "$$\n",
    "\n",
    "\\[\n",
    "\\begin{aligned}\n",
    "1 \\text{ vs } 4 &\\rightarrow \\text{not } 1 \\quad & \\{2, 3, 4\\} \\\\\n",
    "2 \\text{ vs } 4 &\\rightarrow \\text{not } 2 \\quad & \\{3, 4\\} \\\\\n",
    "3 \\text{ vs } 4 &\\rightarrow \\text{Class } 3 \\quad & \\{3\\}\n",
    "\\end{aligned}\n",
    "\\]\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The DAGSVM method generalizes decision trees by efficiently eliminating classes in a directed acyclic graph. It provides an efficient testing procedure for multiclass classification by eliminating one class at each node.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7f46be",
   "metadata": {},
   "source": [
    "![Decision Directed Acyclic Graphs (DDAG)](svm5_1.png)\n",
    "Fig 1: The decision directed acyclic graphs (DDAG) for finding the best class out of four classes\n",
    "\n",
    "The decision directed acyclic graph (DDAG) eliminates one class from the list at each node. Starting at the root node with a decision between Class 1 and Class 4, the binary decision function evaluates which class remains. \n",
    "\n",
    "If the output value does not favor Class 1, Class 1 is eliminated from the list, yielding a new list:\n",
    "\n",
    "$$ \\{2, 3, 4\\} $$\n",
    "\n",
    "Next, the binary decision is made between Classes 2 and 4. \n",
    "\n",
    "If the root node prefers Class 1, then Class 4 is removed from the list, resulting in a new list:\n",
    "\n",
    "$$ \\{1, 2, 3\\} $$\n",
    "\n",
    "The next binary decision is made between Classes 1 and 3. \n",
    "\n",
    "At the second layer, there are two decision nodes:\n",
    "\n",
    "1. $$ 2 \\, \\text{vs} \\, 4 $$\n",
    "2. $$ 1 \\, \\text{vs} \\, 3 $$\n",
    "\n",
    "The DDAG continues this process, proceeding through each binary decision at every node, until only one class remains in the list. \n",
    "\n",
    "This process is depicted in Figure 8.2, showing the DDAG for the four classes. \n",
    "\n",
    "DDAGs generalize the structure of Decision Trees, enabling a more efficient representation of redundancies and repetitions within different branches of the tree by allowing decision paths to merge and streamline the classification process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b77ddb6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7653/2985546523.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# Build and use DDAG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mclassifiers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_ddag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_ddag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifiers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Predicted classes:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_7653/2985546523.py\u001b[0m in \u001b[0;36mpredict_ddag\u001b[0;34m(X_test, classifiers)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mdecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mdecision\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0;31m# Class i is preferred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mremaining_classes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_j\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def build_ddag(X_train, y_train):\n",
    "    classes = np.unique(y_train)\n",
    "    classifiers = {}\n",
    "    \n",
    "    # Train binary classifiers for each pair of classes\n",
    "    for i in range(len(classes)):\n",
    "        for j in range(i+1, len(classes)):\n",
    "            class_i = classes[i]\n",
    "            class_j = classes[j]\n",
    "            # Create binary labels for the i-th vs j-th classifier\n",
    "            y_binary = np.where(y_train == class_i, 1, -1)\n",
    "            \n",
    "            # Train SVM\n",
    "            clf = SVC(kernel='linear')\n",
    "            clf.fit(X_train, y_binary)\n",
    "            \n",
    "            # Store the classifier\n",
    "            classifiers[(class_i, class_j)] = clf\n",
    "    \n",
    "    return classifiers\n",
    "\n",
    "def predict_ddag(X_test, classifiers):\n",
    "    # Start with all classes\n",
    "    remaining_classes = set(c[0] for c in classifiers.keys()).union(c[1] for c in classifiers.keys())\n",
    "    \n",
    "    # Predict using the classifiers\n",
    "    for (class_i, class_j), clf in classifiers.items():\n",
    "        decision = clf.predict(X_test)\n",
    "        \n",
    "        if decision == 1:\n",
    "            # Class i is preferred\n",
    "            remaining_classes.discard(class_j)\n",
    "        else:\n",
    "            # Class j is preferred\n",
    "            remaining_classes.discard(class_i)\n",
    "    \n",
    "    return remaining_classes\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Build and use DDAG\n",
    "classifiers = build_ddag(X_train, y_train)\n",
    "predictions = predict_ddag(X_test, classifiers)\n",
    "\n",
    "print(\"Predicted classes:\", predictions)\n",
    "\n",
    "'''\n",
    "build_ddag Function:\n",
    "\n",
    "Trains binary SVM classifiers for each pair of classes.\n",
    "Uses SVC from scikit-learn for linear classification.\n",
    "predict_ddag Function:\n",
    "\n",
    "Uses the trained classifiers to determine which classes are remaining based on predictions.\n",
    "Loading and Splitting Data:\n",
    "\n",
    "Example uses the Iris dataset. Replace with your own dataset as needed.\n",
    "Include Image:\n",
    "\n",
    "Use Markdown to embed an image showing the DDAG structure.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7051c77e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.78\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Linear SVM Classifier implementation (simplified)\n",
    "class LinearSVM:\n",
    "    def __init__(self, learning_rate=0.01, epochs=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.weights = np.zeros(num_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        for _ in range(self.epochs):\n",
    "            for idx, x_i in enumerate(X):\n",
    "                if y[idx] * (np.dot(x_i, self.weights) + self.bias) < 1:\n",
    "                    self.weights += self.learning_rate * (y[idx] * x_i - 2 * 1/self.epochs * self.weights)\n",
    "                    self.bias += self.learning_rate * y[idx]\n",
    "                else:\n",
    "                    self.weights -= self.learning_rate * 2 * 1/self.epochs * self.weights\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.sign(np.dot(X, self.weights) + self.bias)\n",
    "\n",
    "# Training binary SVM classifiers for DAGSVM\n",
    "def train_binary_svm_classifiers(X_train, y_train):\n",
    "    classifiers = {}\n",
    "    classes = np.unique(y_train)\n",
    "    num_classes = len(classes)\n",
    "    \n",
    "    for i in range(num_classes):\n",
    "        for j in range(i + 1, num_classes):\n",
    "            svm = LinearSVM()\n",
    "            binary_labels = np.where(y_train == i, 1, -1)\n",
    "            svm.fit(X_train, binary_labels)\n",
    "            classifiers[(i, j)] = svm\n",
    "    \n",
    "    return classifiers, classes\n",
    "\n",
    "# Predict using the DAGSVM method\n",
    "def predict_dagsvm(classifiers, classes, X_test):\n",
    "    num_classes = len(classes)\n",
    "    \n",
    "    def classify(sample):\n",
    "        remaining_classes = list(classes)\n",
    "        \n",
    "        for i in range(num_classes - 1):\n",
    "            for j in range(i + 1, num_classes):\n",
    "                clf = classifiers[(i, j)]\n",
    "                pred = clf.predict([sample])[0]\n",
    "                if pred == 1:\n",
    "                    if j in remaining_classes:\n",
    "                        remaining_classes.remove(j)\n",
    "                else:\n",
    "                    if i in remaining_classes:\n",
    "                        remaining_classes.remove(i)\n",
    "                if len(remaining_classes) == 1:\n",
    "                    return remaining_classes[0]\n",
    "        \n",
    "        return remaining_classes[0]\n",
    "    \n",
    "    return np.array([classify(sample) for sample in X_test])\n",
    "\n",
    "# Main script\n",
    "if __name__ == \"__main__\":\n",
    "    # Example data (Iris dataset)\n",
    "    from sklearn.datasets import load_iris\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import accuracy_score\n",
    "\n",
    "    # Load and prepare data\n",
    "    data = load_iris()\n",
    "    X = data.data\n",
    "    y = data.target\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Train classifiers\n",
    "    classifiers, classes = train_binary_svm_classifiers(X_train, y_train)\n",
    "    \n",
    "    # Predict and evaluate\n",
    "    y_pred = predict_dagsvm(classifiers, classes, X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9908a0c7",
   "metadata": {},
   "source": [
    "## Least Squares SVM Multiclass Classifier\n",
    "\n",
    "In the multiclass case with \\( k \\) labels, LS-SVM uses \\( k \\) output nodes to encode the multiclasses, where \\( y_{i,j} \\) denotes the output value of the \\( j \\)-th output node for the training data \\( x_i \\). The primal optimization problem of LS-SVM can be represented as:\n",
    "\n",
    "$$\n",
    "\\min_{w_m, b_m, \\xi_{m,i}} \\sum_{m=1}^k \\left( \\frac{1}{2} \\| w_m \\|^2 + \\frac{C}{2} \\sum_{i=1}^N \\xi_{m,i} \\right)\n",
    "$$\n",
    "\n",
    "subject to:\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "w_m^T \\phi_m(x_i) + b_m = 1 - \\xi_{m,i}, & \\text{for } i = 1, \\dots, N \\text{ and } y_i = m \\\\\n",
    "w_m^T \\phi_m(x_i) + b_m \\geq 1 - \\xi_{m,i}, & \\text{for } i = 1, \\dots, N \\text{ and } y_i \\ne m\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where \\( w_m \\) and \\( b_m \\) are the weights and bias of the \\( m \\)-th classifier, \\( \\xi_{m,i} \\) is the slack variable, and \\( \\phi_m(x_i) \\) represents the feature transformation for the \\( m \\)-th class.\n",
    "\n",
    "### Dual Formulation\n",
    "\n",
    "The Lagrange function in the dual LS-SVM multiclass classifier is given by:\n",
    "\n",
    "$$\n",
    "L_D = \\frac{1}{2} \\sum_{m=1}^k \\left( \\frac{1}{2} \\| w_m \\|^2 + \\frac{C}{2} \\sum_{i=1}^N \\xi_{m,i} \\right) - \\sum_{i=1}^N \\sum_{m=1}^k \\alpha_{m,i} \\left( y_i w_m^T \\phi_m(x_i) + b_m - 1 + \\xi_{m,i} \\right)\n",
    "$$\n",
    "\n",
    "The conditions for optimality are:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L_D}{\\partial w_m} = 0 \\implies w_m = \\sum_{i=1}^N \\alpha_{m,i} y_i \\phi_m(x_i)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L_D}{\\partial b_m} = 0 \\implies \\sum_{i=1}^N \\alpha_{m,i} y_i = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L_D}{\\partial \\xi_{m,i}} = 0 \\implies \\alpha_{m,i} = C \\xi_{m,i}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L_D}{\\partial \\alpha_{m,i}} = 0 \\implies y_i w_m^T \\phi_m(x_i) + b_m - 1 + \\xi_{m,i} = 0\n",
    "$$\n",
    "\n",
    "### KKT Conditions in Matrix Form\n",
    "\n",
    "The KKT conditions can be rewritten in matrix form:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "0 & (y^{(m)})^T \\\\\n",
    "y^{(m)} & \\Omega^{(m)}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "b_m \\\\\n",
    "\\alpha_m\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "1 \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\n",
    "\\Omega^{(m)} = Z_m^T Z_m + C^{-1} I\n",
    "$$\n",
    "\n",
    "with entries:\n",
    "\n",
    "$$\n",
    "\\Omega^{(m)}_{ij} = y_i^{(m)} y_j^{(m)} K_m(x_i, x_j) + C^{-1} \\delta_{ij}\n",
    "$$\n",
    "\n",
    "and \\( K_m(x_i, x_j) = \\phi_m(x_i)^T \\phi_m(x_j) \\) is the kernel function for the \\( m \\)-th SVM.\n",
    "\n",
    "### LS-SVM Multiclass Classification Algorithm\n",
    "\n",
    "Algorithm 8.3 shows the LS-SVM multiclass classification algorithm:\n",
    "\n",
    "**Input**: Training set \\( \\{(x_i, y_i) \\mid x_i \\in \\mathbb{R}^n, y_i \\in \\{1, \\dots, k\\}, i = 1, \\dots, N\\} \\), constant \\( C > 0 \\), and the kernel function \\( K_m(x, x_i) \\), \\( m = 1, \\dots, k \\).\n",
    "\n",
    "**Initialization**: \n",
    "\n",
    "$$\n",
    "y_i^{(m)} = \\begin{cases}\n",
    "+1, & \\text{if } y_i = m \\\\\n",
    "-1, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**Learning Step**:\n",
    "\n",
    "1. Construct the \\( N \\times 1 \\) vector \\( y^{(m)} = [y_1^{(m)}, \\dots, y_N^{(m)}]^T \\).\n",
    "2. Use the kernel function to construct the entries of the \\( N \\times N \\) matrix \\( \\Omega^{(m)} \\).\n",
    "3. Solve the KKT matrix equation for \\( b_m \\) and \\( \\alpha_m = [\\alpha_{m,i}]_{i=1}^N \\).\n",
    "\n",
    "**Testing Step**:\n",
    "\n",
    "For a given test sample \\( x \\in \\mathbb{R}^n \\), the class is:\n",
    "\n",
    "$$\n",
    "\\text{class of } x = \\arg \\max_{m=1, \\dots, k} \\left( \\sum_{i=1}^N \\alpha_{m,i} y_i K_m(x, x_i) + b_m \\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83c4f075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import solve\n",
    "\n",
    "class LeastSquaresSVM:\n",
    "    def __init__(self, C=1.0, kernel='rbf', gamma=1.0):\n",
    "        self.C = C\n",
    "        self.kernel = kernel\n",
    "        self.gamma = gamma\n",
    "        self.alphas = None\n",
    "        self.b = None\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        self.K = None\n",
    "\n",
    "    def _kernel(self, X1, X2):\n",
    "        if self.kernel == 'rbf':\n",
    "            sq_dists = np.sum(X1**2, axis=1).reshape(-1, 1) + np.sum(X2**2, axis=1) - 2 * np.dot(X1, X2.T)\n",
    "            return np.exp(-self.gamma * sq_dists)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported kernel\")\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        self.K = self._kernel(X, X)\n",
    "        \n",
    "        N = X.shape[0]\n",
    "        num_classes = len(np.unique(y))\n",
    "        \n",
    "        # Construct the matrix for each class\n",
    "        alphas = np.zeros((N, num_classes))\n",
    "        biases = np.zeros(num_classes)\n",
    "\n",
    "        for m in range(num_classes):\n",
    "            y_m = np.where(y == m, 1, -1)\n",
    "            K_m = self.K + np.eye(N) / self.C\n",
    "            \n",
    "            # Solve for alpha and bias\n",
    "            alpha_m = solve(K_m, y_m)\n",
    "            alphas[:, m] = alpha_m\n",
    "            \n",
    "            # Calculate bias term for the m-th classifier\n",
    "            bias_m = np.mean(y_m - np.dot(self.K, alpha_m))\n",
    "            biases[m] = bias_m\n",
    "\n",
    "        self.alphas = alphas\n",
    "        self.b = biases\n",
    "\n",
    "    def predict(self, X):\n",
    "        K_test = self._kernel(X, self.X_train)\n",
    "        decision_values = np.dot(K_test, self.alphas) + self.b\n",
    "        return np.argmax(decision_values, axis=1)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    from sklearn.datasets import load_iris\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import accuracy_score\n",
    "\n",
    "    # Load dataset\n",
    "    data = load_iris()\n",
    "    X = data.data\n",
    "    y = data.target\n",
    "\n",
    "    # Split dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Train LS-SVM classifier\n",
    "    classifier = LeastSquaresSVM(C=1.0, kernel='rbf', gamma=0.1)\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Predict and evaluate\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fa85c0",
   "metadata": {},
   "source": [
    "## Proximal Support Vector Machine (PSVM) Multiclass Classification\n",
    "\n",
    "### Primal Constrained Optimization Problem\n",
    "\n",
    "The primal constrained optimization problem for the PSVM multiclass classifier is:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_{w_m, b_m, \\xi_{m,i}} \\quad & L_{PPSVM}(w_m, b_m, \\xi_{m,i}) \\\\\n",
    "= \\quad & \\frac{1}{2} \\sum_{m=1}^k \\left( \\|w_m\\|^2 + b_m^2 \\right) + \\frac{C}{2} \\sum_{i=1}^N \\xi_{m,i}^2 \\\\\n",
    "\\text{subject to} \\quad & y_i \\left( w_m^T \\phi_m(x_i) + b_m \\right) \\geq 1 - \\xi_{m,i}, \\\\\n",
    "& \\text{for } i = 1, \\ldots, N.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### Dual Unconstrained Optimization Problem\n",
    "\n",
    "The corresponding dual unconstrained optimization problem is:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_{\\alpha_{m,i}} \\quad & L_{DPSVM} \\\\\n",
    "= \\quad & \\frac{1}{2} \\sum_{m=1}^k \\left( \\|w_m\\|^2 + b_m^2 \\right) + \\frac{C}{2} \\sum_{i=1}^N \\xi_{m,i}^2 \\\\\n",
    "\\text{subject to} \\quad & \\sum_{i=1}^N \\alpha_{m,i} \\left( y_i^{(m)} \\left( w_m^T \\phi_m(x_i) + b_m \\right) - 1 + \\xi_{m,i} \\right) = 0.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### Optimality Conditions\n",
    "\n",
    "The optimality conditions are:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L_{DPSVM}}{\\partial w_m} = 0 \\implies w_m = \\sum_{i=1}^N \\alpha_{m,i} y_i^{(m)} \\phi_m(x_i),\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L_{DPSVM}}{\\partial b_m} = 0 \\implies b_m = \\sum_{i=1}^N \\alpha_{m,i} y_i^{(m)},\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L_{DPSVM}}{\\partial \\xi_{m,i}} = 0 \\implies \\xi_{m,i} = C^{-1} \\alpha_{m,i},\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L_{DPSVM}}{\\partial \\alpha_{m,i}} = 0 \\implies y_i^{(m)} \\left( w_m^T \\phi_m(x_i) + b_m \\right) - 1 + \\xi_{m,i} = 0.\n",
    "$$\n",
    "\n",
    "### KKT Matrix Equation\n",
    "\n",
    "The KKT matrix equation is:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^N \\alpha_{m,i} y_i^{(m)} = b_m,\n",
    "$$\n",
    "\n",
    "$$\n",
    "(C^{-1} I + Z_m^T Z_m + y_m y_m^T) \\alpha_m = 1,\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "Z_m = \\left[ y_1^{(m)} \\phi_m(x_1), \\ldots, y_N^{(m)} \\phi_m(x_N) \\right],\n",
    "$$\n",
    "\n",
    "$$\n",
    "y_m = \\left[ y_1^{(m)}, \\ldots, y_N^{(m)} \\right]^T,\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\alpha_m = \\left[ \\alpha_{m,1}, \\ldots, \\alpha_{m,N} \\right]^T.\n",
    "$$\n",
    "\n",
    "### PSVM Multiclass Classification Algorithm\n",
    "\n",
    "**Algorithm:**\n",
    "\n",
    "1. **Input**: Training set \\(\\{ (x_i, y_i) \\mid x_i \\in \\mathbb{R}^n, y_i \\in \\{1, \\ldots, k\\}, i = 1, \\ldots, N \\}\\), constant \\(C > 0\\), and kernel functions \\(K_m(x, x_i)\\) for \\(m = 1, \\ldots, k\\).\n",
    "\n",
    "2. **Initialization**: \n",
    "   - Set \\( y_i^{(m)} = 1 \\) if \\( y_i = m \\); otherwise, \\( y_i^{(m)} = -1 \\) for \\( m = 1, \\ldots, k \\) and \\( i = 1, \\ldots, N \\).\n",
    "\n",
    "3. **Learning Step**:\n",
    "   - For \\( m = 1, \\ldots, k \\):\n",
    "     1. Construct the \\( N \\times N \\) matrix \\( \\left[ Z_m^T Z_m \\right]_{ij} = y_i^{(m)} y_j^{(m)} K_m(x_i, x_j) \\).\n",
    "     2. Solve the KKT matrix equation \\( (C^{-1} I + Z_m^T Z_m + y_m y_m^T) \\alpha_m = 1 \\) for \\( \\alpha_m \\).\n",
    "     3. Compute \\( b_m = \\alpha_m^T y_m \\).\n",
    "\n",
    "4. **Testing Step**:\n",
    "   - For a given test sample \\( x \\in \\mathbb{R}^n \\), its class is given by:\n",
    "   $$\n",
    "   \\text{class of } x = \\arg \\max_{m=1, \\ldots, k} \\left( \\sum_{i=1}^N \\alpha_{m,i} y_i^{(m)} K_m(x, x_i) + b_m \\right).\n",
    "   $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7c8dcde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.89\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class PSVM:\n",
    "    def __init__(self, C=1.0, kernel='linear'):\n",
    "        self.C = C\n",
    "        self.kernel = kernel\n",
    "        self.kernels = {\n",
    "            'linear': lambda x, y: np.dot(x, y.T),\n",
    "            'poly': lambda x, y: (1 + np.dot(x, y.T)) ** 3,\n",
    "            'rbf': lambda x, y: np.exp(-np.linalg.norm(x[:, None] - y, axis=2) ** 2 / 2.0)\n",
    "        }\n",
    "        self.kernel_func = self.kernels[kernel]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        N, D = X.shape\n",
    "        k = len(np.unique(y))\n",
    "        \n",
    "        alpha = np.zeros((k, N))\n",
    "        b = np.zeros(k)\n",
    "        \n",
    "        # Construct Z_m and y_m\n",
    "        Z = [self.kernel_func(X, X) for _ in range(k)]\n",
    "        y_m = [np.where(y == m+1, 1, -1) for m in range(k)]\n",
    "        \n",
    "        # Learning step\n",
    "        for m in range(k):\n",
    "            # Construct matrix for (C^-1 I + Z_m^T Z_m + y_m y_m^T)\n",
    "            K_m = Z[m]\n",
    "            K_m_y = K_m * y_m[m][:, np.newaxis]  # Element-wise multiplication\n",
    "            M = (1 / self.C) * np.eye(N) + K_m @ K_m.T + np.outer(y_m[m], y_m[m])\n",
    "            \n",
    "            # Solve for alpha_m\n",
    "            alpha_m = np.linalg.solve(M, np.ones(N))\n",
    "            alpha[m, :] = alpha_m\n",
    "            b[m] = np.mean(alpha_m * y_m[m])\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        self.b = b\n",
    "        self.support_vectors = X\n",
    "        self.support_vector_labels = y\n",
    "\n",
    "    def predict(self, X):\n",
    "        k = len(self.b)\n",
    "        predictions = np.zeros((X.shape[0], k))\n",
    "        \n",
    "        for m in range(k):\n",
    "            K_m = self.kernel_func(X, self.support_vectors)\n",
    "            predictions[:, m] = K_m @ self.alpha[m, :] + self.b[m]\n",
    "        \n",
    "        return np.argmax(predictions, axis=1) + 1\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    from sklearn.datasets import load_iris\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    \n",
    "    # Load the Iris dataset\n",
    "    data = load_iris()\n",
    "    X = data.data\n",
    "    y = data.target + 1  # PSVM expects labels to start from 1\n",
    "    \n",
    "    # Split into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Initialize and train PSVM classifier\n",
    "    classifier = PSVM(C=1.0, kernel='rbf')\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict and evaluate\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
