{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaf8ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " * Copyright (c) 2018 Radhamadhab Dalai\n",
    " *\n",
    " * Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    " * of this software and associated documentation files (the \"Software\"), to deal\n",
    " * in the Software without restriction, including without limitation the rights\n",
    " * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    " * copies of the Software, and to permit persons to whom the Software is\n",
    " * furnished to do so, subject to the following conditions:\n",
    " *\n",
    " * The above copyright notice and this permission notice shall be included in\n",
    " * all copies or substantial portions of the Software.\n",
    " *\n",
    " * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    " * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    " * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    " * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    " * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    " * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
    " * THE SOFTWARE.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9901e03f",
   "metadata": {},
   "source": [
    "$$\n",
    "\\textbf{ν-Support Vector Machine Binary Classifier}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{The primal optimization problem of ν-SVC is described by:}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\min_{w, \\xi, \\rho} \\quad \\frac{1}{2} \\lVert w \\rVert^2 - \\nu \\rho + \\sum_{i=1}^{N} \\xi_i \n",
    "$$\n",
    "$$\n",
    "\\text{subject to:} \\quad y_i (w^T \\phi(x_i) + b) \\geq \\rho - \\xi_i, \\quad \\xi_i \\geq 0 \\quad (i = 1, \\dots, N), \\quad \\rho \\geq 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Let Lagrange multipliers} \\, \\alpha_i, \\beta_i, \\delta \\geq 0, \\text{ and the Lagrange function is:}\n",
    "$$\n",
    "\n",
    "$$\n",
    "L(w, \\xi, b, \\rho, \\alpha, \\beta, \\delta) = \\frac{1}{2} \\lVert w \\rVert^2 - \\nu \\rho + \\sum_{i=1}^{N} \\xi_i - \\delta \\rho - \\sum_{i=1}^{N} \\alpha_i \\left( y_i (w^T x_i + b) - \\rho + \\xi_i \\right) + \\sum_{i=1}^{N} \\beta_i \\xi_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{The first-order optimization conditions give:}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w} = 0 \\quad \\Rightarrow \\quad w = \\sum_{i=1}^{N} \\alpha_i y_i x_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\xi_i} = 0 \\quad \\Rightarrow \\quad \\alpha_i + \\beta_i = \\frac{1}{N}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b} = 0 \\quad \\Rightarrow \\quad \\sum_{i=1}^{N} \\alpha_i y_i = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\rho} = 0 \\quad \\Rightarrow \\quad \\sum_{i=1}^{N} \\alpha_i - \\delta = \\nu\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{The Wolfe dual optimization problem for ν-SVC is:}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\max_{\\alpha} W(\\alpha) = - \\frac{1}{2} \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\alpha_i \\alpha_j y_i y_j K(x_i, x_j)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{subject to:} \\quad 0 \\leq \\alpha_i \\leq \\frac{1}{N}, \\quad \\sum_{i=1}^{N} \\alpha_i y_i = 0, \\quad \\alpha_i \\geq \\nu\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{To compute the parameters \\( b \\) and \\( \\rho \\), define two sets} \\, S^+ \\, \\text{and} \\, S^-:\n",
    "$$\n",
    "\n",
    "$$\n",
    "s_1 = |S^+| = |\\{i \\mid 0 < \\alpha_i < 1, y_i = 1\\}|\n",
    "$$\n",
    "\n",
    "$$\n",
    "s_2 = |S^-| = |\\{i \\mid 0 < \\alpha_i < 1, y_i = -1\\}|\n",
    "$$\n",
    "\n",
    "$$\n",
    "r_1 = \\frac{1}{s_1} \\sum_{x \\in S^+} \\sum_{j=1}^{N} \\alpha_j y_j K(x, x_j)\n",
    "$$\n",
    "\n",
    "$$\n",
    "r_2 = \\frac{1}{s_2} \\sum_{x \\in S^-} \\sum_{j=1}^{N} \\alpha_j y_j K(x, x_j)\n",
    "$$\n",
    "\n",
    "$$\n",
    "b = -\\frac{r_1 - r_2}{2}, \\quad \\rho = \\frac{r_1 + r_2}{2}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95c6bfc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 93.33%\n",
      "Support Vectors:\n",
      "[[-0.48760622  1.34700796 -2.03812454 ... -0.40807537 -0.32602353\n",
      "   1.20121392]\n",
      " [-0.76325916  1.89033108 -0.65183611 ... -1.2110162  -1.66152006\n",
      "  -0.0660798 ]\n",
      " [-1.22576566  0.80742726  0.65232288 ...  0.88365994 -0.03850847\n",
      "  -0.1726273 ]\n",
      " ...\n",
      " [-0.9354387   0.15985512  0.7999419  ... -0.30317978 -0.3493168\n",
      "  -0.01941961]\n",
      " [ 0.84064355  0.37531604 -0.96697614 ...  0.42545756  0.76041466\n",
      "   0.78580016]\n",
      " [-0.42018682 -0.24038388  0.9843224  ... -0.99835404  0.23421473\n",
      "   1.55050049]]\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np\n",
    "from sklearn.svm import NuSVC\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate a random binary classification dataset\n",
    "X, y = make_classification(n_samples=100, n_features=20, n_classes=2, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize ν-SVC with a kernel\n",
    "nu = 0.5  # ν parameter\n",
    "classifier = NuSVC(nu=nu, kernel='rbf', gamma='scale')\n",
    "\n",
    "# Train the classifier\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Support Vectors\n",
    "support_vectors = classifier.support_vectors_\n",
    "print(f'Support Vectors:\\n{support_vectors}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4aaf9365",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (70,) (100,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8958/3165143181.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;31m# Train ν-SVM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;31m# Test on some data points\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_8958/3165143181.py\u001b[0m in \u001b[0;36mupdate_alpha\u001b[0;34m(X, y, alpha, C, kernel, tol, max_passes)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mnum_changed_alphas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mEi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mEi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mtol\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mEi\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mtol\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (70,) (100,) "
     ]
    }
   ],
   "source": [
    "def kernel(x1, x2):\n",
    "    return np.dot(x1, x2)  # Linear kernel\n",
    "N = len(X_train)  # Number of training samples\n",
    "alpha = np.zeros(N)  # Lagrange multipliers\n",
    "b = 0  # Bias term\n",
    "C = 1 / N  # Constant related to the number of samples\n",
    "rho = 0  # Threshold parameter\n",
    "def compute_objective(X, y, alpha, kernel):\n",
    "    # Dual objective function\n",
    "    total = 0\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            total += alpha[i] * alpha[j] * y[i] * y[j] * kernel(X[i], X[j])\n",
    "    return np.sum(alpha) - 0.5 * total\n",
    "def update_alpha(X, y, alpha, C, kernel, tol=1e-4, max_passes=10):\n",
    "    passes = 0\n",
    "    while passes < max_passes:\n",
    "        num_changed_alphas = 0\n",
    "        for i in range(N):\n",
    "            Ei = np.dot(alpha * y, kernel(X, X[i])) + b - y[i]\n",
    "            if (y[i] * Ei < -tol and alpha[i] < C) or (y[i] * Ei > tol and alpha[i] > 0):\n",
    "                j = np.random.randint(0, N)\n",
    "                while j == i:\n",
    "                    j = np.random.randint(0, N)\n",
    "                Ej = np.dot(alpha * y, kernel(X, X[j])) + b - y[j]\n",
    "                \n",
    "                alpha_i_old = alpha[i]\n",
    "                alpha_j_old = alpha[j]\n",
    "\n",
    "                # Compute bounds L and H\n",
    "                if y[i] != y[j]:\n",
    "                    L = max(0, alpha[j] - alpha[i])\n",
    "                    H = min(C, C + alpha[j] - alpha[i])\n",
    "                else:\n",
    "                    L = max(0, alpha[i] + alpha[j] - C)\n",
    "                    H = min(C, alpha[i] + alpha[j])\n",
    "\n",
    "                if L == H:\n",
    "                    continue\n",
    "                \n",
    "                # Update alpha_j\n",
    "                eta = 2 * kernel(X[i], X[j]) - kernel(X[i], X[i]) - kernel(X[j], X[j])\n",
    "                if eta >= 0:\n",
    "                    continue\n",
    "                alpha[j] -= y[j] * (Ei - Ej) / eta\n",
    "                alpha[j] = np.clip(alpha[j], L, H)\n",
    "                \n",
    "                if abs(alpha[j] - alpha_j_old) < tol:\n",
    "                    continue\n",
    "\n",
    "                # Update alpha_i\n",
    "                alpha[i] += y[i] * y[j] * (alpha_j_old - alpha[j])\n",
    "\n",
    "                # Compute b\n",
    "                b1 = b - Ei - y[i] * (alpha[i] - alpha_i_old) * kernel(X[i], X[i]) - y[j] * (alpha[j] - alpha_j_old) * kernel(X[i], X[j])\n",
    "                b2 = b - Ej - y[i] * (alpha[i] - alpha_i_old) * kernel(X[i], X[j]) - y[j] * (alpha[j] - alpha_j_old) * kernel(X[j], X[j])\n",
    "                if 0 < alpha[i] < C:\n",
    "                    b = b1\n",
    "                elif 0 < alpha[j] < C:\n",
    "                    b = b2\n",
    "                else:\n",
    "                    b = (b1 + b2) / 2\n",
    "\n",
    "                num_changed_alphas += 1\n",
    "        \n",
    "        if num_changed_alphas == 0:\n",
    "            passes += 1\n",
    "        else:\n",
    "            passes = 0\n",
    "\n",
    "    return alpha, b\n",
    "def predict(X, y, alpha, b, kernel, X_test):\n",
    "    y_pred = []\n",
    "    for i in range(len(X_test)):\n",
    "        prediction = np.dot(alpha * y, [kernel(x, X_test[i]) for x in X]) + b\n",
    "        y_pred.append(np.sign(prediction))\n",
    "    return np.array(y_pred)\n",
    "# Generate synthetic data\n",
    "N = 100\n",
    "X = np.random.randn(N, 2)\n",
    "y = np.where(np.dot(X, np.array([2, -1])) > 0, 1, -1)\n",
    "\n",
    "# Train ν-SVM\n",
    "alpha, b = update_alpha(X, y, alpha, C, kernel)\n",
    "\n",
    "# Test on some data points\n",
    "X_test = np.random.randn(10, 2)\n",
    "y_pred = predict(X, y, alpha, b, kernel, X_test)\n",
    "\n",
    "print('Predictions:', y_pred)\n",
    "# Fix it------- May be a small issue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee1510a",
   "metadata": {},
   "source": [
    "Standard SVMs are powerful tools for data classification by assigning them to one\n",
    "of two disjoint halfspaces in either the original input space for linear classifiers,\n",
    "or in a higher-dimensional feature space for nonlinear classifiers. The least squares\n",
    "SVMs (LS-SVMs) developed in [37] and the proximal SVMs (PSVMs) presented in\n",
    "[14, 15] are two much simpler classifiers, in which each class of points is assigned\n",
    "to the closest of two parallel planes (in input or feature space) such that they are\n",
    "pushed apart as far as possible.\n",
    "\n",
    "The LS-SVMs formulate the binary classification problem as:\n",
    "\n",
    "$$\n",
    "\\min_{\\mathbf{w}, b, \\mathbf{e}} \\quad \\frac{1}{2} \\|\\mathbf{w}\\|^2 + \\frac{C}{2} \\sum_{k=1}^{N} e_k\n",
    "$$\n",
    "subject to:\n",
    "$$\n",
    "y_k (\\mathbf{w}^T \\phi(x_k) + b) = 1 - e_k, \\quad k = 1, \\dots, N\n",
    "$$\n",
    "for the binary classification \\(y_k \\in \\{-1, 1\\}\\).\n",
    "\n",
    "Define the Lagrange function (Lagrangian):\n",
    "$$\n",
    "L = L(\\mathbf{w}, b, \\mathbf{e}; \\boldsymbol{\\alpha}) = \\frac{1}{2} \\|\\mathbf{w}\\|^2 + \\frac{C}{2} \\sum_{k=1}^{N} e_k - \\sum_{k=1}^{N} \\alpha_k \\left( y_k (\\mathbf{w}^T \\phi(x_k) + b) - 1 + e_k \\right)\n",
    "$$\n",
    "where \\(\\alpha_k\\) are Lagrange multipliers. Unlike Lagrange multipliers in SVM\n",
    "with inequality constraints, in LS-SVM, Lagrange multipliers \\(\\alpha_k\\) can be either\n",
    "positive or negative due to the equality constraints.\n",
    "\n",
    "Based on the KKT conditions, we get the optimality conditions as follows:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{w}} = 0 \\Rightarrow \\mathbf{w} = \\sum_{k=1}^{N} \\alpha_k y_k \\phi(x_k) \\Rightarrow \\mathbf{w} = Z \\boldsymbol{\\alpha}\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b} = 0 \\Rightarrow \\sum_{k=1}^{N} \\alpha_k y_k = 0 \\Rightarrow \\mathbf{y}^T \\boldsymbol{\\alpha} = 0\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{e}} = 0 \\Rightarrow \\alpha_k = C e_k, \\quad k = 1, \\dots, N \\Rightarrow \\boldsymbol{\\alpha} = C \\mathbf{e}\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\alpha_k} = 0 \\Rightarrow y_k (\\mathbf{w}^T \\phi(x_k) + b) - 1 + e_k = 0, \\quad k = 1, \\dots, N \\Rightarrow Z^T \\mathbf{w} + b \\mathbf{y} + \\mathbf{e} = 1\n",
    "$$\n",
    "\n",
    "Where \\(Z = [y_1 \\phi(x_1), \\dots, y_N \\phi(x_N)] \\in \\mathbb{R}^{m \\times N}\\), \\(\\mathbf{y} = [y_1, \\dots, y_N]^T\\), \\(\\mathbf{1} = [1, \\dots, 1]^T\\), \\(\\mathbf{e} = [e_1, \\dots, e_N]^T\\), and \\(\\boldsymbol{\\alpha} = [\\alpha_1, \\dots, \\alpha_N]^T\\).\n",
    "\n",
    "The KKT conditions can be written as a matrix equation:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "I & 0 & 0 & -Z \\\\\n",
    "0 & 0 & 0 & -\\mathbf{y}^T \\\\\n",
    "0 & 0 & C I & I \\\\\n",
    "-Z^T & -\\mathbf{y} & I & 0\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{w} \\\\\n",
    "b \\\\\n",
    "\\mathbf{e} \\\\\n",
    "\\boldsymbol{\\alpha}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "By eliminating \\(\\mathbf{w}\\) and \\(\\mathbf{e}\\), we simplify the KKT system as:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "0 & \\mathbf{y}^T \\\\\n",
    "\\mathbf{y} & Z^T Z + C^{-1} I\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "b \\\\\n",
    "\\boldsymbol{\\alpha}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "\\mathbf{1}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Mercer’s condition can be applied to the matrix \\(Z^T Z\\):\n",
    "\n",
    "$$\n",
    "[Z^T Z]_{ij} = y_i y_j \\phi(x_i)^T \\phi(x_j) = y_i y_j K(x_i, x_j)\n",
    "$$\n",
    "\n",
    "Given a training set \\(\\{(x_i, y_i) \\,|\\, x_i \\in \\mathbb{R}^n, y_i \\in \\{-1, 1\\}, i = 1, \\dots, N \\}\\), constant \\(C > 0\\), and the kernel function \\(K(x_i, x_j)\\), the LS-SVM binary classification algorithm performs the following learning step:\n",
    "\n",
    "- Construct the \\(N \\times N\\) matrix \\([Z^T Z]_{ij} = y_i y_j K(x_i, x_j)\\).\n",
    "- Solve the KKT matrix equation for \\(\\boldsymbol{\\alpha} = [\\alpha_1, \\dots, \\alpha_N]^T\\) and \\(b\\).\n",
    "\n",
    "In the testing step, for a given test sample \\(x \\in \\mathbb{R}^n\\), the decision function is:\n",
    "\n",
    "$$\n",
    "\\text{class of } x = \\text{sign}\\left( \\sum_{j=1}^{N} \\alpha_j y_j K(x, x_j) + b \\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403af059",
   "metadata": {},
   "source": [
    "$$\n",
    "b = \\sum_{i=1}^{N} \\alpha_i y_i\n",
    "\\tag{8.4.53}\n",
    "$$\n",
    "\n",
    "Here,\n",
    "\n",
    "$$\n",
    "Z = \\begin{bmatrix} y_1 x_1, \\dots, y_N x_N \\end{bmatrix} \\tag{8.4.54}\n",
    "$$\n",
    "\n",
    "$$\n",
    "y = \\begin{bmatrix} y_1, \\dots, y_N \\end{bmatrix}^T \\tag{8.4.55}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\alpha = \\begin{bmatrix} \\alpha_1, \\dots, \\alpha_N \\end{bmatrix}^T \\tag{8.4.56}\n",
    "$$\n",
    "\n",
    "Similar to LS-SVM, the training data \\( x \\) can be mapped from the input space \\( \\mathbb{R}^n \\) into the feature space \\( \\phi : x \\to \\phi(x) \\). Hence, the nonlinear PSVM classifier still has the KKT equation (8.4.52) with:\n",
    "\n",
    "$$\n",
    "Z = \\begin{bmatrix} y_1 \\phi(x_1), \\dots, y_N \\phi(x_N) \\end{bmatrix}\n",
    "\\tag{8.4.54}\n",
    "$$\n",
    "\n",
    "### Algorithm 8.1: PSVM Binary Classification Algorithm\n",
    "\n",
    "**Input:**  \n",
    "Training set \\( \\{(x_i, y_i) | x_i \\in \\mathbb{R}^n, y_i \\in \\{-1, 1\\}, i = 1, \\dots, N\\} \\), constant \\( C > 0 \\), and the kernel function \\( K(x, x_i) \\).\n",
    "\n",
    "**Initialization:**  \n",
    "\\( y = \\begin{bmatrix} y_1, \\dots, y_N \\end{bmatrix}^T \\)\n",
    "\n",
    "**Learning Step:**\n",
    "\n",
    "1. Construct the \\( N \\times N \\) matrix:\n",
    "   $$\n",
    "   [Z^T Z]_{ij} = y_i y_j \\phi^T(x_i) \\phi(x_j) = y_i y_j K(x_i, x_j)\n",
    "   $$\n",
    "\n",
    "2. Solve the KKT matrix equation (8.4.52) for:\n",
    "   $$\n",
    "   \\alpha = \\begin{bmatrix} \\alpha_1, \\dots, \\alpha_N \\end{bmatrix}^T\n",
    "   $$\n",
    "\n",
    "3. Compute:\n",
    "   $$\n",
    "   b = \\sum_{i=1}^{N} \\alpha_i y_i\n",
    "   $$\n",
    "\n",
    "**Testing Step:**  \n",
    "For a given testing sample \\( x \\in \\mathbb{R}^n \\), its decision is given by:\n",
    "\n",
    "$$\n",
    "\\text{class of } x = \\text{sign} \\left( \\sum_{i=1}^{N} \\alpha_i y_i K(x, x_i) + b \\right)\n",
    "$$\n",
    "\n",
    "The decision functions of binary SVM, LS-SVM, and PSVM classifiers have the same form:\n",
    "\n",
    "$$\n",
    "f(x) = \\text{sign} \\left( \\sum_{i=1}^{N} \\alpha_i y_i K(x, x_i) + b \\right) \\tag{8.4.57}\n",
    "$$\n",
    "\n",
    "where \\( y_i \\) is the corresponding target class label of the training data \\( x_i \\), \\( \\alpha_i \\) is the Lagrange multiplier, and \\( K(x, x_i) \\) is a suitable kernel function.\n",
    "\n",
    "### Comparisons of SVM, LS-SVM, and PSVM\n",
    "\n",
    "- **SVM**, **LS-SVM**, and **PSVM** are originally proposed for binary classification.\n",
    "- LS-SVM and PSVM provide faster implementations of traditional SVMs.\n",
    "- Both LS-SVM and PSVM use equality constraints instead of inequality constraints, avoiding quadratic programming and allowing a direct least-square solution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e457ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define a kernel function (linear or RBF kernel)\n",
    "def linear_kernel(x1, x2):\n",
    "    return np.dot(x1, x2)\n",
    "\n",
    "def rbf_kernel(x1, x2, sigma=1.0):\n",
    "    return np.exp(-np.linalg.norm(x1 - x2)**2 / (2 * (sigma**2)))\n",
    "\n",
    "# PSVM Implementation\n",
    "class PSVM:\n",
    "    def __init__(self, C=1.0, kernel=linear_kernel):\n",
    "        self.C = C  # Regularization parameter\n",
    "        self.kernel = kernel  # Kernel function\n",
    "    \n",
    "    # Fit the PSVM model\n",
    "    def fit(self, X, y):\n",
    "        N = X.shape[0]  # Number of samples\n",
    "        # Compute the kernel matrix K\n",
    "        K = np.array([[self.kernel(X[i], X[j]) for j in range(N)] for i in range(N)])\n",
    "\n",
    "        # Construct matrix A and vector B\n",
    "        A = np.vstack([np.hstack([K, np.ones((N, 1))]), np.hstack([y[:, np.newaxis].T, np.array([[0]])])])\n",
    "        B = np.hstack([np.zeros(N), [0]])\n",
    "        \n",
    "        # Solve the linear system A * alpha = B\n",
    "        result = np.linalg.solve(A, B)\n",
    "        self.alpha = result[:-1]  # Alpha coefficients\n",
    "        self.b = result[-1]  # Bias term\n",
    "\n",
    "    # Predict the class of new data points\n",
    "    def predict(self, X_test):\n",
    "        y_pred = []\n",
    "        for x in X_test:\n",
    "            decision_value = sum(self.alpha[i] * y[i] * self.kernel(X[i], x) for i in range(len(X))) + self.b\n",
    "            y_pred.append(np.sign(decision_value))\n",
    "        return np.array(y_pred)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example training data (binary classification)\n",
    "    X = np.array([[1, 2], [2, 3], [3, 3], [4, 5], [1, 0], [0, 1]])  # Training features\n",
    "    y = np.array([1, 1, 1, -1, -1, -1])  # Labels (+1 or -1)\n",
    "\n",
    "    # Initialize and train PSVM\n",
    "    psvm = PSVM(C=1.0, kernel=linear_kernel)  # Using linear kernel\n",
    "    psvm.fit(X, y)\n",
    "\n",
    "    # Predict on new data points\n",
    "    X_test = np.array([[3, 2], [1, 1], [2, 2]])\n",
    "    predictions = psvm.predict(X_test)\n",
    "\n",
    "    print(\"Predictions:\", predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9590ba1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
