{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369218ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " * Copyright (c) 2018 Radhamadhab Dalai\n",
    " *\n",
    " * Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    " * of this software and associated documentation files (the \"Software\"), to deal\n",
    " * in the Software without restriction, including without limitation the rights\n",
    " * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    " * copies of the Software, and to permit persons to whom the Software is\n",
    " * furnished to do so, subject to the following conditions:\n",
    " *\n",
    " * The above copyright notice and this permission notice shall be included in\n",
    " * all copies or substantial portions of the Software.\n",
    " *\n",
    " * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    " * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    " * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    " * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    " * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    " * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
    " * THE SOFTWARE.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d6b6f8",
   "metadata": {},
   "source": [
    "## Kernel Partial Least Squares Regression\n",
    "\n",
    "Given two data blocks \\( \\mathbf{X} \\) and \\( \\mathbf{Y} \\), the kernel partial least squares (PLS) regression is a natural extension of PLS regression. The key steps involved in the PLS regression are:\n",
    "\n",
    "1. $$ \\mathbf{w} = \\frac{\\mathbf{X}^T \\mathbf{u}}{\\mathbf{u}^T \\mathbf{u}} $$\n",
    "2. $$ \\mathbf{t} = \\mathbf{Xw} $$\n",
    "3. $$ \\mathbf{c} = \\frac{\\mathbf{Y}^T \\mathbf{t}}{\\mathbf{t}^T \\mathbf{t}} $$\n",
    "4. $$ \\mathbf{u} = \\frac{\\mathbf{Y}^T \\mathbf{c}}{\\mathbf{c}^T \\mathbf{c}} $$\n",
    "5. $$ \\mathbf{p} = \\frac{\\mathbf{X}^T \\mathbf{t}}{\\mathbf{t}^T \\mathbf{t}} $$\n",
    "6. $$ \\mathbf{q} = \\frac{\\mathbf{Y}^T \\mathbf{u}}{\\mathbf{u}^T \\mathbf{u}} $$\n",
    "7. $$ \\mathbf{X} = \\mathbf{X} - \\mathbf{tp}^T $$\n",
    "8. $$ \\mathbf{Y} = \\mathbf{Y} - \\mathbf{tc}^T $$\n",
    "\n",
    "These steps can also be expressed as:\n",
    "\n",
    "$$ \\mathbf{t} = \\frac{\\mathbf{X} \\mathbf{X}^T \\mathbf{u}}{\\mathbf{u}^T \\mathbf{u}} $$\n",
    "\n",
    "$$ \\mathbf{c} = \\mathbf{Y}^T \\mathbf{t} $$\n",
    "\n",
    "$$ \\mathbf{u} = \\frac{\\mathbf{Y}^T \\mathbf{c}}{\\mathbf{c}^T \\mathbf{c}} $$\n",
    "\n",
    "$$ \\mathbf{X} = \\mathbf{X} - \\mathbf{tt}^T \\mathbf{X} $$\n",
    "\n",
    "$$ \\mathbf{Y} = \\mathbf{Y} - \\mathbf{tc}^T $$\n",
    "\n",
    "Using \\( \\Phi = \\Phi(\\mathbf{X}) \\) instead of \\( \\mathbf{X} \\), the equations become:\n",
    "\n",
    "$$ \\mathbf{t} = \\frac{\\Phi \\Phi^T \\mathbf{u}}{\\mathbf{u}^T \\mathbf{u}} $$\n",
    "\n",
    "$$ \\Phi = \\Phi - \\mathbf{tt}^T \\Phi $$\n",
    "\n",
    "Therefore, the key steps of kernel nonlinear iterative partial least squares (NIPALS) regression are as follows:\n",
    "\n",
    "Given \\( \\Phi_0 = \\Phi \\) and the data block \\( \\mathbf{Y}_0 = \\mathbf{Y} \\):\n",
    "\n",
    "1. Randomly initialize \\( \\mathbf{u} \\).\n",
    "2. $$ \\mathbf{t} = \\Phi \\Phi^T \\mathbf{u}, \\quad \\mathbf{t} \\leftarrow \\frac{\\mathbf{t}}{\\mathbf{t}^T \\mathbf{t}} $$\n",
    "3. $$ \\mathbf{c} = \\mathbf{Y}^T \\mathbf{t} $$\n",
    "4. $$ \\mathbf{u} = \\mathbf{Y}^T \\mathbf{u}, \\quad \\mathbf{u} \\leftarrow \\frac{\\mathbf{u}}{\\mathbf{u}^T \\mathbf{u}} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88acd69a",
   "metadata": {},
   "source": [
    "\\textbf{Kernel NIPALS Regression Algorithm}\n",
    "\n",
    "5. Repeat Steps 2–4 until convergence of \\( t \\).\n",
    "\n",
    "6. Deflate the matrix:\n",
    "$$\n",
    "\\mathbf{T} = (\\mathbf{I} - \\mathbf{t}\\mathbf{t}^T)\\mathbf{T}(\\mathbf{I} - \\mathbf{t}\\mathbf{t}^T)^T.\n",
    "$$\n",
    "\n",
    "7. Deflate the matrix:\n",
    "$$\n",
    "\\mathbf{Y} = \\mathbf{Y} - \\mathbf{t}\\mathbf{c}^T.\n",
    "$$\n",
    "\n",
    "The kernel NIPALS regression is an iterative process: after extraction of the first component \\( t_1 \\), the algorithm starts again using the deflated matrices \\( \\mathbf{T} \\) and \\( \\mathbf{Y} \\) computed in Step 6 and Step 7, and repeat Steps 2–7 until the deflated matrix \\( \\mathbf{T} \\) or \\( \\mathbf{Y} \\) becomes a null matrix.\n",
    "\n",
    "Once two matrices \\( \\mathbf{T} = [t_1, \\ldots, t_p] \\) and \\( \\mathbf{U} = [u_1 , \\ldots, u_p] \\) are found by using the NIPALS regression algorithm, then the matrix regression coefficients \\( \\mathbf{B} \\) can be computed in the form similar to (6.9.26):\n",
    "$$\n",
    "\\mathbf{B} = \\mathbf{T}^0 \\mathbf{U}(\\mathbf{T}^T \\mathbf{T}^0 \\mathbf{U})^{-1} \\mathbf{T}^T \\mathbf{Y}^0.\n",
    "$$\n",
    "\n",
    "Then for a given new data block \\( X_{\\text{new}} \\) and \\( \\mathbf{T}_{\\text{new}} = \\mathbf{T}(X_{\\text{new}}) \\), the unknown \\( \\mathbf{Y} \\)-values can be predicted as:\n",
    "$$\n",
    "\\hat{\\mathbf{Y}}_{\\text{new}} = \\mathbf{T}_{\\text{new}}\\mathbf{B}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12776f96",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1 is different from 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7187/398673940.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkernel_nipals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;31m# Predict new data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_7187/398673940.py\u001b[0m in \u001b[0;36mkernel_nipals\u001b[0;34m(X, Y, n_components)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;31m# Step 3: Compute new t = (X @ u) / (u.T @ u)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mu_u\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mu\u001b[0m  \u001b[0;31m# Scalar value (1x1 matrix)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mt_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mu_u\u001b[0m  \u001b[0;31m# Shape (n_samples, 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;31m# Check for convergence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1 is different from 2)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def kernel_nipals(X, Y, n_components):\n",
    "    # X: Input data matrix (n_samples, n_features)\n",
    "    # Y: Output data matrix (n_samples, n_targets)\n",
    "    # n_components: Number of components to extract\n",
    "    \n",
    "    n_samples, n_features = X.shape\n",
    "    n_targets = Y.shape[1]\n",
    "    \n",
    "    # Initialize matrices to store results\n",
    "    T = np.zeros((n_samples, n_components))\n",
    "    P = np.zeros((n_features, n_components))\n",
    "    C = np.zeros((n_targets, n_components))\n",
    "    \n",
    "    for i in range(n_components):\n",
    "        # Step 1: Initialize t (randomly or with the first column of X)\n",
    "        t = X[:, 0].reshape(-1, 1)  # Shape (n_samples, 1)\n",
    "        \n",
    "        while True:\n",
    "            # Step 2: Compute u = (Y.T @ t) / (t.T @ t)\n",
    "            t_t = t.T @ t  # Scalar value (1x1 matrix)\n",
    "            u = (Y.T @ t) / t_t  # Shape (n_targets, 1)\n",
    "            u = u / np.linalg.norm(u)  # Normalize u\n",
    "            \n",
    "            # Step 3: Compute new t = (X @ u) / (u.T @ u)\n",
    "            u_u = u.T @ u  # Scalar value (1x1 matrix)\n",
    "            t_new = (X @ u) / u_u  # Shape (n_samples, 1)\n",
    "            \n",
    "            # Check for convergence\n",
    "            if np.linalg.norm(t_new - t) < 1e-10:\n",
    "                t = t_new\n",
    "                break\n",
    "            t = t_new\n",
    "        \n",
    "        # Step 4: Deflation\n",
    "        p = (X.T @ t) / t_t  # Shape (n_features, 1)\n",
    "        c = (Y.T @ t) / t_t  # Shape (n_targets, 1)\n",
    "        \n",
    "        # Store components\n",
    "        T[:, i] = t.flatten()\n",
    "        P[:, i] = p.flatten()\n",
    "        C[:, i] = c.flatten()\n",
    "        \n",
    "        # Deflate X and Y\n",
    "        X -= t @ p.T  # X shape (n_samples, n_features), p.T shape (1, n_features)\n",
    "        Y -= t @ c.T  # Y shape (n_samples, n_targets), c.T shape (1, n_targets)\n",
    "    \n",
    "    # Compute regression coefficients B\n",
    "    T_T_T = T.T @ T  # Shape (n_components, n_components)\n",
    "    T_T_Y = T.T @ Y  # Shape (n_components, n_targets)\n",
    "    B = np.linalg.pinv(T_T_T) @ T_T_Y  # Shape (n_features, n_targets)\n",
    "    \n",
    "    return B, T, P, C\n",
    "\n",
    "# Predict function\n",
    "def predict(X_new, B):\n",
    "    return X_new @ B  # Shape (n_samples, n_features) @ (n_features, n_targets)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example input data\n",
    "    X = np.array([[1, 2], [2, 3], [3, 4]])  # (3 samples, 2 features)\n",
    "    Y = np.array([[1], [2], [3]])           # (3 samples, 1 target)\n",
    "    \n",
    "    # Number of components\n",
    "    n_components = 1\n",
    "    \n",
    "    # Train the model\n",
    "    B, T, P, C = kernel_nipals(X, Y, n_components)\n",
    "    \n",
    "    # Predict new data\n",
    "    X_new = np.array([[4, 5]])  # (1 sample, 2 features)\n",
    "    Y_pred = predict(X_new, B)\n",
    "    \n",
    "    print(\"Predicted Y:\", Y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78eefeb8",
   "metadata": {},
   "source": [
    "### Laplacian Support Vector Machines (LapSVM)\n",
    "\n",
    "**Problem Statement:**\n",
    "\n",
    "Consider a set of labeled graph examples \\(\\{(x_i, y_i)\\}_{i=1}^l\\) and a set of unlabeled graph examples \\(\\{x_j\\}_{j=l+1}^{l+u}\\). The goal is to solve the following optimization problem for a semi-supervised SVM:\n",
    "\n",
    "$$\n",
    "f^* = \\arg\\min_f \\left[ \\frac{1}{l} \\sum_{i=1}^l \\left( 1 - y_i f(x_i) \\right)_+ + \\gamma_A \\frac{1}{2} \\|f\\|_K^2 + \\frac{(u - l)^2}{2} f^T L f \\right]\n",
    "$$\n",
    "\n",
    "where:\n",
    "- \\(\\left(1 - y_i f(x_i)\\right)_+\\) denotes the hinge loss for the \\(i\\)-th labeled example.\n",
    "- \\(\\gamma_A\\) is the regularization parameter for the kernel norm.\n",
    "- \\(L\\) is the Laplacian matrix of the graph.\n",
    "\n",
    "**Solution Representation:**\n",
    "\n",
    "According to the extended representer theorem, the solution \\(f^*\\) can be expressed as:\n",
    "\n",
    "$$\n",
    "f^* = \\sum_{i=1}^{l+u} \\alpha_i^* K(x, x_i) + b\n",
    "$$\n",
    "\n",
    "**Primal Problem for \\(\\alpha\\):**\n",
    "\n",
    "Substituting \\(f^*\\) into the optimization problem and adding a bias term \\(b\\), the primal problem for optimizing \\(\\alpha\\) can be written as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_{\\alpha, \\xi} & \\quad \\frac{1}{2} \\alpha^T \\left( \\gamma_A K + \\frac{(u - l)^2}{2} KLK \\right) \\alpha + \\sum_{i=1}^l \\xi_i \\\\\n",
    "\\text{subject to} & \\quad y_i \\left( \\sum_{j=1}^{l+u} \\alpha_j K(x_i, x_j) + b \\right) \\geq 1 - \\xi_i, \\quad \\forall i = 1, \\ldots, l \\\\\n",
    "& \\quad \\xi_i \\geq 0, \\quad \\forall i = 1, \\ldots, l\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Lagrangian Formulation:**\n",
    "\n",
    "The Lagrangian function incorporating the constraints is:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L(\\alpha, \\xi, b, \\beta, \\zeta) = & \\frac{1}{2} \\alpha^T \\left( \\gamma_A K + \\frac{(u - l)^2}{2} KLK \\right) \\alpha + \\sum_{i=1}^l \\xi_i \\\\\n",
    "& - \\sum_{i=1}^l \\beta_i \\left( y_i \\left( \\sum_{j=1}^{l+u} \\alpha_j K(x_i, x_j) + b \\right) - 1 + \\xi_i \\right) \\\\\n",
    "& - \\sum_{i=1}^l \\zeta_i \\xi_i\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "From the first-order optimization conditions, we get:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b} = 0 \\implies \\beta_i y_i = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\xi_i} = 0 \\implies \\beta_i + \\zeta_i = 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "0 \\leq \\beta_i \\leq \\zeta_i, \\quad \\text{where } \\zeta_i \\text{ and } \\xi_i \\text{ are nonnegative}\n",
    "$$\n",
    "\n",
    "**Reduced Lagrangian Function:**\n",
    "\n",
    "The reduced Lagrangian function is:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L_R(\\alpha, \\beta) = & \\frac{1}{2} \\alpha^T \\left( \\gamma_A K + \\frac{(u - l)^2}{2} KLK \\right) \\alpha - \\sum_{i=1}^l \\beta_i \\left( y_i \\left( \\sum_{j=1}^{l+u} \\alpha_j K(x_i, x_j) - 1 \\right) \\right) \\\\\n",
    "& + \\beta_i\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where \\(J = [1, \\ldots, 1, 0, \\ldots, 0]\\) is a \\(1 \\times (l + u)\\) matrix with the first \\(l\\) entries as 1 and the rest as 0, and \\(Y = \\text{Diag}(y_1, \\ldots, y_l)\\).\n",
    "\n",
    "From the first-order optimization condition:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L_R}{\\partial \\alpha} = \\gamma_A K \\alpha + \\frac{(u - l)^2}{2} KLK \\alpha - K J^T Y \\beta = 0\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61b78f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal alpha: [ 1.07962249 -1.14805856  1.14805856 -1.07962249]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_kernel(X, gamma=1.0):\n",
    "    \"\"\"Compute the Gaussian RBF kernel matrix.\"\"\"\n",
    "    pairwise_dists = np.linalg.norm(X[:, None] - X, axis=2) ** 2\n",
    "    K = np.exp(-gamma * pairwise_dists)\n",
    "    return K\n",
    "\n",
    "def compute_laplacian(K):\n",
    "    \"\"\"Compute the Laplacian matrix L from the kernel matrix K.\"\"\"\n",
    "    D = np.diag(K.sum(axis=1))\n",
    "    L = D - K\n",
    "    return L\n",
    "\n",
    "def solve_linear_system(A, b):\n",
    "    \"\"\"Solve the linear system Ax = b.\"\"\"\n",
    "    return np.linalg.solve(A, b)\n",
    "\n",
    "def laplacian_svm(X, y, gamma_A=1.0, C=1.0):\n",
    "    \"\"\"Train a Laplacian Support Vector Machine.\"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    K = compute_kernel(X)\n",
    "    L = compute_laplacian(K)\n",
    "    \n",
    "    # Define matrix for the quadratic term\n",
    "    A = gamma_A * K + (C / n_samples) * L\n",
    "    \n",
    "    # Define vector for the linear term\n",
    "    b = y\n",
    "    \n",
    "    # Solve the system A * alpha = b\n",
    "    alpha = solve_linear_system(A, b)\n",
    "    \n",
    "    return alpha\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample data\n",
    "    X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n",
    "    y = np.array([1, -1, 1, -1])\n",
    "    \n",
    "    # Parameters\n",
    "    gamma_A = 1.0\n",
    "    C = 1.0\n",
    "    \n",
    "    # Train the LapSVM\n",
    "    alpha = laplacian_svm(X, y, gamma_A=gamma_A, C=C)\n",
    "    \n",
    "    print(\"Optimal alpha:\", alpha)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30816a8",
   "metadata": {},
   "source": [
    "## Support Vector Machine Regression\n",
    "\n",
    "Support Vector Machine Regression (SVR) is an algorithm that looks for an optimal hyperplane to predict continuous values in a high-dimensional space.\n",
    "\n",
    "### 8.3.1 Support Vector Machine Regressor\n",
    "\n",
    "Given a training set of \\( N \\) data points \\( \\{ (x_k, y_k) \\}_{k=1}^N \\), where \\( x_k \\in \\mathbb{R}^n \\) is the \\( k \\)-th input pattern and \\( y_k \\in \\mathbb{R} \\) is the associated \"truth\", we aim to find a hyperplane \\( (w, b) \\) that satisfies certain conditions.\n",
    "\n",
    "The SVM learning algorithm seeks to minimize the following function:\n",
    "\n",
    "$$\n",
    "\\min_{w, b} \\quad \\frac{1}{2} \\|w\\|^2\n",
    "$$\n",
    "\n",
    "subject to:\n",
    "\n",
    "$$\n",
    "y_i \\left(w^T \\phi(x_i) - b \\right) \\geq \\epsilon\n",
    "$$\n",
    "\n",
    "where \\( \\phi : I \\subseteq \\mathbb{R}^n \\rightarrow F \\subseteq \\mathbb{R}^N \\) is a mapping from the input space \\( I \\) to the feature space \\( F \\), and \\( \\phi(x_i) \\) is the extracted feature of the input \\( x_i \\).\n",
    "\n",
    "Here, the distance between the point \\( x_i \\) and the decision boundary is given by:\n",
    "\n",
    "$$\n",
    "\\text{Quality} = \\left\\langle w, \\phi(x_i) \\right\\rangle - b\n",
    "$$\n",
    "\n",
    "The margin \\( \\gamma \\) is defined as:\n",
    "\n",
    "$$\n",
    "\\gamma = \\frac{1}{N} \\sum_{i=1}^N y_i \\left(\\left\\langle w, \\phi(x_i) \\right\\rangle - b \\right)\n",
    "$$\n",
    "\n",
    "The constrained optimization problem can be rewritten in Lagrangian form as an unconstrained optimization problem:\n",
    "\n",
    "$$\n",
    "\\min_{w, b} \\quad L(w, b) = \\frac{1}{2} \\|w\\|^2 - \\sum_{i=1}^N \\alpha_i \\left[ y_i \\left\\langle w, \\phi(x_i) \\right\\rangle - b \\right]\n",
    "$$\n",
    "\n",
    "where the Lagrange multipliers \\( \\alpha_i \\) are nonnegative.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8415679",
   "metadata": {},
   "source": [
    "## Optimization Conditions for Support Vector Machine Regression\n",
    "\n",
    "From the optimization conditions, we have:\n",
    "\n",
    "1. The gradient of the Lagrangian with respect to \\( w \\) is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L(w, b)}{\\partial w} = w - \\sum_{i=1}^N \\alpha_i y_i \\phi(x_i) = 0 \\quad \\Rightarrow \\quad w = \\sum_{i=1}^N \\alpha_i y_i \\phi(x_i)\n",
    "$$\n",
    "\n",
    "2. The gradient of the Lagrangian with respect to \\( b \\) is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L(w, b)}{\\partial b} = -\\sum_{i=1}^N \\alpha_i y_i = 0\n",
    "$$\n",
    "\n",
    "### Constrained Optimization with Respect to \\( \\alpha \\)\n",
    "\n",
    "Substituting these results into the original constrained optimization problem, we obtain the following dual optimization problems:\n",
    "\n",
    "#### Minimization Problem\n",
    "\n",
    "$$\n",
    "\\min_{\\alpha} \\quad J_1(\\alpha) = \\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j K(x_i, x_j) - \\sum_{i=1}^N \\alpha_i\n",
    "$$\n",
    "\n",
    "#### Maximization Problem\n",
    "\n",
    "$$\n",
    "\\max_{\\alpha} \\quad J_2(\\alpha) = \\sum_{i=1}^N \\alpha_i - \\frac{1}{2} \\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j K(x_i, x_j)\n",
    "$$\n",
    "\n",
    "subject to:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^N \\alpha_i y_i = 0\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\alpha_i \\geq 0, \\quad i = 1, \\ldots, N\n",
    "$$\n",
    "\n",
    "where \\( K(x_i, x_j) = \\langle \\phi(x_i), \\phi(x_j) \\rangle = \\phi(x_i)^T \\phi(x_j) \\) is the kernel function.\n",
    "\n",
    "### Algorithm for Support Vector Machine Regression\n",
    "\n",
    "1. Solve the maximization problem (8.3.8) with constraints (8.3.9) to obtain the Lagrange multipliers \\( \\alpha_i \\), \\( i = 1, \\ldots, N \\).\n",
    "2. Update the bias \\( b \\) using:\n",
    "\n",
    "$$\n",
    "b \\leftarrow b - \\eta \\sum_{i=1}^N \\alpha_i y_i\n",
    "$$\n",
    "\n",
    "3. Calculate the support vector regressor \\( w \\) using:\n",
    "\n",
    "$$\n",
    "w = \\sum_{i=1}^N \\alpha_i y_i \\phi(x_i)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30ec5d82",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cvxopt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7187/2107277591.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcvxopt\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolvers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m\"\"\"Compute the Gaussian RBF kernel matrix.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cvxopt'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from cvxopt import matrix, solvers\n",
    "\n",
    "def compute_kernel(X, gamma=1.0):\n",
    "    \"\"\"Compute the Gaussian RBF kernel matrix.\"\"\"\n",
    "    pairwise_dists = np.linalg.norm(X[:, None] - X, axis=2) ** 2\n",
    "    K = np.exp(-gamma * pairwise_dists)\n",
    "    return K\n",
    "\n",
    "def svr(X, y, C=1.0, epsilon=0.1, gamma=1.0):\n",
    "    \"\"\"Train a Support Vector Machine Regressor using quadratic programming.\"\"\"\n",
    "    N = X.shape[0]\n",
    "    K = compute_kernel(X, gamma)\n",
    "    \n",
    "    # Define the parameters for quadratic programming\n",
    "    H = np.outer(y, y) * K\n",
    "    H = (H + H.T) / 2  # Ensure symmetry\n",
    "    P = matrix(H)\n",
    "    \n",
    "    # Linear term\n",
    "    q = np.ones(N) * -1\n",
    "    q = matrix(q)\n",
    "    \n",
    "    # Constraints\n",
    "    G = np.vstack([-np.eye(N), np.eye(N)])\n",
    "    h = np.hstack([np.zeros(N), np.ones(N) * C])\n",
    "    G = matrix(G)\n",
    "    h = matrix(h)\n",
    "    \n",
    "    # Equality constraint: sum of alpha_i * y_i = 0\n",
    "    A = np.array(y, dtype=float)\n",
    "    A = matrix(A, (1, N))\n",
    "    b = matrix(np.zeros(1))\n",
    "    \n",
    "    # Solve the quadratic programming problem\n",
    "    solution = solvers.qp(P, q, G, h, A, b)\n",
    "    alpha = np.array(solution['x']).flatten()\n",
    "    \n",
    "    # Compute the bias term\n",
    "    support_vector_indices = alpha > 1e-5\n",
    "    support_vectors = X[support_vector_indices]\n",
    "    support_vector_labels = y[support_vector_indices]\n",
    "    support_vector_alphas = alpha[support_vector_indices]\n",
    "    \n",
    "    K_sv = compute_kernel(support_vectors, gamma)\n",
    "    b = np.mean(support_vector_labels - np.dot(K_sv, support_vector_alphas))\n",
    "    \n",
    "    # Compute the weights\n",
    "    w = np.dot(alpha * y, K)\n",
    "    \n",
    "    return w, b, alpha\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample data\n",
    "    X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n",
    "    y = np.array([1, -1, 1, -1])\n",
    "    \n",
    "    # Parameters\n",
    "    C = 1.0\n",
    "    epsilon = 0.1\n",
    "    gamma = 1.0\n",
    "    \n",
    "    # Train the SVR\n",
    "    w, b, alpha = svr(X, y, C=C, epsilon=epsilon, gamma=gamma)\n",
    "    \n",
    "    print(\"Weights:\", w)\n",
    "    print(\"Bias:\", b)\n",
    "    print(\"Alpha:\", alpha)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c6ae52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: [0. 0. 0. 0.]\n",
      "Bias: nan\n",
      "Alpha: [0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/radha/.local/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/radha/.local/lib/python3.7/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_kernel(X, gamma=1.0):\n",
    "    \"\"\"Compute the Gaussian RBF kernel matrix.\"\"\"\n",
    "    pairwise_dists = np.linalg.norm(X[:, None] - X, axis=2) ** 2\n",
    "    K = np.exp(-gamma * pairwise_dists)\n",
    "    return K\n",
    "\n",
    "def svr(X, y, C=1.0, epsilon=0.1, gamma=1.0, tol=1e-6, max_iter=1000):\n",
    "    \"\"\"Train a Support Vector Machine Regressor using a basic quadratic solver.\"\"\"\n",
    "    N = X.shape[0]\n",
    "    K = compute_kernel(X, gamma)\n",
    "    \n",
    "    # Initialize alpha values\n",
    "    alpha = np.zeros(N)\n",
    "    b = 0\n",
    "    \n",
    "    # Define the function for the dual problem\n",
    "    def objective(alpha):\n",
    "        return 0.5 * np.sum(alpha[:, None] * alpha[None, :] * y[:, None] * y[None, :] * K) - np.sum(alpha)\n",
    "    \n",
    "    # Simple gradient ascent for solving dual problem\n",
    "    for _ in range(max_iter):\n",
    "        gradient = np.dot((alpha * y), K) - 1\n",
    "        alpha += 0.01 * gradient  # Basic step size, needs tuning\n",
    "        alpha = np.clip(alpha, 0, C)\n",
    "        if np.linalg.norm(gradient) < tol:\n",
    "            break\n",
    "    \n",
    "    # Calculate the bias term\n",
    "    support_vector_indices = alpha > 1e-5\n",
    "    support_vectors = X[support_vector_indices]\n",
    "    support_vector_labels = y[support_vector_indices]\n",
    "    support_vector_alphas = alpha[support_vector_indices]\n",
    "    \n",
    "    K_sv = compute_kernel(support_vectors, gamma)\n",
    "    b = np.mean(support_vector_labels - np.dot(K_sv, support_vector_alphas))\n",
    "    \n",
    "    # Compute the weights\n",
    "    w = np.dot(alpha * y, K)\n",
    "    \n",
    "    return w, b, alpha\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample data\n",
    "    X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n",
    "    y = np.array([1, -1, 1, -1])\n",
    "    \n",
    "    # Parameters\n",
    "    C = 1.0\n",
    "    epsilon = 0.1\n",
    "    gamma = 1.0\n",
    "    \n",
    "    # Train the SVR\n",
    "    w, b, alpha = svr(X, y, C=C, epsilon=epsilon, gamma=gamma)\n",
    "    \n",
    "    print(\"Weights:\", w)\n",
    "    print(\"Bias:\", b)\n",
    "    print(\"Alpha:\", alpha)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9256e2",
   "metadata": {},
   "source": [
    "## Support Vector Regression (SVR)\n",
    "\n",
    "The basic idea of \\(\\epsilon\\)-support vector regression is to find a function \\( f(x) = w^T \\phi(x) + b \\) that has at most \\(\\epsilon\\) deviation from the actual target values \\(y_i\\) for all training data \\(x_1, \\dots, x_N\\). In other words:\n",
    "\n",
    "$$\n",
    "|f(x_i) - y_i| \\leq \\epsilon \\quad \\text{for} \\quad i = 1, \\dots, N,\n",
    "$$\n",
    "\n",
    "while keeping \\(w\\) as flat as possible. Deviations smaller than \\(\\epsilon\\) are acceptable, but deviations larger than this are penalized.\n",
    "\n",
    "To ensure the flatness of \\(w\\), we minimize its norm \\(\\|w\\|^2 = \\langle w, w \\rangle\\). Hence, the basic form of \\(\\epsilon\\)-support vector regression can be written as a convex optimization problem:\n",
    "\n",
    "$$\n",
    "\\min_w \\frac{1}{2} \\|w\\|^2,\n",
    "$$\n",
    "\n",
    "subject to:\n",
    "\n",
    "$$\n",
    "y_k - \\langle w, \\phi(x_k) \\rangle - b \\leq \\epsilon,\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\langle w, \\phi(x_k) \\rangle + b - y_k \\leq \\epsilon,\n",
    "$$\n",
    "\n",
    "where \\(\\epsilon > 0\\) represents the allowed regression error.\n",
    "\n",
    "### Slackness Parameters\n",
    "\n",
    "To handle cases where the constraints might be violated, we introduce slackness parameters \\(\\xi_k\\) and \\(\\xi_k^*\\), along with a regularization parameter \\(C > 0\\) and \\(\\epsilon > 0\\). The optimization problem then becomes:\n",
    "\n",
    "$$\n",
    "\\min_{w, b, \\xi_k, \\xi_k^*} \\frac{1}{2} \\langle w, w \\rangle + C \\sum_{k=1}^N (\\xi_k + \\xi_k^*),\n",
    "$$\n",
    "\n",
    "subject to:\n",
    "\n",
    "$$\n",
    "y_k - \\langle w, \\phi(x_k) \\rangle - b \\leq \\epsilon + \\xi_k,\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\langle w, \\phi(x_k) \\rangle + b - y_k \\leq \\epsilon + \\xi_k^*,\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\xi_k, \\xi_k^* \\geq 0, \\quad k = 1, \\dots, N.\n",
    "$$\n",
    "\n",
    "Here, \\(C > 0\\) controls the SVM's tolerance for misclassification, \\(\\xi_k\\) represents the upper training error, and \\(\\xi_k^*\\) the lower training error subject to the \\(\\epsilon\\)-insensitive tube \\( |y_k - (\\langle w, \\phi(x_k) \\rangle + b)| \\leq \\epsilon \\).\n",
    "\n",
    "### Lagrangian Formulation\n",
    "\n",
    "To solve the constrained optimization problem, we define the Lagrangian as follows:\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{2} \\|w\\|^2 + C \\sum_{k=1}^N (\\xi_k + \\xi_k^*) - \\sum_{k=1}^N (\\eta_k \\xi_k + \\eta_k^* \\xi_k^*) \n",
    "$$\n",
    "\n",
    "$$\n",
    "- \\sum_{k=1}^N \\alpha_k \\left( \\epsilon + \\xi_k - y_k + \\langle w, \\phi(x_k) \\rangle + b \\right) \n",
    "- \\sum_{k=1}^N \\alpha_k^* \\left( \\epsilon + \\xi_k^* + y_k - \\langle w, \\phi(x_k) \\rangle - b \\right),\n",
    "$$\n",
    "\n",
    "where \\(\\eta_k, \\eta_k^*, \\alpha_k, \\alpha_k^*\\) are Lagrange multipliers, subject to the following positivity constraints:\n",
    "\n",
    "$$\n",
    "\\eta_k, \\eta_k^*, \\alpha_k, \\alpha_k^* \\geq 0.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05404efb",
   "metadata": {},
   "source": [
    "## Support Vector Regression (SVR)\n",
    "\n",
    "The basic idea of \\( \\epsilon \\)-support vector regression is to find a function \\( f(x) = w^T \\phi(x) + b \\) that has at most \\( \\epsilon \\) deviation from the target values \\( y_i \\) for the training data \\( x_1, \\dots, x_N \\), namely:\n",
    "\n",
    "$$\n",
    "|f(x_i) - y_i| \\leq \\epsilon \\quad \\text{for} \\quad i = 1, \\dots, N,\n",
    "$$\n",
    "\n",
    "while keeping \\( w \\) as flat as possible. In other words, errors less than \\( \\epsilon \\) are acceptable, but deviations larger than this are penalized.\n",
    "\n",
    "One way to ensure flatness is to minimize the norm \\( \\|w\\|^2 = \\langle w, w \\rangle \\). Hence, the basic form of \\( \\epsilon \\)-support vector regression can be written as a convex optimization problem:\n",
    "\n",
    "$$\n",
    "\\min_w \\frac{1}{2} \\|w\\|^2,\n",
    "$$\n",
    "\n",
    "subject to:\n",
    "\n",
    "$$\n",
    "y_k - \\langle w, \\phi(x_k) \\rangle - b \\leq \\epsilon,\n",
    "$$\n",
    "$$\n",
    "\\langle w, \\phi(x_k) \\rangle + b - y_k \\leq \\epsilon,\n",
    "$$\n",
    "\n",
    "where \\( \\epsilon > 0 \\) is a regression error.\n",
    "\n",
    "To avoid a violation of the constrained conditions, slackness parameters \\( (\\xi_k, \\xi_k^*) \\) are introduced, with given parameters \\( C > 0 \\) and \\( \\epsilon > 0 \\). Thus, we have the standard form of support vector regression:\n",
    "\n",
    "$$\n",
    "\\min_{w, b, \\xi_k, \\xi_k^*} \\frac{1}{2} \\langle w, w \\rangle + C \\sum_{k=1}^N (\\xi_k + \\xi_k^*),\n",
    "$$\n",
    "\n",
    "subject to:\n",
    "\n",
    "$$\n",
    "y_k - (\\langle w, \\phi(x_k) \\rangle + b) \\leq \\epsilon + \\xi_k,\n",
    "$$\n",
    "$$\n",
    "(\\langle w, \\phi(x_k) \\rangle + b) - y_k \\leq \\epsilon + \\xi_k^*,\n",
    "$$\n",
    "$$\n",
    "\\xi_k, \\xi_k^* \\geq 0, \\quad k = 1, \\dots, N.\n",
    "$$\n",
    "\n",
    "Here, \\( C > 0 \\) is the regularization parameter controlling the SVM misclassification tolerance. \\( \\xi_k \\) and \\( \\xi_k^* \\) represent the upper and lower training errors subject to the \\( \\epsilon \\)-insensitive tube \\( |y_k - (\\langle w, \\phi(x_k) \\rangle + b)| \\leq \\epsilon \\).\n",
    "\n",
    "### Lagrange Function\n",
    "\n",
    "To solve the above constrained optimization problem, we define the Lagrangian:\n",
    "\n",
    "$$\n",
    "L = L(w, b, \\xi_k, \\xi_k^*) = \\frac{1}{2} \\|w\\|^2 + C \\sum_{k=1}^N (\\xi_k + \\xi_k^*) - \\sum_{k=1}^N (\\eta_k \\xi_k + \\eta_k^* \\xi_k^*)\n",
    "$$\n",
    "\n",
    "$$\n",
    "- \\sum_{k=1}^N \\alpha_k \\left( \\epsilon + \\xi_k - y_k + \\langle w, \\phi(x_k) \\rangle + b \\right)\n",
    "- \\sum_{k=1}^N \\alpha_k^* \\left( \\epsilon + \\xi_k^* + y_k - \\langle w, \\phi(x_k) \\rangle - b \\right),\n",
    "$$\n",
    "\n",
    "where \\( \\eta_k, \\eta_k^*, \\alpha_k, \\alpha_k^* \\) are Lagrange multipliers, subject to the positivity constraints:\n",
    "\n",
    "$$\n",
    "\\eta_k, \\eta_k^*, \\alpha_k, \\alpha_k^* \\geq 0.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3920b803",
   "metadata": {},
   "source": [
    "## First-Order Optimality Conditions\n",
    "\n",
    "From the first-order optimality conditions, the following equations hold:\n",
    "\n",
    "1. For \\(w\\):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w} = 0 \\quad \\Rightarrow \\quad w = \\sum_{k=1}^N (\\alpha_k - \\alpha_k^*) \\phi(x_k),\n",
    "$$\n",
    "which implies that \\(w\\) can be expressed as a linear combination of the training patterns \\(x_i\\). This is called the support vector expansion.\n",
    "\n",
    "2. For \\(b\\):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b} = 0 \\quad \\Rightarrow \\quad \\sum_{k=1}^N (\\alpha_k - \\alpha_k^*) = 0.\n",
    "$$\n",
    "\n",
    "3. For \\(\\xi_k\\):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\xi_k} = 0 \\quad \\Rightarrow \\quad \\eta_k + \\alpha_k = C,\n",
    "$$\n",
    "\n",
    "and similarly,\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\xi_k^*} = 0 \\quad \\Rightarrow \\quad \\eta_k^* + \\alpha_k^* = C.\n",
    "$$\n",
    "\n",
    "### Dual Problem\n",
    "\n",
    "Substituting the expressions for \\(w\\) and \\(b\\) from the first-order optimality conditions into the Lagrangian \\(L\\), we obtain the dual function:\n",
    "\n",
    "$$\n",
    "L_D(\\alpha, \\alpha^*) = -\\frac{1}{2} \\sum_{i=1}^N \\sum_{j=1}^N (\\alpha_i - \\alpha_i^*)(\\alpha_j - \\alpha_j^*) \\langle \\phi(x_i), \\phi(x_j) \\rangle \n",
    "$$\n",
    "\n",
    "$$\n",
    "- \\epsilon \\sum_{i=1}^N (\\alpha_i + \\alpha_i^*) + \\sum_{i=1}^N y_i (\\alpha_i - \\alpha_i^*).\n",
    "$$\n",
    "\n",
    "### Constraints\n",
    "\n",
    "From the conditions \\(\\eta_i, \\eta_i^*, \\alpha_i, \\alpha_i^* \\geq 0\\), we know:\n",
    "\n",
    "$$\n",
    "0 \\leq \\alpha_i, \\alpha_i^* \\leq C \\quad \\text{for} \\quad i = 1, \\dots, N.\n",
    "$$\n",
    "\n",
    "### Wolfe Dual Problem\n",
    "\n",
    "The Wolfe dual problem for \\(\\epsilon\\)-support vector regression becomes:\n",
    "\n",
    "$$\n",
    "\\max_{\\alpha, \\alpha^*} \\quad -\\frac{1}{2} (\\alpha - \\alpha^*)^T Q (\\alpha - \\alpha^*) - \\epsilon \\sum_{i=1}^N (\\alpha_i + \\alpha_i^*) + \\sum_{i=1}^N y_i (\\alpha_i - \\alpha_i^*),\n",
    "$$\n",
    "\n",
    "subject to:\n",
    "\n",
    "$$\n",
    "\\sum_{k=1}^N (\\alpha_k - \\alpha_k^*) = 0,\n",
    "$$\n",
    "\n",
    "and the bounds:\n",
    "\n",
    "$$\n",
    "0 \\leq \\alpha_i, \\alpha_i^* \\leq C \\quad \\text{for} \\quad i = 1, \\dots, N,\n",
    "$$\n",
    "\n",
    "where \\(Q\\) is the kernel matrix \\( Q_{ij} = \\langle \\phi(x_i), \\phi(x_j) \\rangle \\).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1abafb6",
   "metadata": {},
   "source": [
    "## Kernel Function and Regression Function\n",
    "\n",
    "The matrix \\( Q = [K(x_i, x_j)]_{i,j=1}^{N,N} \\) is an \\(N \\times N\\) positive semi-definite matrix where the kernel function \\(K(x_i, x_j)\\) is defined as:\n",
    "\n",
    "$$\n",
    "K(x_i, x_j) = \\langle \\phi(x_i), \\phi(x_j) \\rangle.\n",
    "$$\n",
    "\n",
    "The regression function for the \\(\\epsilon\\)-SVR is given by:\n",
    "\n",
    "$$\n",
    "f(x) = w^T \\phi(x) + b = \\sum_{i=1}^{N} (\\alpha_i - \\alpha_i^*) K(x_i, x) + b,\n",
    "$$\n",
    "\n",
    "where \\(w\\) is described by the support vector expansion:\n",
    "\n",
    "$$\n",
    "w = \\sum_{i=1}^{N} (\\alpha_i - \\alpha_i^*) \\phi(x_i).\n",
    "$$\n",
    "\n",
    "## KKT Conditions\n",
    "\n",
    "The KKT conditions for the dual constrained optimization problem are given by:\n",
    "\n",
    "1. \\( \\alpha_i \\left( \\epsilon + \\xi_i - y_i + \\langle w, x_i \\rangle + b \\right) = 0 \\),\n",
    "2. \\( \\alpha_i^* \\left( \\epsilon + \\xi_i^* + y_i - \\langle w, x_i \\rangle - b \\right) = 0 \\),\n",
    "3. \\( (C - \\alpha_i)\\xi_i = 0 \\),\n",
    "4. \\( (C - \\alpha_i^*) \\xi_i^* = 0 \\).\n",
    "\n",
    "### Implications of KKT Conditions\n",
    "\n",
    "From these conditions, the following inequalities are derived:\n",
    "\n",
    "- If \\( \\alpha_i < C \\), then \\( \\epsilon - y_i + \\langle w, x_i \\rangle + b \\geq 0 \\) and \\( \\xi_i = 0 \\),\n",
    "- If \\( \\alpha_i > 0 \\), then \\( \\epsilon - y_i + \\langle w, x_i \\rangle + b \\leq 0 \\),\n",
    "- If \\( \\alpha_i^* < C \\), then \\( \\epsilon + y_i - \\langle w, x_i \\rangle - b \\geq 0 \\) and \\( \\xi_i^* = 0 \\),\n",
    "- If \\( \\alpha_i^* > 0 \\), then \\( \\epsilon + y_i - \\langle w, x_i \\rangle - b \\leq 0 \\).\n",
    "\n",
    "### Computation of \\(b\\)\n",
    "\n",
    "The computation of the bias term \\(b\\) is given by Smola and Schölkopf [36] as follows:\n",
    "\n",
    "$$\n",
    "\\max \\left\\{ -\\epsilon + y_i - \\langle w, x_i \\rangle \\mid \\alpha_i < C \\text{ or } \\alpha_i^* > 0 \\right\\} \\leq b \\leq\n",
    "\\min \\left\\{ -\\epsilon + y_i - \\langle w, x_i \\rangle \\mid \\alpha_i > 0 \\text{ or } \\alpha_i^* < C \\right\\}.\n",
    "$$\n",
    "\n",
    "If some \\( \\alpha_i \\) or \\( \\alpha_i^* \\in (0, C) \\), then the inequalities become equalities, which can be used to compute \\(b\\).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99471f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 0.0036575530080911215\n",
      "Testing MSE: 0.0036475810521609045\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generating synthetic data for SVR\n",
    "np.random.seed(42)\n",
    "X = np.sort(5 * np.random.rand(100, 1), axis=0)  # 100 random samples between 0 and 5\n",
    "y = np.sin(X).ravel() + 0.2 * (0.5 - np.random.rand(100))  # Adding noise to the sine function\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Creating and training the SVR model (with a radial basis function kernel)\n",
    "svr_rbf = SVR(kernel='rbf', C=1.0, epsilon=0.1)\n",
    "svr_rbf.fit(X_train, y_train)\n",
    "\n",
    "# Predicting using the model\n",
    "y_pred_train = svr_rbf.predict(X_train)\n",
    "y_pred_test = svr_rbf.predict(X_test)\n",
    "\n",
    "# Calculating Mean Squared Error (MSE) for train and test sets\n",
    "mse_train = mean_squared_error(y_train, y_pred_train)\n",
    "mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Training MSE: {mse_train}\")\n",
    "print(f\"Testing MSE: {mse_test}\")\n",
    "\n",
    "# Plotting the regression result\n",
    "plt.scatter(X, y, color='darkorange', label='Data')\n",
    "plt.plot(X, svr_rbf.predict(X), color='navy', lw=2, label='SVR Model')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('SVR Regression with RBF Kernel')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9286d8d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'EpsilonSVR' object has no attribute 'X_fit_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12949/2446692815.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;31m# Predicting on training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;31m# Plotting results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_12949/2446692815.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_kernel_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_fit_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha_pos\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha_neg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'EpsilonSVR' object has no attribute 'X_fit_'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class EpsilonSVR:\n",
    "    def __init__(self, C=1.0, epsilon=0.1, kernel='linear', gamma=None):\n",
    "        self.C = C            # Regularization parameter\n",
    "        self.epsilon = epsilon # Epsilon-insensitive loss\n",
    "        self.kernel = kernel   # Kernel type: 'linear' or 'rbf'\n",
    "        self.gamma = gamma     # Parameter for RBF kernel\n",
    "\n",
    "    def _linear_kernel(self, X1, X2):\n",
    "        return np.dot(X1, X2.T)\n",
    "\n",
    "    def _rbf_kernel(self, X1, X2):\n",
    "        if self.gamma is None:\n",
    "            self.gamma = 1 / X1.shape[1]\n",
    "        K = np.zeros((X1.shape[0], X2.shape[0]))\n",
    "        for i in range(X1.shape[0]):\n",
    "            for j in range(X2.shape[0]):\n",
    "                K[i, j] = np.exp(-self.gamma * np.linalg.norm(X1[i] - X2[j]) ** 2)\n",
    "        return K\n",
    "\n",
    "    def _kernel_function(self, X1, X2):\n",
    "        if self.kernel == 'linear':\n",
    "            return self._linear_kernel(X1, X2)\n",
    "        elif self.kernel == 'rbf':\n",
    "            return self._rbf_kernel(X1, X2)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        K = self._kernel_function(X, X)\n",
    "\n",
    "        # Setting up the optimization problem: maximize the dual objective\n",
    "        P = np.block([[K, -K], [-K, K]])  # Kernel matrix\n",
    "        q = np.hstack([self.epsilon + y, self.epsilon - y])\n",
    "        G = np.vstack([-np.eye(2 * n_samples), np.eye(2 * n_samples)])\n",
    "        h = np.hstack([np.zeros(2 * n_samples), self.C * np.ones(2 * n_samples)])\n",
    "\n",
    "        # Solve the quadratic programming problem using a simple iterative approach\n",
    "        alpha = np.zeros(2 * n_samples)\n",
    "        for _ in range(1000):\n",
    "            grad = np.dot(P, alpha) - q\n",
    "            alpha = np.maximum(0, alpha - 0.001 * grad)  # Gradient descent step\n",
    "\n",
    "        # Extract Lagrange multipliers\n",
    "        self.alpha_pos = alpha[:n_samples]\n",
    "        self.alpha_neg = alpha[n_samples:]\n",
    "\n",
    "        # Support vectors have non-zero alpha\n",
    "        self.support_ = np.where((self.alpha_pos - self.alpha_neg) > 1e-5)[0]\n",
    "\n",
    "        # Calculate weights (w) in case of a linear kernel\n",
    "        if self.kernel == 'linear':\n",
    "            self.w = np.sum(\n",
    "                (self.alpha_pos - self.alpha_neg)[:, None] * X, axis=0\n",
    "            )\n",
    "\n",
    "        # Compute intercept term 'b' using support vectors\n",
    "        self.b = np.mean(\n",
    "            y[self.support_] - np.sum((self.alpha_pos - self.alpha_neg) * K[self.support_], axis=1)\n",
    "        )\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.kernel == 'linear':\n",
    "            return np.dot(X, self.w) + self.b\n",
    "        else:\n",
    "            K = self._kernel_function(X, self.X_fit_)\n",
    "            return np.sum((self.alpha_pos - self.alpha_neg) * K, axis=1) + self.b\n",
    "\n",
    "\n",
    "# Testing the implementation with synthetic data\n",
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(42)\n",
    "    X = np.sort(5 * np.random.rand(100, 1), axis=0)  # Random samples between 0 and 5\n",
    "    y = np.sin(X).ravel() + 0.2 * (0.5 - np.random.rand(100))  # Adding noise\n",
    "\n",
    "    # Training Epsilon-SVR\n",
    "    svr = EpsilonSVR(C=1.0, epsilon=0.1, kernel='rbf', gamma=0.5)\n",
    "    svr.fit(X, y)\n",
    "\n",
    "    # Predicting on training data\n",
    "    y_pred = svr.predict(X)\n",
    "\n",
    "    # Plotting results\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.scatter(X, y, color='darkorange', label='Data')\n",
    "    plt.plot(X, y_pred, color='navy', lw=2, label='SVR Model')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('y')\n",
    "    plt.title('Custom Epsilon-SVR Regression with RBF Kernel')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197bbaf5",
   "metadata": {},
   "source": [
    "### \\(\\nu\\)-Support Vector Machine Regression\n",
    "\n",
    "The primal form of the \\(\\nu\\)-SVR problem is:\n",
    "\n",
    "$$\n",
    "\\min_{w, \\xi_i, \\xi_i^*, \\epsilon} \\frac{1}{2} \\| w \\|^2 + C \\left( \\nu \\epsilon + \\frac{1}{N} \\sum_{i=1}^{N} (\\xi_i + \\xi_i^*) \\right)\n",
    "$$\n",
    "\n",
    "Subject to:\n",
    "\n",
    "$$\n",
    "y_i - \\left( w^T \\phi(x_i) + b \\right) \\leq \\epsilon + \\xi_i^*\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\left( w^T \\phi(x_i) + b \\right) - y_i \\leq \\epsilon + \\xi_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\xi_i, \\xi_i^* \\geq 0\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( C \\) is the regularization parameter.\n",
    "- \\( \\nu \\) controls the trade-off between the size of \\(\\epsilon\\) and model complexity.\n",
    "- \\( \\xi_i, \\xi_i^* \\) are slack variables for points outside the \\(\\epsilon\\)-tube.\n",
    "\n",
    "The dual form (Wolfe dual) of this problem is:\n",
    "\n",
    "$$\n",
    "\\max_{\\alpha, \\alpha^*} \\sum_{i=1}^{N} (\\alpha_i^* - \\alpha_i) y_i - \\frac{1}{2} \\sum_{i=1}^{N} \\sum_{j=1}^{N} (\\alpha_i^* - \\alpha_i)(\\alpha_j^* - \\alpha_j) K(x_i, x_j)\n",
    "$$\n",
    "\n",
    "Subject to:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{N} (\\alpha_i - \\alpha_i^*) = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "0 \\leq \\alpha_i, \\alpha_i^* \\leq \\frac{C}{N}, \\quad \\sum_{i=1}^{N} (\\alpha_i + \\alpha_i^*) \\leq C \\nu\n",
    "$$\n",
    "\n",
    "Once the Lagrange multipliers \\( \\alpha_i, \\alpha_i^* \\) are found, the regression function for a new data point \\( x \\) is:\n",
    "\n",
    "$$\n",
    "f(x) = \\sum_{i=1}^{N} (\\alpha_i^* - \\alpha_i) K(x, x_i) + b\n",
    "$$\n",
    "\n",
    "### Example Function\n",
    "\n",
    "A popular example function for SVR regression is:\n",
    "\n",
    "$$\n",
    "y(x) = \n",
    "\\begin{cases}\n",
    "\\frac{\\sin(x)}{x}, & \\text{if } x \\neq 0 \\\\\n",
    "0, & \\text{if } x = 0\n",
    "\\end{cases}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d1436d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.4950e+00 -1.4649e+01  5e+01  2e+00  2e-16\n",
      " 1: -1.0039e+00 -6.9411e+00  6e+00  3e-16  3e-16\n",
      " 2: -1.5640e+00 -2.5080e+00  9e-01  1e-16  5e-16\n",
      " 3: -1.8203e+00 -1.9378e+00  1e-01  2e-16  1e-16\n",
      " 4: -1.8713e+00 -1.8946e+00  2e-02  1e-16  2e-16\n",
      " 5: -1.8774e+00 -1.8777e+00  3e-04  2e-16  1e-16\n",
      " 6: -1.8775e+00 -1.8775e+00  3e-06  1e-16  2e-16\n",
      " 7: -1.8775e+00 -1.8775e+00  3e-08  1e-16  2e-16\n",
      "Optimal solution found.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (3,) (10,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_23455/2712548961.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0malphas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_nu_svr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;31m# Predict on new data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_23455/2712548961.py\u001b[0m in \u001b[0;36mtrain_nu_svr\u001b[0;34m(X, y, nu, C)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m# Calculate the bias term\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0msupport_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0malphas\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msupport_vectors\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupport_vectors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malphas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msupport_vectors\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msupport_vectors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0malphas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (3,) (10,) "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def kernel(x1, x2):\n",
    "    \"\"\"\n",
    "    Define the kernel function. Here we use a linear kernel.\n",
    "    \"\"\"\n",
    "    return np.dot(x1, x2)\n",
    "\n",
    "def train_nu_svr(X, y, nu, C):\n",
    "    \"\"\"\n",
    "    Train a ν-Support Vector Regression model.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Training data, shape (N, d)\n",
    "    - y: Target values, shape (N,)\n",
    "    - nu: Upper bound on the fraction of margin errors\n",
    "    - C: Regularization parameter\n",
    "\n",
    "    Returns:\n",
    "    - alphas: Lagrange multipliers\n",
    "    - b: Bias term\n",
    "    \"\"\"\n",
    "    N, d = X.shape\n",
    "    K = np.array([[kernel(X[i], X[j]) for j in range(N)] for i in range(N)])\n",
    "    \n",
    "    # Set up matrices for the quadratic programming problem\n",
    "    P = np.outer(y, y) * K\n",
    "    q = -np.ones(N)\n",
    "    G = np.vstack([np.eye(N), -np.eye(N)])\n",
    "    h = np.concatenate([C * np.ones(N), np.zeros(N)])\n",
    "    \n",
    "    # Solve the quadratic programming problem\n",
    "    from cvxopt import matrix, solvers\n",
    "\n",
    "    P = matrix(P)\n",
    "    q = matrix(q)\n",
    "    G = matrix(G)\n",
    "    h = matrix(h)\n",
    "    \n",
    "    solution = solvers.qp(P, q, G, h)\n",
    "    alphas = np.array(solution['x']).flatten()\n",
    "\n",
    "    # Calculate the bias term\n",
    "    support_vectors = (alphas > 1e-5)\n",
    "    b = np.mean(y[support_vectors] - np.dot(K[:, support_vectors], alphas[support_vectors] * y[support_vectors]))\n",
    "\n",
    "    return alphas, b\n",
    "\n",
    "def predict(X_train, X_test, alphas, b):\n",
    "    \"\"\"\n",
    "    Predict using the ν-Support Vector Regression model.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train: Training data, shape (N_train, d)\n",
    "    - X_test: Test data, shape (N_test, d)\n",
    "    - alphas: Lagrange multipliers\n",
    "    - b: Bias term\n",
    "\n",
    "    Returns:\n",
    "    - y_pred: Predictions for X_test\n",
    "    \"\"\"\n",
    "    N_train = X_train.shape[0]\n",
    "    K_test = np.array([[kernel(X_test[i], X_train[j]) for j in range(N_train)] for i in range(X_test.shape[0])])\n",
    "    y_pred = np.dot(K_test, alphas * y) + b\n",
    "    return y_pred\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    from cvxopt import matrix, solvers\n",
    "    \n",
    "    # Generate some example data\n",
    "    np.random.seed(0)\n",
    "    X = np.random.rand(10, 2)  # 10 samples, 2 features\n",
    "    y = np.sin(X[:, 0]) + np.cos(X[:, 1])\n",
    "    \n",
    "    nu = 0.1\n",
    "    C = 1.0\n",
    "\n",
    "    # Train the model\n",
    "    alphas, b = train_nu_svr(X, y, nu, C)\n",
    "    \n",
    "    # Predict on new data\n",
    "    X_test = np.random.rand(5, 2)  # 5 new samples\n",
    "    y_pred = predict(X, X_test, alphas, b)\n",
    "    \n",
    "    print(\"Predictions:\", y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71c0ab2",
   "metadata": {},
   "source": [
    "Given a training set of \\( N \\) data points \\( \\{ (x_k, y_k) \\}_{k=1}^N \\), where \\( x_k \\) is the \\( k \\)-th input pattern and \\( y_k \\) is the \\( k \\)-th output pattern, and \\( x_k \\in \\mathbb{R}^n \\), \\( y_k \\in \\{-1, +1\\} \\), the dataset can be represented as:\n",
    "\n",
    "$$\n",
    "D = \\{ (x_1, y_1), \\ldots, (x_N, y_N) \\}\n",
    "$$\n",
    "\n",
    "A Support Vector Machine (SVM) classifier finds the hyperplane that separates the data with the largest margin between the hyperplane and the closest data points (called support vectors). The linear separating hyperplane is described by the decision function:\n",
    "\n",
    "$$\n",
    "f(x) = \\text{sign}(w^T x + b)\n",
    "$$\n",
    "\n",
    "where \\( x \\) is the input pattern, \\( w \\) is the weight vector, and \\( b \\) is the bias term. This can be interpreted as:\n",
    "\n",
    "$$\n",
    "\\text{if } w^T x + b > 0, \\text{ then } x \\in \\text{class } S^+\n",
    "$$\n",
    "$$\n",
    "\\text{if } w^T x + b < 0, \\text{ then } x \\in \\text{class } S^-\n",
    "$$\n",
    "$$\n",
    "\\text{if } w^T x + b = 0, \\text{ then } x \\text{ is on the decision boundary.}\n",
    "$$\n",
    "\n",
    "For a nonlinear classifier with a nonlinear mapping \\( \\phi : x \\to \\phi(x) \\), the testing output is given by:\n",
    "\n",
    "$$\n",
    "y = f(x) = w^T \\phi(x) + b\n",
    "$$\n",
    "\n",
    "where \\( b \\) denotes the bias term. The classification decision is:\n",
    "\n",
    "$$\n",
    "\\text{if } w^T \\phi(x) + b > 0, \\text{ then } x \\in \\text{class } S^+\n",
    "$$\n",
    "$$\n",
    "\\text{if } w^T \\phi(x) + b < 0, \\text{ then } x \\in \\text{class } S^-\n",
    "$$\n",
    "$$\n",
    "\\text{if } w^T \\phi(x) + b = 0, \\text{ then } x \\text{ is on the decision boundary.}\n",
    "$$\n",
    "\n",
    "Therefore, the decision function of a nonlinear SVM classifier is given by:\n",
    "\n",
    "$$\n",
    "\\text{class of } x = \\text{sign}(w^T \\phi(x) + b)\n",
    "$$\n",
    "\n",
    "For an SVM with a kernel function \\( K(x, x_i) = \\phi(x)^T \\phi(x_i) \\), the weight vector \\( w \\) is usually designed as:\n",
    "\n",
    "$$\n",
    "w = \\sum_{i=1}^N \\alpha_i y_i \\phi(x_i)\n",
    "$$\n",
    "\n",
    "Substituting this into the decision function, the classifier's decision function becomes:\n",
    "\n",
    "$$\n",
    "\\text{class of } x = \\text{sign}\\left( \\sum_{i=1}^N \\alpha_i y_i K(x, x_i) + b \\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5bfd1b",
   "metadata": {},
   "source": [
    "Hence, when designing any SVM classifier, its weighting vector \\( w \\) must have the form given by:\n",
    "\n",
    "$$\n",
    "w = \\sum_{i=1}^N \\alpha_i y_i \\phi(x_i)\n",
    "$$\n",
    "\n",
    "The set of vectors \\(\\{x_1, \\ldots, x_N\\}\\) is said to be optimally separated by the hyperplane if it is separated without error and the distance between the closest vector to the hyperplane is maximal.\n",
    "\n",
    "For designing the classifier \\( w \\), we assume that:\n",
    "\n",
    "$$\n",
    "w^T \\phi(x_k) + b \\geq 1, \\text{ if } y_k = +1\n",
    "$$\n",
    "\n",
    "$$\n",
    "w^T \\phi(x_k) + b \\leq -1, \\text{ if } y_k = -1\n",
    "$$\n",
    "\n",
    "In other words, the hyperplane should ensure:\n",
    "\n",
    "$$\n",
    "y_k (w^T \\phi(x_k) + b) > 0, \\text{ for all } k = 1, \\ldots, N\n",
    "$$\n",
    "\n",
    "assuming that \\( y_k \\in \\{-1, +1\\} \\). Here, \\(\\phi(x_k)\\) is a nonlinear vector function mapping the input space \\(\\mathbb{R}^n\\) into a higher-dimensional space, but this function is not explicitly constructed.\n",
    "\n",
    "The distance of a point \\( x_k \\) from the hyperplane, denoted as \\( d(w, b; x_k) \\), is defined as:\n",
    "\n",
    "$$\n",
    "d(w, b; x_k) = \\frac{|w^T \\phi(x_k) + b|}{\\|w\\|}\n",
    "$$\n",
    "\n",
    "By the constraint condition \\( y_k (w^T \\phi(x_k) + b) \\geq 1 \\), the margin of a classifier is given by:\n",
    "\n",
    "$$\n",
    "\\rho(w, b) = \\frac{ \\min_{y_k = -1} \\left | w^T \\phi(x_k) + b \\right | + \\min_{y_k = +1} \\left | w^T \\phi(x_k) + b \\right | }{\\|w\\|}\n",
    "$$\n",
    "\n",
    "The optimal hyperplane \\( w_{\\text{opt}} \\) is given by maximizing the above margin, namely:\n",
    "\n",
    "$$\n",
    "w = \\arg \\max_w \\rho(w, b) = \\frac{2}{\\|w\\|}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d5320b",
   "metadata": {},
   "source": [
    "which is equivalent to:\n",
    "\n",
    "$$\n",
    "w = \\arg \\min_w \\frac{1}{2} \\|w\\|^2\n",
    "$$\n",
    "\n",
    "In order to avoid the possibility of violating \\( y_k (w^T \\phi(x_k) + b) \\geq 1 \\), we need to introduce slack variables \\( \\xi_k \\) such that:\n",
    "\n",
    "$$\n",
    "y_k (w^T \\phi(x_k) + b) \\geq 1 - \\xi_k, \\quad k = 1, \\ldots, N\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\xi_k \\geq 0, \\quad k = 1, \\ldots, N\n",
    "$$\n",
    "\n",
    "According to the structural risk minimization principle, the risk bound is minimized by formulating the optimization problem:\n",
    "\n",
    "$$\n",
    "\\min_{w, \\xi_k} \\frac{1}{2} \\|w\\|^2 + C \\sum_{k=1}^N \\xi_k\n",
    "$$\n",
    "\n",
    "subject to:\n",
    "\n",
    "$$\n",
    "y_k (w^T \\phi(x_k) + b) \\geq 1 - \\xi_k\n",
    "$$\n",
    "\n",
    "where \\( C \\) is a user-specified parameter that provides a trade-off between the distance of the separating margin and the training error.\n",
    "\n",
    "Therefore, the primary problem for the SVM binary classifier is a constrained optimization problem:\n",
    "\n",
    "$$\n",
    "\\min_{w, b, \\xi_i} L_{\\text{PSVM}} = \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^N \\xi_i\n",
    "$$\n",
    "\n",
    "subject to:\n",
    "\n",
    "$$\n",
    "y_i (w^T \\phi(x_i) + b) \\geq 1 - \\xi_i, \\quad \\xi_i \\geq 0, \\quad i = 1, \\ldots, N\n",
    "$$\n",
    "\n",
    "The dual form of the above primary optimization problem is given by:\n",
    "\n",
    "$$\n",
    "\\max_{\\alpha_i} L_{\\text{DSVM}} = \\sum_{i=1}^N \\alpha_i - \\frac{1}{2} \\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j \\langle \\phi(x_i), \\phi(x_j) \\rangle\n",
    "$$\n",
    "\n",
    "subject to:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^N \\alpha_i y_i = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "0 \\leq \\alpha_i \\leq C, \\quad i = 1, \\ldots, N\n",
    "$$\n",
    "\n",
    "where \\( \\alpha_i \\) is the Lagrange multiplier corresponding to the \\( i \\)-th training sample \\( (x_i, y_i) \\), and vectors \\( x_i \\) satisfying \\( y_i (w^T \\phi(x_i) + b) = 1 \\) are termed support vectors.\n",
    "In SVM learning algorithms, kernel functions \\( K(u, v) = \\langle \\phi(u), \\phi(v) \\rangle \\) are usually used, and the dual optimization problem for the SVM binary classifier is represented as:\n",
    "\n",
    "$$\n",
    "\\min_{\\alpha_i} L_{\\text{DSVM}} = \\frac{1}{2} \\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j K(x_i, x_j) - \\sum_{i=1}^N \\alpha_i\n",
    "$$\n",
    "\n",
    "subject to:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^N \\alpha_i y_i = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "0 \\leq \\alpha_i \\leq C, \\quad i = 1, \\ldots, N\n",
    "$$\n",
    "\n",
    "By Bousquet et al. [5, pp. 14–15], several important practical points should be taken into account when designing classifiers:\n",
    "\n",
    "1. To reduce the likelihood of overfitting the classifier to the training data, the ratio of the number of training examples to the number of features should be at least 10:1. For the same reason, the ratio of the number of training examples to the number of unknown parameters should be at least 10:1.\n",
    "\n",
    "2. Proper error-estimation methods should be used, especially when selecting parameters for the classifier.\n",
    "\n",
    "3. Some algorithms require the input features to be scaled to similar ranges, such as some kind of weighted average of the inputs.\n",
    "\n",
    "4. There is no single best classification algorithm!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa96151e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "constraints() missing 1 required positional argument: 'C'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_23455/2854797235.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;31m# Train SVM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_svm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;31m# Predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_23455/2854797235.py\u001b[0m in \u001b[0;36mtrain_svm\u001b[0;34m(X, y, C, gamma)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# Solve the quadratic programming problem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobjective_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha_initial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstraints\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'SLSQP'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m# Optimal alphas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cv37/lib/python3.7/site-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    630\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'slsqp'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m         return _minimize_slsqp(fun, x0, args, jac, bounds,\n\u001b[0;32m--> 632\u001b[0;31m                                constraints, callback=callback, **options)\n\u001b[0m\u001b[1;32m    633\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'trust-constr'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m         return _minimize_trustregion_constr(fun, x0, args, jac, hess, hessp,\n",
      "\u001b[0;32m~/anaconda3/envs/cv37/lib/python3.7/site-packages/scipy/optimize/slsqp.py\u001b[0m in \u001b[0;36m_minimize_slsqp\u001b[0;34m(func, x0, args, jac, bounds, constraints, maxiter, ftol, iprint, disp, eps, callback, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0;31m# meq, mieq: number of equality and inequality constraints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     meq = sum(map(len, [atleast_1d(c['fun'](x, *c['args']))\n\u001b[0;32m--> 329\u001b[0;31m               for c in cons['eq']]))\n\u001b[0m\u001b[1;32m    330\u001b[0m     mieq = sum(map(len, [atleast_1d(c['fun'](x, *c['args']))\n\u001b[1;32m    331\u001b[0m                for c in cons['ineq']]))\n",
      "\u001b[0;32m~/anaconda3/envs/cv37/lib/python3.7/site-packages/scipy/optimize/slsqp.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0;31m# meq, mieq: number of equality and inequality constraints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     meq = sum(map(len, [atleast_1d(c['fun'](x, *c['args']))\n\u001b[0;32m--> 329\u001b[0;31m               for c in cons['eq']]))\n\u001b[0m\u001b[1;32m    330\u001b[0m     mieq = sum(map(len, [atleast_1d(c['fun'](x, *c['args']))\n\u001b[1;32m    331\u001b[0m                for c in cons['ineq']]))\n",
      "\u001b[0;31mTypeError\u001b[0m: constraints() missing 1 required positional argument: 'C'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def rbf_kernel(x1, x2, gamma=1.0):\n",
    "    \"\"\" Radial Basis Function (RBF) Kernel \"\"\"\n",
    "    return np.exp(-gamma * np.linalg.norm(x1 - x2) ** 2)\n",
    "\n",
    "def compute_kernel_matrix(X, gamma=1.0):\n",
    "    \"\"\" Compute the Kernel Matrix K(x_i, x_j) \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    K = np.zeros((n_samples, n_samples))\n",
    "    for i in range(n_samples):\n",
    "        for j in range(n_samples):\n",
    "            K[i, j] = rbf_kernel(X[i], X[j], gamma)\n",
    "    return K\n",
    "\n",
    "def objective_function(alpha, K, y, C):\n",
    "    \"\"\" Objective function for the dual SVM problem \"\"\"\n",
    "    return 0.5 * np.dot(alpha, np.dot(K, alpha)) - np.sum(alpha)\n",
    "\n",
    "def constraints(alpha, y, C):\n",
    "    \"\"\" Constraints for the dual problem \"\"\"\n",
    "    return [np.dot(y, alpha)]\n",
    "\n",
    "def train_svm(X, y, C=1.0, gamma=1.0):\n",
    "    \"\"\" Train SVM model using the dual problem \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    \n",
    "    # Compute Kernel Matrix\n",
    "    K = compute_kernel_matrix(X, gamma)\n",
    "    \n",
    "    # Initial guess for alpha\n",
    "    alpha_initial = np.zeros(n_samples)\n",
    "    \n",
    "    # Bounds for alpha\n",
    "    bounds = [(0, C) for _ in range(n_samples)]\n",
    "    \n",
    "    # Constraints: sum(alpha_i * y_i) = 0\n",
    "    cons = {'type': 'eq', 'fun': constraints, 'args': (y,)}\n",
    "    \n",
    "    # Solve the quadratic programming problem\n",
    "    result = minimize(fun=objective_function, x0=alpha_initial, args=(K, y, C), bounds=bounds, constraints=cons, method='SLSQP')\n",
    "    \n",
    "    # Optimal alphas\n",
    "    alpha = result.x\n",
    "    \n",
    "    # Compute the weight vector and bias\n",
    "    w = np.sum(alpha[:, np.newaxis] * y[:, np.newaxis] * X, axis=0)\n",
    "    support_vectors = np.where((alpha > 1e-5) & (alpha < C - 1e-5))[0]\n",
    "    b = np.mean(y[support_vectors] - np.dot(X[support_vectors], w))\n",
    "    \n",
    "    return w, b, alpha\n",
    "\n",
    "def predict(X, w, b, kernel_function):\n",
    "    \"\"\" Predict using the trained SVM model \"\"\"\n",
    "    return np.sign(np.dot(X, w) + b)\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example data\n",
    "    X = np.array([[2, 3], [4, 5], [1, 1], [7, 8]])\n",
    "    y = np.array([1, 1, -1, -1])\n",
    "    \n",
    "    # Train SVM\n",
    "    w, b, alpha = train_svm(X, y, C=1.0, gamma=1.0)\n",
    "    \n",
    "    # Predict\n",
    "    X_test = np.array([[3, 4], [5, 6]])\n",
    "    predictions = predict(X_test, w, b, rbf_kernel)\n",
    "    \n",
    "    print(\"Weights:\", w)\n",
    "    print(\"Bias:\", b)\n",
    "    print(\"Predictions:\", predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b1207bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: [-2.00640242 -1.01327247]\n",
      "Bias: 11.328816480809465\n",
      "Predictions: [ 1. -1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def rbf_kernel(x1, x2, gamma=1.0):\n",
    "    \"\"\" Radial Basis Function (RBF) Kernel \"\"\"\n",
    "    return np.exp(-gamma * np.linalg.norm(x1 - x2) ** 2)\n",
    "\n",
    "def compute_kernel_matrix(X, gamma=1.0):\n",
    "    \"\"\" Compute the Kernel Matrix K(x_i, x_j) \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    K = np.zeros((n_samples, n_samples))\n",
    "    for i in range(n_samples):\n",
    "        for j in range(n_samples):\n",
    "            K[i, j] = rbf_kernel(X[i], X[j], gamma)\n",
    "    return K\n",
    "\n",
    "def objective_function(alpha, K, y):\n",
    "    \"\"\" Objective function for the dual SVM problem \"\"\"\n",
    "    return 0.5 * np.dot(alpha, np.dot(K, alpha)) - np.sum(alpha)\n",
    "\n",
    "def constraints(alpha, y, C):\n",
    "    \"\"\" Constraints for the dual problem \"\"\"\n",
    "    return np.dot(y, alpha)\n",
    "\n",
    "def train_svm(X, y, C=1.0, gamma=1.0):\n",
    "    \"\"\" Train SVM model using the dual problem \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    \n",
    "    # Compute Kernel Matrix\n",
    "    K = compute_kernel_matrix(X, gamma)\n",
    "    \n",
    "    # Initial guess for alpha\n",
    "    alpha_initial = np.zeros(n_samples)\n",
    "    \n",
    "    # Bounds for alpha\n",
    "    bounds = [(0, C) for _ in range(n_samples)]\n",
    "    \n",
    "    # Constraints: sum(alpha_i * y_i) = 0\n",
    "    cons = {'type': 'eq', 'fun': constraints, 'args': (y, C)}\n",
    "    \n",
    "    # Solve the quadratic programming problem\n",
    "    result = minimize(fun=objective_function, x0=alpha_initial, args=(K, y), bounds=bounds, constraints=cons, method='SLSQP')\n",
    "    \n",
    "    # Optimal alphas\n",
    "    alpha = result.x\n",
    "    \n",
    "    # Compute the weight vector and bias\n",
    "    w = np.sum(alpha[:, np.newaxis] * y[:, np.newaxis] * X, axis=0)\n",
    "    support_vectors = np.where((alpha > 1e-5) & (alpha < C - 1e-5))[0]\n",
    "    b = np.mean(y[support_vectors] - np.dot(X[support_vectors], w))\n",
    "    \n",
    "    return w, b, alpha\n",
    "\n",
    "def predict(X, w, b):\n",
    "    \"\"\" Predict using the trained SVM model \"\"\"\n",
    "    return np.sign(np.dot(X, w) + b)\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example data\n",
    "    X = np.array([[2, 3], [4, 5], [1, 1], [7, 8]])\n",
    "    y = np.array([1, 1, -1, -1])\n",
    "    \n",
    "    # Train SVM\n",
    "    w, b, alpha = train_svm(X, y, C=1.0, gamma=1.0)\n",
    "    \n",
    "    # Predict\n",
    "    X_test = np.array([[3, 4], [5, 6]])\n",
    "    predictions = predict(X_test, w, b)\n",
    "    \n",
    "    print(\"Weights:\", w)\n",
    "    print(\"Bias:\", b)\n",
    "    print(\"Predictions:\", predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077929b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
