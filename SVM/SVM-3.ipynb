{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369218ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " * Copyright (c) 2018 Radhamadhab Dalai\n",
    " *\n",
    " * Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    " * of this software and associated documentation files (the \"Software\"), to deal\n",
    " * in the Software without restriction, including without limitation the rights\n",
    " * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    " * copies of the Software, and to permit persons to whom the Software is\n",
    " * furnished to do so, subject to the following conditions:\n",
    " *\n",
    " * The above copyright notice and this permission notice shall be included in\n",
    " * all copies or substantial portions of the Software.\n",
    " *\n",
    " * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    " * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    " * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    " * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    " * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    " * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
    " * THE SOFTWARE.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d6b6f8",
   "metadata": {},
   "source": [
    "## Kernel Partial Least Squares Regression\n",
    "\n",
    "Given two data blocks \\( \\mathbf{X} \\) and \\( \\mathbf{Y} \\), the kernel partial least squares (PLS) regression is a natural extension of PLS regression. The key steps involved in the PLS regression are:\n",
    "\n",
    "1. $$ \\mathbf{w} = \\frac{\\mathbf{X}^T \\mathbf{u}}{\\mathbf{u}^T \\mathbf{u}} $$\n",
    "2. $$ \\mathbf{t} = \\mathbf{Xw} $$\n",
    "3. $$ \\mathbf{c} = \\frac{\\mathbf{Y}^T \\mathbf{t}}{\\mathbf{t}^T \\mathbf{t}} $$\n",
    "4. $$ \\mathbf{u} = \\frac{\\mathbf{Y}^T \\mathbf{c}}{\\mathbf{c}^T \\mathbf{c}} $$\n",
    "5. $$ \\mathbf{p} = \\frac{\\mathbf{X}^T \\mathbf{t}}{\\mathbf{t}^T \\mathbf{t}} $$\n",
    "6. $$ \\mathbf{q} = \\frac{\\mathbf{Y}^T \\mathbf{u}}{\\mathbf{u}^T \\mathbf{u}} $$\n",
    "7. $$ \\mathbf{X} = \\mathbf{X} - \\mathbf{tp}^T $$\n",
    "8. $$ \\mathbf{Y} = \\mathbf{Y} - \\mathbf{tc}^T $$\n",
    "\n",
    "These steps can also be expressed as:\n",
    "\n",
    "$$ \\mathbf{t} = \\frac{\\mathbf{X} \\mathbf{X}^T \\mathbf{u}}{\\mathbf{u}^T \\mathbf{u}} $$\n",
    "\n",
    "$$ \\mathbf{c} = \\mathbf{Y}^T \\mathbf{t} $$\n",
    "\n",
    "$$ \\mathbf{u} = \\frac{\\mathbf{Y}^T \\mathbf{c}}{\\mathbf{c}^T \\mathbf{c}} $$\n",
    "\n",
    "$$ \\mathbf{X} = \\mathbf{X} - \\mathbf{tt}^T \\mathbf{X} $$\n",
    "\n",
    "$$ \\mathbf{Y} = \\mathbf{Y} - \\mathbf{tc}^T $$\n",
    "\n",
    "Using \\( \\Phi = \\Phi(\\mathbf{X}) \\) instead of \\( \\mathbf{X} \\), the equations become:\n",
    "\n",
    "$$ \\mathbf{t} = \\frac{\\Phi \\Phi^T \\mathbf{u}}{\\mathbf{u}^T \\mathbf{u}} $$\n",
    "\n",
    "$$ \\Phi = \\Phi - \\mathbf{tt}^T \\Phi $$\n",
    "\n",
    "Therefore, the key steps of kernel nonlinear iterative partial least squares (NIPALS) regression are as follows:\n",
    "\n",
    "Given \\( \\Phi_0 = \\Phi \\) and the data block \\( \\mathbf{Y}_0 = \\mathbf{Y} \\):\n",
    "\n",
    "1. Randomly initialize \\( \\mathbf{u} \\).\n",
    "2. $$ \\mathbf{t} = \\Phi \\Phi^T \\mathbf{u}, \\quad \\mathbf{t} \\leftarrow \\frac{\\mathbf{t}}{\\mathbf{t}^T \\mathbf{t}} $$\n",
    "3. $$ \\mathbf{c} = \\mathbf{Y}^T \\mathbf{t} $$\n",
    "4. $$ \\mathbf{u} = \\mathbf{Y}^T \\mathbf{u}, \\quad \\mathbf{u} \\leftarrow \\frac{\\mathbf{u}}{\\mathbf{u}^T \\mathbf{u}} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88acd69a",
   "metadata": {},
   "source": [
    "\\textbf{Kernel NIPALS Regression Algorithm}\n",
    "\n",
    "5. Repeat Steps 2–4 until convergence of \\( t \\).\n",
    "\n",
    "6. Deflate the matrix:\n",
    "$$\n",
    "\\mathbf{T} = (\\mathbf{I} - \\mathbf{t}\\mathbf{t}^T)\\mathbf{T}(\\mathbf{I} - \\mathbf{t}\\mathbf{t}^T)^T.\n",
    "$$\n",
    "\n",
    "7. Deflate the matrix:\n",
    "$$\n",
    "\\mathbf{Y} = \\mathbf{Y} - \\mathbf{t}\\mathbf{c}^T.\n",
    "$$\n",
    "\n",
    "The kernel NIPALS regression is an iterative process: after extraction of the first component \\( t_1 \\), the algorithm starts again using the deflated matrices \\( \\mathbf{T} \\) and \\( \\mathbf{Y} \\) computed in Step 6 and Step 7, and repeat Steps 2–7 until the deflated matrix \\( \\mathbf{T} \\) or \\( \\mathbf{Y} \\) becomes a null matrix.\n",
    "\n",
    "Once two matrices \\( \\mathbf{T} = [t_1, \\ldots, t_p] \\) and \\( \\mathbf{U} = [u_1 , \\ldots, u_p] \\) are found by using the NIPALS regression algorithm, then the matrix regression coefficients \\( \\mathbf{B} \\) can be computed in the form similar to (6.9.26):\n",
    "$$\n",
    "\\mathbf{B} = \\mathbf{T}^0 \\mathbf{U}(\\mathbf{T}^T \\mathbf{T}^0 \\mathbf{U})^{-1} \\mathbf{T}^T \\mathbf{Y}^0.\n",
    "$$\n",
    "\n",
    "Then for a given new data block \\( X_{\\text{new}} \\) and \\( \\mathbf{T}_{\\text{new}} = \\mathbf{T}(X_{\\text{new}}) \\), the unknown \\( \\mathbf{Y} \\)-values can be predicted as:\n",
    "$$\n",
    "\\hat{\\mathbf{Y}}_{\\text{new}} = \\mathbf{T}_{\\text{new}}\\mathbf{B}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12776f96",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1 is different from 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7187/398673940.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkernel_nipals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;31m# Predict new data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_7187/398673940.py\u001b[0m in \u001b[0;36mkernel_nipals\u001b[0;34m(X, Y, n_components)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;31m# Step 3: Compute new t = (X @ u) / (u.T @ u)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mu_u\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mu\u001b[0m  \u001b[0;31m# Scalar value (1x1 matrix)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mt_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mu_u\u001b[0m  \u001b[0;31m# Shape (n_samples, 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;31m# Check for convergence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1 is different from 2)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def kernel_nipals(X, Y, n_components):\n",
    "    # X: Input data matrix (n_samples, n_features)\n",
    "    # Y: Output data matrix (n_samples, n_targets)\n",
    "    # n_components: Number of components to extract\n",
    "    \n",
    "    n_samples, n_features = X.shape\n",
    "    n_targets = Y.shape[1]\n",
    "    \n",
    "    # Initialize matrices to store results\n",
    "    T = np.zeros((n_samples, n_components))\n",
    "    P = np.zeros((n_features, n_components))\n",
    "    C = np.zeros((n_targets, n_components))\n",
    "    \n",
    "    for i in range(n_components):\n",
    "        # Step 1: Initialize t (randomly or with the first column of X)\n",
    "        t = X[:, 0].reshape(-1, 1)  # Shape (n_samples, 1)\n",
    "        \n",
    "        while True:\n",
    "            # Step 2: Compute u = (Y.T @ t) / (t.T @ t)\n",
    "            t_t = t.T @ t  # Scalar value (1x1 matrix)\n",
    "            u = (Y.T @ t) / t_t  # Shape (n_targets, 1)\n",
    "            u = u / np.linalg.norm(u)  # Normalize u\n",
    "            \n",
    "            # Step 3: Compute new t = (X @ u) / (u.T @ u)\n",
    "            u_u = u.T @ u  # Scalar value (1x1 matrix)\n",
    "            t_new = (X @ u) / u_u  # Shape (n_samples, 1)\n",
    "            \n",
    "            # Check for convergence\n",
    "            if np.linalg.norm(t_new - t) < 1e-10:\n",
    "                t = t_new\n",
    "                break\n",
    "            t = t_new\n",
    "        \n",
    "        # Step 4: Deflation\n",
    "        p = (X.T @ t) / t_t  # Shape (n_features, 1)\n",
    "        c = (Y.T @ t) / t_t  # Shape (n_targets, 1)\n",
    "        \n",
    "        # Store components\n",
    "        T[:, i] = t.flatten()\n",
    "        P[:, i] = p.flatten()\n",
    "        C[:, i] = c.flatten()\n",
    "        \n",
    "        # Deflate X and Y\n",
    "        X -= t @ p.T  # X shape (n_samples, n_features), p.T shape (1, n_features)\n",
    "        Y -= t @ c.T  # Y shape (n_samples, n_targets), c.T shape (1, n_targets)\n",
    "    \n",
    "    # Compute regression coefficients B\n",
    "    T_T_T = T.T @ T  # Shape (n_components, n_components)\n",
    "    T_T_Y = T.T @ Y  # Shape (n_components, n_targets)\n",
    "    B = np.linalg.pinv(T_T_T) @ T_T_Y  # Shape (n_features, n_targets)\n",
    "    \n",
    "    return B, T, P, C\n",
    "\n",
    "# Predict function\n",
    "def predict(X_new, B):\n",
    "    return X_new @ B  # Shape (n_samples, n_features) @ (n_features, n_targets)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example input data\n",
    "    X = np.array([[1, 2], [2, 3], [3, 4]])  # (3 samples, 2 features)\n",
    "    Y = np.array([[1], [2], [3]])           # (3 samples, 1 target)\n",
    "    \n",
    "    # Number of components\n",
    "    n_components = 1\n",
    "    \n",
    "    # Train the model\n",
    "    B, T, P, C = kernel_nipals(X, Y, n_components)\n",
    "    \n",
    "    # Predict new data\n",
    "    X_new = np.array([[4, 5]])  # (1 sample, 2 features)\n",
    "    Y_pred = predict(X_new, B)\n",
    "    \n",
    "    print(\"Predicted Y:\", Y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78eefeb8",
   "metadata": {},
   "source": [
    "### Laplacian Support Vector Machines (LapSVM)\n",
    "\n",
    "**Problem Statement:**\n",
    "\n",
    "Consider a set of labeled graph examples \\(\\{(x_i, y_i)\\}_{i=1}^l\\) and a set of unlabeled graph examples \\(\\{x_j\\}_{j=l+1}^{l+u}\\). The goal is to solve the following optimization problem for a semi-supervised SVM:\n",
    "\n",
    "$$\n",
    "f^* = \\arg\\min_f \\left[ \\frac{1}{l} \\sum_{i=1}^l \\left( 1 - y_i f(x_i) \\right)_+ + \\gamma_A \\frac{1}{2} \\|f\\|_K^2 + \\frac{(u - l)^2}{2} f^T L f \\right]\n",
    "$$\n",
    "\n",
    "where:\n",
    "- \\(\\left(1 - y_i f(x_i)\\right)_+\\) denotes the hinge loss for the \\(i\\)-th labeled example.\n",
    "- \\(\\gamma_A\\) is the regularization parameter for the kernel norm.\n",
    "- \\(L\\) is the Laplacian matrix of the graph.\n",
    "\n",
    "**Solution Representation:**\n",
    "\n",
    "According to the extended representer theorem, the solution \\(f^*\\) can be expressed as:\n",
    "\n",
    "$$\n",
    "f^* = \\sum_{i=1}^{l+u} \\alpha_i^* K(x, x_i) + b\n",
    "$$\n",
    "\n",
    "**Primal Problem for \\(\\alpha\\):**\n",
    "\n",
    "Substituting \\(f^*\\) into the optimization problem and adding a bias term \\(b\\), the primal problem for optimizing \\(\\alpha\\) can be written as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_{\\alpha, \\xi} & \\quad \\frac{1}{2} \\alpha^T \\left( \\gamma_A K + \\frac{(u - l)^2}{2} KLK \\right) \\alpha + \\sum_{i=1}^l \\xi_i \\\\\n",
    "\\text{subject to} & \\quad y_i \\left( \\sum_{j=1}^{l+u} \\alpha_j K(x_i, x_j) + b \\right) \\geq 1 - \\xi_i, \\quad \\forall i = 1, \\ldots, l \\\\\n",
    "& \\quad \\xi_i \\geq 0, \\quad \\forall i = 1, \\ldots, l\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Lagrangian Formulation:**\n",
    "\n",
    "The Lagrangian function incorporating the constraints is:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L(\\alpha, \\xi, b, \\beta, \\zeta) = & \\frac{1}{2} \\alpha^T \\left( \\gamma_A K + \\frac{(u - l)^2}{2} KLK \\right) \\alpha + \\sum_{i=1}^l \\xi_i \\\\\n",
    "& - \\sum_{i=1}^l \\beta_i \\left( y_i \\left( \\sum_{j=1}^{l+u} \\alpha_j K(x_i, x_j) + b \\right) - 1 + \\xi_i \\right) \\\\\n",
    "& - \\sum_{i=1}^l \\zeta_i \\xi_i\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "From the first-order optimization conditions, we get:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b} = 0 \\implies \\beta_i y_i = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\xi_i} = 0 \\implies \\beta_i + \\zeta_i = 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "0 \\leq \\beta_i \\leq \\zeta_i, \\quad \\text{where } \\zeta_i \\text{ and } \\xi_i \\text{ are nonnegative}\n",
    "$$\n",
    "\n",
    "**Reduced Lagrangian Function:**\n",
    "\n",
    "The reduced Lagrangian function is:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L_R(\\alpha, \\beta) = & \\frac{1}{2} \\alpha^T \\left( \\gamma_A K + \\frac{(u - l)^2}{2} KLK \\right) \\alpha - \\sum_{i=1}^l \\beta_i \\left( y_i \\left( \\sum_{j=1}^{l+u} \\alpha_j K(x_i, x_j) - 1 \\right) \\right) \\\\\n",
    "& + \\beta_i\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where \\(J = [1, \\ldots, 1, 0, \\ldots, 0]\\) is a \\(1 \\times (l + u)\\) matrix with the first \\(l\\) entries as 1 and the rest as 0, and \\(Y = \\text{Diag}(y_1, \\ldots, y_l)\\).\n",
    "\n",
    "From the first-order optimization condition:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L_R}{\\partial \\alpha} = \\gamma_A K \\alpha + \\frac{(u - l)^2}{2} KLK \\alpha - K J^T Y \\beta = 0\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61b78f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal alpha: [ 1.07962249 -1.14805856  1.14805856 -1.07962249]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_kernel(X, gamma=1.0):\n",
    "    \"\"\"Compute the Gaussian RBF kernel matrix.\"\"\"\n",
    "    pairwise_dists = np.linalg.norm(X[:, None] - X, axis=2) ** 2\n",
    "    K = np.exp(-gamma * pairwise_dists)\n",
    "    return K\n",
    "\n",
    "def compute_laplacian(K):\n",
    "    \"\"\"Compute the Laplacian matrix L from the kernel matrix K.\"\"\"\n",
    "    D = np.diag(K.sum(axis=1))\n",
    "    L = D - K\n",
    "    return L\n",
    "\n",
    "def solve_linear_system(A, b):\n",
    "    \"\"\"Solve the linear system Ax = b.\"\"\"\n",
    "    return np.linalg.solve(A, b)\n",
    "\n",
    "def laplacian_svm(X, y, gamma_A=1.0, C=1.0):\n",
    "    \"\"\"Train a Laplacian Support Vector Machine.\"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    K = compute_kernel(X)\n",
    "    L = compute_laplacian(K)\n",
    "    \n",
    "    # Define matrix for the quadratic term\n",
    "    A = gamma_A * K + (C / n_samples) * L\n",
    "    \n",
    "    # Define vector for the linear term\n",
    "    b = y\n",
    "    \n",
    "    # Solve the system A * alpha = b\n",
    "    alpha = solve_linear_system(A, b)\n",
    "    \n",
    "    return alpha\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample data\n",
    "    X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n",
    "    y = np.array([1, -1, 1, -1])\n",
    "    \n",
    "    # Parameters\n",
    "    gamma_A = 1.0\n",
    "    C = 1.0\n",
    "    \n",
    "    # Train the LapSVM\n",
    "    alpha = laplacian_svm(X, y, gamma_A=gamma_A, C=C)\n",
    "    \n",
    "    print(\"Optimal alpha:\", alpha)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30816a8",
   "metadata": {},
   "source": [
    "## Support Vector Machine Regression\n",
    "\n",
    "Support Vector Machine Regression (SVR) is an algorithm that looks for an optimal hyperplane to predict continuous values in a high-dimensional space.\n",
    "\n",
    "### 8.3.1 Support Vector Machine Regressor\n",
    "\n",
    "Given a training set of \\( N \\) data points \\( \\{ (x_k, y_k) \\}_{k=1}^N \\), where \\( x_k \\in \\mathbb{R}^n \\) is the \\( k \\)-th input pattern and \\( y_k \\in \\mathbb{R} \\) is the associated \"truth\", we aim to find a hyperplane \\( (w, b) \\) that satisfies certain conditions.\n",
    "\n",
    "The SVM learning algorithm seeks to minimize the following function:\n",
    "\n",
    "$$\n",
    "\\min_{w, b} \\quad \\frac{1}{2} \\|w\\|^2\n",
    "$$\n",
    "\n",
    "subject to:\n",
    "\n",
    "$$\n",
    "y_i \\left(w^T \\phi(x_i) - b \\right) \\geq \\epsilon\n",
    "$$\n",
    "\n",
    "where \\( \\phi : I \\subseteq \\mathbb{R}^n \\rightarrow F \\subseteq \\mathbb{R}^N \\) is a mapping from the input space \\( I \\) to the feature space \\( F \\), and \\( \\phi(x_i) \\) is the extracted feature of the input \\( x_i \\).\n",
    "\n",
    "Here, the distance between the point \\( x_i \\) and the decision boundary is given by:\n",
    "\n",
    "$$\n",
    "\\text{Quality} = \\left\\langle w, \\phi(x_i) \\right\\rangle - b\n",
    "$$\n",
    "\n",
    "The margin \\( \\gamma \\) is defined as:\n",
    "\n",
    "$$\n",
    "\\gamma = \\frac{1}{N} \\sum_{i=1}^N y_i \\left(\\left\\langle w, \\phi(x_i) \\right\\rangle - b \\right)\n",
    "$$\n",
    "\n",
    "The constrained optimization problem can be rewritten in Lagrangian form as an unconstrained optimization problem:\n",
    "\n",
    "$$\n",
    "\\min_{w, b} \\quad L(w, b) = \\frac{1}{2} \\|w\\|^2 - \\sum_{i=1}^N \\alpha_i \\left[ y_i \\left\\langle w, \\phi(x_i) \\right\\rangle - b \\right]\n",
    "$$\n",
    "\n",
    "where the Lagrange multipliers \\( \\alpha_i \\) are nonnegative.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8415679",
   "metadata": {},
   "source": [
    "## Optimization Conditions for Support Vector Machine Regression\n",
    "\n",
    "From the optimization conditions, we have:\n",
    "\n",
    "1. The gradient of the Lagrangian with respect to \\( w \\) is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L(w, b)}{\\partial w} = w - \\sum_{i=1}^N \\alpha_i y_i \\phi(x_i) = 0 \\quad \\Rightarrow \\quad w = \\sum_{i=1}^N \\alpha_i y_i \\phi(x_i)\n",
    "$$\n",
    "\n",
    "2. The gradient of the Lagrangian with respect to \\( b \\) is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L(w, b)}{\\partial b} = -\\sum_{i=1}^N \\alpha_i y_i = 0\n",
    "$$\n",
    "\n",
    "### Constrained Optimization with Respect to \\( \\alpha \\)\n",
    "\n",
    "Substituting these results into the original constrained optimization problem, we obtain the following dual optimization problems:\n",
    "\n",
    "#### Minimization Problem\n",
    "\n",
    "$$\n",
    "\\min_{\\alpha} \\quad J_1(\\alpha) = \\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j K(x_i, x_j) - \\sum_{i=1}^N \\alpha_i\n",
    "$$\n",
    "\n",
    "#### Maximization Problem\n",
    "\n",
    "$$\n",
    "\\max_{\\alpha} \\quad J_2(\\alpha) = \\sum_{i=1}^N \\alpha_i - \\frac{1}{2} \\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j K(x_i, x_j)\n",
    "$$\n",
    "\n",
    "subject to:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^N \\alpha_i y_i = 0\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\alpha_i \\geq 0, \\quad i = 1, \\ldots, N\n",
    "$$\n",
    "\n",
    "where \\( K(x_i, x_j) = \\langle \\phi(x_i), \\phi(x_j) \\rangle = \\phi(x_i)^T \\phi(x_j) \\) is the kernel function.\n",
    "\n",
    "### Algorithm for Support Vector Machine Regression\n",
    "\n",
    "1. Solve the maximization problem (8.3.8) with constraints (8.3.9) to obtain the Lagrange multipliers \\( \\alpha_i \\), \\( i = 1, \\ldots, N \\).\n",
    "2. Update the bias \\( b \\) using:\n",
    "\n",
    "$$\n",
    "b \\leftarrow b - \\eta \\sum_{i=1}^N \\alpha_i y_i\n",
    "$$\n",
    "\n",
    "3. Calculate the support vector regressor \\( w \\) using:\n",
    "\n",
    "$$\n",
    "w = \\sum_{i=1}^N \\alpha_i y_i \\phi(x_i)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30ec5d82",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cvxopt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7187/2107277591.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcvxopt\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolvers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m\"\"\"Compute the Gaussian RBF kernel matrix.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cvxopt'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from cvxopt import matrix, solvers\n",
    "\n",
    "def compute_kernel(X, gamma=1.0):\n",
    "    \"\"\"Compute the Gaussian RBF kernel matrix.\"\"\"\n",
    "    pairwise_dists = np.linalg.norm(X[:, None] - X, axis=2) ** 2\n",
    "    K = np.exp(-gamma * pairwise_dists)\n",
    "    return K\n",
    "\n",
    "def svr(X, y, C=1.0, epsilon=0.1, gamma=1.0):\n",
    "    \"\"\"Train a Support Vector Machine Regressor using quadratic programming.\"\"\"\n",
    "    N = X.shape[0]\n",
    "    K = compute_kernel(X, gamma)\n",
    "    \n",
    "    # Define the parameters for quadratic programming\n",
    "    H = np.outer(y, y) * K\n",
    "    H = (H + H.T) / 2  # Ensure symmetry\n",
    "    P = matrix(H)\n",
    "    \n",
    "    # Linear term\n",
    "    q = np.ones(N) * -1\n",
    "    q = matrix(q)\n",
    "    \n",
    "    # Constraints\n",
    "    G = np.vstack([-np.eye(N), np.eye(N)])\n",
    "    h = np.hstack([np.zeros(N), np.ones(N) * C])\n",
    "    G = matrix(G)\n",
    "    h = matrix(h)\n",
    "    \n",
    "    # Equality constraint: sum of alpha_i * y_i = 0\n",
    "    A = np.array(y, dtype=float)\n",
    "    A = matrix(A, (1, N))\n",
    "    b = matrix(np.zeros(1))\n",
    "    \n",
    "    # Solve the quadratic programming problem\n",
    "    solution = solvers.qp(P, q, G, h, A, b)\n",
    "    alpha = np.array(solution['x']).flatten()\n",
    "    \n",
    "    # Compute the bias term\n",
    "    support_vector_indices = alpha > 1e-5\n",
    "    support_vectors = X[support_vector_indices]\n",
    "    support_vector_labels = y[support_vector_indices]\n",
    "    support_vector_alphas = alpha[support_vector_indices]\n",
    "    \n",
    "    K_sv = compute_kernel(support_vectors, gamma)\n",
    "    b = np.mean(support_vector_labels - np.dot(K_sv, support_vector_alphas))\n",
    "    \n",
    "    # Compute the weights\n",
    "    w = np.dot(alpha * y, K)\n",
    "    \n",
    "    return w, b, alpha\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample data\n",
    "    X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n",
    "    y = np.array([1, -1, 1, -1])\n",
    "    \n",
    "    # Parameters\n",
    "    C = 1.0\n",
    "    epsilon = 0.1\n",
    "    gamma = 1.0\n",
    "    \n",
    "    # Train the SVR\n",
    "    w, b, alpha = svr(X, y, C=C, epsilon=epsilon, gamma=gamma)\n",
    "    \n",
    "    print(\"Weights:\", w)\n",
    "    print(\"Bias:\", b)\n",
    "    print(\"Alpha:\", alpha)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c6ae52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: [0. 0. 0. 0.]\n",
      "Bias: nan\n",
      "Alpha: [0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/radha/.local/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/radha/.local/lib/python3.7/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_kernel(X, gamma=1.0):\n",
    "    \"\"\"Compute the Gaussian RBF kernel matrix.\"\"\"\n",
    "    pairwise_dists = np.linalg.norm(X[:, None] - X, axis=2) ** 2\n",
    "    K = np.exp(-gamma * pairwise_dists)\n",
    "    return K\n",
    "\n",
    "def svr(X, y, C=1.0, epsilon=0.1, gamma=1.0, tol=1e-6, max_iter=1000):\n",
    "    \"\"\"Train a Support Vector Machine Regressor using a basic quadratic solver.\"\"\"\n",
    "    N = X.shape[0]\n",
    "    K = compute_kernel(X, gamma)\n",
    "    \n",
    "    # Initialize alpha values\n",
    "    alpha = np.zeros(N)\n",
    "    b = 0\n",
    "    \n",
    "    # Define the function for the dual problem\n",
    "    def objective(alpha):\n",
    "        return 0.5 * np.sum(alpha[:, None] * alpha[None, :] * y[:, None] * y[None, :] * K) - np.sum(alpha)\n",
    "    \n",
    "    # Simple gradient ascent for solving dual problem\n",
    "    for _ in range(max_iter):\n",
    "        gradient = np.dot((alpha * y), K) - 1\n",
    "        alpha += 0.01 * gradient  # Basic step size, needs tuning\n",
    "        alpha = np.clip(alpha, 0, C)\n",
    "        if np.linalg.norm(gradient) < tol:\n",
    "            break\n",
    "    \n",
    "    # Calculate the bias term\n",
    "    support_vector_indices = alpha > 1e-5\n",
    "    support_vectors = X[support_vector_indices]\n",
    "    support_vector_labels = y[support_vector_indices]\n",
    "    support_vector_alphas = alpha[support_vector_indices]\n",
    "    \n",
    "    K_sv = compute_kernel(support_vectors, gamma)\n",
    "    b = np.mean(support_vector_labels - np.dot(K_sv, support_vector_alphas))\n",
    "    \n",
    "    # Compute the weights\n",
    "    w = np.dot(alpha * y, K)\n",
    "    \n",
    "    return w, b, alpha\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample data\n",
    "    X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n",
    "    y = np.array([1, -1, 1, -1])\n",
    "    \n",
    "    # Parameters\n",
    "    C = 1.0\n",
    "    epsilon = 0.1\n",
    "    gamma = 1.0\n",
    "    \n",
    "    # Train the SVR\n",
    "    w, b, alpha = svr(X, y, C=C, epsilon=epsilon, gamma=gamma)\n",
    "    \n",
    "    print(\"Weights:\", w)\n",
    "    print(\"Bias:\", b)\n",
    "    print(\"Alpha:\", alpha)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612437bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
