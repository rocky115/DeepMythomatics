{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5153487",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " * Copyright (c) 2004 Radhamadhab Dalai\n",
    " *\n",
    " * Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    " * of this software and associated documentation files (the \"Software\"), to deal\n",
    " * in the Software without restriction, including without limitation the rights\n",
    " * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    " * copies of the Software, and to permit persons to whom the Software is\n",
    " * furnished to do so, subject to the following conditions:\n",
    " *\n",
    " * The above copyright notice and this permission notice shall be included in\n",
    " * all copies or substantial portions of the Software.\n",
    " *\n",
    " * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    " * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    " * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    " * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    " * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    " * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
    " * THE SOFTWARE.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91744f75",
   "metadata": {},
   "source": [
    "\n",
    "1. Defining the Model\n",
    "\n",
    "Before we can begin optimizing our model's parameters by minibatch SGD, we need to have some parameters in the first place. In the following, we initialize weights by drawing random numbers from a normal distribution with mean 0 and a standard deviation of 0.01. The magic number 0.01 often works well in practice, but you can specify a different value through the argument `sigma`. Moreover, we set the bias to 0. Note that for object-oriented design, we add the code to the `__init__` method of a subclass of `d2l.Module` (introduced in Section 3.2.2).\n",
    "\n",
    "```python\n",
    "class LinearRegressionScratch(d.Module):\n",
    "    #@save\n",
    "    \"\"\"The linear regression model implemented from scratch.\"\"\"\n",
    "    def __init__(self, num_inputs, lr, sigma=0.01):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.w = torch.normal(0, sigma, (num_inputs, 1), requires_grad=True)\n",
    "        self.b = torch.zeros(1, requires_grad=True)\n",
    "```\n",
    "\n",
    "Next, we must define our model, relating its input and parameters to its output. Using the same notation in (3.1.4), for our linear model, we simply take the matrix-vector product of the input features `X` and the model weights `w`, and add the offset `b` to each example. `Xw` is a vector and `b` is a scalar. Due to the broadcasting mechanism (see Section 2.1.4), when we add a vector and a scalar, the scalar is added to each component of the vector. The resulting `forward` method is registered in the `LinearRegressionScratch` class via `add_to_class` (introduced in Section 3.2.1).\n",
    "\n",
    "```python\n",
    "@d.add_to_class(LinearRegressionScratch)\n",
    "#@save\n",
    "def forward(self, X):\n",
    "    return torch.matmul(X, self.w) + self.b\n",
    "```\n",
    "\n",
    "2. Defining the Loss Function\n",
    "\n",
    "Since updating our model requires taking the gradient of our loss function, we ought to define the loss function first. Here we use the squared loss function in (3.1.5). In the implementation, we need to transform the true value `y` into the predicted value's shape `y_hat`. The result returned by the following method will also have the same shape as `y_hat`. We also return the averaged loss value among all examples in the minibatch.\n",
    "\n",
    "```python\n",
    "@d.add_to_class(LinearRegressionScratch)\n",
    "#@save\n",
    "def squared_loss(self, y_hat, y):\n",
    "    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "# Implementing Linear Regression from Scratch\n",
    "\n",
    "## Defining the Loss Function\n",
    "\n",
    "```python\n",
    "@d.add_to_class(LinearRegressionScratch)  # @save\n",
    "def loss(self, y_hat, y):\n",
    "    l = (y_hat - y) ** 2 / 2\n",
    "    return l.mean()\n",
    "```\n",
    "\n",
    "## Defining the Optimization Algorithm\n",
    "\n",
    "As discussed in Section 3.1, linear regression has a closed-form solution. However, our goal here is to illustrate how to train more general neural networks, which requires us to use minibatch stochastic gradient descent (SGD).\n",
    "\n",
    "At each step, using a minibatch randomly drawn from our dataset, we estimate the gradient of the loss with respect to the parameters. Then, we update the parameters in the direction that may reduce the loss.\n",
    "\n",
    "### Stochastic Gradient Descent (SGD)\n",
    "The following code defines the SGD optimizer:\n",
    "\n",
    "```python\n",
    "class SGD(d.HyperParameters):  # @save\n",
    "    \"\"\"Minibatch stochastic gradient descent.\"\"\"\n",
    "    def __init__(self, params, lr):\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def step(self):\n",
    "        for param in self.params:\n",
    "            param -= self.lr * param.grad\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for param in self.params:\n",
    "            if param.grad is not None:\n",
    "                param.grad.zero_()\n",
    "```\n",
    "\n",
    "We next define the `configure_optimizers` method, which returns an instance of the `SGD` class.\n",
    "\n",
    "```python\n",
    "@d.add_to_class(LinearRegressionScratch)  # @save\n",
    "def configure_optimizers(self):\n",
    "    return SGD([self.w, self.b], self.lr)\n",
    "```\n",
    "\n",
    "## Training the Model\n",
    "\n",
    "Now that we have all of the components (parameters, loss function, model, and optimizer), we are ready to implement the main training loop.\n",
    "\n",
    "### Training Loop\n",
    "\n",
    "At each epoch:\n",
    "\n",
    "- Iterate through the entire dataset.\n",
    "- Compute the loss on a minibatch.\n",
    "- Compute the gradient.\n",
    "- Update the parameters using the optimizer.\n",
    "\n",
    "Mathematically, this can be represented as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    &\\text{Initialize parameters } (w, b) \\\\\n",
    "    &\\text{Repeat until convergence:} \\\\\n",
    "    &\\quad \\text{Compute gradient: } g \\leftarrow \\frac{\\partial}{\\partial (w,b)} \\sum_{i \\in B} l(x^{(i)}, y^{(i)}, w, b) \\\\\n",
    "    &\\quad \\text{Update parameters: } (w, b) \\leftarrow (w, b) - \\eta g\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The training function is implemented as follows:\n",
    "\n",
    "```python\n",
    "@d.add_to_class(d.Trainer)  # @save\n",
    "def prepare_batch(self, batch):\n",
    "    return batch\n",
    "\n",
    "@d.add_to_class(d.Trainer)  # @save\n",
    "def fit_epoch(self):\n",
    "    self.model.train()\n",
    "    for batch in self.train_dataloader:\n",
    "        loss = self.model.training_step(self.prepare_batch(batch))\n",
    "        self.optim.zero_grad()\n",
    "        with torch.no_grad():\n",
    "            loss.backward()\n",
    "        if self.gradient_clip_val > 0:  # To be discussed later\n",
    "            self.clip_gradients(self.gradient_clip_val, self.model)\n",
    "        self.optim.step()\n",
    "        self.train_batch_idx += 1\n",
    "\n",
    "    if self.val_dataloader is None:\n",
    "        return\n",
    "    self.model.eval()\n",
    "    for batch in self.val_dataloader:\n",
    "        with torch.no_grad():\n",
    "            self.model.validation_step(self.prepare_batch(batch))\n",
    "        self.val_batch_idx += 1\n",
    "```\n",
    "\n",
    "## Generating Synthetic Data\n",
    "\n",
    "We now generate synthetic data for training. Here, we use the `SyntheticRegressionData` class with ground-truth parameters.\n",
    "\n",
    "```python\n",
    "data = d.SyntheticRegressionData(w=[2, -3.4], b=4.2)\n",
    "```\n",
    "\n",
    "## Training the Model\n",
    "\n",
    "We set the learning rate to \\(\\eta = 0.03\\) and train the model for 3 epochs:\n",
    "\n",
    "```python\n",
    "model = LinearRegressionScratch(lr=0.03)\n",
    "trainer = d.Trainer(max_epochs=3)\n",
    "trainer.fit(model, data)\n",
    "```\n",
    "\n",
    "This concludes the implementation of linear regression using stochastic gradient descent.\n",
    "```\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fe318574",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)\n",
    "\n",
    "## Implementing Linear Regression from Scratch\n",
    "\n",
    "### Defining the Loss Function\n",
    "\n",
    "```python\n",
    "@d.add_to_class(LinearRegressionScratch)  # @save\n",
    "def loss(self, y_hat, y):\n",
    "    l = (y_hat - y) ** 2 / 2\n",
    "    return l.mean()\n",
    "```\n",
    "\n",
    "## Defining the Optimization Algorithm\n",
    "\n",
    "As discussed in Section 3.1, linear regression has a closed-form solution. However, our goal here is to illustrate how to train more general neural networks, which requires us to use minibatch stochastic gradient descent (SGD). \n",
    "\n",
    "At each step, using a minibatch randomly drawn from our dataset, we estimate the gradient of the loss with respect to the parameters. Then, we update the parameters in the direction that may reduce the loss.\n",
    "\n",
    "### Stochastic Gradient Descent (SGD)\n",
    "The following code defines the SGD optimizer:\n",
    "\n",
    "```python\n",
    "class SGD(d.HyperParameters):  # @save\n",
    "    \"\"\"Minibatch stochastic gradient descent.\"\"\"\n",
    "    def __init__(self, params, lr):\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def step(self):\n",
    "        for param in self.params:\n",
    "            param -= self.lr * param.grad\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for param in self.params:\n",
    "            if param.grad is not None:\n",
    "                param.grad.zero_()\n",
    "```\n",
    "\n",
    "We next define the `configure_optimizers` method, which returns an instance of the `SGD` class.\n",
    "\n",
    "```python\n",
    "@d.add_to_class(LinearRegressionScratch)  # @save\n",
    "def configure_optimizers(self):\n",
    "    return SGD([self.w, self.b], self.lr)\n",
    "```\n",
    "\n",
    "## Training the Model\n",
    "\n",
    "Now that we have all of the components (parameters, loss function, model, and optimizer), we are ready to implement the main training loop.\n",
    "\n",
    "### Training Loop\n",
    "\n",
    "At each epoch:\n",
    "\n",
    "- Iterate through the entire dataset.\n",
    "- Compute the loss on a minibatch.\n",
    "- Compute the gradient.\n",
    "- Update the parameters using the optimizer.\n",
    "\n",
    "Mathematically, this can be represented as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    &\\text{Initialize parameters } (w, b) \\\\\n",
    "    &\\text{Repeat until convergence:} \\\\\n",
    "    &\\quad \\text{Compute gradient: } g \\leftarrow \\frac{\\partial}{\\partial (w,b)} \\sum_{i \\in B} l(x^{(i)}, y^{(i)}, w, b) \\\\\n",
    "    &\\quad \\text{Update parameters: } (w, b) \\leftarrow (w, b) - \\eta g\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The training function is implemented as follows:\n",
    "\n",
    "```python\n",
    "@d.add_to_class(d.Trainer)  # @save\n",
    "def prepare_batch(self, batch):\n",
    "    return batch\n",
    "\n",
    "@d.add_to_class(d.Trainer)  # @save\n",
    "def fit_epoch(self):\n",
    "    self.model.train()\n",
    "    for batch in self.train_dataloader:\n",
    "        loss = self.model.training_step(self.prepare_batch(batch))\n",
    "        self.optim.zero_grad()\n",
    "        with torch.no_grad():\n",
    "            loss.backward()\n",
    "        if self.gradient_clip_val > 0:  # To be discussed later\n",
    "            self.clip_gradients(self.gradient_clip_val, self.model)\n",
    "        self.optim.step()\n",
    "        self.train_batch_idx += 1\n",
    "\n",
    "    if self.val_dataloader is None:\n",
    "        return\n",
    "    self.model.eval()\n",
    "    for batch in self.val_dataloader:\n",
    "        with torch.no_grad():\n",
    "            self.model.validation_step(self.prepare_batch(batch))\n",
    "        self.val_batch_idx += 1\n",
    "```\n",
    "\n",
    "## Hyperparameter Selection and Training\n",
    "\n",
    "Both the number of epochs and the learning rate are hyperparameters. Setting hyperparameters is tricky, and we will usually want to use a 3-way split: one set for training, a second for hyperparameter selection, and the third reserved for final evaluation. We elide these details for now but will revise them later.\n",
    "\n",
    "```python\n",
    "model = LinearRegressionScratch(2, lr=0.03)\n",
    "data = d.SyntheticRegressionData(w=torch.tensor([2, -3.4]), b=4.2)\n",
    "trainer = d.Trainer(max_epochs=3)\n",
    "trainer.fit(model, data)\n",
    "```\n",
    "\n",
    "Because we synthesized the dataset ourselves, we know precisely what the true parameters are. Thus, we can evaluate our success in training by comparing the true parameters with those that we learned through our training loop. Indeed, they turn out to be very close to each other.\n",
    "\n",
    "```python\n",
    "print(f'error in estimating w: {data.w - model.w.reshape(data.w.shape)}')\n",
    "print(f'error in estimating b: {data.b - model.b}')\n",
    "```\n",
    "\n",
    "Example output:\n",
    "```python\n",
    "error in estimating w: tensor([ 0.1006, -0.1535], grad_fn=<SubBackward0>)\n",
    "error in estimating b: tensor([0.2132], grad_fn=<RsubBackward1>)\n",
    "\n",
    "\n",
    "We should not take the ability to exactly recover the ground-truth parameters for granted. In general, for deep models, unique solutions for the parameters do not exist, and even for linear models, exactly recovering the parameters is only possible when no feature is linearly dependent on the others. However, in machine learning, we are often less concerned with recovering true underlying parameters and more concerned with parameters that lead to highly accurate predictions (Vapnik, 1992). Fortunately, even on difficult optimization problems, stochastic gradient descent can often find remarkably good solutions, partly because, for deep networks, there exist many configurations of the parameters that lead to highly accurate predictions.\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73b27725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 148.1430\n",
      "Epoch 2, Loss: 83.4955\n",
      "Epoch 3, Loss: 47.4871\n",
      "Estimated Weights: [ 0.8811886 -1.1149743]\n",
      "Estimated Bias: -1.0005\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate Synthetic Data\n",
    "def generate_data(n=1000):\n",
    "    np.random.seed(42)\n",
    "    X = np.random.rand(n, 2) * 10  # Two features\n",
    "    w_true = np.array([2, -3.4])  # True weights\n",
    "    b_true = 4.2  # True bias\n",
    "    y = X @ w_true + b_true + np.random.randn(n) * 0.5  # Adding noise\n",
    "    return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "X_train, y_train = generate_data(1000)\n",
    "\n",
    "# Define the Linear Regression Model\n",
    "class LinearRegressionScratch(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "        self.w = nn.Parameter(torch.randn(num_features, 1))  # Random weights\n",
    "        self.b = nn.Parameter(torch.randn(1))  # Random bias\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return X @ self.w + self.b  # Linear transformation\n",
    "\n",
    "# Define the Loss Function & Optimizer\n",
    "def loss(y_hat, y):\n",
    "    return ((y_hat - y) ** 2 / 2).mean()  # MSE / 2\n",
    "\n",
    "class SGD:\n",
    "    def __init__(self, params, lr=0.03):\n",
    "        self.params = params\n",
    "        self.lr = lr\n",
    "    \n",
    "    def step(self):\n",
    "        for param in self.params:\n",
    "            param.data -= self.lr * param.grad  # Update rule\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for param in self.params:\n",
    "            if param.grad is not None:\n",
    "                param.grad.zero_()\n",
    "\n",
    "# Training the Model\n",
    "def train(model, X, y, lr=0.03, epochs=3):\n",
    "    optimizer = SGD([model.w, model.b], lr=lr)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        y_hat = model(X)  # Forward pass\n",
    "        l = loss(y_hat, y)  # Compute loss\n",
    "        l.backward()  # Compute gradients\n",
    "        optimizer.step()  # Update parameters\n",
    "        optimizer.zero_grad()  # Reset gradients\n",
    "        print(f\"Epoch {epoch+1}, Loss: {l.item():.4f}\")\n",
    "\n",
    "# Train the model\n",
    "model = LinearRegressionScratch(2)\n",
    "train(model, X_train, y_train)\n",
    "\n",
    "# Evaluate the Model\n",
    "print(f\"Estimated Weights: {model.w.data.flatten().numpy()}\")\n",
    "print(f\"Estimated Bias: {model.b.item():.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2408a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 58.1083\n",
      "Epoch 2, Loss: 34.8290\n",
      "Epoch 3, Loss: 21.0880\n",
      "Estimated Weights: [0.826513471323397, -2.410363336209654]\n",
      "Estimated Bias: 0.3743\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import random\n",
    "\n",
    "# Generate Synthetic Data\n",
    "def generate_data(n=1000):\n",
    "    random.seed(42)\n",
    "    X = [[random.uniform(0, 10), random.uniform(0, 10)] for _ in range(n)]  # Two features\n",
    "    w_true = [2, -3.4]  # True weights\n",
    "    b_true = 4.2  # True bias\n",
    "    y = [x[0] * w_true[0] + x[1] * w_true[1] + b_true + random.gauss(0, 0.5) for x in X]  # Adding noise\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = generate_data(1000)\n",
    "\n",
    "# Define the Linear Regression Model\n",
    "class LinearRegressionScratch:\n",
    "    def __init__(self, num_features):\n",
    "        self.w = [random.uniform(-1, 1) for _ in range(num_features)]  # Random weights\n",
    "        self.b = random.uniform(-1, 1)  # Random bias\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return [sum(x[i] * self.w[i] for i in range(len(self.w))) + self.b for x in X]\n",
    "\n",
    "# Define the Loss Function & Optimizer\n",
    "def loss(y_hat, y):\n",
    "    return sum((yh - yt) ** 2 / 2 for yh, yt in zip(y_hat, y)) / len(y)\n",
    "\n",
    "class SGD:\n",
    "    def __init__(self, params, lr=0.03):\n",
    "        self.params = params\n",
    "        self.lr = lr\n",
    "    \n",
    "    def step(self, gradients):\n",
    "        for i in range(len(self.params['w'])):\n",
    "            self.params['w'][i] -= self.lr * gradients['w'][i]\n",
    "        self.params['b'] -= self.lr * gradients['b']\n",
    "\n",
    "# Compute Gradients\n",
    "def compute_gradients(model, X, y):\n",
    "    y_hat = model.predict(X)\n",
    "    n = len(y)\n",
    "    gradients = {'w': [0] * len(model.w), 'b': 0}\n",
    "    \n",
    "    for i in range(n):\n",
    "        error = y_hat[i] - y[i]\n",
    "        for j in range(len(model.w)):\n",
    "            gradients['w'][j] += error * X[i][j] / n\n",
    "        gradients['b'] += error / n\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "# Training the Model\n",
    "def train(model, X, y, lr=0.03, epochs=3):\n",
    "    optimizer = SGD({'w': model.w, 'b': model.b}, lr=lr)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        gradients = compute_gradients(model, X, y)\n",
    "        optimizer.step(gradients)\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss(model.predict(X), y):.4f}\")\n",
    "\n",
    "# Train the model\n",
    "model = LinearRegressionScratch(2)\n",
    "train(model, X_train, y_train)\n",
    "\n",
    "# Evaluate the Model\n",
    "print(f\"Estimated Weights: {model.w}\")\n",
    "print(f\"Estimated Bias: {model.b:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa8dc8a",
   "metadata": {},
   "source": [
    "## 5. Concise Implementation of Linear Regression\n",
    "\n",
    "Deep learning has witnessed a Cambrian explosion of sorts over the past decade. The sheer number of techniques, applications, and algorithms by far surpasses the progress of previous decades. This is due to a fortuitous combination of multiple factors, one of which is the powerful free tools offered by a number of open-source deep learning frameworks. Theano (Bergstra et al., 2010), DistBelief (Dean et al., 2012), and Caffe (Jia et al., 2014) arguably represent the first generation of such models that found widespread adoption. In contrast to earlier (seminal) works like SN2 (Simulateur Neuristique) (Bottou and Le Cun, 1988), which provided a Lisp-like programming experience, modern frameworks offer automatic differentiation and the convenience of Python. These frameworks allow us to automate and modularize the repetitive work of implementing gradient-based learning algorithms.\n",
    "\n",
    "In Section 3.4, we relied only on (i) tensors for data storage and linear algebra; and (ii) automatic differentiation for calculating gradients. In practice, because data iterators, loss functions, optimizers, and neural network layers are so common, modern libraries implement these components for us as well. In this section, we will show you how to implement the linear regression model from Section 3.4 concisely by using high-level APIs of deep learning frameworks.\n",
    "\n",
    "### 5.1 Defining the Model\n",
    "\n",
    "When we implemented linear regression from scratch in Section 3.4, we defined our model parameters explicitly and coded up the calculations to produce output using basic linear algebra operations. You should know how to do this. But once your models get more complex, and once you have to do this nearly every day, you will be glad for the assistance. The situation is similar to coding up your own blog from scratch. Doing it once or twice is rewarding and instructive, but you would be a lousy web developer if you spent a month reinventing the wheel. For standard operations, we can use a framework’s predefined layers, which allow us to focus on the layers used to construct the model rather than worrying about their implementation.\n",
    "\n",
    "```python\n",
    "class LinearRegression:\n",
    "    def __init__(self, input_dim, lr=0.01):\n",
    "        self.weights = [0.0] * input_dim\n",
    "        self.bias = 0.0\n",
    "        self.lr = lr\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return [sum(x * w for x, w in zip(sample, self.weights)) + self.bias for sample in X]\n",
    "    \n",
    "    def update_weights(self, gradients):\n",
    "        self.weights = [w - self.lr * g for w, g in zip(self.weights, gradients)]\n",
    "        self.bias -= self.lr * sum(gradients) / len(gradients)\n",
    "```\n",
    "\n",
    "Recall the architecture of a single-layer network as described in Figure 3.1.2. The layer is called fully connected since each of its inputs is connected to each of its outputs by means of a matrix-vector multiplication. In modern deep learning frameworks, the fully connected layer is defined in `Linear` and `LazyLinear` classes. The latter allows users to only specify the output dimension, while the former additionally asks for how many inputs go into this layer. Specifying input shapes is inconvenient, which may require nontrivial calculations (such as in convolutional layers). Thus, for simplicity, we will use such “lazy” layers whenever we can.\n",
    "\n",
    "### 5.2 Defining the Loss Function\n",
    "\n",
    "The `MSELoss` class computes the mean squared error (without the 1/2 factor). By default, `MSELoss` returns the average loss over examples. It is faster (and easier to use) than implementing our own loss function.\n",
    "\n",
    "```python\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return sum((yt - yp) ** 2 for yt, yp in zip(y_true, y_pred)) / len(y_true)\n",
    "```\n",
    "\n",
    "### 5.3 Defining the Optimization Algorithm\n",
    "\n",
    "Minibatch SGD is a standard tool for optimizing neural networks, and modern deep learning frameworks support it alongside a number of variations on this algorithm in their optimization modules. When we instantiate an SGD instance, we specify the parameters to optimize over, obtainable from our model via `self.parameters()`, and the learning rate (`self.lr`) required by our optimization algorithm.\n",
    "\n",
    "```python\n",
    "def stochastic_gradient_descent(model, X, y, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        y_pred = model.predict(X)\n",
    "        loss = mean_squared_error(y, y_pred)\n",
    "        gradients = [(yp - yt) for yp, yt in zip(y_pred, y)]\n",
    "        model.update_weights(gradients)\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss:.4f}')\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0787f3bc",
   "metadata": {},
   "source": [
    "\n",
    "## Concise Implementation of Linear Regression\n",
    "\n",
    "Deep learning has witnessed significant advancements over the past decade. Modern frameworks allow us to automate and modularize gradient-based learning algorithms. In this section, we will implement the linear regression model concisely using high-level APIs.\n",
    "\n",
    "## Importing Libraries\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "```\n",
    "\n",
    "## Defining the Model\n",
    "\n",
    "```python\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, lr):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.net = nn.Linear(2, 1)  # Fully connected layer\n",
    "        self.net.weight.data.normal_(0, 0.01)\n",
    "        self.net.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.net(X)\n",
    "\n",
    "    def loss(self, y_hat, y):\n",
    "        fn = nn.MSELoss()\n",
    "        return fn(y_hat, y)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.SGD(self.parameters(), self.lr)\n",
    "```\n",
    "\n",
    "## Training the Model\n",
    "\n",
    "```python\n",
    "# Generating synthetic data\n",
    "true_w = torch.tensor([2, -3.4])\n",
    "true_b = 4.2\n",
    "data = torch.randn(1000, 2)\n",
    "labels = data @ true_w + true_b + torch.randn(1000) * 0.01\n",
    "\n",
    "def train_model():\n",
    "    model = LinearRegression(lr=0.03)\n",
    "    optimizer = model.configure_optimizers()\n",
    "    loss_fn = model.loss\n",
    "    \n",
    "    for epoch in range(3):\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = model(data)\n",
    "        loss = loss_fn(y_hat, labels.view(-1, 1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = train_model()\n",
    "```\n",
    "\n",
    "## Evaluating the Model\n",
    "\n",
    "```python\n",
    "def get_w_b(model):\n",
    "    return model.net.weight.data, model.net.bias.data\n",
    "\n",
    "w, b = get_w_b(model)\n",
    "print(f'error in estimating w: {true_w - w.reshape(true_w.shape)}')\n",
    "print(f'error in estimating b: {true_b - b}')\n",
    "```\n",
    "\n",
    "### Output Example:\n",
    "```\n",
    "error in estimating w: tensor([ 0.0022, -0.0069])\n",
    "error in estimating b: tensor([0.0080])\n",
    "```\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5eee8889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 41.8427\n",
      "Epoch 2, Loss: 39.4340\n",
      "Epoch 3, Loss: 37.1641\n",
      "Error in estimating w: [ 3.57940776 -2.33460353]\n",
      "Error in estimating b: 4.211997049641287\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate synthetic data\n",
    "def generate_data(n_samples=1000):\n",
    "    np.random.seed(42)\n",
    "    true_w = np.array([2, -3.4])\n",
    "    true_b = 4.2\n",
    "    X = np.random.randn(n_samples, 2)\n",
    "    y = X @ true_w + true_b + np.random.randn(n_samples) * 0.01\n",
    "    return X, y, true_w, true_b\n",
    "\n",
    "X, y, true_w, true_b = generate_data()\n",
    "\n",
    "# Linear Regression Model\n",
    "class LinearRegression:\n",
    "    def __init__(self, lr=0.03):\n",
    "        self.lr = lr\n",
    "        self.w = np.random.randn(2)\n",
    "        self.b = np.random.randn()\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return X @ self.w + self.b\n",
    "    \n",
    "    def loss(self, y_hat, y):\n",
    "        return ((y_hat - y) ** 2).mean()\n",
    "    \n",
    "    def train(self, X, y, epochs=3):\n",
    "        n = len(y)\n",
    "        for epoch in range(epochs):\n",
    "            y_hat = self.predict(X)\n",
    "            grad_w = (X.T @ (y_hat - y)) / n\n",
    "            grad_b = (y_hat - y).mean()\n",
    "            self.w -= self.lr * grad_w\n",
    "            self.b -= self.lr * grad_b\n",
    "            print(f'Epoch {epoch+1}, Loss: {self.loss(y_hat, y):.4f}')\n",
    "\n",
    "# Train the model\n",
    "model = LinearRegression(lr=0.03)\n",
    "model.train(X, y, epochs=3)\n",
    "\n",
    "# Evaluate the model\n",
    "print(f'Error in estimating w: {true_w - model.w}')\n",
    "print(f'Error in estimating b: {true_b - model.b}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "255abbfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 18.1382\n",
      "Epoch 2, Loss: 16.0748\n",
      "Epoch 3, Loss: 14.2468\n",
      "Error in estimating w: tensor([ 0.7516, -2.7659])\n",
      "Error in estimating b: tensor([2.1833])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Generate synthetic data\n",
    "def generate_data(n_samples=1000):\n",
    "    torch.manual_seed(42)\n",
    "    true_w = torch.tensor([2.0, -3.4])\n",
    "    true_b = 4.2\n",
    "    X = torch.randn(n_samples, 2)\n",
    "    y = X @ true_w + true_b + torch.randn(n_samples) * 0.01\n",
    "    return X, y, true_w, true_b\n",
    "\n",
    "X, y, true_w, true_b = generate_data()\n",
    "\n",
    "# Linear Regression Model\n",
    "class LinearRegression(torch.nn.Module):\n",
    "    def __init__(self, lr=0.03):\n",
    "        super().__init__()\n",
    "        self.w = torch.nn.Parameter(torch.randn(2, requires_grad=True))\n",
    "        self.b = torch.nn.Parameter(torch.randn(1, requires_grad=True))\n",
    "        self.lr = lr\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return X @ self.w + self.b\n",
    "    \n",
    "    def loss(self, y_hat, y):\n",
    "        return torch.mean((y_hat - y) ** 2)\n",
    "    \n",
    "    def train(self, X, y, epochs=3):\n",
    "        optimizer = torch.optim.SGD(self.parameters(), lr=self.lr)\n",
    "        for epoch in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = self.forward(X)\n",
    "            loss = self.loss(y_hat, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}')\n",
    "\n",
    "# Train the model\n",
    "model = LinearRegression(lr=0.03)\n",
    "model.train(X, y, epochs=3)\n",
    "\n",
    "# Evaluate the model\n",
    "print(f'Error in estimating w: {true_w - model.w.detach()}')\n",
    "print(f'Error in estimating b: {true_b - model.b.detach()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "446f7881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 30.5174\n",
      "Epoch 2, Loss: 28.6857\n",
      "Epoch 3, Loss: 26.9640\n",
      "Error in estimating w: [1.0355043396349708, -3.692129574485173]\n",
      "Error in estimating b: 3.200872572734384\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Generate synthetic data\n",
    "def generate_data(n_samples=1000):\n",
    "    random.seed(42)\n",
    "    true_w = [2.0, -3.4]\n",
    "    true_b = 4.2\n",
    "    X = [[random.gauss(0, 1) for _ in range(2)] for _ in range(n_samples)]\n",
    "    y = [sum(x[i] * true_w[i] for i in range(2)) + true_b + random.gauss(0, 0.01) for x in X]\n",
    "    return X, y, true_w, true_b\n",
    "\n",
    "X, y, true_w, true_b = generate_data()\n",
    "\n",
    "# Linear Regression Model\n",
    "class LinearRegression:\n",
    "    def __init__(self, lr=0.03):\n",
    "        self.w = [random.random(), random.random()]\n",
    "        self.b = random.random()\n",
    "        self.lr = lr\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return [sum(X[i][j] * self.w[j] for j in range(2)) + self.b for i in range(len(X))]\n",
    "    \n",
    "    def loss(self, y_hat, y):\n",
    "        return sum((y_hat[i] - y[i]) ** 2 for i in range(len(y))) / len(y)\n",
    "    \n",
    "    def train(self, X, y, epochs=3):\n",
    "        for epoch in range(epochs):\n",
    "            y_hat = self.forward(X)\n",
    "            grad_w = [sum((y_hat[i] - y[i]) * X[i][j] for i in range(len(y))) / len(y) for j in range(2)]\n",
    "            grad_b = sum(y_hat[i] - y[i] for i in range(len(y))) / len(y)\n",
    "            \n",
    "            self.w = [self.w[j] - self.lr * grad_w[j] for j in range(2)]\n",
    "            self.b -= self.lr * grad_b\n",
    "            \n",
    "            print(f'Epoch {epoch+1}, Loss: {self.loss(y_hat, y):.4f}')\n",
    "\n",
    "# Train the model\n",
    "model = LinearRegression(lr=0.03)\n",
    "model.train(X, y, epochs=3)\n",
    "\n",
    "# Evaluate the model\n",
    "print(f'Error in estimating w: {[true_w[i] - model.w[i] for i in range(2)]}')\n",
    "print(f'Error in estimating b: {true_b - model.b}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a27d2d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
