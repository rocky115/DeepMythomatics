{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7c76fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " * Copyright (c) 2004 Radhamadhab Dalai\n",
    " *\n",
    " * Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    " * of this software and associated documentation files (the \"Software\"), to deal\n",
    " * in the Software without restriction, including without limitation the rights\n",
    " * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    " * copies of the Software, and to permit persons to whom the Software is\n",
    " * furnished to do so, subject to the following conditions:\n",
    " *\n",
    " * The above copyright notice and this permission notice shall be included in\n",
    " * all copies or substantial portions of the Software.\n",
    " *\n",
    " * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    " * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    " * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    " * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    " * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    " * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
    " * THE SOFTWARE.\n",
    "'''"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "c6d76145",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec9e27e",
   "metadata": {},
   "source": [
    "I'll help you create a markdown explanation of the Module class using LaTeX formatting for a Jupyter notebook. I'll break down the key components and add mathematical notation where appropriate.\n",
    "\n",
    "# Neural Network Module Base Class\n",
    "\n",
    "## Overview\n",
    "\n",
    "The `Module` class serves as the foundational class for all models, inheriting from PyTorch's `nn.Module` and a custom `HyperParameters` class. Let's examine its core components.\n",
    "\n",
    "## Class Definition\n",
    "\n",
    "```python\n",
    "class Module(nn.Module, d2l.HyperParameters):\n",
    "    \"\"\"The base class of models.\"\"\"\n",
    "    def __init__(self, plot_train_per_epoch=2, plot_valid_per_epoch=1):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.board = ProgressBoard()\n",
    "```\n",
    "\n",
    "## Essential Methods\n",
    "\n",
    "### Loss Function\n",
    "The abstract loss function that must be implemented by subclasses:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\hat{y}, y) \\rightarrow \\text{loss value}\n",
    "$$\n",
    "\n",
    "```python\n",
    "def loss(self, y_hat, y):\n",
    "    raise NotImplementedError\n",
    "```\n",
    "\n",
    "### Forward Pass\n",
    "The forward computation expects a defined neural network:\n",
    "\n",
    "$$\n",
    "f(X; \\theta) = \\text{net}(X)\n",
    "$$\n",
    "\n",
    "```python\n",
    "def forward(self, X):\n",
    "    assert hasattr(self, 'net'), 'Neural network is defined'\n",
    "    return self.net(X)\n",
    "```\n",
    "\n",
    "### Training Step\n",
    "For each batch $\\mathcal{B}$, computes and plots the loss:\n",
    "\n",
    "$$\n",
    "\\text{step}(\\mathcal{B}) = \\mathcal{L}(f(X_\\mathcal{B}), y_\\mathcal{B})\n",
    "$$\n",
    "\n",
    "```python\n",
    "def training_step(self, batch):\n",
    "    l = self.loss(self(*batch[:-1]), batch[-1])\n",
    "    self.plot('loss', l, train=True)\n",
    "    return l\n",
    "```\n",
    "\n",
    "### Validation Step\n",
    "Similar to training step but for validation data:\n",
    "\n",
    "$$\n",
    "\\text{validate}(\\mathcal{B}) = \\mathcal{L}(f(X_\\mathcal{B}), y_\\mathcal{B})\n",
    "$$\n",
    "\n",
    "```python\n",
    "def validation_step(self, batch):\n",
    "    l = self.loss(self(*batch[:-1]), batch[-1])\n",
    "    self.plot('loss', l, train=False)\n",
    "```\n",
    "\n",
    "### Progress Plotting\n",
    "Visualizes training progress with the following parameters:\n",
    "- Training points plotted per epoch: $n_{\\text{train}} = 2$\n",
    "- Validation points plotted per epoch: $n_{\\text{valid}} = 1$\n",
    "\n",
    "Training point position is calculated as:\n",
    "\n",
    "$$\n",
    "x_{\\text{train}} = \\frac{\\text{batch\\_idx}}{\\text{num\\_train\\_batches}}\n",
    "$$\n",
    "\n",
    "Validation point position is:\n",
    "\n",
    "$$\n",
    "x_{\\text{valid}} = \\text{epoch} + 1\n",
    "$$\n",
    "\n",
    "## Notes\n",
    "\n",
    "1. This class inherits from `nn.Module`, enabling automatic parameter management and the convenient calling interface:\n",
    "   \n",
    "   $$\n",
    "   \\text{model}(X) \\equiv \\text{model.forward}(X)\n",
    "   $$\n",
    "\n",
    "2. The `configure_optimizers` method must be implemented by subclasses to specify the optimization algorithm:\n",
    "\n",
    "   $$\n",
    "   \\text{opt}(\\theta, \\nabla_\\theta \\mathcal{L})\n",
    "   $$\n",
    "\n",
    "3. Progress visualization frequency is controlled by:\n",
    "   - `plot_train_per_epoch`: Points plotted during training per epoch\n",
    "   - `plot_valid_per_epoch`: Points plotted during validation per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd682c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "class ProgressBoard:\n",
    "    \"\"\"Simple progress board for tracking and visualizing training metrics.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.xlabel = None\n",
    "        self.metrics = {}\n",
    "        \n",
    "    def draw(self, x: float, y: float, metric_name: str, every_n: int = 1):\n",
    "        if metric_name not in self.metrics:\n",
    "            self.metrics[metric_name] = {'x': [], 'y': []}\n",
    "        \n",
    "        if len(self.metrics[metric_name]['x']) % every_n == 0:\n",
    "            self.metrics[metric_name]['x'].append(x)\n",
    "            self.metrics[metric_name]['y'].append(y)\n",
    "\n",
    "class HyperParameters:\n",
    "    \"\"\"Mixin class for saving hyperparameters.\"\"\"\n",
    "    def save_hyperparameters(self):\n",
    "        \"\"\"Save arguments from __init__ as class attributes.\"\"\"\n",
    "        frame = inspect.currentframe().f_back\n",
    "        _, _, _, local_vars = inspect.getargvalues(frame)\n",
    "        self.hparams = {k: v for k, v in local_vars.items() if k != 'self'}\n",
    "        for k, v in self.hparams.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "class Module(nn.Module, HyperParameters):\n",
    "    \"\"\"Base neural network module with training utilities.\"\"\"\n",
    "    \n",
    "    def __init__(self, plot_train_per_epoch: int = 2, plot_valid_per_epoch: int = 1):\n",
    "        \"\"\"\n",
    "        Initialize the module.\n",
    "        \n",
    "        Args:\n",
    "            plot_train_per_epoch: Number of training points to plot per epoch\n",
    "            plot_valid_per_epoch: Number of validation points to plot per epoch\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.board = ProgressBoard()\n",
    "        self.trainer = None  # Will be set by Trainer class\n",
    "        \n",
    "    def loss(self, y_hat: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Calculate loss between predictions and targets.\n",
    "        Must be implemented by subclasses.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "        Requires self.net to be defined.\n",
    "        \"\"\"\n",
    "        assert hasattr(self, 'net'), 'Neural network is not defined'\n",
    "        return self.net(X)\n",
    "        \n",
    "    def plot(self, key: str, value: torch.Tensor, train: bool):\n",
    "        \"\"\"\n",
    "        Plot a point in the progress animation.\n",
    "        \n",
    "        Args:\n",
    "            key: Metric name\n",
    "            value: Metric value\n",
    "            train: Whether this is a training or validation metric\n",
    "        \"\"\"\n",
    "        assert hasattr(self, 'trainer'), 'Trainer is not initialized'\n",
    "        self.board.xlabel = 'epoch'\n",
    "        \n",
    "        if train:\n",
    "            x = self.trainer.train_batch_idx / self.trainer.num_train_batches\n",
    "            n = self.trainer.num_train_batches / self.plot_train_per_epoch\n",
    "        else:\n",
    "            x = self.trainer.epoch + 1\n",
    "            n = self.trainer.num_val_batches / self.plot_valid_per_epoch\n",
    "            \n",
    "        self.board.draw(\n",
    "            x=x,\n",
    "            y=value.cpu().detach().numpy(),\n",
    "            metric_name=('train_' if train else 'val_') + key,\n",
    "            every_n=int(n)\n",
    "        )\n",
    "        \n",
    "    def training_step(self, batch: Tuple[torch.Tensor, ...]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Perform a training step on a batch of data.\n",
    "        \n",
    "        Args:\n",
    "            batch: Tuple of (inputs..., targets)\n",
    "        Returns:\n",
    "            Loss value\n",
    "        \"\"\"\n",
    "        l = self.loss(self(*batch[:-1]), batch[-1])\n",
    "        self.plot('loss', l, train=True)\n",
    "        return l\n",
    "        \n",
    "    def validation_step(self, batch: Tuple[torch.Tensor, ...]):\n",
    "        \"\"\"\n",
    "        Perform a validation step on a batch of data.\n",
    "        \n",
    "        Args:\n",
    "            batch: Tuple of (inputs..., targets)\n",
    "        \"\"\"\n",
    "        l = self.loss(self(*batch[:-1]), batch[-1])\n",
    "        self.plot('loss', l, train=False)\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "        Configure optimizers for training.\n",
    "        Must be implemented by subclasses.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "# Example implementation of a simple regression model\n",
    "class LinearRegression(Module):\n",
    "    \"\"\"Linear regression model implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int, learning_rate: float = 0.01):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = nn.Linear(input_dim, 1)\n",
    "        \n",
    "    def loss(self, y_hat: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Mean squared error loss.\"\"\"\n",
    "        return torch.mean((y_hat - y) ** 2)\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Configure SGD optimizer.\"\"\"\n",
    "        return torch.optim.SGD(self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "# Example implementation of a classification model\n",
    "class MLPClassifier(Module):\n",
    "    \"\"\"Multi-layer perceptron classifier.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int, hidden_dim: int, num_classes: int, \n",
    "                 learning_rate: float = 0.01):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "        \n",
    "    def loss(self, y_hat: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Cross entropy loss.\"\"\"\n",
    "        return nn.functional.cross_entropy(y_hat, y)\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Configure Adam optimizer.\"\"\"\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "# Example usage\n",
    "def train_model(model: Module, train_data, val_data, epochs: int = 10):\n",
    "    \"\"\"Simple training loop example.\"\"\"\n",
    "    optimizer = model.configure_optimizers()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        for batch in train_data:\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.training_step(batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in val_data:\n",
    "                model.validation_step(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2860aee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import time\n",
    "from IPython import display\n",
    "\n",
    "def use_svg_display():\n",
    "    \"\"\"Use the svg format to display a plot in Jupyter.\"\"\"\n",
    "    display.set_matplotlib_formats('svg')\n",
    "\n",
    "def set_figsize(figsize=(3.5, 2.5)):\n",
    "    \"\"\"Set the figure size for matplotlib.\"\"\"\n",
    "    use_svg_display()\n",
    "    plt.rcParams['figure.figsize'] = figsize\n",
    "\n",
    "class Animator:\n",
    "    \"\"\"For plotting data in animation.\"\"\"\n",
    "    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,\n",
    "                 ylim=None, xscale='linear', yscale='linear',\n",
    "                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,\n",
    "                 figsize=(3.5, 2.5)):\n",
    "        \"\"\"Defined in :numref:`sec_utils`\"\"\"\n",
    "        if legend is None:\n",
    "            legend = []\n",
    "        use_svg_display()\n",
    "        self.fig, self.axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
    "        if nrows * ncols == 1:\n",
    "            self.axes = [self.axes, ]\n",
    "        # Use a lambda to capture arguments\n",
    "        self.config_axes = lambda: set_axes(\n",
    "            self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n",
    "        self.X, self.Y, self.fmts = None, None, fmts\n",
    "\n",
    "    def add(self, x, y):\n",
    "        \"\"\"Add multiple data points into the figure.\"\"\"\n",
    "        if not hasattr(y, \"__len__\"):\n",
    "            y = [y]\n",
    "        n = len(y)\n",
    "        if not hasattr(x, \"__len__\"):\n",
    "            x = [x] * n\n",
    "        if not self.X:\n",
    "            self.X = [[] for _ in range(n)]\n",
    "        if not self.Y:\n",
    "            self.Y = [[] for _ in range(n)]\n",
    "        for i, (a, b) in enumerate(zip(x, y)):\n",
    "            if a is not None and b is not None:\n",
    "                self.X[i].append(a)\n",
    "                self.Y[i].append(b)\n",
    "        self.axes[0].cla()\n",
    "        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n",
    "            self.axes[0].plot(x, y, fmt)\n",
    "        self.config_axes()\n",
    "        display.display(self.fig)\n",
    "        display.clear_output(wait=True)\n",
    "\n",
    "class Module(nn.Module):\n",
    "    \"\"\"The base class of models.\"\"\"\n",
    "    def __init__(self, plot_train_per_epoch=2, plot_valid_per_epoch=1):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.animator = Animator()\n",
    "        self.training_stats = {'loss': [], 'epoch': []}\n",
    "        self.validation_stats = {'loss': [], 'epoch': []}\n",
    "        \n",
    "    def save_hyperparameters(self):\n",
    "        \"\"\"Save function arguments into class attributes.\"\"\"\n",
    "        frame = inspect.currentframe().f_back\n",
    "        _, _, _, local_vars = inspect.getargvalues(frame)\n",
    "        self.hparams = {k:v for k, v in local_vars.items() if k != 'self'}\n",
    "        for k, v in self.hparams.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "    def loss(self, y_hat, y):\n",
    "        \"\"\"Calculate loss between predictions and targets.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward computation.\"\"\"\n",
    "        assert hasattr(self, 'net'), 'Neural network is not defined'\n",
    "        return self.net(X)\n",
    "\n",
    "    def plot(self, key, value, train):\n",
    "        \"\"\"Plot a point in animation.\"\"\"\n",
    "        assert hasattr(self, 'trainer'), 'Trainer is not inited'\n",
    "        \n",
    "        if train:\n",
    "            x = self.trainer.train_batch_idx / self.trainer.num_train_batches\n",
    "            self.training_stats['loss'].append(value.item())\n",
    "            self.training_stats['epoch'].append(x)\n",
    "        else:\n",
    "            x = self.trainer.epoch + 1\n",
    "            self.validation_stats['loss'].append(value.item())\n",
    "            self.validation_stats['epoch'].append(x)\n",
    "            \n",
    "        # Update plot\n",
    "        self.animator.add(\n",
    "            x=[x], \n",
    "            y=[value.item()]\n",
    "        )\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        \"\"\"Perform one training step.\"\"\"\n",
    "        l = self.loss(self(*batch[:-1]), batch[-1])\n",
    "        self.plot('loss', l, train=True)\n",
    "        return l\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        \"\"\"Perform one validation step.\"\"\"\n",
    "        l = self.loss(self(*batch[:-1]), batch[-1])\n",
    "        self.plot('loss', l, train=False)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Configure optimizer.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "class LinearRegressionScratch(Module):\n",
    "    \"\"\"Linear regression implemented from scratch.\"\"\"\n",
    "    def __init__(self, num_inputs, lr, sigma=0.01):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.w = torch.normal(0, sigma, (num_inputs, 1), requires_grad=True)\n",
    "        self.b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"The linear regression model.\"\"\"\n",
    "        return torch.matmul(X, self.w) + self.b\n",
    "\n",
    "    def loss(self, y_hat, y):\n",
    "        \"\"\"Mean squared error loss.\"\"\"\n",
    "        l = (y_hat - y.reshape(y_hat.shape)) ** 2\n",
    "        return l.mean()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Return optimizer.\"\"\"\n",
    "        return torch.optim.SGD([self.w, self.b], self.lr)\n",
    "\n",
    "def train_epoch(model, train_iter, loss, optimizer):\n",
    "    \"\"\"Train the model for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    for batch in train_iter:\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.training_step(batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def train(model, train_iter, valid_iter, num_epochs):\n",
    "    \"\"\"Train the model.\"\"\"\n",
    "    optimizer = model.configure_optimizers()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_epoch(model, train_iter, model.loss, optimizer)\n",
    "        \n",
    "        if valid_iter is not None:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for batch in valid_iter:\n",
    "                    model.validation_step(batch)\n",
    "\n",
    "# Example usage:\n",
    "def synthetic_data(w, b, num_examples):\n",
    "    \"\"\"Generate y = Xw + b + noise.\"\"\"\n",
    "    X = torch.normal(0, 1, (num_examples, len(w)))\n",
    "    y = torch.matmul(X, w) + b\n",
    "    y += torch.normal(0, 0.01, y.shape)\n",
    "    return X, y.reshape((-1, 1))\n",
    "\n",
    "# Generate synthetic data\n",
    "true_w = torch.tensor([2, -3.4])\n",
    "true_b = 4.2\n",
    "features, labels = synthetic_data(true_w, true_b, 1000)\n",
    "\n",
    "# Create data iterator\n",
    "def load_array(data_arrays, batch_size, is_train=True):\n",
    "    \"\"\"Construct a PyTorch data iterator.\"\"\"\n",
    "    dataset = torch.utils.data.TensorDataset(*data_arrays)\n",
    "    return torch.utils.data.DataLoader(dataset, batch_size, shuffle=is_train)\n",
    "\n",
    "batch_size = 10\n",
    "data_iter = load_array((features, labels), batch_size)\n",
    "\n",
    "# Create and train model\n",
    "model = LinearRegressionScratch(2, lr=0.03)\n",
    "train(model, data_iter, None, num_epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc17ae05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "class Tensor:\n",
    "    \"\"\"Simple tensor implementation with autograd.\"\"\"\n",
    "    def __init__(self, data: List[List[float]], requires_grad: bool = False):\n",
    "        self.data = data\n",
    "        self.grad = None\n",
    "        self.requires_grad = requires_grad\n",
    "        self.grad_fn = None\n",
    "        \n",
    "    def __add__(self, other: 'Tensor') -> 'Tensor':\n",
    "        result = Tensor([[self.data[i][j] + other.data[i][j]\n",
    "                         for j in range(len(self.data[0]))]\n",
    "                        for i in range(len(self.data))])\n",
    "        if self.requires_grad or other.requires_grad:\n",
    "            result.requires_grad = True\n",
    "            result.grad_fn = ('add', self, other)\n",
    "        return result\n",
    "    \n",
    "    def __mul__(self, other: 'Tensor') -> 'Tensor':\n",
    "        result = Tensor([[self.data[i][j] * other.data[i][j]\n",
    "                         for j in range(len(self.data[0]))]\n",
    "                        for i in range(len(self.data))])\n",
    "        if self.requires_grad or other.requires_grad:\n",
    "            result.requires_grad = True\n",
    "            result.grad_fn = ('mul', self, other)\n",
    "        return result\n",
    "    \n",
    "    def matmul(self, other: 'Tensor') -> 'Tensor':\n",
    "        result = [[sum(self.data[i][k] * other.data[k][j]\n",
    "                      for k in range(len(other.data)))\n",
    "                  for j in range(len(other.data[0]))]\n",
    "                 for i in range(len(self.data))]\n",
    "        tensor = Tensor(result)\n",
    "        if self.requires_grad or other.requires_grad:\n",
    "            tensor.requires_grad = True\n",
    "            tensor.grad_fn = ('matmul', self, other)\n",
    "        return tensor\n",
    "    \n",
    "    def mean(self) -> 'Tensor':\n",
    "        total = sum(sum(row) for row in self.data)\n",
    "        size = len(self.data) * len(self.data[0])\n",
    "        result = Tensor([[total / size]])\n",
    "        if self.requires_grad:\n",
    "            result.requires_grad = True\n",
    "            result.grad_fn = ('mean', self)\n",
    "        return result\n",
    "    \n",
    "    def backward(self, grad: Optional['Tensor'] = None):\n",
    "        if grad is None:\n",
    "            grad = Tensor([[1.0]])\n",
    "            \n",
    "        if self.grad_fn is None:\n",
    "            self.grad = grad\n",
    "            return\n",
    "            \n",
    "        op, *inputs = self.grad_fn\n",
    "        if op == 'add':\n",
    "            for input_tensor in inputs:\n",
    "                if input_tensor.requires_grad:\n",
    "                    input_tensor.backward(grad)\n",
    "        elif op == 'mul':\n",
    "            for i, input_tensor in enumerate(inputs):\n",
    "                if input_tensor.requires_grad:\n",
    "                    other = inputs[1 - i]\n",
    "                    input_grad = Tensor([[grad.data[0][0] * other.data[i][j]\n",
    "                                        for j in range(len(other.data[0]))]\n",
    "                                       for i in range(len(other.data))])\n",
    "                    input_tensor.backward(input_grad)\n",
    "        elif op == 'matmul':\n",
    "            # Simplified gradient computation for matrix multiplication\n",
    "            if inputs[0].requires_grad:\n",
    "                inputs[0].backward(grad.matmul(inputs[1]))\n",
    "            if inputs[1].requires_grad:\n",
    "                inputs[1].backward(inputs[0].matmul(grad))\n",
    "\n",
    "class Module:\n",
    "    \"\"\"Base neural network module.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.training = True\n",
    "        self.parameters = []\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        \"\"\"Zero out gradients of all parameters.\"\"\"\n",
    "        for param in self.parameters:\n",
    "            param.grad = None\n",
    "    \n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        return self.forward(x)\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "class Linear(Module):\n",
    "    \"\"\"Linear (fully connected) layer.\"\"\"\n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        super().__init__()\n",
    "        # Initialize weights and bias\n",
    "        self.weight = Tensor([[random.gauss(0, 0.01) \n",
    "                             for _ in range(in_features)]\n",
    "                            for _ in range(out_features)],\n",
    "                           requires_grad=True)\n",
    "        self.bias = Tensor([[0.0] for _ in range(out_features)],\n",
    "                          requires_grad=True)\n",
    "        self.parameters = [self.weight, self.bias]\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return x.matmul(self.weight) + self.bias\n",
    "\n",
    "class SGD:\n",
    "    \"\"\"Stochastic Gradient Descent optimizer.\"\"\"\n",
    "    def __init__(self, parameters: List[Tensor], lr: float = 0.01):\n",
    "        self.parameters = parameters\n",
    "        self.lr = lr\n",
    "    \n",
    "    def step(self):\n",
    "        \"\"\"Update parameters using their gradients.\"\"\"\n",
    "        for param in self.parameters:\n",
    "            if param.grad is not None:\n",
    "                for i in range(len(param.data)):\n",
    "                    for j in range(len(param.data[0])):\n",
    "                        param.data[i][j] -= self.lr * param.grad.data[i][j]\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        \"\"\"Zero out gradients.\"\"\"\n",
    "        for param in self.parameters:\n",
    "            param.grad = None\n",
    "\n",
    "class MSELoss:\n",
    "    \"\"\"Mean Squared Error Loss.\"\"\"\n",
    "    def __call__(self, pred: Tensor, target: Tensor) -> Tensor:\n",
    "        return ((pred + target * -1) * (pred + target * -1)).mean()\n",
    "\n",
    "# Example usage:\n",
    "def train_model():\n",
    "    # Create synthetic data\n",
    "    X = Tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "    y = Tensor([[5.0], [11.0], [17.0]])\n",
    "    \n",
    "    # Create model\n",
    "    model = Linear(2, 1)\n",
    "    criterion = MSELoss()\n",
    "    optimizer = SGD(model.parameters, lr=0.01)\n",
    "    \n",
    "    # Training loop\n",
    "    losses = []\n",
    "    for epoch in range(100):\n",
    "        # Forward pass\n",
    "        y_pred = model(X)\n",
    "        loss = criterion(y_pred, y)\n",
    "        losses.append(loss.data[0][0])\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch}, Loss: {loss.data[0][0]:.4f}')\n",
    "    \n",
    "    return losses\n",
    "\n",
    "# Simple plotting function using ASCII art\n",
    "def plot_losses(losses: List[float], width: int = 50, height: int = 20):\n",
    "    \"\"\"Plot losses using ASCII art.\"\"\"\n",
    "    if not losses:\n",
    "        return\n",
    "    \n",
    "    min_loss = min(losses)\n",
    "    max_loss = max(losses)\n",
    "    loss_range = max_loss - min_loss\n",
    "    \n",
    "    canvas = [[' ' for _ in range(width)] for _ in range(height)]\n",
    "    \n",
    "    for i, loss in enumerate(losses):\n",
    "        x = int((i / len(losses)) * (width - 1))\n",
    "        y = int(((loss - min_loss) / loss_range) * (height - 1))\n",
    "        y = height - 1 - y  # Flip y-axis\n",
    "        canvas[y][x] = '*'\n",
    "    \n",
    "    # Print the plot\n",
    "    print('\\nLoss Plot:')\n",
    "    print('-' * width)\n",
    "    for row in canvas:\n",
    "        print(''.join(row))\n",
    "    print('-' * width)\n",
    "\n",
    "# Run training and plot results\n",
    "if __name__ == \"__main__\":\n",
    "    losses = train_model()\n",
    "    plot_losses(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2bdde25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scalar tensor: [[5.0]]\n",
      "1D tensor: [[1.0, 2.0, 3.0]]\n",
      "2D tensor: [[1.0, 2.0], [3.0, 4.0]]\n",
      "Scalar + 1D tensor: [[6.0, 7.0, 8.0]]\n",
      "Scalar * 1D tensor: [[5.0, 10.0, 15.0]]\n",
      "Matrix multiplication result: [[19.0, 22.0], [43.0, 50.0]]\n"
     ]
    }
   ],
   "source": [
    "class Tensor:\n",
    "    \"\"\"Simple tensor implementation with autograd.\"\"\"\n",
    "    def __init__(self, data, requires_grad: bool = False):\n",
    "        # Handle scalar inputs\n",
    "        if isinstance(data, (int, float)):\n",
    "            self.data = [[float(data)]]\n",
    "        elif isinstance(data, list):\n",
    "            # Handle 1D list\n",
    "            if not isinstance(data[0], list):\n",
    "                self.data = [data]\n",
    "            else:\n",
    "                self.data = [[float(x) for x in row] for row in data]\n",
    "        else:\n",
    "            raise TypeError(\"Data must be a number or a list\")\n",
    "            \n",
    "        self.grad = None\n",
    "        self.requires_grad = requires_grad\n",
    "        self.grad_fn = None\n",
    "        \n",
    "    def shape(self):\n",
    "        return (len(self.data), len(self.data[0]))\n",
    "        \n",
    "    def __add__(self, other: 'Tensor') -> 'Tensor':\n",
    "        # Convert scalar to tensor if needed\n",
    "        if isinstance(other, (int, float)):\n",
    "            other = Tensor(other)\n",
    "            \n",
    "        # Handle broadcasting for scalar-like tensors\n",
    "        if self.shape() == (1, 1):\n",
    "            result = Tensor([[self.data[0][0] + other.data[i][j]\n",
    "                            for j in range(len(other.data[0]))]\n",
    "                           for i in range(len(other.data))])\n",
    "        elif other.shape() == (1, 1):\n",
    "            result = Tensor([[self.data[i][j] + other.data[0][0]\n",
    "                            for j in range(len(self.data[0]))]\n",
    "                           for i in range(len(self.data))])\n",
    "        else:\n",
    "            # Regular element-wise addition\n",
    "            if len(self.data) != len(other.data) or len(self.data[0]) != len(other.data[0]):\n",
    "                raise ValueError(f\"Cannot add tensors of shapes {self.shape()} and {other.shape()}\")\n",
    "            result = Tensor([[self.data[i][j] + other.data[i][j]\n",
    "                            for j in range(len(self.data[0]))]\n",
    "                           for i in range(len(self.data))])\n",
    "            \n",
    "        if self.requires_grad or other.requires_grad:\n",
    "            result.requires_grad = True\n",
    "            result.grad_fn = ('add', self, other)\n",
    "        return result\n",
    "    \n",
    "    def __mul__(self, other: 'Tensor') -> 'Tensor':\n",
    "        # Convert scalar to tensor if needed\n",
    "        if isinstance(other, (int, float)):\n",
    "            other = Tensor(other)\n",
    "            \n",
    "        # Handle broadcasting for scalar-like tensors\n",
    "        if self.shape() == (1, 1):\n",
    "            result = Tensor([[self.data[0][0] * other.data[i][j]\n",
    "                            for j in range(len(other.data[0]))]\n",
    "                           for i in range(len(other.data))])\n",
    "        elif other.shape() == (1, 1):\n",
    "            result = Tensor([[self.data[i][j] * other.data[0][0]\n",
    "                            for j in range(len(self.data[0]))]\n",
    "                           for i in range(len(self.data))])\n",
    "        else:\n",
    "            # Regular element-wise multiplication\n",
    "            if len(self.data) != len(other.data) or len(self.data[0]) != len(other.data[0]):\n",
    "                raise ValueError(f\"Cannot multiply tensors of shapes {self.shape()} and {other.shape()}\")\n",
    "            result = Tensor([[self.data[i][j] * other.data[i][j]\n",
    "                            for j in range(len(self.data[0]))]\n",
    "                           for i in range(len(self.data))])\n",
    "            \n",
    "        if self.requires_grad or other.requires_grad:\n",
    "            result.requires_grad = True\n",
    "            result.grad_fn = ('mul', self, other)\n",
    "        return result\n",
    "    \n",
    "    def matmul(self, other: 'Tensor') -> 'Tensor':\n",
    "        if len(self.data[0]) != len(other.data):\n",
    "            raise ValueError(f\"Cannot multiply matrices of shapes {self.shape()} and {other.shape()}\")\n",
    "            \n",
    "        result = [[sum(self.data[i][k] * other.data[k][j]\n",
    "                      for k in range(len(other.data)))\n",
    "                  for j in range(len(other.data[0]))]\n",
    "                 for i in range(len(self.data))]\n",
    "        tensor = Tensor(result)\n",
    "        if self.requires_grad or other.requires_grad:\n",
    "            tensor.requires_grad = True\n",
    "            tensor.grad_fn = ('matmul', self, other)\n",
    "        return tensor\n",
    "    \n",
    "    def mean(self) -> 'Tensor':\n",
    "        total = sum(sum(row) for row in self.data)\n",
    "        size = len(self.data) * len(self.data[0])\n",
    "        result = Tensor(total / size)\n",
    "        if self.requires_grad:\n",
    "            result.requires_grad = True\n",
    "            result.grad_fn = ('mean', self)\n",
    "        return result\n",
    "    \n",
    "    def backward(self, grad: Optional['Tensor'] = None):\n",
    "        if grad is None:\n",
    "            grad = Tensor(1.0)\n",
    "            \n",
    "        if self.grad_fn is None:\n",
    "            if self.requires_grad:\n",
    "                if self.grad is None:\n",
    "                    self.grad = grad\n",
    "                else:\n",
    "                    self.grad = self.grad + grad\n",
    "            return\n",
    "            \n",
    "        op, *inputs = self.grad_fn\n",
    "        if op == 'add':\n",
    "            for input_tensor in inputs:\n",
    "                if input_tensor.requires_grad:\n",
    "                    input_tensor.backward(grad)\n",
    "        elif op == 'mul':\n",
    "            for i, input_tensor in enumerate(inputs):\n",
    "                if input_tensor.requires_grad:\n",
    "                    other = inputs[1 - i]\n",
    "                    input_grad = other * grad\n",
    "                    input_tensor.backward(input_grad)\n",
    "        elif op == 'matmul':\n",
    "            # Simplified gradient computation for matrix multiplication\n",
    "            if inputs[0].requires_grad:\n",
    "                inputs[0].backward(grad.matmul(inputs[1]))\n",
    "            if inputs[1].requires_grad:\n",
    "                inputs[1].backward(inputs[0].matmul(grad))\n",
    "        elif op == 'mean':\n",
    "            if inputs[0].requires_grad:\n",
    "                scale = 1.0 / (len(self.data) * len(self.data[0]))\n",
    "                input_grad = Tensor([[scale * grad.data[0][0] \n",
    "                                    for _ in range(len(inputs[0].data[0]))]\n",
    "                                   for _ in range(len(inputs[0].data))])\n",
    "                inputs[0].backward(input_grad)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Tensor(data={self.data}, requires_grad={self.requires_grad})\"\n",
    "\n",
    "# Test the fixed implementation\n",
    "def test_tensor_operations():\n",
    "    # Test scalar creation\n",
    "    t1 = Tensor(5.0)\n",
    "    print(\"Scalar tensor:\", t1.data)\n",
    "    \n",
    "    # Test 1D list creation\n",
    "    t2 = Tensor([1.0, 2.0, 3.0])\n",
    "    print(\"1D tensor:\", t2.data)\n",
    "    \n",
    "    # Test 2D list creation\n",
    "    t3 = Tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "    print(\"2D tensor:\", t3.data)\n",
    "    \n",
    "    # Test addition\n",
    "    result = t1 + t2\n",
    "    print(\"Scalar + 1D tensor:\", result.data)\n",
    "    \n",
    "    # Test multiplication\n",
    "    result = t1 * t2\n",
    "    print(\"Scalar * 1D tensor:\", result.data)\n",
    "    \n",
    "    # Test matrix multiplication\n",
    "    t4 = Tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "    t5 = Tensor([[5.0, 6.0], [7.0, 8.0]])\n",
    "    result = t4.matmul(t5)\n",
    "    print(\"Matrix multiplication result:\", result.data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_tensor_operations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99df7068",
   "metadata": {},
   "source": [
    "# Neural Network Training Framework\n",
    "\n",
    "## Data Module\n",
    "\n",
    "The `DataModule` serves as the base class for handling data operations. It is represented formally as:\n",
    "\n",
    "$$\n",
    "\\text{DataModule} := \\{\\mathcal{D}_{\\text{train}}, \\mathcal{D}_{\\text{val}}, \\text{root}, \\text{num\\_workers}\\}\n",
    "$$\n",
    "\n",
    "### Key Components:\n",
    "1. **Initialization**:\n",
    "   $$\n",
    "   \\text{init}(\\text{root}, \\text{num\\_workers}) \\rightarrow \\text{void}\n",
    "   $$\n",
    "\n",
    "2. **Data Loader Interface**:\n",
    "   $$\n",
    "   \\text{get\\_dataloader}(\\text{train}: \\text{bool}) \\rightarrow \\text{Iterator}[\\mathcal{B}]\n",
    "   $$\n",
    "   where $\\mathcal{B}$ represents a batch of data\n",
    "\n",
    "3. **Specific Loaders**:\n",
    "   - Training: $\\text{train\\_dataloader}() \\rightarrow \\text{Iterator}[\\mathcal{B}_{\\text{train}}]$\n",
    "   - Validation: $\\text{val\\_dataloader}() \\rightarrow \\text{Iterator}[\\mathcal{B}_{\\text{val}}]$\n",
    "\n",
    "## Trainer\n",
    "\n",
    "The `Trainer` class orchestrates the training process. Its formal definition:\n",
    "\n",
    "$$\n",
    "\\text{Trainer} := \\{\\text{model}, \\text{data}, \\text{max\\_epochs}, \\text{num\\_gpus}, \\text{gradient\\_clip\\_val}\\}\n",
    "$$\n",
    "\n",
    "### Methods:\n",
    "\n",
    "1. **Data Preparation**:\n",
    "   $$\n",
    "   \\begin{align*}\n",
    "   \\text{prepare\\_data}(\\mathcal{D}) &\\rightarrow \\text{void} \\\\\n",
    "   \\text{num\\_train\\_batches} &= |\\mathcal{D}_{\\text{train}}| \\\\\n",
    "   \\text{num\\_val\\_batches} &= |\\mathcal{D}_{\\text{val}}|\n",
    "   \\end{align*}\n",
    "   $$\n",
    "\n",
    "2. **Model Preparation**:\n",
    "   $$\n",
    "   \\text{prepare\\_model}(M) \\rightarrow \\text{void}\n",
    "   $$\n",
    "\n",
    "3. **Training Loop**:\n",
    "   $$\n",
    "   \\begin{align*}\n",
    "   \\text{fit}(M, \\mathcal{D}) &\\rightarrow \\text{void} \\\\\n",
    "   \\text{epoch} &\\in [0, \\text{max\\_epochs}) \\\\\n",
    "   \\text{train\\_batch\\_idx} &\\in [0, |\\mathcal{D}_{\\text{train}}|) \\\\\n",
    "   \\text{val\\_batch\\_idx} &\\in [0, |\\mathcal{D}_{\\text{val}}|)\n",
    "   \\end{align*}\n",
    "   $$\n",
    "\n",
    "### Training Process:\n",
    "\n",
    "For each epoch $e \\in [0, \\text{max\\_epochs})$:\n",
    "\n",
    "1. Model training:\n",
    "   $$\n",
    "   \\mathcal{L}_{\\text{train}} = \\frac{1}{|\\mathcal{D}_{\\text{train}}|} \\sum_{\\mathcal{B} \\in \\mathcal{D}_{\\text{train}}} \\text{model.training\\_step}(\\mathcal{B})\n",
    "   $$\n",
    "\n",
    "2. Validation (if available):\n",
    "   $$\n",
    "   \\mathcal{L}_{\\text{val}} = \\frac{1}{|\\mathcal{D}_{\\text{val}}|} \\sum_{\\mathcal{B} \\in \\mathcal{D}_{\\text{val}}} \\text{model.validation\\_step}(\\mathcal{B})\n",
    "   $$\n",
    "\n",
    "Note: The actual implementation of `fit_epoch()` is deferred to specific model implementations, allowing for customization of the training process for different architectures and tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d8f8cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperParameters:\n",
    "    \"\"\"Stores and manages hyperparameters.\"\"\"\n",
    "    def save_hyperparameters(self):\n",
    "        \"\"\"Store hyperparameters from initialization.\"\"\"\n",
    "        frame = inspect.currentframe().f_back\n",
    "        _, _, _, local_vars = inspect.getargvalues(frame)\n",
    "        self.hparams = {k: v for k, v in local_vars.items() \n",
    "                       if k != 'self' and not k.startswith('_')}\n",
    "        for k, v in self.hparams.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "class DataModule(HyperParameters):\n",
    "    \"\"\"Base class for data modules.\"\"\"\n",
    "    \n",
    "    def __init__(self, root='../data', num_workers=4, batch_size=16):\n",
    "        \"\"\"\n",
    "        Initialize DataModule.\n",
    "        \n",
    "        Args:\n",
    "            root: Root directory for data\n",
    "            num_workers: Number of workers for data loading\n",
    "            batch_size: Batch size for data loaders\n",
    "        \"\"\"\n",
    "        self.save_hyperparameters()\n",
    "        self.train_data = None\n",
    "        self.val_data = None\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        \"\"\"\n",
    "        Download and prepare raw data.\n",
    "        Should be called only once and on 1 GPU.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def setup(self):\n",
    "        \"\"\"\n",
    "        Process data operations: splitting, etc.\n",
    "        Should be called on every GPU.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def get_dataloader(self, train=True):\n",
    "        \"\"\"Get a data loader for either training or validation.\"\"\"\n",
    "        data = self.train_data if train else self.val_data\n",
    "        if data is None:\n",
    "            return None\n",
    "            \n",
    "        start_idx = 0\n",
    "        while start_idx < len(data):\n",
    "            batch = data[start_idx:start_idx + self.batch_size]\n",
    "            yield batch\n",
    "            start_idx += self.batch_size\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        \"\"\"Get training data loader.\"\"\"\n",
    "        return self.get_dataloader(train=True)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        \"\"\"Get validation data loader.\"\"\"\n",
    "        return self.get_dataloader(train=False)\n",
    "\n",
    "class Trainer(HyperParameters):\n",
    "    \"\"\"Handles model training.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_epochs=10, num_gpus=0, gradient_clip_val=0):\n",
    "        \"\"\"\n",
    "        Initialize Trainer.\n",
    "        \n",
    "        Args:\n",
    "            max_epochs: Maximum number of epochs\n",
    "            num_gpus: Number of GPUs (currently only CPU supported)\n",
    "            gradient_clip_val: Gradient clipping value\n",
    "        \"\"\"\n",
    "        self.save_hyperparameters()\n",
    "        assert num_gpus == 0, 'No GPU support yet'\n",
    "        \n",
    "    def prepare_data(self, data):\n",
    "        \"\"\"Prepare data loaders.\"\"\"\n",
    "        data.prepare_data()\n",
    "        data.setup()\n",
    "        self.train_dataloader = data.train_dataloader()\n",
    "        self.val_dataloader = data.val_dataloader()\n",
    "        \n",
    "        # Count number of batches\n",
    "        self.num_train_batches = sum(1 for _ in data.train_dataloader())\n",
    "        self.num_val_batches = (sum(1 for _ in data.val_dataloader())\n",
    "                               if self.val_dataloader is not None else 0)\n",
    "    \n",
    "    def prepare_model(self, model):\n",
    "        \"\"\"Prepare model for training.\"\"\"\n",
    "        model.trainer = self\n",
    "        if hasattr(model, 'board'):\n",
    "            model.board.xlim = [0, self.max_epochs]\n",
    "        self.model = model\n",
    "    \n",
    "    def fit(self, model, data):\n",
    "        \"\"\"\n",
    "        Train the model with the given data.\n",
    "        \n",
    "        Args:\n",
    "            model: Model to train\n",
    "            data: DataModule instance with training data\n",
    "        \"\"\"\n",
    "        self.prepare_data(data)\n",
    "        self.prepare_model(model)\n",
    "        self.optim = model.configure_optimizers()\n",
    "        \n",
    "        self.epoch = 0\n",
    "        self.train_batch_idx = 0\n",
    "        self.val_batch_idx = 0\n",
    "        \n",
    "        for self.epoch in range(self.max_epochs):\n",
    "            self.fit_epoch()\n",
    "    \n",
    "    def fit_epoch(self):\n",
    "        \"\"\"Run one epoch of training and validation.\"\"\"\n",
    "        # Training phase\n",
    "        self.model.train()\n",
    "        for batch in self.train_dataloader:\n",
    "            loss = self.model.training_step(batch)\n",
    "            self.optim.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Apply gradient clipping if specified\n",
    "            if self.gradient_clip_val > 0:\n",
    "                params = [p for p in self.model.parameters() if p.requires_grad]\n",
    "                norm = math.sqrt(sum(p.grad.data.norm()**2 for p in params))\n",
    "                if norm > self.gradient_clip_val:\n",
    "                    for p in params:\n",
    "                        p.grad.data *= self.gradient_clip_val / norm\n",
    "            \n",
    "            self.optim.step()\n",
    "            self.train_batch_idx += 1\n",
    "        \n",
    "        # Validation phase\n",
    "        if self.val_dataloader is not None:\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                for batch in self.val_dataloader:\n",
    "                    self.model.validation_step(batch)\n",
    "                    self.val_batch_idx += 1\n",
    "\n",
    "# Example implementation of a simple data module\n",
    "class SimpleDataModule(DataModule):\n",
    "    \"\"\"Example data module for synthetic data.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_examples=1000, input_dim=10, \n",
    "                 root='../data', num_workers=4, batch_size=16):\n",
    "        super().__init__(root, num_workers, batch_size)\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        \"\"\"Generate synthetic data.\"\"\"\n",
    "        rng = random.Random(42)\n",
    "        # Generate random input data\n",
    "        X = [[rng.random() for _ in range(self.input_dim)] \n",
    "             for _ in range(self.num_examples)]\n",
    "        # Generate random target values\n",
    "        y = [sum(x) / self.input_dim + 0.1 * rng.random() for x in X]\n",
    "        \n",
    "        # Split into train and validation\n",
    "        split = int(0.8 * self.num_examples)\n",
    "        self.train_data = list(zip(X[:split], y[:split]))\n",
    "        self.val_data = list(zip(X[split:], y[split:]))\n",
    "\n",
    "# Example usage\n",
    "def train_example_model():\n",
    "    # Create synthetic data\n",
    "    data = SimpleDataModule(num_examples=1000, input_dim=10, batch_size=32)\n",
    "    \n",
    "    # Create model (assuming we have a SimpleModel class)\n",
    "    model = SimpleModel(input_dim=10)\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = Trainer(max_epochs=10)\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fe961f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class HyperParameters:\n",
    "    \"\"\"Stores and manages hyperparameters.\"\"\"\n",
    "    def save_hyperparameters(self):\n",
    "        \"\"\"Store hyperparameters from initialization.\"\"\"\n",
    "        frame = inspect.currentframe().f_back\n",
    "        _, _, _, local_vars = inspect.getargvalues(frame)\n",
    "        self.hparams = {k: v for k, v in local_vars.items() \n",
    "                       if k != 'self' and not k.startswith('_')}\n",
    "        for k, v in self.hparams.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "class DataModule(HyperParameters):\n",
    "    \"\"\"Base class for data modules.\"\"\"\n",
    "    \n",
    "    def __init__(self, root='../data', num_workers=4, batch_size=16):\n",
    "        \"\"\"\n",
    "        Initialize DataModule.\n",
    "        \n",
    "        Args:\n",
    "            root: Root directory for data\n",
    "            num_workers: Number of workers for data loading\n",
    "            batch_size: Batch size for data loaders\n",
    "        \"\"\"\n",
    "        self.save_hyperparameters()\n",
    "        self.train_data = None\n",
    "        self.val_data = None\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        \"\"\"\n",
    "        Download and prepare raw data.\n",
    "        Should be called only once and on 1 GPU.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def setup(self):\n",
    "        \"\"\"\n",
    "        Process data operations: splitting, etc.\n",
    "        Should be called on every GPU.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def get_dataloader(self, train=True):\n",
    "        \"\"\"Get a data loader for either training or validation.\"\"\"\n",
    "        data = self.train_data if train else self.val_data\n",
    "        if data is None:\n",
    "            return None\n",
    "            \n",
    "        start_idx = 0\n",
    "        while start_idx < len(data):\n",
    "            batch = data[start_idx:start_idx + self.batch_size]\n",
    "            yield batch\n",
    "            start_idx += self.batch_size\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        \"\"\"Get training data loader.\"\"\"\n",
    "        return self.get_dataloader(train=True)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        \"\"\"Get validation data loader.\"\"\"\n",
    "        return self.get_dataloader(train=False)\n",
    "\n",
    "class Trainer(HyperParameters):\n",
    "    \"\"\"Handles model training.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_epochs=10, num_gpus=0, gradient_clip_val=0):\n",
    "        \"\"\"\n",
    "        Initialize Trainer.\n",
    "        \n",
    "        Args:\n",
    "            max_epochs: Maximum number of epochs\n",
    "            num_gpus: Number of GPUs (currently only CPU supported)\n",
    "            gradient_clip_val: Gradient clipping value\n",
    "        \"\"\"\n",
    "        self.save_hyperparameters()\n",
    "        assert num_gpus == 0, 'No GPU support yet'\n",
    "        \n",
    "    def prepare_data(self, data):\n",
    "        \"\"\"Prepare data loaders.\"\"\"\n",
    "        data.prepare_data()\n",
    "        data.setup()\n",
    "        self.train_dataloader = data.train_dataloader()\n",
    "        self.val_dataloader = data.val_dataloader()\n",
    "        \n",
    "        # Count number of batches\n",
    "        self.num_train_batches = sum(1 for _ in data.train_dataloader())\n",
    "        self.num_val_batches = (sum(1 for _ in data.val_dataloader())\n",
    "                               if self.val_dataloader is not None else 0)\n",
    "    \n",
    "    def prepare_model(self, model):\n",
    "        \"\"\"Prepare model for training.\"\"\"\n",
    "        model.trainer = self\n",
    "        if hasattr(model, 'board'):\n",
    "            model.board.xlim = [0, self.max_epochs]\n",
    "        self.model = model\n",
    "    \n",
    "    def fit(self, model, data):\n",
    "        \"\"\"\n",
    "        Train the model with the given data.\n",
    "        \n",
    "        Args:\n",
    "            model: Model to train\n",
    "            data: DataModule instance with training data\n",
    "        \"\"\"\n",
    "        self.prepare_data(data)\n",
    "        self.prepare_model(model)\n",
    "        self.optim = model.configure_optimizers()\n",
    "        \n",
    "        self.epoch = 0\n",
    "        self.train_batch_idx = 0\n",
    "        self.val_batch_idx = 0\n",
    "        \n",
    "        for self.epoch in range(self.max_epochs):\n",
    "            self.fit_epoch()\n",
    "    \n",
    "    def fit_epoch(self):\n",
    "        \"\"\"Run one epoch of training and validation.\"\"\"\n",
    "        # Training phase\n",
    "        self.model.train()\n",
    "        for batch in self.train_dataloader:\n",
    "            loss = self.model.training_step(batch)\n",
    "            self.optim.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Apply gradient clipping if specified\n",
    "            if self.gradient_clip_val > 0:\n",
    "                params = [p for p in self.model.parameters() if p.requires_grad]\n",
    "                norm = math.sqrt(sum(p.grad.data.norm()**2 for p in params))\n",
    "                if norm > self.gradient_clip_val:\n",
    "                    for p in params:\n",
    "                        p.grad.data *= self.gradient_clip_val / norm\n",
    "            \n",
    "            self.optim.step()\n",
    "            self.train_batch_idx += 1\n",
    "        \n",
    "        # Validation phase\n",
    "        if self.val_dataloader is not None:\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                for batch in self.val_dataloader:\n",
    "                    self.model.validation_step(batch)\n",
    "                    self.val_batch_idx += 1\n",
    "\n",
    "# Example implementation of a simple data module\n",
    "class SimpleDataModule(DataModule):\n",
    "    \"\"\"Example data module for synthetic data.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_examples=1000, input_dim=10, \n",
    "                 root='../data', num_workers=4, batch_size=16):\n",
    "        super().__init__(root, num_workers, batch_size)\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        \"\"\"Generate synthetic data.\"\"\"\n",
    "        rng = random.Random(42)\n",
    "        # Generate random input data\n",
    "        X = [[rng.random() for _ in range(self.input_dim)] \n",
    "             for _ in range(self.num_examples)]\n",
    "        # Generate random target values\n",
    "        y = [sum(x) / self.input_dim + 0.1 * rng.random() for x in X]\n",
    "        \n",
    "        # Split into train and validation\n",
    "        split = int(0.8 * self.num_examples)\n",
    "        self.train_data = list(zip(X[:split], y[:split]))\n",
    "        self.val_data = list(zip(X[split:], y[split:]))\n",
    "\n",
    "# Example usage\n",
    "def train_example_model():\n",
    "    # Create synthetic data\n",
    "    data = SimpleDataModule(num_examples=1000, input_dim=10, batch_size=32)\n",
    "    \n",
    "    # Create model (assuming we have a SimpleModel class)\n",
    "    model = SimpleModel(input_dim=10)\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = Trainer(max_epochs=10)\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.fit(model, data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a4c97d",
   "metadata": {},
   "source": [
    "## Synthetic Regression Data\n",
    "\n",
    "Machine learning is all about extracting information from data. So you might wonder, what could we possibly learn from synthetic data? While we might not care intrinsically about the patterns that we ourselves baked into an artificial data generating model, such datasets are nevertheless useful for didactic purposes, helping us to evaluate the properties of our learning algorithms and to confirm that our implementations work as expected. For example, if we create data for which the correct parameters are known *a priori*, then we can verify that our model can in fact recover them.\n",
    "if we create data for which the correct parameters are known a priori, then we can verify that our model can in fact recover them.\n",
    "\n",
    "\n",
    "\n",
    "## Generating the Dataset\n",
    "\n",
    "For this example, we will work low-dimensional for succinctness. The following code snippet generates 1000 examples with 2-dimensional features drawn from a standard normal distribution. The resulting design matrix X belongs to $\\mathbb{R}^{1000\\times2}$. We generate each label by applying a ground truth linear function, corrupted them via additive noise $\\epsilon$, drawn independently and identically for each example:\n",
    "\n",
    "$$y = Xw + b + \\zeta. \\quad (3.3.1)$$\n",
    "\n",
    "For convenience, we assume that $\\epsilon$ is drawn from a normal distribution with mean $\\mu = 0$ and standard deviation $\\sigma = 0.01$. Note that for object-oriented design, we add the code to the `__init__` method of a subclass of `d2l.DataModule` (introduced in Section 3.2.3). It is good practice to allow setting any additional hyperparameters. We accomplish this with `save_hyperparameters()`. The batch_size will be determined later on.\n",
    "\n",
    "features: tensor([-0.0499, -0.2817])\n",
    "label: tensor([5.0533])\n",
    "\n",
    "``` python\n",
    "\n",
    "%matplotlib inline\n",
    "import random\n",
    "import torch\n",
    "from d import torch as d\n",
    "\n",
    "class SyntheticRegressionData(d.DataModule):\n",
    "    #@save\n",
    "    \"\"\"Synthetic data for linear regression.\"\"\"\n",
    "    def __init__(self, w, b, noise=0.01, num_train=1000, num_val=1000, batch_size=32):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        n = num_train + num_val\n",
    "        self.X = torch.randn(n, len(w))\n",
    "        noise = torch.randn(n, 1) * noise\n",
    "        self.y = torch.matmul(self.X, w.reshape((-1, 1))) + b + noise\n",
    "\n",
    "\n",
    "```\n",
    "2. Reading the Dataset\n",
    "\n",
    "Training machine learning models often requires multiple passes over a dataset, grabbing one minibatch of examples at a time. This data is then used to update the model. To illustrate how this works, we implement the `get_dataloader` method, registering it in the `SyntheticRe- gressionData` class via `add_to_class` (introduced in Section 3.2.1). It takes a batch size, a matrix of features, and a vector of labels, and generates minibatches of size `batch_size`. As such, each minibatch consists of a tuple of features and labels. Note that we need to be mindful of whether were in training or validation mode: in the former, we will want to read the data in random order, whereas for the latter, being able to read data in a pre-defined order may be important for debugging purposes.\n",
    "\n",
    "```python\n",
    "@d2l.add_to_class(SyntheticRegressionData)\n",
    "def get_dataloader(self, train):\n",
    "    if train:\n",
    "        indices = list(range(0, self.num_train))  # The examples are read in random order\n",
    "        random.shuffle(indices)\n",
    "    else:\n",
    "        indices = list(range(self.num_train, self.num_train + self.num_val))\n",
    "\n",
    "    for i in range(0, len(indices), self.batch_size):\n",
    "        batch_indices = torch.tensor(indices[i: i + self.batch_size])\n",
    "        yield self.X[batch_indices], self.y[batch_indices]\n",
    "```\n",
    "\n",
    "To build some intuition, lets inspect the first minibatch of data. Each minibatch of features provides us with both its size and the dimensionality of input features. Likewise, our mini-batch of labels will have a matching shape given by `batch_size`.\n",
    "\n",
    "```python\n",
    "X, y = next(iter(data.train_dataloader()))\n",
    "print('X shape:', X.shape, '\\ny shape:', y.shape)\n",
    "```\n",
    "\n",
    "```\n",
    "X shape: torch.Size([32, 2])\n",
    "y shape: torch.Size([32, 1])\n",
    "```\n",
    "\n",
    "While seemingly innocuous, the invocation of `iter(data.train_dataloader())` illus- trates the power of Pythons object-oriented design. Note that we added a method to the `SyntheticRegressionData` class after creating the data object. Nonetheless, the object benets from the ex post facto addition of functionality to the class. Throughout the iteration, we obtain distinct minibatches until the entire dataset has been ex- hausted (try this).\n",
    "\n",
    "While the iteration implemented above is good for didactic purposes, it is inecient in ways that might get us in trouble on real problems. For example, it requires that we load all the data in memory and that we perform lots of random memory access. The built-in iterators implemented in a deep learning framework are considerably more ecient and they can deal with sources such as data stored in les, data received via a stream, and data generated or processed on the y. Next, lets try to implement the same method using built-in iterators.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73c1bf0",
   "metadata": {},
   "source": [
    "\n",
    "3. Concise Implementation of the Data Loader\n",
    "\n",
    "Rather than writing our own iterator, we can call the existing API in a framework to load data. As before, we need a dataset with features `X` and labels `y`. Beyond that, we set `batch_size` in the built-in data loader and let it take care of shuffling examples efficiently.\n",
    "\n",
    "```python\n",
    "@d2l.add_to_class(d.DataModule)\n",
    "#@save\n",
    "def get_tensorloader(self, tensors, train, indices=slice(0, None)):\n",
    "    tensors = tuple(a[indices] for a in tensors)\n",
    "    dataset = torch.utils.data.TensorDataset(*tensors)\n",
    "    return torch.utils.data.DataLoader(dataset, self.batch_size, shuffle=train)\n",
    "\n",
    "@d2l.add_to_class(SyntheticRegressionData)\n",
    "#@save\n",
    "def get_dataloader(self, train):\n",
    "    i = slice(0, self.num_train) if train else slice(self.num_train, None)\n",
    "    return self.get_tensorloader((self.X, self.y), train, i)\n",
    "```\n",
    "\n",
    "The new data loader behaves just as the previous one, except that it is more efficient and has some added functionality.\n",
    "\n",
    "```python\n",
    "X, y = next(iter(data.train_dataloader()))\n",
    "print('X shape:', X.shape, '\\ny shape:', y.shape)\n",
    "```\n",
    "\n",
    "```\n",
    "X shape: torch.Size([32, 2])\n",
    "y shape: torch.Size([32, 1])\n",
    "```\n",
    "\n",
    "For instance, the data loader provided by the framework API supports the built-in `__len__` method, so we can query its length, i.e., the number of batches.\n",
    "\n",
    "```python\n",
    "len(data.train_dataloader())\n",
    "```\n",
    "\n",
    "```\n",
    "32\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "901dd21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([32, 2]) \n",
      "y shape: torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class SyntheticRegressionData:\n",
    "    def __init__(self, w, b, noise=0.01, num_train=1000, num_val=1000, batch_size=32):\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "        self.noise = noise\n",
    "        self.num_train = num_train\n",
    "        self.num_val = num_val\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.X = torch.randn(num_train + num_val, len(w))\n",
    "        noise = torch.randn(num_train + num_val, 1) * noise\n",
    "        self.y = torch.matmul(self.X, w.reshape((-1, 1))) + b + noise\n",
    "\n",
    "    def get_dataloader(self, train):\n",
    "        if train:\n",
    "            indices = list(range(0, self.num_train))\n",
    "            random.shuffle(indices)\n",
    "        else:\n",
    "            indices = list(range(self.num_train, self.num_train + self.num_val))\n",
    "\n",
    "        dataset = TensorDataset(self.X[indices], self.y[indices])\n",
    "        return DataLoader(dataset, batch_size=self.batch_size, shuffle=train)\n",
    "\n",
    "data = SyntheticRegressionData(w=torch.tensor([2, -3.4]), b=4.2)\n",
    "\n",
    "X, y = next(iter(data.get_dataloader(train=True)))\n",
    "print('X shape:', X.shape, '\\ny shape:', y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d2434a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: 32 2 \n",
      "y shape: 32\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "class SyntheticRegressionData:\n",
    "    def __init__(self, w, b, noise=0.01, num_train=1000, num_val=1000, batch_size=32):\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "        self.noise = noise\n",
    "        self.num_train = num_train\n",
    "        self.num_val = num_val\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.X = [[random.gauss(0, 1) for _ in range(len(w))] for _ in range(num_train + num_val)]\n",
    "        self.y = [sum(x * w_i for x, w_i in zip(x_i, w)) + b + random.gauss(0, noise) for x_i in self.X]\n",
    "\n",
    "    def get_dataloader(self, train):\n",
    "        if train:\n",
    "            indices = list(range(0, self.num_train))\n",
    "            random.shuffle(indices)\n",
    "        else:\n",
    "            indices = list(range(self.num_train, self.num_train + self.num_val))\n",
    "\n",
    "        for i in range(0, len(indices), self.batch_size):\n",
    "            batch_indices = indices[i: i+self.batch_size]\n",
    "            yield [self.X[i] for i in batch_indices], [self.y[i] for i in batch_indices]\n",
    "\n",
    "data = SyntheticRegressionData(w=[2, -3.4], b=4.2)\n",
    "\n",
    "X, y = next(iter(data.get_dataloader(train=True)))\n",
    "print('X shape:', len(X), len(X[0]), '\\ny shape:', len(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168d9dba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
