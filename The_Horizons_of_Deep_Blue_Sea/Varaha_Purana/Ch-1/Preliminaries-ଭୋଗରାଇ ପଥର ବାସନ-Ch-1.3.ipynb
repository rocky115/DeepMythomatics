{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4074df94",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " * Copyright (c) 2004 Radhamadhab Dalai\n",
    " *\n",
    " * Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    " * of this software and associated documentation files (the \"Software\"), to deal\n",
    " * in the Software without restriction, including without limitation the rights\n",
    " * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    " * copies of the Software, and to permit persons to whom the Software is\n",
    " * furnished to do so, subject to the following conditions:\n",
    " *\n",
    " * The above copyright notice and this permission notice shall be included in\n",
    " * all copies or substantial portions of the Software.\n",
    " *\n",
    " * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    " * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    " * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    " * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    " * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    " * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
    " * THE SOFTWARE.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0db888",
   "metadata": {},
   "source": [
    "## 2.3.4 Tensors\n",
    "\n",
    "While you can go far in your machine learning journey with only scalars, vectors, and matrices, eventually you may need to work with higher-order tensors. Tensors give us a generic way to describe extensions to $n^{th}$-order arrays. We call software objects of the tensor class “tensors” precisely because they too can have arbitrary numbers of axes. While it may be confusing to use the word tensor for both the mathematical object and its realization in code, our meaning should usually be clear from context. We denote general tensors by capital letters with a special font face (e.g., $\\mathcal{X}$, $\\mathcal{Y}$, and $\\mathcal{Z}$) and their indexing mechanism (e.g., $x_{ijk}$ and $[\\mathcal{X}]_{1, 2i-1, 3}$) follows naturally from that of matrices.\n",
    "\n",
    "Tensors will become more important when we start working with images. Each image arrives as a $3^{rd}$-order tensor with axes corresponding to the height, width, and channel. At each spatial location, the intensities of each color (red, green, and blue) are stacked along the channel. Moreover a collection of images is represented in code by a $4^{th}$-order tensor, where distinct images are indexed along the first axis. Higher-order tensors are constructed analogously to vectors and matrices, by growing the number of shape components.\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "torch.arange(24).reshape(2, 3, 4)\n",
    "\n",
    "tensor([[[ 0,  1,  2,  3],\n",
    "         [ 4,  5,  6,  7],\n",
    "         [ 8,  9, 10, 11]],\n",
    "\n",
    "        [[12, 13, 14, 15],\n",
    "         [16, 17, 18, 19],\n",
    "         [20, 21, 22, 23]]])\n",
    "\n",
    " Basic Properties of Tensor Arithmetic\n",
    "Scalars, vectors, matrices, and higher-order tensors all have some handy properties. For example, elementwise operations produce outputs that have the same shape as their operands."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681ec8b7",
   "metadata": {},
   "source": [
    "## Basic Properties of Tensor Arithmetic\n",
    "\n",
    "Scalars, vectors, matrices, and higher-order tensors all have some handy properties. For example, elementwise operations produce outputs that have the same shape as their operands.\n",
    "\n",
    "### Elementwise Addition\n",
    "Given a matrix $A$:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "A = torch.arange(6, dtype=torch.float32).reshape(2, 3)\n",
    "B = A.clone() # Assign a copy of A to B by allocating new memory\n",
    "A, A + B\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix} 0 & 1 & 2 \\\\ 3 & 4 & 5 \\end{bmatrix}, \\quad A + B = \\begin{bmatrix} 0+0 & 1+1 & 2+2 \\\\ 3+3 & 4+4 & 5+5 \\end{bmatrix} = \\begin{bmatrix} 0 & 2 & 4 \\\\ 6 & 8 & 10 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### Hadamard Product (Elementwise Multiplication)\n",
    "The Hadamard product of two matrices $A, B \\in \\mathbb{R}^{m \\times n}$ is computed as follows:\n",
    "\n",
    "$$\n",
    "A \\odot B = \\begin{bmatrix} a_{11} b_{11} & a_{12} b_{12} & \\dots & a_{1n} b_{1n} \\\\\n",
    "                          a_{21} b_{21} & a_{22} b_{22} & \\dots & a_{2n} b_{2n} \\\\\n",
    "                          \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "                          a_{m1} b_{m1} & a_{m2} b_{m2} & \\dots & a_{mn} b_{mn} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "In Python:\n",
    "\n",
    "```python\n",
    "A * B\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} 0 \\cdot 0 & 1 \\cdot 1 & 2 \\cdot 2 \\\\\n",
    "                  3 \\cdot 3 & 4 \\cdot 4 & 5 \\cdot 5 \\end{bmatrix} = \\begin{bmatrix} 0 & 1 & 4 \\\\\n",
    "                  9 & 16 & 25 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### Scalar-Tensor Operations\n",
    "Adding or multiplying a scalar to a tensor produces a result with the same shape as the original tensor. Each element of the tensor is added to (or multiplied by) the scalar.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "a = 2\n",
    "X = torch.arange(24).reshape(2, 3, 4)\n",
    "a + X, (a * X).shape\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} \n",
    "\\begin{bmatrix} 2 & 3 & 4 & 5 \\\\\n",
    "6 & 7 & 8 & 9 \\\\\n",
    "10 & 11 & 12 & 13 \\end{bmatrix},\n",
    "\\begin{bmatrix} 14 & 15 & 16 & 17 \\\\\n",
    "18 & 19 & 20 & 21 \\\\\n",
    "22 & 23 & 24 & 25 \\end{bmatrix}\n",
    "\\end{bmatrix}, \\quad \\text{shape} = (2,3,4)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117057e3",
   "metadata": {},
   "source": [
    "## Reduction Operations in Tensor Arithmetic\n",
    "\n",
    "### Summation\n",
    "Often, we wish to calculate the sum of a tensor’s elements. To express the sum of the elements $\\sum$ in a vector $x$ of length $n$, we write:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{n} x_i\n",
    "$$\n",
    "\n",
    "There’s a simple function for it:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "x = torch.arange(3, dtype=torch.float32)\n",
    "x, x.sum()\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "$$\n",
    "(\\text{tensor}([0., 1., 2.]), \\text{tensor}(3.))\n",
    "$$\n",
    "\n",
    "To express sums over the elements of tensors of arbitrary shape, we simply sum over all of its axes. For example, the sum of the elements of an $m \\times n$ matrix $A$ could be written:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{m} \\sum_{j=1}^{n} a_{ij}\n",
    "$$\n",
    "\n",
    "```python\n",
    "A = torch.arange(6, dtype=torch.float32).reshape(2, 3)\n",
    "A.shape, A.sum()\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "$$\n",
    "(\\text{torch.Size}([2, 3]), \\text{tensor}(15.))\n",
    "$$\n",
    "\n",
    "By default, invoking the `sum` function reduces a tensor along all of its axes, eventually producing a scalar. Our libraries also allow us to specify the axes along which the tensor should be reduced.\n",
    "\n",
    "Summing over all elements along the rows ($\\text{axis}=0$):\n",
    "\n",
    "```python\n",
    "A.shape, A.sum(axis=0).shape\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "$$\n",
    "(\\text{torch.Size}([2, 3]), \\text{torch.Size}([3]))\n",
    "$$\n",
    "\n",
    "Summing over columns ($\\text{axis}=1$):\n",
    "\n",
    "```python\n",
    "A.shape, A.sum(axis=1).shape\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "$$\n",
    "(\\text{torch.Size}([2, 3]), \\text{torch.Size}([2]))\n",
    "$$\n",
    "\n",
    "Reducing along both rows and columns:\n",
    "\n",
    "```python\n",
    "A.sum(axis=[0, 1]) == A.sum()\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "$$\n",
    "\\text{tensor}(\\text{True})\n",
    "$$\n",
    "\n",
    "### Mean Calculation\n",
    "A related quantity is the mean, also called the average:\n",
    "\n",
    "$$\n",
    "\\text{mean}(A) = \\frac{\\sum A}{\\text{num elements}}\n",
    "$$\n",
    "\n",
    "```python\n",
    "A.mean(), A.sum() / A.numel()\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "$$\n",
    "(\\text{tensor}(2.5000), \\text{tensor}(2.5000))\n",
    "$$\n",
    "\n",
    "We can also reduce along specific axes:\n",
    "\n",
    "```python\n",
    "A.mean(axis=0), A.sum(axis=0) / A.shape[0]\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "$$\n",
    "(\\text{tensor}([1.5000, 2.5000, 3.5000]), \\text{tensor}([1.5000, 2.5000, 3.5000]))\n",
    "$$\n",
    "\n",
    "## Non-Reduction Sum\n",
    "Sometimes it can be useful to keep the number of axes unchanged when invoking the function for calculating the sum or mean. This is important when using broadcasting.\n",
    "\n",
    "```python\n",
    "sum_A = A.sum(axis=1, keepdims=True)\n",
    "sum_A, sum_A.shape\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "$$\n",
    "(\\text{tensor}([[ 3.], [12.]]), \\text{torch.Size}([2, 1]))\n",
    "$$\n",
    "\n",
    "Since `sum_A` keeps its two axes after summing each row, we can divide `A` by `sum_A` using broadcasting to create a matrix where each row sums up to 1:\n",
    "\n",
    "```python\n",
    "A / sum_A\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "$$\n",
    "\\text{tensor}([[0.0000, 0.3333, 0.6667], [0.2500, 0.3333, 0.4167]])\n",
    "$$\n",
    "\n",
    "To calculate the cumulative sum along an axis, say `axis=0` (row by row), we use `cumsum`, which does not reduce the input tensor:\n",
    "\n",
    "```python\n",
    "A.cumsum(axis=0)\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "$$\n",
    "\\text{tensor}([[0., 1., 2.], [3., 5., 7.]])\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dd8bb0",
   "metadata": {},
   "source": [
    "\n",
    "## Tensor Arithmetic and Operations\n",
    "\n",
    "###  Basic Properties of Tensor Arithmetic\n",
    "\n",
    "Scalars, vectors, matrices, and higher-order tensors all have some handy properties. For example, elementwise operations produce outputs that have the same shape as their operands.\n",
    "\n",
    "```python\n",
    "A = torch.arange(6, dtype=torch.float32).reshape(2, 3)\n",
    "B = A.clone() # Assign a copy of A to B by allocating new memory\n",
    "A, A + B\n",
    "```\n",
    "\n",
    "$$\n",
    "A \\odot B = \\begin{bmatrix} a_{11} b_{11} & a_{12} b_{12} & ... & a_{1n} b_{1n} \\\\ a_{21} b_{21} & a_{22} b_{22} & ... & a_{2n} b_{2n} \\\\ ... & ... & ... & ... \\\\ a_{m1} b_{m1} & a_{m2} b_{m2} & ... & a_{mn} b_{mn} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "```python\n",
    "A * B\n",
    "```\n",
    "\n",
    "## 6 Reduction\n",
    "\n",
    "Often, we wish to calculate the sum of a tensor’s elements.\n",
    "\n",
    "```python\n",
    "x = torch.arange(3, dtype=torch.float32)\n",
    "x, x.sum()\n",
    "```\n",
    "\n",
    "To express sums over the elements of tensors of arbitrary shape:\n",
    "\n",
    "```python\n",
    "A.shape, A.sum()\n",
    "```\n",
    "\n",
    "By default, invoking the sum function reduces a tensor along all of its axes, eventually producing a scalar. We can also specify an axis for reduction:\n",
    "\n",
    "```python\n",
    "A.shape, A.sum(axis=0).shape\n",
    "A.shape, A.sum(axis=1).shape\n",
    "```\n",
    "\n",
    "The mean of a tensor can be computed as follows:\n",
    "\n",
    "```python\n",
    "A.mean(), A.sum() / A.numel()\n",
    "A.mean(axis=0), A.sum(axis=0) / A.shape[0]\n",
    "```\n",
    "\n",
    "## 7 Non-Reduction Sum\n",
    "\n",
    "Sometimes it is useful to keep the number of axes unchanged:\n",
    "\n",
    "```python\n",
    "sum_A = A.sum(axis=1, keepdims=True)\n",
    "sum_A, sum_A.shape\n",
    "A / sum_A\n",
    "```\n",
    "\n",
    "To calculate cumulative sums:\n",
    "\n",
    "```python\n",
    "A.cumsum(axis=0)\n",
    "```\n",
    "\n",
    "## 8 Dot Products\n",
    "\n",
    "One of the most fundamental operations is the dot product:\n",
    "\n",
    "$$\n",
    "x^T y = \\sum_{i=1}^{d} x_i y_i\n",
    "$$\n",
    "\n",
    "```python\n",
    "y = torch.ones(3, dtype=torch.float32)\n",
    "x, y, torch.dot(x, y)\n",
    "```\n",
    "\n",
    "The dot product can also be computed using:\n",
    "\n",
    "```python\n",
    "torch.sum(x * y)\n",
    "```\n",
    "\n",
    "## 9 Matrix-Vector Products\n",
    "\n",
    "Given an $m \\times n$ matrix $A$ and an $n$-dimensional vector $x$:\n",
    "\n",
    "$$\n",
    "Ax = \\begin{bmatrix} a_1^T x \\\\ a_2^T x \\\\ ... \\\\ a_m^T x \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "```python\n",
    "A.shape, x.shape, torch.mv(A, x), A @ x\n",
    "```\n",
    "\n",
    "## 10 Matrix-Matrix Multiplication\n",
    "\n",
    "Matrix-matrix multiplication is straightforward:\n",
    "\n",
    "$$\n",
    "C = AB = \\begin{bmatrix} a_1^T b_1 & a_1^T b_2 & ... & a_1^T b_m \\\\ a_2^T b_1 & a_2^T b_2 & ... & a_2^T b_m \\\\ ... & ... & ... & ... \\\\ a_n^T b_1 & a_n^T b_2 & ... & a_n^T b_m \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "```python\n",
    "B = torch.ones(3, 4)\n",
    "torch.mm(A, B), A @ B\n",
    "```\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7776b094",
   "metadata": {},
   "source": [
    "## 11 Norms\n",
    "\n",
    "Some of the most useful operators in linear algebra are norms. Informally, the norm of a vector tells us how big it is. For instance, the $\\ell_2$ norm measures the (Euclidean) length of a vector. Here, we are employing a notion of size that concerns the magnitude of a vector’s components (not its dimensionality). A norm is a function $\\| \\cdot \\|$ that maps a vector to a scalar and satisfies the following three properties:\n",
    "\n",
    "1. Given any vector $x$, if we scale (all elements of) the vector by a scalar $\\alpha \\in \\mathbb{R}$, its norm scales accordingly:\n",
    "   $$\\|\\alpha x\\| = |\\alpha|\\|x\\|.$$ (2.3.10)\n",
    "\n",
    "2. For any vectors $x$ and $y$: norms satisfy the triangle inequality:\n",
    "   $$\\|x + y\\| \\leq \\|x\\| + \\|y\\|.$$ (2.3.11)\n",
    "\n",
    "3. The norm of a vector is nonnegative and it only vanishes if the vector is zero:\n",
    "   $$\\|x\\| > 0 \\quad \\text{for all } x \\neq 0.$$ (2.3.12)\n",
    "\n",
    "Many functions are valid norms and different norms encode different notions of size. The Euclidean norm that we all learned in elementary school geometry when calculating the hypotenuse of a right triangle is the square root of the sum of squares of a vector’s elements. Formally, this is called the $\\ell_2$ norm and expressed as:\n",
    "   $$\\|x\\|_2 = \\sqrt{\\sum_{i=1}^{n} x_i^2}.$$ (2.3.13)\n",
    "\n",
    "The method `norm` calculates the $\\ell_2$ norm:\n",
    "```python\n",
    "u = torch.tensor([3.0, -4.0])\n",
    "torch.norm(u)\n",
    "```\n",
    "```\n",
    "tensor(5.)\n",
    "```\n",
    "\n",
    "The $\\ell_1$ norm is also popular and the associated metric is called the Manhattan distance. By definition, the $\\ell_1$ norm sums the absolute values of a vector’s elements:\n",
    "   $$\\|x\\|_1 = \\sum_{i=1}^{n} |x_i|.$$ (2.3.14)\n",
    "Compared to the $\\ell_2$ norm, it is less sensitive to outliers. To compute the $\\ell_1$ norm, we compose the absolute value with the sum operation:\n",
    "```python\n",
    "torch.abs(u).sum()\n",
    "```\n",
    "```\n",
    "tensor(7.)\n",
    "```\n",
    "\n",
    "Both the $\\ell_2$ and $\\ell_1$ norms are special cases of the more general $\\ell_p$ norms:\n",
    "   $$\\|x\\|_p = \\left(\\sum_{i=1}^{n} |x_i|^p \\right)^{1/p}.$$ (2.3.15)\n",
    "\n",
    "In the case of matrices, matters are more complicated. After all, matrices can be viewed both as collections of individual entries and as objects that operate on vectors and transform them into other vectors. For instance, we can ask by how much longer the matrix-vector product $Xv$ could be relative to $v$. This line of thought leads to a norm called the spectral norm. For now, we introduce the Frobenius norm, which is much easier to compute and defined as the square root of the sum of the squares of a matrix’s elements:\n",
    "   $$\\|X\\|_F = \\sqrt{\\sum_{i=1}^{m} \\sum_{j=1}^{n} x_{ij}^2}.$$ (2.3.16)\n",
    "\n",
    "The Frobenius norm behaves as if it were an $\\ell_2$ norm of a matrix-shaped vector. Invoking the following function will calculate the Frobenius norm of a matrix:\n",
    "```python\n",
    "torch.norm(torch.ones((4, 9)))\n",
    "```\n",
    "```\n",
    "tensor(6.)\n",
    "```\n",
    "\n",
    "While we do not want to get too far ahead of ourselves, we can plant some intuition already about why these concepts are useful. In deep learning, we are often trying to solve optimization problems: maximize the probability assigned to observed data; maximize the revenue associated with a recommender model; minimize the distance between predictions and the ground-truth observations; minimize the distance between representations of photos of the same person while maximizing the distance between representations of photos of different people. These distances, which constitute the objectives of deep learning algorithms, are often expressed as norms.\n",
    "\n",
    "## 11 Norms\n",
    "\n",
    "Some of the most useful operators in linear algebra are norms. Informally, the norm of a vector tells us how big it is. For instance, the $\\ell_2$ norm measures the (Euclidean) length of a vector. Here, we are employing a notion of size that concerns the magnitude of a vector’s components (not its dimensionality). A norm is a function $\\| \\cdot \\|$ that maps a vector to a scalar and satisfies the following three properties:\n",
    "\n",
    "1. Given any vector $x$, if we scale (all elements of) the vector by a scalar $\\alpha \\in \\mathbb{R}$, its norm scales accordingly:\n",
    "   $$\\|\\alpha x\\| = |\\alpha|\\|x\\|.$$ (2.3.10)\n",
    "\n",
    "2. For any vectors $x$ and $y$: norms satisfy the triangle inequality:\n",
    "   $$\\|x + y\\| \\leq \\|x\\| + \\|y\\|.$$ (2.3.11)\n",
    "\n",
    "3. The norm of a vector is nonnegative and it only vanishes if the vector is zero:\n",
    "   $$\\|x\\| > 0 \\quad \\text{for all } x \\neq 0.$$ (2.3.12)\n",
    "\n",
    "Many functions are valid norms and different norms encode different notions of size. The Euclidean norm that we all learned in elementary school geometry when calculating the hypotenuse of a right triangle is the square root of the sum of squares of a vector’s elements. Formally, this is called the $\\ell_2$ norm and expressed as:\n",
    "   $$\\|x\\|_2 = \\sqrt{\\sum_{i=1}^{n} x_i^2}.$$ (2.3.13)\n",
    "\n",
    "The method `norm` calculates the \\(\\ell_2\\) norm:\n",
    "```python\n",
    "u = torch.tensor([3.0, -4.0])\n",
    "torch.norm(u)\n",
    "```\n",
    "```\n",
    "tensor(5.)\n",
    "```\n",
    "\n",
    "The $\\ell_1$ norm is also popular and the associated metric is called the Manhattan distance. By definition, the $\\ell_1$ norm sums the absolute values of a vector’s elements:\n",
    "   $$\\|x\\|_1 = \\sum_{i=1}^{n} |x_i|.$$ (2.3.14)\n",
    "Compared to the $\\ell_2$ norm, it is less sensitive to outliers. To compute the $\\ell_1$ norm, we compose the absolute value with the sum operation:\n",
    "```python\n",
    "torch.abs(u).sum()\n",
    "```\n",
    "```\n",
    "tensor(7.)\n",
    "```\n",
    "\n",
    "Both the $\\ell_2$ and $\\ell_1$ norms are special cases of the more general $\\ell_p$ norms:\n",
    "   $$\\|x\\|_p = \\left(\\sum_{i=1}^{n} |x_i|^p \\right)^{1/p}.$$ (2.3.15)\n",
    "\n",
    "In the case of matrices, matters are more complicated. After all, matrices can be viewed both as collections of individual entries and as objects that operate on vectors and transform them into other vectors. For instance, we can ask by how much longer the matrix-vector product $Xv$ could be relative to $v$. This line of thought leads to a norm called the spectral norm. For now, we introduce the Frobenius norm, which is much easier to compute and defined as the square root of the sum of the squares of a matrix’s elements:\n",
    "   $$\\|X\\|_F = \\sqrt{\\sum_{i=1}^{m} \\sum_{j=1}^{n} x_{ij}^2}.$ (2.3.16)\n",
    "\n",
    "The Frobenius norm behaves as if it were an $\\ell_2$ norm of a matrix-shaped vector. Invoking the following function will calculate the Frobenius norm of a matrix:\n",
    "```python\n",
    "torch.norm(torch.ones((4, 9)))\n",
    "```\n",
    "```\n",
    "tensor(6.)\n",
    "```\n",
    "\n",
    "While we do not want to get too far ahead of ourselves, we can plant some intuition already about why these concepts are useful. In deep learning, we are often trying to solve optimization problems: maximize the probability assigned to observed data; maximize the revenue associated with a recommender model; minimize the distance between predictions and the ground-truth observations; minimize the distance between representations of photos of the same person while maximizing the distance between representations of photos of different people. These distances, which constitute the objectives of deep learning algorithms, are often expressed as norms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc564b8",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "In this section, we reviewed all the linear algebra that you will need to understand a remarkable chunk of modern deep learning. There is a lot more to linear algebra and much of it is useful for machine learning. For example, matrices can be decomposed into factors, and these decompositions can reveal low-dimensional structure in real-world datasets. There are entire subfields of machine learning that focus on using matrix decompositions and their generalizations to high-order tensors to discover structure in datasets and solve prediction problems.\n",
    "\n",
    "But this book focuses on deep learning. And we believe you will be more inclined to learn more mathematics once you have gotten your hands dirty applying machine learning to real datasets. So while we reserve the right to introduce more mathematics later on, we wrap up this section here. If you are eager to learn more linear algebra, there are many excellent books and online resources. For a more advanced crash course, consider checking out Kolter (2008), Petersen et al. (2008), Strang (1993).\n",
    "\n",
    "### Recap:\n",
    "\n",
    "- Scalars, vectors, matrices, and tensors are the basic mathematical objects used in linear algebra and have zero, one, two, and an arbitrary number of axes, respectively.\n",
    "- Tensors can be sliced or reduced along specified axes via indexing, or operations such as sum and mean, respectively.\n",
    "- Elementwise products are called Hadamard products. By contrast, dot products, matrix-vector products, and matrix-matrix products are not elementwise operations and in general return objects that have different shapes than the operands.\n",
    "- Compared to Hadamard products, matrix-matrix products take considerably longer to compute (cubic rather than quadratic time).\n",
    "- Norms capture various notions of the magnitude of a vector and are commonly applied to the difference of two vectors to measure their distance.\n",
    "- Common vector norms include the $\\ell_1$ and $\\ell_2$ norms, and common matrix norms include the spectral and Frobenius norms.\n",
    "\n",
    "$$\n",
    "\\|x\\|_1 = \\sum_{i=1}^{n} |x_i| \\\\\n",
    "\\|x\\|_2 = \\sqrt{\\sum_{i=1}^{n} x_i^2} \\\\\n",
    "\\|X\\|_F = \\sqrt{\\sum_{i=1}^{m} \\sum_{j=1}^{n} x_{ij}^2}\n",
    "$$\n",
    "\n",
    "These fundamental concepts form the foundation for understanding deep learning algorithms and models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93e9909",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
