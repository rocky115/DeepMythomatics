{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ed09dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " * Copyright (c) 2004 Radhamadhab Dalai\n",
    " *\n",
    " * Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    " * of this software and associated documentation files (the \"Software\"), to deal\n",
    " * in the Software without restriction, including without limitation the rights\n",
    " * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    " * copies of the Software, and to permit persons to whom the Software is\n",
    " * furnished to do so, subject to the following conditions:\n",
    " *\n",
    " * The above copyright notice and this permission notice shall be included in\n",
    " * all copies or substantial portions of the Software.\n",
    " *\n",
    " * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    " * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    " * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    " * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    " * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    " * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
    " * THE SOFTWARE.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b28aff",
   "metadata": {},
   "source": [
    "## The Basic Ingredients\n",
    "# 2. Preparation for Deep Learning\n",
    "\n",
    "To prepare for your dive into deep learning, you will need a few survival skills:\n",
    "\n",
    "1. **Techniques for Storing and Manipulating Data**\n",
    "2. **Libraries for Ingesting and Preprocessing Data** from a variety of sources.\n",
    "3. **Basic Linear Algebraic Operations** applied to high-dimensional data elements.\n",
    "4. **Just Enough Calculus** to determine which direction to adjust each parameter in order to decrease the loss function.\n",
    "5. **Automatic Derivative Computation** so that you can focus less on manual calculus.\n",
    "6. **Basic Fluency in Probability**, our primary language for reasoning under uncertainty.\n",
    "7. **Aptitude for Finding Answers** in the official documentation when you get stuck.\n",
    "\n",
    "In short, this chapter provides a rapid introduction to the basics that you will need to follow most of the technical content in this book.\n",
    "\n",
    "## 2.1 Data Manipulation\n",
    "\n",
    "In order to get anything done, we need some way to store and manipulate data. Generally, there are two important things we need to do with data:\n",
    "\n",
    "1. **Acquire Data**\n",
    "2. **Process Data** once it is inside the computer.\n",
    "\n",
    "### Tensors\n",
    "\n",
    "There is no point in acquiring data without some way to store it, so to start, let’s get our hands dirty with n-dimensional arrays, which we also call **tensors**. If you already know the NumPy scientific computing package, this will be a breeze. \n",
    "\n",
    "For all modern deep learning frameworks, the tensor class (ndarray in MXNet, Tensor in PyTorch and TensorFlow) resembles NumPy’s ndarray, with a few killer features added:\n",
    "\n",
    "- **Automatic Differentiation**: The tensor class supports automatic differentiation.\n",
    "- **GPU Acceleration**: It leverages GPUs to accelerate numerical computation, whereas NumPy only runs on CPUs.\n",
    "\n",
    "These properties make neural networks both easy to code and fast to run.\n",
    "\n",
    "### 2.1.1 Getting Started\n",
    "\n",
    "To start, we import the PyTorch library. Note that the package name is `torch`.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10207347",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning and Tensors\n",
    "\n",
    "## 1.7 The Essence of Deep Learning\n",
    "\n",
    "Thus far, we have talked about machine learning broadly. **Deep learning** is the subset of machine learning concerned with models based on many-layered neural networks. It is deep in precisely the sense that its models learn many layers of transformations. While this might sound narrow, deep learning has given rise to a dizzying array of models, techniques, problem formulations, and applications.\n",
    "\n",
    "Many intuitions have been developed to explain the benefits of depth. Arguably, all machine learning has many layers of computation, the first consisting of feature processing steps. What differentiates deep learning is that the operations learned at each of the many layers of representations are learned jointly from data.\n",
    "\n",
    "The problems that we have discussed so far, such as learning from the raw audio signal, the raw pixel values of images, or mapping between sentences of arbitrary lengths and their counterparts in foreign languages, are those where deep learning excels and traditional methods falter. \n",
    "\n",
    "It turns out that these many-layered models are capable of addressing low-level perceptual data in a way that previous tools could not. Arguably the most significant commonality in deep learning methods is **end-to-end training**. That is, rather than assembling a system based on components that are individually tuned, one builds the system and then tunes their performance jointly. \n",
    "\n",
    "For instance, in computer vision, scientists used to separate the process of feature engineering from the process of building machine learning models. The Canny edge detector (Canny, 1987) and Lowe’s SIFT feature extractor (Lowe, 2004) reigned supreme for over a decade as algorithms for mapping images into feature vectors. \n",
    "\n",
    "In bygone days, the crucial part of applying machine learning to these problems consisted of coming up with manually-engineered ways of transforming the data into some form amenable to shallow models. Unfortunately, there is only so much that humans can accomplish by ingenuity in comparison with a consistent evaluation over millions of choices carried out automatically by an algorithm. \n",
    "\n",
    "When deep learning took over, these feature extractors were replaced by automatically tuned filters, yielding superior accuracy. Thus, one key advantage of deep learning is that it replaces not only the shallow models at the end of traditional learning pipelines but also the labor-intensive process of feature engineering. \n",
    "\n",
    "Moreover, by replacing much of the domain-specific preprocessing, deep learning has eliminated many of the boundaries that previously separated computer vision, speech recognition, natural language processing, medical informatics, and other application areas, offering a unified set of tools for tackling diverse problems.\n",
    "\n",
    "Beyond end-to-end training, we are experiencing a transition from parametric statistical descriptions to fully nonparametric models. When data is scarce, one needs to rely on simplifying assumptions about reality in order to obtain useful models. When data is abundant, these can be replaced by nonparametric models that better fit the data.\n",
    "\n",
    "To some extent, this mirrors the progress that physics experienced in the middle of the previous century with the availability of computers. Rather than solving parametric approximations of how electrons behave by hand, one can now resort to numerical simulations of the associated partial differential equations. \n",
    "\n",
    "This has led to much more accurate models, albeit often at the expense of explainability. Another difference to previous work is the acceptance of suboptimal solutions, dealing with nonconvex nonlinear optimization problems, and the willingness to try things before proving them. \n",
    "\n",
    "This newfound empiricism in dealing with statistical problems, combined with a rapid influx of talent, has led to rapid progress in practical algorithms, albeit in many cases at the expense of modifying and reinventing tools that existed for decades. \n",
    "\n",
    "In the end, the deep learning community prides itself on sharing tools across academic and corporate boundaries, releasing many excellent libraries, statistical models, and trained networks as open source. It is in this spirit that the notebooks forming this book are freely available for distribution and use. We have worked hard to lower the barriers of access for everyone to learn about deep learning, and we hope that our readers will benefit from this.\n",
    "\n",
    "## 1.8 Summary\n",
    "\n",
    "Machine learning studies how computer systems can leverage experience (often data) to improve performance at specific tasks. It combines ideas from statistics, data mining, and optimization. Often, it is used as a means of implementing AI solutions. As a class of machine learning, representational learning focuses on how to automatically find the appropriate way to represent data.\n",
    "\n",
    "As multi-level representation learning through learning many layers of transformations, deep learning replaces not only the shallow models at the end of traditional machine learning pipelines but also the labor-intensive process of feature engineering. \n",
    "\n",
    "Much of the recent progress in deep learning has been triggered by an abundance of data arising from cheap sensors and Internet-scale applications, and by significant progress in computation, mostly through GPUs. \n",
    "\n",
    "Besides, the availability of efficient deep learning frameworks has made the design and implementation of whole system optimization significantly easier, which is a key component in obtaining high performance.\n",
    "\n",
    "## Preparing for Deep Learning\n",
    "\n",
    "To prepare for your dive into deep learning, you will need a few survival skills:\n",
    "\n",
    "1. Techniques for storing and manipulating data.\n",
    "2. Libraries for ingesting and preprocessing data from a variety of sources.\n",
    "3. Knowledge of the basic linear algebraic operations that we apply to high-dimensional data elements.\n",
    "4. Just enough calculus to determine which direction to adjust each parameter in order to decrease the loss function.\n",
    "5. The ability to automatically compute derivatives so that you can forget much of the calculus you just learned.\n",
    "6. Some basic fluency in probability, our primary language for reasoning under uncertainty.\n",
    "7. Some aptitude for finding answers in the official documentation when you get stuck.\n",
    "\n",
    "In short, this chapter provides a rapid introduction to the basics that you will need to follow most of the technical content in this book.\n",
    "\n",
    "## 2.1 Data Manipulation\n",
    "\n",
    "In order to get anything done, we need some way to store and manipulate data. Generally, there are two important things we need to do with data: \n",
    "\n",
    "1. Acquire it.\n",
    "2. Process it once it is inside the computer.\n",
    "\n",
    "There is no point in acquiring data without some way to store it, so to start, let’s get our hands dirty with **n-dimensional arrays**, which we also call **tensors**. If you already know the NumPy scientific computing package, this will be a breeze. For all modern deep learning frameworks, the tensor class (ndarray in MXNet, Tensor in PyTorch and TensorFlow) resembles NumPy’s ndarray, with a few killer features added.\n",
    "\n",
    "First, the tensor class supports **automatic differentiation**. Second, it leverages **GPUs** to accelerate numerical computation, whereas NumPy only runs on CPUs. These properties make neural networks both easy to code and fast to run.\n",
    "\n",
    "### 2.1.1 Getting Started\n",
    "\n",
    "To start, we import the PyTorch library. Note that the package name is `torch`.\n",
    "\n",
    "```python\n",
    "# Importing the PyTorch library\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e1b3219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Creating a tensor using arange\n",
    "x = torch.arange(12, dtype=torch.float32)\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e17fffc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12])\n"
     ]
    }
   ],
   "source": [
    "# Shape of tensor x\n",
    "tensor_shape = x.shape\n",
    "print(tensor_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b88c1acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.]])\n"
     ]
    }
   ],
   "source": [
    "# Reshaping tensor x to a matrix X\n",
    "X = x.reshape(3, 4)\n",
    "print(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f804d47",
   "metadata": {},
   "source": [
    "### 2.1.2 Indexing and Slicing\n",
    "\n",
    "As with Python lists, we can access tensor elements by indexing (starting with 0). To access an element based on its position relative to the end of the list, we can use **negative indexing**. Additionally, we can access whole ranges of indices via **slicing** (e.g., `X[start:stop)`), where the returned value includes the first index (`start`) but not the last (`stop`).\n",
    "\n",
    "When only one index (or slice) is specified for a \\(k\\)-th order tensor, it is applied along axis 0. Thus, in the following code, `[-1]` selects the last row and `[1:3]` selects the second and third rows.\n",
    "\n",
    "```python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "999ab279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 3, 2, 1])\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [4, 3, 2, 1]])\n"
     ]
    }
   ],
   "source": [
    "X = torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "print(X[-1])   # Last row\n",
    "print(X[1:3])  # Second and third rows\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62ad0a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2,  1,  4,  3],\n",
      "        [ 1,  2, 17,  4],\n",
      "        [ 4,  3,  2,  1]])\n"
     ]
    }
   ],
   "source": [
    "# Assigning a new value to an element\n",
    "X[1, 2] = 17\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85378c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[12, 12, 12, 12],\n",
      "        [12, 12, 12, 12],\n",
      "        [ 4,  3,  2,  1]])\n"
     ]
    }
   ],
   "source": [
    "# Assigning a value to multiple elements\n",
    "X[:2, :] = 12\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c084a0b4",
   "metadata": {},
   "source": [
    "### 2.1.3 Operations\n",
    "\n",
    "Now that we know how to construct tensors and how to read from and write to their elements, we can begin to manipulate them with various mathematical operations. Among the most useful tools are the **elementwise operations**. These apply a standard scalar operation to each element of a tensor. For functions that take two tensors as inputs, elementwise operations apply some standard binary operator on each pair of corresponding elements.\n",
    "\n",
    "We can create an elementwise function from any function that maps from a scalar to a scalar. In mathematical notation, we denote such **unary scalar operators** (taking one input) by the signature $ f: \\mathbb{R} \\to \\mathbb{R} $. This just means that the function maps from any real number onto some other real number. Most standard operators can be applied elementwise, including unary operators like $ e^x $.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "174e484b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e+00, 2.7183e+00, 7.3891e+00, 2.0086e+01, 5.4598e+01, 1.4841e+02,\n",
       "        4.0343e+02, 1.0966e+03, 2.9810e+03, 8.1031e+03, 2.2026e+04, 5.9874e+04])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Example of applying the exponential function elementwise\n",
    "x = torch.arange(12, dtype=torch.float32)\n",
    "torch.exp(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556db00d",
   "metadata": {},
   "source": [
    "Likewise, we denote binary scalar operators, which map pairs of real numbers to a (single) real number via the signature $f : R, R → R$. Given any two vectors u and v of the same shape, and a binary operator $f$ , we can produce a vector $c = F(u, v)$ by setting $ci ← f (ui, vi )$ for all i, where $c_i$, $u_i$ , and $v_i$ are the i th elements of vectors c, u, and v. Here, we produced the vector-valued $F : R^d, R^d → R^d$ by lifting the scalar function to an elementwise vector operation. The common standard arithmetic operators for addition (+), subtraction (-), multiplication (*), division (/), and exponentiation (**) have all been lifted to elementwise operations for identically-shaped tensors of arbitrary shape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba25ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example tensors\n",
    "x = torch.tensor([1.0, 2, 4, 8])\n",
    "y = torch.tensor([2, 2, 2, 2])\n",
    "\n",
    "# Elementwise operations\n",
    "x + y, x - y, x * y, x / y, x ** y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ecb453f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x + y: tensor([ 3.,  4.,  6., 10.])\n",
      "x - y: tensor([-1.,  0.,  2.,  6.])\n",
      "x * y: tensor([ 2.,  4.,  8., 16.])\n",
      "x / y: tensor([0.5000, 1.0000, 2.0000, 4.0000])\n",
      "x ** y: tensor([ 1.,  4., 16., 64.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Example tensors\n",
    "x = torch.tensor([1.0, 2, 4, 8])  # Float tensor\n",
    "y = torch.tensor([2, 2, 2, 2], dtype=torch.float32)  # Convert y to Float tensor\n",
    "\n",
    "# Elementwise operations\n",
    "sum_result = x + y\n",
    "diff_result = x - y\n",
    "prod_result = x * y\n",
    "div_result = x / y\n",
    "pow_result = x ** y\n",
    "\n",
    "# Print results\n",
    "print(\"x + y:\", sum_result)\n",
    "print(\"x - y:\", diff_result)\n",
    "print(\"x * y:\", prod_result)\n",
    "print(\"x / y:\", div_result)\n",
    "print(\"x ** y:\", pow_result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fa8477a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tensor:\n",
      " tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "Transposed Tensor:\n",
      " tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a 2D tensor (matrix)\n",
    "tensor_2d = torch.tensor([[1, 2, 3],\n",
    "                           [4, 5, 6]])\n",
    "\n",
    "# Transpose using .t() method\n",
    "transposed_tensor = tensor_2d.t()\n",
    "\n",
    "# Alternatively, you can use torch.transpose\n",
    "# transposed_tensor = torch.transpose(tensor_2d, 0, 1)\n",
    "\n",
    "# Print the original and transposed tensors\n",
    "print(\"Original Tensor:\\n\", tensor_2d)\n",
    "print(\"Transposed Tensor:\\n\", transposed_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e81543e",
   "metadata": {},
   "source": [
    "## Tensor Operations in PyTorch\n",
    "\n",
    "Given two tensors:\n",
    "\n",
    "$$\n",
    "X = \\text{torch.arange}(12, \\text{dtype=torch.float32}).\\text{reshape}((3, 4))\n",
    "$$\n",
    "$$\n",
    "Y = \\text{torch.tensor}([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "$$\n",
    "\n",
    "### Concatenation\n",
    "\n",
    "When we concatenate the tensors along different dimensions, we can observe the following:\n",
    "\n",
    "1. **Concatenating along axis 0 (rows)**:\n",
    "   - The axis-0 lengths of the two tensors are \\(3\\) and \\(3\\). Thus, the resulting axis-0 length will be:\n",
    "   $$\n",
    "   \\text{axis-0 length} = 3 + 3 = 6\n",
    "   $$\n",
    "\n",
    "   Resulting tensor:\n",
    "   $$\n",
    "   \\text{torch.cat}((X, Y), \\text{dim}=0) = \n",
    "   \\begin{bmatrix}\n",
    "   0. & 1. & 2. & 3. \\\\\n",
    "   4. & 5. & 6. & 7. \\\\\n",
    "   8. & 9. & 10. & 11. \\\\\n",
    "   2. & 1. & 4. & 3. \\\\\n",
    "   1. & 2. & 3. & 4. \\\\\n",
    "   4. & 3. & 2. & 1.\n",
    "   \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "2. **Concatenating along axis 1 (columns)**:\n",
    "   - The axis-1 lengths of the two tensors are \\(4\\) and \\(4\\). Thus, the resulting axis-1 length will be:\n",
    "   $$\n",
    "   \\text{axis-1 length} = 4 + 4 = 8\n",
    "   $$\n",
    "\n",
    "   Resulting tensor:\n",
    "   $$\n",
    "   \\text{torch.cat}((X, Y), \\text{dim}=1) = \n",
    "   \\begin{bmatrix}\n",
    "   0. & 1. & 2. & 3. & 2. & 1. & 4. & 3. \\\\\n",
    "   4. & 5. & 6. & 7. & 1. & 2. & 3. & 4. \\\\\n",
    "   8. & 9. & 10. & 11. & 4. & 3. & 2. & 1.\n",
    "   \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "### Logical Comparison\n",
    "\n",
    "We can construct a binary tensor by comparing the elements of \\(X\\) and \\(Y\\):\n",
    "$$\n",
    "X == Y \\Rightarrow\n",
    "\\begin{bmatrix}\n",
    "\\text{False} & \\text{True} & \\text{False} & \\text{True} \\\\\n",
    "\\text{False} & \\text{False} & \\text{False} & \\text{False} \\\\\n",
    "\\text{False} & \\text{False} & \\text{False} & \\text{False}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "For each position \\(i, j\\), if \\(X[i, j]\\) is equal to \\(Y[i, j]\\), the corresponding entry in the result takes value \\(1\\); otherwise, it takes value \\(0\\).\n",
    "\n",
    "### Summation of Elements\n",
    "\n",
    "Summing all the elements in tensor \\(X\\):\n",
    "$$\n",
    "\\text{Sum of elements in } X = X.sum() = \\text{tensor}(66.)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ead0498f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated along dim=0:\n",
      " tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [ 2.,  1.,  4.,  3.],\n",
      "        [ 1.,  2.,  3.,  4.],\n",
      "        [ 4.,  3.,  2.,  1.]])\n",
      "Concatenated along dim=1:\n",
      " tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n",
      "        [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]])\n",
      "Element-wise equality:\n",
      " tensor([[0, 1, 0, 1],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0]], dtype=torch.uint8)\n",
      "Sum of elements in X: tensor(66.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create tensor X\n",
    "X = torch.arange(12, dtype=torch.float32).reshape((3, 4))  # Shape: (3, 4)\n",
    "\n",
    "# Create tensor Y\n",
    "Y = torch.tensor([[2.0, 1, 4, 3], \n",
    "                  [1, 2, 3, 4], \n",
    "                  [4, 3, 2, 1]])  # Shape: (3, 4)\n",
    "\n",
    "# Concatenate along dimension 0 (rows)\n",
    "result_dim0 = torch.cat((X, Y), dim=0)\n",
    "print(\"Concatenated along dim=0:\\n\", result_dim0)\n",
    "\n",
    "# Concatenate along dimension 1 (columns)\n",
    "result_dim1 = torch.cat((X, Y), dim=1)\n",
    "print(\"Concatenated along dim=1:\\n\", result_dim1)\n",
    "\n",
    "# Element-wise logical comparison\n",
    "logical_tensor = X == Y\n",
    "print(\"Element-wise equality:\\n\", logical_tensor)\n",
    "\n",
    "# Sum all elements in tensor X\n",
    "sum_result = X.sum()\n",
    "print(\"Sum of elements in X:\", sum_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317a5523",
   "metadata": {},
   "source": [
    "### 2.1.4 Broadcasting \n",
    "\n",
    "By now, you know how to perform elementwise binary operations on two tensors of the same shape. Under certain conditions, even when shapes diﬀer, we can still perform elementwise binary operations by invoking the broadcasting mechanism. Broadcasting works according to the following two-step procedure: (i) expand one or both arrays by copying elements along axes with length 1 so that after this transformation, the two tensors have the same shape; (ii) perform an elementwise operation on the resulting arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1326a60e",
   "metadata": {},
   "source": [
    "## Tensor Operations in PyTorch\n",
    "\n",
    "Given two tensors:\n",
    "\n",
    "$$\n",
    "X = \\text{torch.arange}(12, \\text{dtype=torch.float32}).\\text{reshape}((3, 4))\n",
    "$$\n",
    "$$\n",
    "Y = \\text{torch.tensor}([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "$$\n",
    "\n",
    "### Concatenation\n",
    "\n",
    "When we concatenate the tensors along different dimensions, we can observe the following:\n",
    "\n",
    "1. **Concatenating along axis 0 (rows)**:\n",
    "   - The axis-0 lengths of the two tensors are \\(3\\) and \\(3\\). Thus, the resulting axis-0 length will be:\n",
    "   $$\n",
    "   \\text{axis-0 length} = 3 + 3 = 6\n",
    "   $$\n",
    "\n",
    "   Resulting tensor:\n",
    "   $$\n",
    "   \\text{torch.cat}((X, Y), \\text{dim}=0) = \n",
    "   \\begin{bmatrix}\n",
    "   0. & 1. & 2. & 3. \\\\\n",
    "   4. & 5. & 6. & 7. \\\\\n",
    "   8. & 9. & 10. & 11. \\\\\n",
    "   2. & 1. & 4. & 3. \\\\\n",
    "   1. & 2. & 3. & 4. \\\\\n",
    "   4. & 3. & 2. & 1.\n",
    "   \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "2. **Concatenating along axis 1 (columns)**:\n",
    "   - The axis-1 lengths of the two tensors are \\(4\\) and \\(4\\). Thus, the resulting axis-1 length will be:\n",
    "   $$\n",
    "   \\text{axis-1 length} = 4 + 4 = 8\n",
    "   $$\n",
    "\n",
    "   Resulting tensor:\n",
    "   $$\n",
    "   \\text{torch.cat}((X, Y), \\text{dim}=1) = \n",
    "   \\begin{bmatrix}\n",
    "   0. & 1. & 2. & 3. & 2. & 1. & 4. & 3. \\\\\n",
    "   4. & 5. & 6. & 7. & 1. & 2. & 3. & 4. \\\\\n",
    "   8. & 9. & 10. & 11. & 4. & 3. & 2. & 1.\n",
    "   \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "### Logical Comparison\n",
    "\n",
    "We can construct a binary tensor by comparing the elements of \\(X\\) and \\(Y\\):\n",
    "$$\n",
    "X == Y \\Rightarrow\n",
    "\\begin{bmatrix}\n",
    "\\text{False} & \\text{True} & \\text{False} & \\text{True} \\\\\n",
    "\\text{False} & \\text{False} & \\text{False} & \\text{False} \\\\\n",
    "\\text{False} & \\text{False} & \\text{False} & \\text{False}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "For each position \\(i, j\\), if \\(X[i, j]\\) is equal to \\(Y[i, j]\\), the corresponding entry in the result takes value \\(1\\); otherwise, it takes value \\(0\\).\n",
    "\n",
    "### Summation of Elements\n",
    "\n",
    "Summing all the elements in tensor \\(X\\):\n",
    "$$\n",
    "\\text{Sum of elements in } X = X.sum() = \\text{tensor}(66.)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88ba0a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated along dim=0:\n",
      " tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [ 2.,  1.,  4.,  3.],\n",
      "        [ 1.,  2.,  3.,  4.],\n",
      "        [ 4.,  3.,  2.,  1.]])\n",
      "Concatenated along dim=1:\n",
      " tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n",
      "        [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]])\n",
      "Element-wise equality:\n",
      " tensor([[0, 1, 0, 1],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0]], dtype=torch.uint8)\n",
      "Sum of elements in X: tensor(66.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create tensor X\n",
    "X = torch.arange(12, dtype=torch.float32).reshape((3, 4))  # Shape: (3, 4)\n",
    "\n",
    "# Create tensor Y\n",
    "Y = torch.tensor([[2.0, 1, 4, 3], \n",
    "                  [1, 2, 3, 4], \n",
    "                  [4, 3, 2, 1]])  # Shape: (3, 4)\n",
    "\n",
    "# Concatenate along dimension 0 (rows)\n",
    "result_dim0 = torch.cat((X, Y), dim=0)\n",
    "print(\"Concatenated along dim=0:\\n\", result_dim0)\n",
    "\n",
    "# Concatenate along dimension 1 (columns)\n",
    "result_dim1 = torch.cat((X, Y), dim=1)\n",
    "print(\"Concatenated along dim=1:\\n\", result_dim1)\n",
    "\n",
    "# Element-wise logical comparison\n",
    "logical_tensor = X == Y\n",
    "print(\"Element-wise equality:\\n\", logical_tensor)\n",
    "\n",
    "# Sum all elements in tensor X\n",
    "sum_result = X.sum()\n",
    "print(\"Sum of elements in X:\", sum_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860edbd6",
   "metadata": {},
   "source": [
    "## Saving Memory\n",
    "\n",
    "Running operations can cause new memory to be allocated to host results. For example, if we write \n",
    "\n",
    "$$\n",
    "Y = X + Y\n",
    "$$\n",
    "\n",
    "we dereference the tensor that $Y$ used to point to and instead point $Y$ at the newly allocated memory. We can demonstrate this issue with Python’s `id()` function, which gives us the exact address of the referenced object in memory. \n",
    "\n",
    "Note that after we run \n",
    "\n",
    "$$\n",
    "Y = Y + X\n",
    "$$ \n",
    "\n",
    "the `id(Y)` points to a different location. This is because Python first evaluates $Y + X$, allocating new memory for the result and then points $Y$ to this new location in memory.\n",
    "\n",
    "```python\n",
    "before = id(Y)\n",
    "Y = Y + X\n",
    "print(id(Y) == before)  # Output: False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1cefed5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID before: 139778107280672\n",
      "ID after: 139778107280672\n",
      "Y after in-place operation: tensor([5., 7., 9.])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### Code Example:\n",
    "#Here’s a code example to demonstrate in-place operations:\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "# Create tensor X\n",
    "X = torch.tensor([1.0, 2.0, 3.0])\n",
    "\n",
    "# Create tensor Y\n",
    "Y = torch.tensor([4.0, 5.0, 6.0])\n",
    "\n",
    "# Before updating Y\n",
    "before_id = id(Y)\n",
    "\n",
    "# In-place operation\n",
    "Y[:] = Y + X\n",
    "\n",
    "# After updating Y\n",
    "after_id = id(Y)\n",
    "\n",
    "# Check if the ID changed\n",
    "print(\"ID before:\", before_id)\n",
    "print(\"ID after:\", after_id)\n",
    "print(\"Y after in-place operation:\", Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b09d1c",
   "metadata": {},
   "source": [
    "## 2.1.6 Conversion to Other Python Objects\n",
    "\n",
    "Converting to a NumPy array (ndarray), or vice versa, is easy. The PyTorch tensor and NumPy array will share their underlying memory, and changing one through an in-place operation will also change the other.\n",
    "\n",
    "```python\n",
    "# Example conversion to NumPy and back to a tensor\n",
    "import torch\n",
    "\n",
    "# Create a PyTorch tensor\n",
    "X = torch.tensor([1.0, 2.0, 3.0])\n",
    "\n",
    "# Convert to NumPy array\n",
    "A = X.numpy()\n",
    "\n",
    "# Convert back to a PyTorch tensor\n",
    "B = torch.from_numpy(A)\n",
    "\n",
    "# Check the types\n",
    "print(type(A), type(B))  # Output: (numpy.ndarray, torch.Tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1e05f3",
   "metadata": {},
   "source": [
    "## 2.2 Data Preprocessing\n",
    "\n",
    "So far, we have been working with synthetic data that arrived in ready-made tensors. However, to apply deep learning in the wild, we must extract messy data stored in arbitrary formats and preprocess it to suit our needs. Fortunately, the `pandas` library can do much of the heavy lifting. This section, while no substitute for a proper pandas tutorial, will give you a crash course on some of the most common routines.\n",
    "\n",
    "### 2.2.1 Reading the Dataset\n",
    "\n",
    "Comma-separated values (CSV) files are ubiquitous for storing tabular (spreadsheet-like) data. Here, each line corresponds to one record and consists of several (comma-separated) fields, e.g., “Albert Einstein, March 14 1879, Ulm, Federal polytechnic school, Accomplishments in the field of gravitational physics.”\n",
    "\n",
    "To demonstrate how to load CSV files with `pandas`, we will create a CSV file below `../data/house_tiny.csv`. This file represents a dataset of homes, where each row corresponds to a distinct home and the columns correspond to the number of rooms (`NumRooms`), the roof type (`RoofType`), and the price (`Price`).\n",
    "\n",
    "```python\n",
    "import os\n",
    "\n",
    "# Create the directory for the data file\n",
    "os.makedirs(os.path.join('..', 'data'), exist_ok=True)\n",
    "\n",
    "# Define the file path\n",
    "data_file = os.path.join('..', 'data', 'house_tiny.csv')\n",
    "\n",
    "# Write the CSV file\n",
    "with open(data_file, 'w') as f:\n",
    "    f.write('''NumRooms,RoofType,Price\n",
    "NA,NA,127500\n",
    "2,NA,106000\n",
    "4,Slate,178100\n",
    "NA,NA,140000\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f007048d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create the directory for the data file\n",
    "os.makedirs(os.path.join('..', 'data'), exist_ok=True)\n",
    "\n",
    "# Define the file path\n",
    "data_file = os.path.join('..', 'data', 'house_tiny.csv')\n",
    "\n",
    "# Write the CSV file\n",
    "with open(data_file, 'w') as f:\n",
    "    f.write('''NumRooms,RoofType,Price\n",
    "NA,NA,127500\n",
    "2,NA,106000\n",
    "4,Slate,178100\n",
    "NA,NA,140000\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "585865e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   NumRooms RoofType   Price\n",
      "0       NaN      NaN  127500\n",
      "1       2.0      NaN  106000\n",
      "2       4.0    Slate  178100\n",
      "3       NaN      NaN  140000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "data = pd.read_csv(data_file) \n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917bd72c",
   "metadata": {},
   "source": [
    "## Data Preparation \n",
    "\n",
    "In supervised learning, we train models to predict a designated target value, given some set of input values. Our ﬁrst step in processing the dataset is to separate out columns corresponding to input versus target values. We can select columns either by name or via integer-location based indexing (iloc). You might have noticed that pandas replaced all CSV entries with value NA with a special NaN (not a number) value. This can also happen whenever an entry is empty, e.g., “3,,,270000”. These are called missing values and they are the “bed bugs” of data science, a persistent menace that you will confront throughout your career. Depending upon the context, missing values might be handled either via imputation or deletion. Imputation replaces missing val- ues with estimates of their values while deletion simply discards either those rows or those columns that contain missing values. Here are some common imputation heuristics. For categorical input ﬁelds, we can treat NaN as a category. Since the RoofType column takes values Slate and NaN, pandas can convert this column into two columns RoofType_Slate and RoofType_nan. A row whose roof type is Slate will set values of RoofType_Slate and RoofType_nan to 1 and 0, respectively. The converse holds for a row with a missing RoofType value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aed008e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   NumRooms  RoofType_Slate  RoofType_nan\n",
      "0       NaN               0             1\n",
      "1       2.0               0             1\n",
      "2       4.0               1             0\n",
      "3       NaN               0             1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset from the CSV file\n",
    "data_file = '../data/house_tiny.csv'\n",
    "data = pd.read_csv(data_file)\n",
    "\n",
    "# Separate inputs and targets\n",
    "inputs, targets = data.iloc[:, 0:2], data.iloc[:, 2]\n",
    "\n",
    "# Apply one-hot encoding to the inputs\n",
    "inputs = pd.get_dummies(inputs, dummy_na=True)\n",
    "\n",
    "# Display the processed inputs\n",
    "print(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f945896f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   NumRooms  RoofType_Slate  RoofType_nan\n",
      "0       3.0               0             1\n",
      "1       2.0               0             1\n",
      "2       4.0               1             0\n",
      "3       3.0               0             1\n"
     ]
    }
   ],
   "source": [
    "# Fill missing values with the mean of each column\n",
    "inputs = inputs.fillna(inputs.mean())\n",
    "\n",
    "# Display the updated inputs\n",
    "print(inputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d59e9d",
   "metadata": {},
   "source": [
    "### Conversion to the Tensor Format\n",
    "\n",
    "Now that all the entries in `inputs` and `targets` are numerical, we can load them into tensors (recall Section 2.1).\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "# Convert the inputs and targets to tensors\n",
    "X = torch.tensor(inputs.values)\n",
    "y = torch.tensor(targets.values)\n",
    "\n",
    "# Display the tensors\n",
    "print(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a220eab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 0., 1.],\n",
      "        [2., 0., 1.],\n",
      "        [4., 1., 0.],\n",
      "        [3., 0., 1.]], dtype=torch.float64) tensor([127500, 106000, 178100, 140000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Convert the inputs and targets to tensors\n",
    "X = torch.tensor(inputs.values)\n",
    "y = torch.tensor(targets.values)\n",
    "\n",
    "# Display the tensors\n",
    "print(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0f14ef",
   "metadata": {},
   "source": [
    "## Linear Algebra\n",
    "\n",
    "By now, we can load datasets into tensors and manipulate these tensors with basic mathematical operations. To start building sophisticated models, we will also need a few tools from linear algebra. This section offers a gentle introduction to the most essential concepts, starting from scalar arithmetic and ramping up to matrix multiplication.\n",
    "\n",
    "### Scalars\n",
    "\n",
    "Most everyday mathematics consists of manipulating numbers one at a time. Formally, we call these values **scalars**. For example, the temperature in Palo Alto is a balmy 72 degrees Fahrenheit. If you wanted to convert the temperature to Celsius, you would evaluate the expression:\n",
    "\n",
    "$$\n",
    "c = \\frac{5}{9} (f - 32)\n",
    "$$\n",
    "\n",
    "setting $ f $ to 72. In this equation, the values 5, 9, and 32 are scalars. The variables $ c $ and $ f $ represent unknown scalars.\n",
    "\n",
    "We denote scalars by ordinary lower-cased letters (e.g., $ x, y, z $) and the space of all (continuous) real-valued scalars by $ \\mathbb{R} $. For expedience, we will skip past rigorous definitions of spaces. Just remember that the expression $ x \\in \\mathbb{R} $ is a formal way to say that $ x $ is a real-valued scalar. The symbol $ \\in $ (pronounced “in”) denotes membership in a set. For example, $ x, y \\in \\{0, 1\\} $ indicates that $ x $ and $ y $ are variables that can only take values 0 or 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3365651a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scalars are implemented as tensors that contain only one element. Below, we assign two scalars and perform the familiar addition, multiplication, division, and exponentiation operations.\n",
    "\n",
    "#```python\n",
    "import torch\n",
    "\n",
    "# Assign two scalars\n",
    "x = torch.tensor(3.0)\n",
    "y = torch.tensor(2.0)\n",
    "\n",
    "# Perform operations\n",
    "addition = x + y\n",
    "multiplication = x * y\n",
    "division = x / y\n",
    "exponentiation = x ** y\n",
    "\n",
    "# Display the results\n",
    "#addition, multiplication, division, exponentiation\n"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHAAAAAvCAIAAAC0Wtk+AAAD6UlEQVRoBe2az0sqURTHhZZSYBBYUItatChw0Y+li1q4MHgERbTpL3AVZJsmCgOTgax/IaYWGS2CWgyYaGTETEFiDFhQiwoihCDopdmcBw3cd7vzw8E32H1470LOvefc69yPZ87XueoC1hwl4HJ0NbYYMKAOJwEVQBOJRCAQmJubs9jc0tJSIBDY2tqyiKHBRQVQnucPDw8rlYoFkUqlkk6nI5GIRQwNLlqAplIpDcf9/X02my2VSvl8ngAkSRIDSjAx7vI8j4Du7u56PJ6JiYnV1VUimgElgJh2Y7HY0dERco+MjGxsbADAxcVFNBotlUqa6+zsbHl5GYXRaVBxy4+NjcXjcQ1QoVDo7OwMh8OXl5f7+/ttbW3Pz8+aa3Nzc3R0lE6O6KqoALq4uHhwcAAAa2trwWDw+PjY6/Umk0kA6OnpQUAzmUw4HEaXTqdBBVBUQz8+PjStL5fLGi8cKKuhdnMIASUmRKPR1tbWmZkZbZwBJfiYds2AqqoKANorADCgpgQJB6HyhBd1mcojFFWMlZUVURSrBAGcnp5yHFc17GcD6BIlaxbslrfm89drVkP/RnxZDCgBxLTLgJqiqc2hFyVVVW9vb4nVmCgRQEy7+KOnFqQoSnt7++fnJz6n4R49U6mUIAiiKEqSJAjC+fk5jsPCNlR59LCEJjacyieTyYGBgeHh4VAoND4+ns1mNRaFQuHX90Z8+2E1FCUNaTw8PHi9Xp/PRxy///7eiOzTAzWsoY2o8icnJ36/3+12C4KAYN/c3AS/t4WFBeQFAD1QwxqKAy0Wi4+Pj/gilNhOfrEPhULNzc2JRGJyctLtds/OztrcpF7lAYDIYgDAVZ7juKGhIZvr1zPMSaDoLAMZNndiKEr6ubgoybJM5+9LTgLVI7A5or/lq9bQp6cndApl813qE0Yp0Ko1tD50angXSoEa1lBclGrYan2mUAHUUJT0+8dFSe+lZIQKoPpHT0M6DffoaUjBzmANKm9n2R+JoSJD9SpvyILVUEMsBoMMqAGUfxlCQFVVzefziqIUi8W7uztiTZahBBDTLgL69vYWiUS6urqmpqZ2dnaICQwoAcS0G4/H0+m05i6Xy01NTVdXV/poWZZjsZh+nKoR6kRpe3vb4/Hs7e0pipLL5a6vr3O5nIaMZajd1EG3/PT09Pz8/Pr6el9fn6Iovb29g4ODLS0tLy8v7J8jdmni56Hv7+/qVyt/NZfLJYpid3f36+srA1oLUHxOJpPx+/2SJHV0dMiyzIDicKrYPM/39/ebHUhrv6ZwHOfz+eg8A8W3R4Uo4Rf0v9sMqMOfIAPKgDpMwOHlWIY6DPQP+8YQEg5EiqQAAAAASUVORK5CYII="
    }
   },
   "cell_type": "markdown",
   "id": "b3154789",
   "metadata": {},
   "source": [
    "###  Vectors\n",
    "\n",
    "For our purposes, you can think of vectors as fixed-length arrays of scalars. As with their code counterparts, we call these values the **elements** of the vector (synonyms include entries and components).\n",
    "\n",
    "When vectors represent examples from real-world datasets, their values hold some real-world significance. For example, if we were training a model to predict the risk of a loan defaulting, we might associate each applicant with a vector whose components correspond to quantities like their income, length of employment, or number of previous defaults. If we were studying heart attack risk, each vector might represent a patient, and its components might correspond to their most recent vital signs, cholesterol levels, minutes of exercise per day, etc.\n",
    "\n",
    "We denote vectors by bold lowercase letters (e.g., $\\mathbf{x}, \\mathbf{y}, \\mathbf{z}$). Vectors are implemented as 1st-order tensors. In general, such tensors can have arbitrary lengths, subject to memory limitations.\n",
    "\n",
    "**Caution**: In Python, like in most programming languages, vector indices start at 0 (zero-based indexing), whereas in linear algebra, subscripts begin at 1 (one-based indexing).\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "# Create a vector using arange\n",
    "x = torch.arange(3)\n",
    "\n",
    "# Display the vector\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff8272d",
   "metadata": {},
   "source": [
    "To indicate that a vector contains $ n $ elements, we write $$ \\mathbf{x} \\in \\mathbb{R}^n $$. Formally, we call $ n $ the dimensionality of the vector. In code, this corresponds to the tensor’s length, accessible via Python’s built-in `len` function:\n",
    "\n",
    "$$\n",
    "\\text{len}(\\mathbf{x}) = 3\n",
    "$$\n",
    "\n",
    "We can also access the length via the `shape` attribute. The shape is a tuple that indicates a tensor’s length along each axis. Tensors with just one axis have shapes with just one element:\n",
    "\n",
    "$$\n",
    "\\mathbf{x.shape} = \\text{torch.Size}([3])\n",
    "$$\n",
    "\n",
    "Oftentimes, the word “dimension” gets overloaded to mean both the number of axes and the length along a particular axis. To avoid this confusion, we use **order** to refer to the number of axes and **dimensionality** exclusively to refer to the number of components.\n",
    "\n",
    "### Matrices\n",
    "\n",
    "Just as scalars are 0th-order tensors and vectors are 1st-order tensors, matrices are 2nd-order tensors. We denote matrices by bold capital letters (e.g., $$\\mathbf{X}$$, $$\\mathbf{Y}$$, and $$\\mathbf{Z}$$), and represent them in code by tensors with two axes. The expression \n",
    "\n",
    "$$ \n",
    "\\mathbf{A} \\in \\mathbb{R}^{m \\times n} \n",
    "$$\n",
    "\n",
    "indicates that a matrix $ \\mathbf{A} $ contains $ m \\times n $ real-valued scalars, arranged as $ m $ rows and $ n $ columns. When $ m = n $, we say that a matrix is square. Visually, we can illustrate any matrix as a table. To refer to an individual element, we subscript both the row and column indices, e.g., $ a_{ij} $ is the value that belongs to $ \\mathbf{A} $’s $ i $-th row and $ j $-th column:\n",
    "\n",
    "$$\n",
    "\\mathbf{A} = \n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12} & \\cdots & a_{1n} \\\\\n",
    "a_{21} & a_{22} & \\cdots & a_{2n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "a_{m1} & a_{m2} & \\cdots & a_{mn}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "In code, we represent a matrix $ \\mathbf{A} \\in \\mathbb{R}^{m \\times n} $ by a 2nd-order tensor with shape $ (m, n) $. We can convert any appropriately sized $ m \\times n $ tensor into an $ m \\times n $ matrix by passing the desired shape to `reshape`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8857bded",
   "metadata": {},
   "source": [
    "Let \n",
    "$$ \n",
    "\\mathbf{A} = \\text{torch.arange}(6).reshape(3, 2) \n",
    "$$ \n",
    "which results in the tensor \n",
    "\n",
    "$$ \n",
    "\\mathbf{A} = \n",
    "\\begin{bmatrix}\n",
    "0 & 1 \\\\\n",
    "2 & 3 \\\\\n",
    "4 & 5\n",
    "\\end{bmatrix} \n",
    "$$ \n",
    "\n",
    "Sometimes, we want to flip the axes. When we exchange a matrix’s rows and columns, the result is called its transpose. Formally, we signify a matrix \\( \\mathbf{A} \\)’s transpose by \\( \\mathbf{A}^\\top \\) and if \\( \\mathbf{B} = \\mathbf{A}^\\top \\), then \n",
    "\n",
    "$$ \n",
    "b_{ij} = a_{ji} \n",
    "$$ \n",
    "\n",
    "for all \\( i \\) and \\( j \\). Thus, the transpose of an \\( m \\times n \\) matrix is an \\( n \\times m \\) matrix:\n",
    "\n",
    "$$\n",
    "\\mathbf{A}^\\top = \n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{21} & \\cdots & a_{m1} \\\\\n",
    "a_{12} & a_{22} & \\cdots & a_{m2} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "a_{1n} & a_{2n} & \\cdots & a_{mn}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "In code, we can access any matrix’s transpose as follows:\n",
    "\n",
    "$$ \n",
    "\\mathbf{A}.T \n",
    "$$ \n",
    "\n",
    "which results in the tensor \n",
    "\n",
    "$$ \n",
    "\\begin{bmatrix}\n",
    "0 & 2 & 4 \\\\\n",
    "1 & 3 & 5\n",
    "\\end{bmatrix} \n",
    "$$ \n",
    "\n",
    "Symmetric matrices are the subset of square matrices that are equal to their own transposes: \n",
    "\n",
    "$$ \n",
    "\\mathbf{A} = \\mathbf{A}^\\top \n",
    "$$ \n",
    "\n",
    "The following matrix is symmetric: \n",
    "\n",
    "$$ \n",
    "\\mathbf{A} = \\text{torch.tensor}\\left( \\begin{bmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "2 & 0 & 4 \\\\\n",
    "3 & 4 & 5\n",
    "\\end{bmatrix} \\right) \n",
    "$$ \n",
    "\n",
    "We can verify symmetry with:\n",
    "\n",
    "$$ \n",
    "\\mathbf{A} == \\mathbf{A}^\\top \n",
    "$$ \n",
    "\n",
    "resulting in \n",
    "\n",
    "$$ \n",
    "\\begin{bmatrix}\n",
    "\\text{True} & \\text{True} & \\text{True} \\\\\n",
    "\\text{True} & \\text{True} & \\text{True} \\\\\n",
    "\\text{True} & \\text{True} & \\text{True}\n",
    "\\end{bmatrix} \n",
    "$$ \n",
    "\n",
    "Matrices are useful for representing datasets. Typically, rows correspond to individual records and columns correspond to distinct attributes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76e7acd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
