{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e9d503",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " * Copyright (c) 2004 Radhamadhab Dalai\n",
    " *\n",
    " * Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    " * of this software and associated documentation files (the \"Software\"), to deal\n",
    " * in the Software without restriction, including without limitation the rights\n",
    " * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    " * copies of the Software, and to permit persons to whom the Software is\n",
    " * furnished to do so, subject to the following conditions:\n",
    " *\n",
    " * The above copyright notice and this permission notice shall be included in\n",
    " * all copies or substantial portions of the Software.\n",
    " *\n",
    " * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    " * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    " * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    " * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    " * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    " * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
    " * THE SOFTWARE.\n",
    "'''"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "bc18f3de",
   "metadata": {},
   "source": [
    "##  Probability and Statistics\n",
    "\n",
    "###  Introduction to Uncertainty in Machine Learning\n",
    "\n",
    "Machine learning fundamentally deals with uncertainty in various forms:\n",
    "\n",
    "- In **supervised learning**, we predict unknown targets given known features:\n",
    "  - Predicting most likely values\n",
    "  - Predicting values with minimal expected distance from targets\n",
    "  - Quantifying uncertainty (e.g., probability of patient heart attacks)\n",
    "\n",
    "- In **unsupervised learning**, we assess the likelihood of observations within populations\n",
    "\n",
    "- In **reinforcement learning**, we reason about:\n",
    "  - Expected environmental changes\n",
    "  - Anticipated rewards for different actions\n",
    "\n",
    "### Probability: Mathematical Framework for Uncertainty\n",
    "\n",
    "Probability theory provides tools for reasoning under uncertainty through probabilistic models. Two main interpretations exist:\n",
    "\n",
    "1. **Frequentist Probability**:\n",
    "   - Describes frequencies of repeatable events\n",
    "   - Limited to repeatable phenomena\n",
    "   - Example: coin toss probabilities\n",
    "\n",
    "2. **Bayesian Probability**:\n",
    "   - Broader formalization of uncertainty reasoning\n",
    "   - Distinguished by:\n",
    "     - Assigns belief degrees to non-repeatable events (e.g., $P(\\text{moon is made of cheese})$)\n",
    "     - Embraces subjectivity while providing unambiguous belief update rules\n",
    "     - Allows different prior beliefs among individuals\n",
    "\n",
    "### Statistical Reasoning\n",
    "\n",
    "Statistics enables reverse reasoning:\n",
    "- Starts with data collection and organization\n",
    "- Infers characteristics of data-generating processes\n",
    "- Identifies patterns that might generalize to broader populations\n",
    "\n",
    "```python\n",
    "%matplotlib inline\n",
    "import random\n",
    "import torch\n",
    "from torch.distributions.multinomial import Multinomial\n",
    "from d2l import torch as d2l\n",
    "```\n",
    "\n",
    "##  Coin Tossing: A Basic Probability Example\n",
    "\n",
    "Consider a coin toss experiment where we want to quantify $P(\\text{heads})$ vs $P(\\text{tails})$.\n",
    "\n",
    "For a fair coin:\n",
    "- Both outcomes are equally likely\n",
    "- For $n$ tosses:\n",
    "  - Expected heads fraction = Expected tails fraction\n",
    "  - Mathematically: for $n_h$ heads and $n_t = (n - n_h)$ tails\n",
    "  - By symmetry: each outcome $(n_h, n_t)$ pairs with equally likely $(n_t, n_h)$\n",
    "  - Therefore: $E[\\frac{n_h}{n}] = E[\\frac{n_t}{n}] = \\frac{1}{2}$\n",
    "\n",
    "Note: For large $n$ (e.g., $n = 1,000,000$), exact equality $n_h = n_t$ is extremely unlikely in any single trial.\n",
    "\n",
    "$\\blacksquare$\n",
    "\n",
    "## 2.6.2 Formal Probability and Statistical Estimation\n",
    "\n",
    "### Probability vs Statistics\n",
    "\n",
    "A probability (like $\\frac{1}{2}$ for a fair coin) represents:\n",
    "- A score between 0 and 1 assigned to events\n",
    "- Denoted as $P(\\text{event})$, e.g., $P(\\text{heads})$\n",
    "- Boundary cases:\n",
    "  - $P(\\text{event}) = 1$: absolute certainty (e.g., two-headed coin)\n",
    "  - $P(\\text{event}) = 0$: impossibility (e.g., two-tailed coin)\n",
    "\n",
    "Statistical quantities (like $\\frac{n_h}{n}$ and $\\frac{n_t}{n}$) differ from probabilities:\n",
    "- Probabilities: theoretical quantities underlying data generation\n",
    "- Statistics: empirical quantities computed from observed data\n",
    "- Estimators: special statistics designed to estimate probability parameters\n",
    "  - When consistent, estimates converge to true probabilities\n",
    "  - Inferred probabilities inform future expectations\n",
    "\n",
    "### Empirical Investigation\n",
    "\n",
    "To investigate unknown $P(\\text{heads})$:\n",
    "1. Collect data (sampling)\n",
    "2. Design estimator (e.g., $\\frac{\\text{observed heads}}{\\text{total tosses}}$)\n",
    "\n",
    "### Simulation Examples\n",
    "\n",
    "Basic coin flip simulation using Python's random:\n",
    "\n",
    "```python\n",
    "num_tosses = 100\n",
    "heads = sum([random.random() > 0.5 for _ in range(100)])\n",
    "tails = num_tosses - heads\n",
    "print(\"heads, tails: \", [heads, tails])\n",
    "# Output example: heads, tails: [48, 52]\n",
    "```\n",
    "\n",
    "Using PyTorch's Multinomial distribution:\n",
    "\n",
    "```python\n",
    "fair_probs = torch.tensor([0.5, 0.5])\n",
    "# 100 tosses\n",
    "Multinomial(100, fair_probs).sample()\n",
    "# Output example: tensor([44., 56.])\n",
    "\n",
    "# Frequencies\n",
    "Multinomial(100, fair_probs).sample() / 100\n",
    "# Output example: tensor([0.5300, 0.4700])\n",
    "\n",
    "# Large sample (10000 tosses)\n",
    "counts = Multinomial(10000, fair_probs).sample()\n",
    "counts / 10000\n",
    "# Output example: tensor([0.4970, 0.5030])\n",
    "```\n",
    "\n",
    "### Law of Large Numbers Visualization\n",
    "\n",
    "```python\n",
    "# Generate cumulative estimates\n",
    "counts = Multinomial(1, fair_probs).sample((10000,))\n",
    "cum_counts = counts.cumsum(dim=0)\n",
    "estimates = cum_counts / cum_counts.sum(dim=1, keepdims=True)\n",
    "estimates = estimates.numpy()\n",
    "\n",
    "# Plotting\n",
    "d2l.set_figsize((4.5, 3.5))\n",
    "d2l.plt.plot(estimates[:, 0], label=(\"P(coin=heads)\"))\n",
    "d2l.plt.plot(estimates[:, 1], label=(\"P(coin=tails)\"))\n",
    "d2l.plt.axhline(y=0.5, color='black', linestyle='dashed')\n",
    "d2l.plt.gca().set_xlabel('Samples')\n",
    "d2l.plt.gca().set_ylabel('Estimated probability')\n",
    "d2l.plt.legend();\n",
    "```\n",
    "\n",
    "### Key Theoretical Results\n",
    "\n",
    "1. **Law of Large Numbers**: \n",
    "   - For repeated events, estimates converge to true probabilities as sample size increases\n",
    "\n",
    "2. **Central Limit Theorem**:\n",
    "   - For many scenarios, estimation errors decrease at rate $\\frac{1}{\\sqrt{n}}$\n",
    "   - $n$ = sample size\n",
    "\n",
    "### Advanced Statistical Questions\n",
    "\n",
    "The study raises deeper statistical questions:\n",
    "- Convergence rate analysis\n",
    "- Incorporation of prior information (e.g., data from similar coins)\n",
    "- Optimal sample size determination\n",
    "- Confidence interval estimation\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "$\\blacksquare$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26949d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic coin toss (100 tosses):\n",
      "Heads: 56, Tails: 44\n",
      "\n",
      "Multinomial toss (100 tosses):\n",
      "Counts: tensor([49., 51.])\n",
      "\n",
      "Probability estimation (1000 tosses):\n",
      "P(heads): 0.500, P(tails): 0.500\n",
      "\n",
      "Plotting convergence for 3 experiments...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sum() received an invalid combination of arguments - got (keepdims=bool, dim=int, ), but expected one of:\n * ()\n * (torch.dtype dtype)\n * (tuple of ints dim, torch.dtype dtype)\n      didn't match because some of the keywords were incorrect: keepdims\n * (tuple of ints dim, bool keepdim, torch.dtype dtype)\n * (tuple of ints dim, bool keepdim)\n      didn't match because some of the keywords were incorrect: keepdims\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11174/3162517927.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m     \u001b[0mrun_demonstration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_11174/3162517927.py\u001b[0m in \u001b[0;36mrun_demonstration\u001b[0;34m()\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;31m# Plot convergence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nPlotting convergence for 3 experiments...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     \u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_convergence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_tosses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_experiments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;31m# Error analysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_11174/3162517927.py\u001b[0m in \u001b[0;36mplot_convergence\u001b[0;34m(self, max_tosses, num_experiments)\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mnum_experiments\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNumber\u001b[0m \u001b[0mof\u001b[0m \u001b[0mexperiments\u001b[0m \u001b[0mto\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \"\"\"\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mestimates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvergence_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_tosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_experiments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_11174/3162517927.py\u001b[0m in \u001b[0;36mconvergence_experiment\u001b[0;34m(self, max_tosses, num_experiments)\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mcounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_tosses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mcum_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0mestimates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcum_counts\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mcum_counts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m             \u001b[0mall_estimates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: sum() received an invalid combination of arguments - got (keepdims=bool, dim=int, ), but expected one of:\n * ()\n * (torch.dtype dtype)\n * (tuple of ints dim, torch.dtype dtype)\n      didn't match because some of the keywords were incorrect: keepdims\n * (tuple of ints dim, bool keepdim, torch.dtype dtype)\n * (tuple of ints dim, bool keepdim)\n      didn't match because some of the keywords were incorrect: keepdims\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.distributions.multinomial import Multinomial\n",
    "import numpy as np\n",
    "\n",
    "class CoinTossExperiment:\n",
    "    def __init__(self, prob_heads=0.5):\n",
    "        \"\"\"\n",
    "        Initialize coin toss experiment.\n",
    "        \n",
    "        Args:\n",
    "            prob_heads (float): Probability of heads (default: 0.5 for fair coin)\n",
    "        \"\"\"\n",
    "        self.prob_heads = prob_heads\n",
    "        self.probs = torch.tensor([prob_heads, 1 - prob_heads])\n",
    "        \n",
    "    def basic_toss(self, num_tosses):\n",
    "        \"\"\"\n",
    "        Simulate coin tosses using basic Python random.\n",
    "        \n",
    "        Args:\n",
    "            num_tosses (int): Number of tosses to simulate\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (heads count, tails count)\n",
    "        \"\"\"\n",
    "        heads = sum([random.random() > (1 - self.prob_heads) for _ in range(num_tosses)])\n",
    "        return heads, num_tosses - heads\n",
    "    \n",
    "    def multinomial_toss(self, num_tosses):\n",
    "        \"\"\"\n",
    "        Simulate coin tosses using PyTorch Multinomial.\n",
    "        \n",
    "        Args:\n",
    "            num_tosses (int): Number of tosses to simulate\n",
    "            \n",
    "        Returns:\n",
    "            tensor: Counts of [heads, tails]\n",
    "        \"\"\"\n",
    "        return Multinomial(num_tosses, self.probs).sample()\n",
    "    \n",
    "    def estimate_probability(self, num_tosses):\n",
    "        \"\"\"\n",
    "        Estimate probabilities from tosses.\n",
    "        \n",
    "        Args:\n",
    "            num_tosses (int): Number of tosses to simulate\n",
    "            \n",
    "        Returns:\n",
    "            tensor: Estimated probabilities [P(heads), P(tails)]\n",
    "        \"\"\"\n",
    "        counts = self.multinomial_toss(num_tosses)\n",
    "        return counts / num_tosses\n",
    "    \n",
    "    def convergence_experiment(self, max_tosses, num_experiments=1):\n",
    "        \"\"\"\n",
    "        Run convergence experiment.\n",
    "        \n",
    "        Args:\n",
    "            max_tosses (int): Maximum number of tosses per experiment\n",
    "            num_experiments (int): Number of experiments to run\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: Array of estimates over time for each experiment\n",
    "        \"\"\"\n",
    "        all_estimates = []\n",
    "        \n",
    "        for _ in range(num_experiments):\n",
    "            counts = Multinomial(1, self.probs).sample((max_tosses,))\n",
    "            cum_counts = counts.cumsum(dim=0)\n",
    "            estimates = cum_counts / cum_counts.sum(dim=1, keepdims=True)\n",
    "            all_estimates.append(estimates.numpy())\n",
    "            \n",
    "        return np.array(all_estimates)\n",
    "    \n",
    "    def plot_convergence(self, max_tosses=10000, num_experiments=1):\n",
    "        \"\"\"\n",
    "        Plot probability convergence over multiple tosses.\n",
    "        \n",
    "        Args:\n",
    "            max_tosses (int): Maximum number of tosses\n",
    "            num_experiments (int): Number of experiments to plot\n",
    "        \"\"\"\n",
    "        estimates = self.convergence_experiment(max_tosses, num_experiments)\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        for i in range(num_experiments):\n",
    "            plt.plot(estimates[i, :, 0], alpha=0.5, label=f'Experiment {i+1} - P(heads)')\n",
    "        \n",
    "        plt.axhline(y=self.prob_heads, color='black', linestyle='dashed', label='True P(heads)')\n",
    "        plt.xlabel('Number of Tosses')\n",
    "        plt.ylabel('Estimated Probability')\n",
    "        plt.title('Convergence of Probability Estimates')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    \n",
    "    def error_analysis(self, max_tosses=10000, num_experiments=100):\n",
    "        \"\"\"\n",
    "        Analyze estimation error vs sample size.\n",
    "        \n",
    "        Args:\n",
    "            max_tosses (int): Maximum number of tosses\n",
    "            num_experiments (int): Number of experiments for error analysis\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (sample_sizes, mean_errors, std_errors)\n",
    "        \"\"\"\n",
    "        sample_sizes = np.logspace(1, np.log10(max_tosses), 50).astype(int)\n",
    "        errors = np.zeros((len(sample_sizes), num_experiments))\n",
    "        \n",
    "        for i, n in enumerate(sample_sizes):\n",
    "            for j in range(num_experiments):\n",
    "                estimate = self.estimate_probability(n)[0].item()\n",
    "                errors[i, j] = abs(estimate - self.prob_heads)\n",
    "                \n",
    "        mean_errors = errors.mean(axis=1)\n",
    "        std_errors = errors.std(axis=1)\n",
    "        \n",
    "        return sample_sizes, mean_errors, std_errors\n",
    "\n",
    "# Example usage\n",
    "def run_demonstration():\n",
    "    # Create experiment instance\n",
    "    experiment = CoinTossExperiment(prob_heads=0.5)\n",
    "    \n",
    "    # Basic tosses\n",
    "    print(\"Basic coin toss (100 tosses):\")\n",
    "    heads, tails = experiment.basic_toss(100)\n",
    "    print(f\"Heads: {heads}, Tails: {tails}\")\n",
    "    \n",
    "    # Multinomial tosses\n",
    "    print(\"\\nMultinomial toss (100 tosses):\")\n",
    "    counts = experiment.multinomial_toss(100)\n",
    "    print(f\"Counts: {counts}\")\n",
    "    \n",
    "    # Probability estimation\n",
    "    print(\"\\nProbability estimation (1000 tosses):\")\n",
    "    probs = experiment.estimate_probability(1000)\n",
    "    print(f\"P(heads): {probs[0]:.3f}, P(tails): {probs[1]:.3f}\")\n",
    "    \n",
    "    # Plot convergence\n",
    "    print(\"\\nPlotting convergence for 3 experiments...\")\n",
    "    experiment.plot_convergence(max_tosses=10000, num_experiments=3)\n",
    "    \n",
    "    # Error analysis\n",
    "    print(\"\\nPerforming error analysis...\")\n",
    "    sizes, means, stds = experiment.error_analysis()\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.loglog(sizes, means, label='Mean Error')\n",
    "    plt.fill_between(sizes, means-stds, means+stds, alpha=0.3, label='±1 std')\n",
    "    plt.loglog(sizes, 1/np.sqrt(sizes), '--', label='1/√n reference')\n",
    "    plt.xlabel('Sample Size')\n",
    "    plt.ylabel('Absolute Error')\n",
    "    plt.title('Error Analysis')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_demonstration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d2cf82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating coin toss experiment (fair coin)...\n",
      "\n",
      "1. Basic coin toss experiment (100 tosses):\n",
      "Heads: 54, Tails: 46\n",
      "Proportions - Heads: 0.540, Tails: 0.460\n",
      "\n",
      "2. Probability estimation (1000 tosses):\n",
      "Estimated P(heads): 0.499, P(tails): 0.501\n",
      "\n",
      "3. Convergence experiment (10000 tosses, showing first 5 and last 5 estimates):\n",
      "Tosses      P(heads)    \n",
      "------------------------\n",
      "100         0.5         \n",
      "200         0.525       \n",
      "300         0.506667    \n",
      "400         0.5         \n",
      "500         0.508       \n",
      "...\n",
      "Tosses      P(heads)    \n",
      "------------------------\n",
      "9600        0.496354    \n",
      "9700        0.496289    \n",
      "9800        0.496531    \n",
      "9900        0.497071    \n",
      "10000       0.4961      \n",
      "\n",
      "4. Error analysis:\n",
      "Sample Size Mean Error  Std Error   \n",
      "------------------------------------\n",
      "10          0.137       0.103591    \n",
      "100         0.0382      0.030377    \n",
      "1000        0.0143      0.010217    \n",
      "10000       0.003923    0.003157    \n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "class CoinTossExperiment:\n",
    "    def __init__(self, prob_heads=0.5):\n",
    "        \"\"\"\n",
    "        Initialize coin toss experiment.\n",
    "        \n",
    "        Args:\n",
    "            prob_heads (float): Probability of heads (default: 0.5 for fair coin)\n",
    "        \"\"\"\n",
    "        if not 0 <= prob_heads <= 1:\n",
    "            raise ValueError(\"Probability must be between 0 and 1\")\n",
    "        self.prob_heads = prob_heads\n",
    "        \n",
    "    def toss(self, num_tosses):\n",
    "        \"\"\"\n",
    "        Simulate coin tosses.\n",
    "        \n",
    "        Args:\n",
    "            num_tosses (int): Number of tosses to simulate\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (heads count, tails count)\n",
    "        \"\"\"\n",
    "        heads = sum(random.random() < self.prob_heads for _ in range(num_tosses))\n",
    "        return heads, num_tosses - heads\n",
    "    \n",
    "    def estimate_probability(self, num_tosses):\n",
    "        \"\"\"\n",
    "        Estimate probabilities from tosses.\n",
    "        \n",
    "        Args:\n",
    "            num_tosses (int): Number of tosses to simulate\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (P(heads), P(tails))\n",
    "        \"\"\"\n",
    "        heads, tails = self.toss(num_tosses)\n",
    "        return heads/num_tosses, tails/num_tosses\n",
    "    \n",
    "    def run_convergence_experiment(self, max_tosses):\n",
    "        \"\"\"\n",
    "        Run experiment to show convergence to true probability.\n",
    "        \n",
    "        Args:\n",
    "            max_tosses (int): Maximum number of tosses\n",
    "            \n",
    "        Returns:\n",
    "            list: List of (num_tosses, p_heads) pairs\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        heads_count = 0\n",
    "        \n",
    "        for i in range(1, max_tosses + 1):\n",
    "            # Single toss\n",
    "            if random.random() < self.prob_heads:\n",
    "                heads_count += 1\n",
    "            \n",
    "            # Record current probability estimate\n",
    "            if i % (max_tosses // 100) == 0:  # Record ~100 data points\n",
    "                results.append((i, heads_count/i))\n",
    "                \n",
    "        return results\n",
    "    \n",
    "    def error_analysis(self, sample_sizes, num_experiments=100):\n",
    "        \"\"\"\n",
    "        Analyze estimation error vs sample size.\n",
    "        \n",
    "        Args:\n",
    "            sample_sizes (list): List of sample sizes to test\n",
    "            num_experiments (int): Number of experiments per sample size\n",
    "            \n",
    "        Returns:\n",
    "            list: List of (sample_size, mean_error, std_error) tuples\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for n in sample_sizes:\n",
    "            errors = []\n",
    "            for _ in range(num_experiments):\n",
    "                p_heads, _ = self.estimate_probability(n)\n",
    "                errors.append(abs(p_heads - self.prob_heads))\n",
    "            \n",
    "            # Calculate mean and standard deviation\n",
    "            mean_error = sum(errors) / len(errors)\n",
    "            variance = sum((x - mean_error) ** 2 for x in errors) / len(errors)\n",
    "            std_error = math.sqrt(variance)\n",
    "            \n",
    "            results.append((n, mean_error, std_error))\n",
    "            \n",
    "        return results\n",
    "\n",
    "def print_table(data, headers, col_width=12):\n",
    "    \"\"\"Helper function to print formatted tables.\"\"\"\n",
    "    # Print headers\n",
    "    header_row = \"\".join(str(h).ljust(col_width) for h in headers)\n",
    "    print(header_row)\n",
    "    print(\"-\" * len(header_row))\n",
    "    \n",
    "    # Print data\n",
    "    for row in data:\n",
    "        print(\"\".join(f\"{str(round(x, 6)).ljust(col_width)}\" for x in row))\n",
    "\n",
    "def run_demonstration():\n",
    "    # Create experiment\n",
    "    print(\"Creating coin toss experiment (fair coin)...\")\n",
    "    experiment = CoinTossExperiment()\n",
    "    \n",
    "    # Basic tosses\n",
    "    print(\"\\n1. Basic coin toss experiment (100 tosses):\")\n",
    "    heads, tails = experiment.toss(100)\n",
    "    print(f\"Heads: {heads}, Tails: {tails}\")\n",
    "    print(f\"Proportions - Heads: {heads/100:.3f}, Tails: {tails/100:.3f}\")\n",
    "    \n",
    "    # Probability estimation\n",
    "    print(\"\\n2. Probability estimation (1000 tosses):\")\n",
    "    p_heads, p_tails = experiment.estimate_probability(1000)\n",
    "    print(f\"Estimated P(heads): {p_heads:.3f}, P(tails): {p_tails:.3f}\")\n",
    "    \n",
    "    # Convergence experiment\n",
    "    print(\"\\n3. Convergence experiment (10000 tosses, showing first 5 and last 5 estimates):\")\n",
    "    convergence_results = experiment.run_convergence_experiment(10000)\n",
    "    print_table(convergence_results[:5], [\"Tosses\", \"P(heads)\"])\n",
    "    print(\"...\")\n",
    "    print_table(convergence_results[-5:], [\"Tosses\", \"P(heads)\"])\n",
    "    \n",
    "    # Error analysis\n",
    "    print(\"\\n4. Error analysis:\")\n",
    "    sample_sizes = [10, 100, 1000, 10000]\n",
    "    error_results = experiment.error_analysis(sample_sizes)\n",
    "    print_table(error_results, [\"Sample Size\", \"Mean Error\", \"Std Error\"])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_demonstration()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea049c8c",
   "metadata": {},
   "source": [
    "## 2.6.2 A More Formal Treatment of Probability Theory\n",
    "\n",
    "### Sample Space and Events\n",
    "\n",
    "A **sample space** (outcome space) $S$ contains all possible outcomes of a random experiment:\n",
    "\n",
    "- Single coin flip: $S = \\{\\text{heads}, \\text{tails}\\}$\n",
    "- Single die roll: $S = \\{1, 2, 3, 4, 5, 6\\}$\n",
    "- Two coin flips: $S = \\{(\\text{heads}, \\text{heads}), (\\text{heads}, \\text{tails}), (\\text{tails}, \\text{heads}), (\\text{tails}, \\text{tails})\\}$\n",
    "\n",
    "**Events** are subsets of the sample space. For example:\n",
    "- For two coin flips, \"first coin is heads\" = $\\{(\\text{heads}, \\text{heads}), (\\text{heads}, \\text{tails})\\}$\n",
    "- For a die roll:\n",
    "  - Event A: \"seeing a 5\" = $\\{5\\}$\n",
    "  - Event B: \"seeing an odd number\" = $\\{1, 3, 5\\}$\n",
    "\n",
    "### Probability Functions\n",
    "\n",
    "A probability function maps events to real values:\n",
    "\n",
    "$P: A \\subseteq S \\rightarrow [0, 1]$\n",
    "\n",
    "#### Kolmogorov's Axioms (1933):\n",
    "\n",
    "1. Non-negativity: For any event $A$, $P(A) \\geq 0$\n",
    "\n",
    "2. Normalization: $P(S) = 1$\n",
    "\n",
    "3. Additivity: For mutually exclusive events $A_1, A_2, \\ldots$ (where $A_i \\cap A_j = \\emptyset$ for $i \\neq j$):\n",
    "\n",
    "   $P(\\bigcup_{i=1}^{\\infty} A_i) = \\sum_{i=1}^{\\infty} P(A_i)$\n",
    "\n",
    "#### Important Consequences:\n",
    "\n",
    "1. Complement Rule: For any event $A$:\n",
    "   \n",
    "   $P(A) + P(A') = 1$\n",
    "\n",
    "2. Empty Set Probability:\n",
    "   \n",
    "   $P(\\emptyset) = 0$\n",
    "\n",
    "3. Mutual Exclusivity:\n",
    "   \n",
    "   $P(A \\cap A') = 0$\n",
    "\n",
    "## 2.6.3 Random Variables\n",
    "\n",
    "Random variables are mappings from a sample space to a set of values. They offer several advantages:\n",
    "\n",
    "1. **Abstraction**: Can be coarser than raw sample space\n",
    "   - Example: Binary variable \"greater than 0.5\" on $[0,1]$\n",
    "\n",
    "2. **Multiple Perspectives**: Different random variables can share same sample space\n",
    "   - Example: Home security scenario\n",
    "     - $X$: \"alarm goes off\"\n",
    "     - $Y$: \"house burglarized\"\n",
    "\n",
    "### Probability Notation\n",
    "\n",
    "For a random variable $X$:\n",
    "- Event notation: $P(X = v)$\n",
    "- Distribution notation: $P(X)$\n",
    "- Joint distribution: $P(X,Y) = P(X)P(Y)$\n",
    "- Range probability: $P(1 \\leq X \\leq 3)$\n",
    "\n",
    "### Types of Random Variables\n",
    "\n",
    "1. **Discrete Random Variables**:\n",
    "   - Finite or countable outcomes\n",
    "   - Examples: coin flips, die rolls\n",
    "\n",
    "2. **Continuous Random Variables**:\n",
    "   - Uncountable outcomes\n",
    "   - Examples: height, weight\n",
    "   - Exact values rarely matter\n",
    "   - Typically work with ranges or intervals\n",
    "\n",
    "$\\blacksquare$\n",
    "\n",
    "## 2.6.4 Continuous Random Variables and Joint Distributions\n",
    "\n",
    "### Continuous Random Variables\n",
    "\n",
    "For continuous quantities like height:\n",
    "- Exact values have zero probability but nonzero density\n",
    "- We work with intervals: $P(1.79 \\leq \\text{height} \\leq 1.81)$\n",
    "- Probability requires integration over density function:\n",
    "\n",
    "$P(a \\leq X \\leq b) = \\int_a^b p(x)dx$\n",
    "\n",
    "### Multiple Random Variables\n",
    "\n",
    "Random variables in machine learning often represent:\n",
    "- Customer attributes\n",
    "- Image features\n",
    "- Biological measurements\n",
    "- Population characteristics\n",
    "\n",
    "#### Joint Probabilities\n",
    "\n",
    "For random variables $A$ and $B$:\n",
    "- Joint probability: $P(A=a, B=b)$\n",
    "- Properties:\n",
    "  - $P(A=a, B=b) \\leq P(A=a)$\n",
    "  - $P(A=a, B=b) \\leq P(B=b)$\n",
    "\n",
    "#### Marginalization\n",
    "\n",
    "To recover individual distributions:\n",
    "\n",
    "$P(A=a) = \\sum_v P(A=a, B=v)$\n",
    "\n",
    "### Conditional Probability\n",
    "\n",
    "The conditional probability is defined as:\n",
    "\n",
    "$P(B=b|A=a) = \\frac{P(A=a, B=b)}{P(A=a)}$\n",
    "\n",
    "Properties:\n",
    "- It is a valid probability measure\n",
    "- For disjoint events $B$ and $B'$:\n",
    "  $P(B \\cup B'|A=a) = P(B|A=a) + P(B'|A=a)$\n",
    "\n",
    "### Bayes' Theorem\n",
    "\n",
    "Fundamental form:\n",
    "\n",
    "$P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$\n",
    "\n",
    "Alternative forms:\n",
    "1. Proportional form:\n",
    "   $P(A|B) \\propto P(B|A)P(A)$\n",
    "\n",
    "2. Normalized form:\n",
    "   $P(A|B) = \\frac{P(B|A)P(A)}{\\sum_a P(B|A=a)P(A=a)}$\n",
    "\n",
    "#### Bayesian Interpretation:\n",
    "- Prior: $P(H)$ (initial beliefs)\n",
    "- Likelihood: $P(E|H)$ (evidence given hypothesis)\n",
    "- Posterior: $P(H|E)$ (updated beliefs)\n",
    "- Relationship: $\\text{posterior} = \\frac{\\text{likelihood} \\times \\text{prior}}{\\text{evidence}}$\n",
    "\n",
    "### Marginalization\n",
    "\n",
    "The law of total probability:\n",
    "\n",
    "$P(B) = \\sum_a P(B|A=a)P(A=a) = \\sum_a P(B,A=a)$\n",
    "\n",
    "### Independence\n",
    "\n",
    "Two variables $A$ and $B$ are independent ($A \\perp B$) if:\n",
    "- $P(A|B) = P(A)$\n",
    "- Equivalently: $P(A,B) = P(A)P(B)$\n",
    "\n",
    "This concept is crucial in:\n",
    "- Feature selection\n",
    "- Model simplification\n",
    "- Statistical testing\n",
    "\n",
    "$\\blacksquare$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328b075e",
   "metadata": {},
   "source": [
    "```markdown\n",
    "# Conditional Probability and Independence\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Conditional probability is a fundamental concept in probability theory. It allows us to calculate the probability of an event occurring given that another event has already occurred. Conditional probability is denoted as $P(A|B)$, which represents the probability of event $A$ occurring given that event $B$ has occurred.\n",
    "\n",
    "## Independence\n",
    "\n",
    "Two random variables $A$ and $B$ are said to be independent if the occurrence of one event does not affect the probability of the other event. In other words, knowing the outcome of one random variable does not provide any additional information about the outcome of the other random variable.\n",
    "\n",
    "Mathematically, two random variables $A$ and $B$ are independent if and only if:\n",
    "\n",
    "$$\n",
    "P(A \\cap B) = P(A) \\cdot P(B)\n",
    "$$\n",
    "\n",
    "## Conditional Independence\n",
    "\n",
    "Conditional independence is a concept that extends independence to conditional probabilities. Two random variables $A$ and $B$ are conditionally independent given a third variable $C$ if and only if:\n",
    "\n",
    "$$\n",
    "P(A, B | C) = P(A | C) \\cdot P(B | C)\n",
    "$$\n",
    "\n",
    "This means that knowing the value of $C$ does not provide any additional information about the relationship between $A$ and $B$.\n",
    "\n",
    "## Example: Disease Test\n",
    "\n",
    "Let's consider an example involving an Disease test. We have two random variables: $D_1$ indicating the diagnosis (0 for negative, 1 for positive) and $H$ indicating the Disease status (0 for healthy, 1 for Disease positive).\n",
    "\n",
    "Conditional Probability | H=1 | H=0\n",
    "--- | --- | ---\n",
    "D1=1 | 1 | 0.01\n",
    "D1=0 | 0 | 0.99\n",
    "\n",
    "We want to calculate the probability of the patient having Disease given a positive test result, $P(H=1 | D_1=1)$. Using Bayes' theorem and marginalization, we can compute:\n",
    "\n",
    "$$\n",
    "P(H=1 | D_1=1) = \\frac{P(D_1=1 | H=1) \\cdot P(H=1)}{P(D_1=1)} = \\frac{0.98 \\cdot 0.0015}{0.011485} = 0.1306\n",
    "$$\n",
    "\n",
    "So, there is only a 13.06% chance that the patient actually has HIV, despite using a very accurate test.\n",
    "\n",
    "## Second Test\n",
    "\n",
    "Now, let's consider a second HIV test with different characteristics.\n",
    "\n",
    "Conditional Probability | H=1 | H=0\n",
    "--- | --- | ---\n",
    "D2=1 | 0.98 | 0.03\n",
    "D2=0 | 0.02 | 0.97\n",
    "\n",
    "If the second test also comes back positive, we can calculate the probability of the patient having Disease given both positive test results using conditional independence:\n",
    "\n",
    "$$\n",
    "P(H=1 | D_1=1, D_2=1) = \\frac{P(D_1=1, D_2=1 | H=1) \\cdot P(H=1)}{P(D_1=1, D_2=1)} = \\frac{0.98 \\cdot 0.0015}{0.00176955} = 0.8307\n",
    "$$\n",
    "\n",
    "So, the second test significantly improves our estimate, increasing our confidence that the patient has Disease.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Conditional probability and independence are essential concepts in probability theory. They allow us to reason about the relationships between events and make informed decisions based on available information. The example of the Disease test demonstrates how conditional probability and independence can be applied in real-world scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ba24b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Conditional probabilities\n",
    "P_D1_1_H_1 = 1\n",
    "P_D1_1_H_0 = 0.01\n",
    "P_D1_0_H_0 = 0.99\n",
    "P_D1_0_H_1 = 0\n",
    "\n",
    "P_D2_1_H_1 = 0.98\n",
    "P_D2_1_H_0 = 0.03\n",
    "P_D2_0_H_0 = 0.02\n",
    "P_D2_0_H_1 = 0.97\n",
    "\n",
    "# Population probability of HIV\n",
    "P_H_1 = 0.0015\n",
    "\n",
    "# Calculate P(D1 = 1)\n",
    "P_D1_1 = P_D1_1_H_0 * P_H_0 + P_D1_1_H_1 * P_H_1\n",
    "\n",
    "# Calculate P(H = 1 | D1 = 1) using Bayes' theorem\n",
    "P_H_1_given_D1_1 = (P_D1_1_H_1 * P_H_1) / P_D1_1\n",
    "\n",
    "print(f\"Probability of HIV given positive test result (D1 = 1): {P_H_1_given_D1_1:.4f}\")\n",
    "\n",
    "# Calculate P(D1 = 1, D2 = 1) assuming conditional independence\n",
    "P_D1_1_D2_1_H_0 = P_D1_1_H_0 * P_D2_1_H_0\n",
    "P_D1_1_D2_1_H_1 = P_D1_1_H_1 * P_D2_1_H_1\n",
    "P_D1_1_D2_1 = P_D1_1_D2_1_H_0 * P_H_0 + P_D1_1_D2_1_H_1 * P_H_1\n",
    "\n",
    "# Calculate P(H = 1 | D1 = 1, D2 = 1) using Bayes' theorem\n",
    "P_H_1_given_D1_1_D2_1 = (P_D1_1_D2_1_H_1 * P_H_1) / P_D1_1_D2_1\n",
    "\n",
    "print(f\"Probability of HIV given both tests positive (D1 = 1, D2 = 1): {P_H_1_given_D1_1_D2_1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04efd0b",
   "metadata": {},
   "source": [
    "```markdown\n",
    "# Expectations and Variance\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Expectations and variance are important concepts in probability theory that help us aggregate and analyze the outcomes of random variables. Expectations provide information about the average value we can expect from a random variable, while variance measures how much the actual values deviate from the expected value.\n",
    "\n",
    "## Expectations\n",
    "\n",
    "The expectation (or average) of a random variable $X$ is defined as the sum of all possible values of $X$ multiplied by their corresponding probabilities:\n",
    "\n",
    "$$\n",
    "E[X] = \\sum_x xP(X = x)\n",
    "$$\n",
    "\n",
    "For continuous random variables with a probability density function $p(x)$, the expectation is calculated as:\n",
    "\n",
    "$$\n",
    "E[X] = \\int x p(x) dx\n",
    "$$\n",
    "\n",
    "Expectations can also be calculated for functions of random variables:\n",
    "\n",
    "$$\n",
    "E[f(X)] = \\sum_x f(x)P(X = x) \\text{ for discrete probabilities}\n",
    "$$\n",
    "\n",
    "$$\n",
    "E[f(X)] = \\int f(x)p(x) dx \\text{ for continuous densities}\n",
    "$$\n",
    "\n",
    "## Variance\n",
    "\n",
    "The variance of a random variable $X$ measures how much the actual values deviate from the expected value. It is calculated as the expected value of the squared deviations:\n",
    "\n",
    "$$\n",
    "Var[X] = E[(X - E[X])^2] = E[X^2] - E[X]^2\n",
    "$$\n",
    "\n",
    "The square root of the variance is known as the standard deviation, which has the same units as the original random variable.\n",
    "\n",
    "## Variance of a Function of a Random Variable\n",
    "\n",
    "The variance of a function $f(X)$ of a random variable $X$ is defined analogously:\n",
    "\n",
    "$$\n",
    "Var[f(X)] = E[f(X)^2] - E[f(X)]^2\n",
    "$$\n",
    "\n",
    "## Investment Example\n",
    "\n",
    "Let's consider an investment example to illustrate expectations and variance. Suppose we have the following conditional probabilities for different investment outcomes:\n",
    "\n",
    "- With 50% probability, the investment might fail altogether.\n",
    "- With 40% probability, it might provide a 2x return.\n",
    "- With 10% probability, it might provide a 10x return.\n",
    "\n",
    "## Expected Return\n",
    "\n",
    "To calculate the expected return, we sum over all returns, multiplying each by the probability that they will occur:\n",
    "\n",
    "$$\n",
    "\\text{Expected Return} = 0.5 \\cdot 0 + 0.4 \\cdot 2 + 0.1 \\cdot 10 = 1.8\n",
    "$$\n",
    "\n",
    "So, the expected return is 1.8x.\n",
    "\n",
    "## Expected Happiness\n",
    "\n",
    "If we consider a utility function that associates greater disutility with losing money and sub-linear utility with increasing amounts of money, we can calculate the expected happiness of investing. Let's assume the following utilities:\n",
    "\n",
    "- Utility associated with a total loss: -1\n",
    "- Utilities associated with returns of 1, 2, and 10: 1, 2, and 4, respectively\n",
    "\n",
    "The expected happiness can be calculated as:\n",
    "\n",
    "$$\n",
    "\\text{Expected Happiness} = 0.5 \\cdot (-1) + 0.4 \\cdot 2 + 0.1 \\cdot 4 = 0.7\n",
    "$$\n",
    "\n",
    "So, the expected loss of utility is 30%.\n",
    "\n",
    "## Variance of the Investment\n",
    "\n",
    "To calculate the variance of the investment, we can use the formula for variance:\n",
    "\n",
    "$$\n",
    "\\text{Variance} = E[(X - E[X])^2]\n",
    "$$\n",
    "\n",
    "Plugging in the values, we get:\n",
    "\n",
    "$$\n",
    "\\text{Variance} = 0.5 \\cdot 0 + 0.4 \\cdot 2^2 + 0.1 \\cdot 10^2 - 1.8^2 = 8.36\n",
    "$$\n",
    "\n",
    "So, the investment has a variance of 8.36, indicating a high level of risk.\n",
    "\n",
    "## Vector-Valued Random Variables\n",
    "\n",
    "Expectations and variance can also be defined for vector-valued random variables. Expectations can be applied element-wise, while covariance is calculated using the outer product of the difference between the random variable and its mean.\n",
    "\n",
    "The covariance matrix $\\Sigma$ is defined as:\n",
    "\n",
    "$$\n",
    "\\Sigma = Cov[x] = E[(x - \\mu)(x - \\mu)^T]\n",
    "$$\n",
    "\n",
    "The covariance matrix allows us to compute the variance for any linear function of $x$ by a simple matrix multiplication. The off-diagonal elements of the covariance matrix indicate the correlation between different coordinates of the random variable.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Expectations and variance are powerful tools for analyzing and understanding the behavior of random variables. They provide insights into the average value, variability, and correlation of outcomes, which are essential for making informed decisions in various fields, including finance, economics, and machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdf8454a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected return: 1.80x\n",
      "Expected happiness: 0.70\n",
      "Variance of the investment: 8.36\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Conditional probabilities\n",
    "P_fail = 0.5\n",
    "P_2x_return = 0.4\n",
    "P_10x_return = 0.1\n",
    "\n",
    "# Expected return\n",
    "expected_return = P_fail * 0 + P_2x_return * 2 + P_10x_return * 10\n",
    "print(f\"Expected return: {expected_return:.2f}x\")\n",
    "\n",
    "# Utilities\n",
    "utility_loss = -1\n",
    "utility_1x = 1\n",
    "utility_2x = 2\n",
    "utility_10x = 4\n",
    "\n",
    "# Expected happiness\n",
    "expected_happiness = P_fail * utility_loss + P_2x_return * utility_2x + P_10x_return * utility_10x\n",
    "print(f\"Expected happiness: {expected_happiness:.2f}\")\n",
    "\n",
    "# Variance of the investment\n",
    "variance = (P_fail * 0**2 + P_2x_return * 2**2 + P_10x_return * 10**2) - expected_return**2\n",
    "print(f\"Variance of the investment: {variance:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e445715",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
