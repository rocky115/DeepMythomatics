{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcdfc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " * Copyright (c) 2018 Radhamadhab Dalai\n",
    " *\n",
    " * Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    " * of this software and associated documentation files (the \"Software\"), to deal\n",
    " * in the Software without restriction, including without limitation the rights\n",
    " * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    " * copies of the Software, and to permit persons to whom the Software is\n",
    " * furnished to do so, subject to the following conditions:\n",
    " *\n",
    " * The above copyright notice and this permission notice shall be included in\n",
    " * all copies or substantial portions of the Software.\n",
    " *\n",
    " * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    " * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    " * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    " * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    " * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    " * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
    " * THE SOFTWARE.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c368311a",
   "metadata": {},
   "source": [
    "# Gaussian Process for Regression and Classification\n",
    "\n",
    "In this section, we discuss Gaussian process methods for regression and classification problems.\n",
    "\n",
    "## 8.6 Gaussian Process for Regression and Classification\n",
    "\n",
    "### 8.6.1 Joint, Marginal, and Conditional Probabilities\n",
    "\n",
    "Let \\( y_1, \\ldots, y_n \\) be \\( n \\) random variables with joint probability \\( p(y_1, \\ldots, y_n) \\) or \\( p(y) \\) for short. We partition the variables into two groups, \\( y_A \\) and \\( y_B \\), where \\( A \\cup B = \\{1, \\ldots, n\\} \\) and \\( A \\cap B = \\emptyset \\), so that \\( p(y) = p(y_A, y_B) \\). The marginal probability of \\( y_A \\) is given by:\n",
    "\n",
    "$$\n",
    "p(y_A) = \\int p(y_A, y_B) \\, dy_B\n",
    "$$\n",
    "\n",
    "For discrete variables, the integral is replaced by a sum. If the joint distribution is equal to the product of the marginals, then the variables are said to be independent; otherwise, they are dependent.\n",
    "\n",
    "The conditional probability function is defined as:\n",
    "\n",
    "$$\n",
    "p(y_A \\mid y_B) = \\frac{p(y_A, y_B)}{p(y_B)}\n",
    "$$\n",
    "\n",
    "Bayes' theorem relates these probabilities:\n",
    "\n",
    "$$\n",
    "p(y_A \\mid y_B) = \\frac{p(y_B \\mid y_A) \\, p(y_A)}{p(y_B)}\n",
    "$$\n",
    "\n",
    "### 8.6.2 Gaussian Process\n",
    "\n",
    "A Gaussian Process (GP) generalizes the multivariate Gaussian distribution to functions. \n",
    "\n",
    "#### Multivariate Gaussian Distribution\n",
    "\n",
    "For a multivariate Gaussian distribution, the joint probability density is given by:\n",
    "\n",
    "$$\n",
    "p(x \\mid \\mu, \\Sigma) = \\frac{1}{(2 \\pi)^{N/2} \\, |\\Sigma|^{1/2}} \\exp \\left( -\\frac{1}{2} (x - \\mu)^T \\Sigma^{-1} (x - \\mu) \\right)\n",
    "$$\n",
    "\n",
    "where \\( \\mu \\) is the mean vector and \\( \\Sigma \\) is the covariance matrix.\n",
    "\n",
    "#### Gaussian Process Definition\n",
    "\n",
    "A Gaussian Process \\( f(x) \\) is a collection of random variables, any finite number of which have joint Gaussian distributions. Formally, a Gaussian Process is defined as:\n",
    "\n",
    "$$\n",
    "f \\sim \\text{GP}(\\mu, K)\n",
    "$$\n",
    "\n",
    "where \\( \\mu = E[f(x)] \\) is the mean function and \\( K(x, x') \\) is the covariance function.\n",
    "\n",
    "For a vector-valued Gaussian process, we have:\n",
    "\n",
    "$$\n",
    "\\mu(x) = E[f(x)]\n",
    "$$\n",
    "\n",
    "$$\n",
    "K(x_i, x_j) = E\\left[(f(x_i) - \\mu(x_i))^T (f(x_j) - \\mu(x_j))\\right]\n",
    "$$\n",
    "\n",
    "and the Gaussian process is expressed as:\n",
    "\n",
    "$$\n",
    "f(x) \\sim \\text{GP}(\\mu, \\Sigma)\n",
    "$$\n",
    "\n",
    "where the \\((i, j)\\)-th element of the covariance matrix \\( \\Sigma \\) is:\n",
    "\n",
    "$$\n",
    "\\Sigma_{ij} = K(x_i, x_j)\n",
    "$$\n",
    "\n",
    "The covariance function \\( K(x_i, x_j) \\), also known as the kernel function, plays a crucial role in Gaussian processes, encoding the structure present in the data. According to Mercerâ€™s theorem, the kernel function can be constructed as:\n",
    "\n",
    "$$\n",
    "K(x_i, x_j) = \\phi(x_i)^T \\phi(x_j)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1085f194",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class GaussianProcessRegressor:\n",
    "    def __init__(self, kernel, sigma_n=1e-5):\n",
    "        self.kernel = kernel\n",
    "        self.sigma_n = sigma_n  # Noise variance\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.K = self.kernel(X_train, X_train) + self.sigma_n**2 * np.eye(len(X_train))\n",
    "        self.L = np.linalg.cholesky(self.K)\n",
    "        self.alpha = np.linalg.solve(self.L.T, np.linalg.solve(self.L, y_train))\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        K_s = self.kernel(self.X_train, X_test)\n",
    "        K_ss = self.kernel(X_test, X_test) + 1e-8 * np.eye(len(X_test))\n",
    "        \n",
    "        # Mean\n",
    "        mu_s = K_s.T @ self.alpha\n",
    "        \n",
    "        # Covariance\n",
    "        v = np.linalg.solve(self.L, K_s)\n",
    "        cov_s = K_ss - v.T @ v\n",
    "        \n",
    "        return mu_s, cov_s\n",
    "\n",
    "def rbf_kernel(X1, X2, length_scale=1.0):\n",
    "    sqdist = np.sum(X1**2, 1).reshape(-1, 1) + np.sum(X2**2, 1) - 2 * np.dot(X1, X2.T)\n",
    "    return np.exp(-0.5 / length_scale**2 * sqdist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80699c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "class GaussianProcessClassifier:\n",
    "    def __init__(self, kernel):\n",
    "        self.kernel = kernel\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.n_samples = X_train.shape[0]\n",
    "        self.K = self.kernel(X_train, X_train)\n",
    "        self.K_inv = np.linalg.inv(self.K)\n",
    "        \n",
    "        # Optimization of the log marginal likelihood\n",
    "        result = minimize(self.neg_log_marginal_likelihood, x0=np.zeros(self.n_samples))\n",
    "        self.alpha = result.x\n",
    "\n",
    "    def predict_proba(self, X_test):\n",
    "        K_s = self.kernel(self.X_train, X_test)\n",
    "        f_mean = K_s.T @ (self.alpha * self.y_train)\n",
    "        f_var = self.kernel(X_test, X_test) - K_s.T @ self.K_inv @ K_s\n",
    "        \n",
    "        return expit(f_mean), f_var\n",
    "\n",
    "    def neg_log_marginal_likelihood(self, alpha):\n",
    "        L = np.linalg.cholesky(self.K + 1e-6 * np.eye(self.n_samples))\n",
    "        alpha = np.linalg.solve(L.T, np.linalg.solve(L, alpha * self.y_train))\n",
    "        log_likelihood = -0.5 * self.y_train.T @ alpha - np.sum(np.log(np.diagonal(L))) - 0.5 * self.n_samples * np.log(2 * np.pi)\n",
    "        return -log_likelihood\n",
    "\n",
    "def rbf_kernel(X1, X2, length_scale=1.0):\n",
    "    sqdist = np.sum(X1**2, 1).reshape(-1, 1) + np.sum(X2**2, 1) - 2 * np.dot(X1, X2.T)\n",
    "    return np.exp(-0.5 / length_scale**2 * sqdist)\n",
    "# Generate synthetic data\n",
    "X_train = np.array([[1], [2], [3], [4], [5]])\n",
    "y_train = np.sin(X_train).ravel()\n",
    "\n",
    "# GP Regression\n",
    "gp = GaussianProcessRegressor(kernel=rbf_kernel)\n",
    "gp.fit(X_train, y_train)\n",
    "X_test = np.linspace(0, 6, 100)[:, np.newaxis]\n",
    "mu_s, cov_s = gp.predict(X_test)\n",
    "\n",
    "# GP Classification\n",
    "X_train_cls = np.array([[1], [2], [3], [4], [5]])\n",
    "y_train_cls = np.array([0, 0, 1, 1, 1])  # Binary classification\n",
    "gp_cls = GaussianProcessClassifier(kernel=rbf_kernel)\n",
    "gp_cls.fit(X_train_cls, y_train_cls)\n",
    "X_test_cls = np.linspace(0, 6, 100)[:, np.newaxis]\n",
    "probs, _ = gp_cls.predict_proba(X_test_cls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b0fcbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
