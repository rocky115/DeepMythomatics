{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ca4857",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "'''Copyright (c) 2004- 2024 , Prof. Radhamadhab Dalai Odisha, India\n",
    "Author's email address :  rmdi115@gmail.com'''\n",
    "###################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e1d50f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum value found at x = [ 5.55111512e-17 -5.00000000e-01] , f(x) = 0.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Define your convex objective function\n",
    "def objective_function(x):\n",
    "    return np.dot(x.T, np.dot(A, x)) + np.dot(b.T, x) + c  # Example: f(x) = x'Ax + b'x + c\n",
    "\n",
    "# Define the derivative of your objective function\n",
    "def derivative_function(x):\n",
    "    return 2 * np.dot(A, x) + b  # Example: f'(x) = 2Ax + b\n",
    "\n",
    "# Define the problem parameters\n",
    "A = np.array([[2, 1], [1, 2]])  # Quadratic term matrix\n",
    "b = np.array([1, 2])  # Linear term vector\n",
    "c = 1  # Constant term\n",
    "\n",
    "initial_guess = np.array([0, 0])  # Initial guess for the minimum\n",
    "\n",
    "# Perform optimization using SciPy's minimize function\n",
    "result = minimize(objective_function, initial_guess, jac=derivative_function)\n",
    "\n",
    "# Output the result\n",
    "print(\"Minimum value found at x =\", result.x, \", f(x) =\", result.fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c85cfee4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (2,1) and (2,) not aligned: 1 (dim 1) != 2 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8364/2234605093.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mx_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mx_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprojected_gradient_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_new\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mx_opt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_8364/2234605093.py\u001b[0m in \u001b[0;36mprojected_gradient_update\u001b[0;34m(x, step_size)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprojected_gradient_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mgradient_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mprojected_gradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient_x\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mx_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstep_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mprojected_gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (2,1) and (2,) not aligned: 1 (dim 1) != 2 (dim 0)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import solve\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Define problem parameters\n",
    "Q = np.array([[4, 1], [1, 2]])  # Positive definite matrix\n",
    "b = np.array([1, 2])  # Linear term vector\n",
    "A = np.array([[1, 1]])  # Constraint matrix\n",
    "b_constraint = np.array([1])  # Constraint vector\n",
    "\n",
    "# Define objective function and its gradient\n",
    "def objective_function(x):\n",
    "    return 0.5 * np.dot(x.T, np.dot(Q, x)) + np.dot(b.T, x)\n",
    "\n",
    "def gradient(x):\n",
    "    return np.dot(Q, x) + b\n",
    "\n",
    "# Define projection operator onto feasible set\n",
    "def project(x):\n",
    "    # Solve the equality-constrained least squares problem\n",
    "    x_proj = solve(np.dot(A.T, A), np.dot(A.T, b_constraint))\n",
    "    return x_proj\n",
    "\n",
    "# Projected gradient update\n",
    "def projected_gradient_update(x, step_size):\n",
    "    gradient_x = gradient(x)\n",
    "    projected_gradient = gradient_x - np.dot(A.T, solve(np.dot(A, np.dot(A.T, gradient_x)), np.dot(A, gradient_x)))\n",
    "    x_new = x - step_size * projected_gradient\n",
    "    return x_new\n",
    "\n",
    "# Perform optimization\n",
    "x0 = np.zeros_like(b)  # Initial guess\n",
    "max_iter = 1000  # Maximum number of iterations\n",
    "tolerance = 1e-6  # Tolerance for convergence\n",
    "step_size = 0.1  # Step size for projected gradient update\n",
    "\n",
    "x_opt = x0\n",
    "for _ in range(max_iter):\n",
    "    x_new = projected_gradient_update(x_opt, step_size)\n",
    "    if np.linalg.norm(x_new - x_opt) < tolerance:\n",
    "        break\n",
    "    x_opt = project(x_new)\n",
    "\n",
    "# Output the result\n",
    "print(\"Minimum value found at x =\", x_opt, \", f(x) =\", objective_function(x_opt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93669c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum value found at x = [ 5.55111512e-17 -5.00000000e-01] , f(x) = 0.5\n"
     ]
    }
   ],
   "source": [
    "'''We define a quadratic objective function f(x)=xTAx+bTx+c, where A is a symmetric matrix, b is a vector, and c is a constant.\n",
    "We define the derivative of the objective function f′(x)=2Ax+b.\n",
    "We use the gradient descent algorithm to minimize the objective function.\n",
    "We define problem parameters A, b, and c, the initial guess for the minimum, learning rate, and maximum number of iterations.\n",
    "We perform gradient descent to find the minimum of the convex function.\n",
    "We output the result.'''\n",
    "\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Define your convex objective function\n",
    "def objective_function(x):\n",
    "    return np.dot(x.T, np.dot(A, x)) + np.dot(b.T, x) + c  # Example: f(x) = x'Ax + b'x + c\n",
    "\n",
    "# Define the derivative of your objective function\n",
    "def derivative_function(x):\n",
    "    return 2 * np.dot(A, x) + b  # Example: f'(x) = 2Ax + b\n",
    "\n",
    "# Define the problem parameters\n",
    "A = np.array([[2, 1], [1, 2]])  # Quadratic term matrix\n",
    "b = np.array([1, 2])  # Linear term vector\n",
    "c = 1  # Constant term\n",
    "\n",
    "initial_guess = np.array([0, 0])  # Initial guess for the minimum\n",
    "\n",
    "# Perform optimization using SciPy's minimize function\n",
    "result = minimize(objective_function, initial_guess, jac=derivative_function)\n",
    "\n",
    "# Output the result\n",
    "print(\"Minimum value found at x =\", result.x, \", f(x) =\", result.fun)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afc79962",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (2,1) and (2,) not aligned: 1 (dim 1) != 2 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5465/957874161.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mx_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mx_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprojected_gradient_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_new\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mx_opt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_5465/957874161.py\u001b[0m in \u001b[0;36mprojected_gradient_update\u001b[0;34m(x, step_size)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprojected_gradient_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mgradient_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mprojected_gradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient_x\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mx_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstep_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mprojected_gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (2,1) and (2,) not aligned: 1 (dim 1) != 2 (dim 0)"
     ]
    }
   ],
   "source": [
    "'''minimizef(x)=21​xTQx+bTx\n",
    "subject to Ax=b, where Q is a positive definite matrix, b and x are vectors, and A is a constraint matrix. \n",
    "We'll use the Mahalanobis norm to define the objective function. '''\n",
    "\n",
    "import numpy as np\n",
    "from scipy.linalg import solve\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Define problem parameters\n",
    "Q = np.array([[4, 1], [1, 2]])  # Positive definite matrix\n",
    "b = np.array([1, 2])  # Linear term vector\n",
    "A = np.array([[1, 1]])  # Constraint matrix\n",
    "b_constraint = np.array([1])  # Constraint vector\n",
    "\n",
    "# Define objective function and its gradient\n",
    "def objective_function(x):\n",
    "    return 0.5 * np.dot(x.T, np.dot(Q, x)) + np.dot(b.T, x)\n",
    "\n",
    "def gradient(x):\n",
    "    return np.dot(Q, x) + b\n",
    "\n",
    "# Define projection operator onto feasible set\n",
    "def project(x):\n",
    "    # Solve the equality-constrained least squares problem\n",
    "    x_proj = solve(np.dot(A.T, A), np.dot(A.T, b_constraint))\n",
    "    return x_proj\n",
    "\n",
    "# Projected gradient update\n",
    "def projected_gradient_update(x, step_size):\n",
    "    gradient_x = gradient(x)\n",
    "    projected_gradient = gradient_x - np.dot(A.T, solve(np.dot(A, np.dot(A.T, gradient_x)), np.dot(A, gradient_x)))\n",
    "    x_new = x - step_size * projected_gradient\n",
    "    return x_new\n",
    "\n",
    "# Perform optimization\n",
    "x0 = np.zeros_like(b)  # Initial guess\n",
    "max_iter = 1000  # Maximum number of iterations\n",
    "tolerance = 1e-6  # Tolerance for convergence\n",
    "step_size = 0.1  # Step size for projected gradient update\n",
    "\n",
    "x_opt = x0\n",
    "for _ in range(max_iter):\n",
    "    x_new = projected_gradient_update(x_opt, step_size)\n",
    "    if np.linalg.norm(x_new - x_opt) < tolerance:\n",
    "        break\n",
    "    x_opt = project(x_new)\n",
    "\n",
    "# Output the result\n",
    "print(\"Minimum value found at x =\", x_opt, \", f(x) =\", objective_function(x_opt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61c576b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum value found at x = [-1.50211396e-14 -1.00000000e+00] , f(x) = -1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define problem parameters\n",
    "Q = np.array([[4, 1], [1, 2]], dtype=np.float64)  # Positive definite matrix\n",
    "b = np.array([1, 2], dtype=np.float64)  # Linear term vector\n",
    "\n",
    "# Define the objective function\n",
    "def objective_function(x):\n",
    "    return 0.5 * np.dot(x.T, np.dot(Q, x)) + np.dot(b.T, x)\n",
    "\n",
    "# Define the gradient of the objective function\n",
    "def gradient(x):\n",
    "    return np.dot(Q, x) + b\n",
    "\n",
    "# AdaGrad optimization function\n",
    "def adagrad(x0, learning_rate, epsilon, max_iter):\n",
    "    x = x0.astype(np.float64)  # Ensure x0 is of type float64\n",
    "    gradient_squared_sum = np.zeros_like(x, dtype=np.float64)\n",
    "    \n",
    "    for _ in range(max_iter):\n",
    "        grad = gradient(x)\n",
    "        gradient_squared_sum += grad ** 2\n",
    "        adjusted_grad = grad / (np.sqrt(gradient_squared_sum) + epsilon)\n",
    "        x -= learning_rate * adjusted_grad\n",
    "        \n",
    "    return x\n",
    "\n",
    "# Perform optimization\n",
    "x0 = np.zeros_like(b, dtype=np.float64)  # Initial guess\n",
    "learning_rate = 0.1  # Learning rate\n",
    "epsilon = 1e-8  # Small constant to avoid division by zero\n",
    "max_iter = 1000  # Maximum number of iterations\n",
    "\n",
    "x_opt = adagrad(x0, learning_rate, epsilon, max_iter)\n",
    "\n",
    "# Output the result\n",
    "print(\"Minimum value found at x =\", x_opt, \", f(x) =\", objective_function(x_opt))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7a4eec",
   "metadata": {},
   "source": [
    "Equations for AdaGrad\n",
    "1. Objective Function:\n",
    "\n",
    "$$f(x) = \\frac{1}{2} x^T Q x + b^T x $$\n",
    "\n",
    "2. Gradient of the Objective Function:\n",
    "\n",
    "$$ \\nabla f(x) = Qx + b $$\n",
    "\n",
    "3. AdaGrad Update Rule:\n",
    "\n",
    "$$ x_{t+1} = x_t - \\frac{\\eta}{\\sqrt{G_t + \\epsilon}} \\odot \\nabla f(x_t) $$\n",
    "\n",
    "Where:\n",
    "\n",
    " 1. $$x_t$$  is the parameter vector at iteration t.\n",
    " \n",
    " 2. $$\\etaη$$ is the learning rate.\n",
    " \n",
    " 3. $$\\nabla f(x_t)∇f(x_t)$$ is the gradient of the objective function at x_tx \n",
    "\n",
    "is a diagonal matrix (or vector in implementation) where each element is the sum of the squares of past gradients up to time tt:\n",
    "$$ G_t = \\sum_{i=1}^{t} (\\nabla f(x_i))^2 $$\n",
    " \n",
    "$$\\epsilon$$ is a small constant to avoid division by zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "88c96fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum value found at x = [-2.9637514e-16 -1.0000000e+00] , f(x) = -1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define problem parameters\n",
    "Q = np.array([[4, 1], [1, 2]], dtype=np.float64)  # Positive definite matrix\n",
    "b = np.array([1, 2], dtype=np.float64)  # Linear term vector\n",
    "\n",
    "# Define the objective function\n",
    "def objective_function(x):\n",
    "    return 0.5 * np.dot(x.T, np.dot(Q, x)) + np.dot(b.T, x)\n",
    "\n",
    "# Define the gradient of the objective function\n",
    "def gradient(x):\n",
    "    return np.dot(Q, x) + b\n",
    "\n",
    "# AdaGrad + Adam optimization function\n",
    "def adagrad_adam(x0, learning_rate, beta1, beta2, epsilon, max_iter):\n",
    "    x = x0.astype(np.float64)  # Ensure x0 is of type float64\n",
    "    m = np.zeros_like(x, dtype=np.float64)\n",
    "    v = np.zeros_like(x, dtype=np.float64)\n",
    "    G = np.zeros_like(x, dtype=np.float64)\n",
    "    \n",
    "    for t in range(1, max_iter + 1):\n",
    "        grad = gradient(x)\n",
    "        m = beta1 * m + (1 - beta1) * grad\n",
    "        v = beta2 * v + (1 - beta2) * (grad ** 2)\n",
    "        G += grad ** 2\n",
    "        \n",
    "        m_hat = m / (1 - beta1 ** t)\n",
    "        v_hat = v / (1 - beta2 ** t)\n",
    "        \n",
    "        x -= learning_rate * m_hat / (np.sqrt(G) + epsilon)\n",
    "        \n",
    "    return x\n",
    "\n",
    "# Perform optimization\n",
    "x0 = np.zeros_like(b, dtype=np.float64)  # Initial guess\n",
    "learning_rate = 0.1  # Learning rate\n",
    "beta1 = 0.9  # Exponential decay rate for the first moment estimates\n",
    "beta2 = 0.999  # Exponential decay rate for the second moment estimates\n",
    "epsilon = 1e-8  # Small constant to avoid division by zero\n",
    "max_iter = 1000  # Maximum number of iterations\n",
    "\n",
    "x_opt = adagrad_adam(x0, learning_rate, beta1, beta2, epsilon, max_iter)\n",
    "\n",
    "# Output the result\n",
    "print(\"Minimum value found at x =\", x_opt, \", f(x) =\", objective_function(x_opt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39473b16",
   "metadata": {},
   "source": [
    "### The update rules for Adam involve both the first moment (mean) and the second moment (uncentered variance). The AdaGrad part will contribute to the adaptive learning rate.\n",
    "\n",
    "1. Initialization\n",
    "\n",
    "- $m_0 = 0$ (Initialize the first moment vector)\n",
    "- $v_0 = 0$ (Initialize the second moment vector)\n",
    "- $G_0 = 0$ (Initialize the gradient sum of squares)\n",
    "\n",
    "2. Update Rules:\n",
    "\n",
    "- Compute the gradient:  $ g_t = \\nabla f(x_t)g t$\n",
    "\n",
    "- Update the first moment estimate: $ m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_tm $\n",
    "\n",
    "- Update the second moment estimate: $ v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t $\n",
    "- AdaGrad-like cumulative gradient sum: $G_t = G_{t-1} + g_t^2$\n",
    "- Bias-corrected first moment estimate: $\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}$ \n",
    "- Bias-corrected second moment estimate: $\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t} $\n",
    "- Update the parameters:$x_{t+1} = x_t - \\frac{\\eta}{\\sqrt{G_t + \\epsilon}} \\hat{m}_tx$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "406c9fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum value found at x = [-2.9637514e-16 -1.0000000e+00] , f(x) = -1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define problem parameters\n",
    "Q = np.array([[4, 1], [1, 2]], dtype=np.float64)  # Positive definite matrix\n",
    "b = np.array([1, 2], dtype=np.float64)  # Linear term vector\n",
    "\n",
    "# Define the objective function\n",
    "def objective_function(x):\n",
    "    return 0.5 * np.dot(x.T, np.dot(Q, x)) + np.dot(b.T, x)\n",
    "\n",
    "# Define the gradient of the objective function\n",
    "def gradient(x):\n",
    "    return np.dot(Q, x) + b\n",
    "\n",
    "# AdaGrad + Adam optimization function\n",
    "def adagrad_adam(x0, learning_rate, beta1, beta2, epsilon, max_iter):\n",
    "    x = x0.astype(np.float64)  # Ensure x0 is of type float64\n",
    "    m = np.zeros_like(x, dtype=np.float64)\n",
    "    v = np.zeros_like(x, dtype=np.float64)\n",
    "    G = np.zeros_like(x, dtype=np.float64)\n",
    "    \n",
    "    for t in range(1, max_iter + 1):\n",
    "        grad = gradient(x)\n",
    "        m = beta1 * m + (1 - beta1) * grad\n",
    "        v = beta2 * v + (1 - beta2) * (grad ** 2)\n",
    "        G += grad ** 2\n",
    "        \n",
    "        m_hat = m / (1 - beta1 ** t)\n",
    "        v_hat = v / (1 - beta2 ** t)\n",
    "        \n",
    "        x -= learning_rate * m_hat / (np.sqrt(G) + epsilon)\n",
    "        \n",
    "    return x\n",
    "\n",
    "# Perform optimization\n",
    "x0 = np.zeros_like(b, dtype=np.float64)  # Initial guess\n",
    "learning_rate = 0.1  # Learning rate\n",
    "beta1 = 0.9  # Exponential decay rate for the first moment estimates\n",
    "beta2 = 0.999  # Exponential decay rate for the second moment estimates\n",
    "epsilon = 1e-8  # Small constant to avoid division by zero\n",
    "max_iter = 1000  # Maximum number of iterations\n",
    "\n",
    "x_opt = adagrad_adam(x0, learning_rate, beta1, beta2, epsilon, max_iter)\n",
    "\n",
    "# Output the result\n",
    "print(\"Minimum value found at x =\", x_opt, \", f(x) =\", objective_function(x_opt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7517664",
   "metadata": {},
   "source": [
    "### To minimize the objective function $f(x) = \\langle u, x \\rangle + \\frac{1}{2} \\langle x, Hx \\rangle + \\lambda \\|x\\|^2$ using the AdaGrad algorithm, we need to:\n",
    "\n",
    "1. Define the objective function.\n",
    "2. Compute its gradient.\n",
    "3. Implement the AdaGrad algorithm to minimize this function..\n",
    "\n",
    "## Equations\n",
    "1. Objective Function:\n",
    "\n",
    "$ f(x) = \\langle u, x \\rangle + \\frac{1}{2} \\langle x, Hx \\rangle + \\lambda \\|x\\|^2 $\n",
    " \n",
    "where:\n",
    "\n",
    "- $\\langle u, x \\rangle$ is the dot product between u and x.\n",
    "- $\\langle x, Hx \\rangle$ is the quadratic term with matrix H.\n",
    "- $\\lambda \\|x\\|^2$ is the regularization term with \\lambdaλ as the regularization parameter.\n",
    "\n",
    "2. Gradient of the Objective Function:\n",
    "\n",
    " $\\nabla f(x) = u + Hx + 2\\lambda x$\n",
    "3. AdaGrad Update Rule:\n",
    "\n",
    "$ x_{t+1} = x_t - \\frac{\\eta}{\\sqrt{G_t + \\epsilon}} \\odot \\nabla $\n",
    "\n",
    "where:\n",
    "\n",
    "$G_t = \\sum_{i=1}^{t} (\\nabla f(x_i))^2$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "259197a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum value found at x = [-0.09646302 -0.59485531] , f(x) = -0.6430868167202571\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define problem parameters\n",
    "u = np.array([1, 2], dtype=np.float64)  # Example vector u\n",
    "H = np.array([[4, 1], [1, 3]], dtype=np.float64)  # Positive definite matrix H\n",
    "lambda_reg = 0.1  # Regularization parameter\n",
    "\n",
    "# Define the objective function\n",
    "def objective_function(x):\n",
    "    return np.dot(u, x) + 0.5 * np.dot(x.T, np.dot(H, x)) + lambda_reg * np.dot(x, x)\n",
    "\n",
    "# Define the gradient of the objective function\n",
    "def gradient(x):\n",
    "    return u + np.dot(H, x) + 2 * lambda_reg * x\n",
    "\n",
    "# AdaGrad optimization function\n",
    "def adagrad(x0, learning_rate, epsilon, max_iter):\n",
    "    x = x0.astype(np.float64)  # Ensure x0 is of type float64\n",
    "    gradient_squared_sum = np.zeros_like(x, dtype=np.float64)\n",
    "    \n",
    "    for t in range(1, max_iter + 1):\n",
    "        grad = gradient(x)\n",
    "        gradient_squared_sum += grad ** 2\n",
    "        adjusted_grad = grad / (np.sqrt(gradient_squared_sum) + epsilon)\n",
    "        x -= learning_rate * adjusted_grad\n",
    "        \n",
    "    return x\n",
    "\n",
    "# Perform optimization\n",
    "x0 = np.zeros_like(u, dtype=np.float64)  # Initial guess\n",
    "learning_rate = 0.1  # Learning rate\n",
    "epsilon = 1e-8  # Small constant to avoid division by zero\n",
    "max_iter = 1000  # Maximum number of iterations\n",
    "\n",
    "x_opt = adagrad(x0, learning_rate, epsilon, max_iter)\n",
    "\n",
    "# Output the result\n",
    "print(\"Minimum value found at x =\", x_opt, \", f(x) =\", objective_function(x_opt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713aee3e",
   "metadata": {},
   "source": [
    "### Logistic regression with sigmoid and coss entropy function.\n",
    "The cross-entropy loss function for logistic regression can be written as:\n",
    "\n",
    "$ H(y, \\hat{y}) = - \\frac{1}{T} \\sum_{t=1}^{T} \\left( y_t \\log(\\hat{y}_t) + (1 - y_t) \\log(1 - \\hat{y}_t) \\right)$\n",
    "where:\n",
    "\n",
    "- T is the total number of samples.\n",
    "- $y_t$  is the true label for sample t, where $y_t \\in \\{0, 1\\}y$.\n",
    "- $\\hat{y}_t$   is the predicted probability of the positive class for sample tt, obtained from the sigmoid function.\n",
    "The gradient of the cross-entropy loss function with respect to the predicted probabilities \n",
    "$\\hat{y}_t$ is given by:\n",
    "\n",
    "$\\frac{\\partial H(y, \\hat{y})}{\\partial \\hat{y}_t} = \\frac{\\hat{y}_t - y_t}{T}$\n",
    " \n",
    "Let us start"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
