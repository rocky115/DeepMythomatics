{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb46fa7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " * Copyright (c) 2018 Radhamadhab Dalai\n",
    " *\n",
    " * Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    " * of this software and associated documentation files (the \"Software\"), to deal\n",
    " * in the Software without restriction, including without limitation the rights\n",
    " * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    " * copies of the Software, and to permit persons to whom the Software is\n",
    " * furnished to do so, subject to the following conditions:\n",
    " *\n",
    " * The above copyright notice and this permission notice shall be included in\n",
    " * all copies or substantial portions of the Software.\n",
    " *\n",
    " * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    " * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    " * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    " * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    " * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    " * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
    " * THE SOFTWARE.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736a1191",
   "metadata": {},
   "source": [
    "# 2.7 Vector and Matrix Differentiation\n",
    "\n",
    "We define derivatives of vectors, matrices, products of vectors and matrices, as well as scalar functions of vectors and matrices, results which commonly appear in linear model theory; see Magnus and Neudecker (1988) for more details. We assume throughout that all the derivatives exist and are continuous.\n",
    "\n",
    "## Definition 2.7.1\n",
    "\n",
    "Let $\\boldsymbol{\\beta} = (\\beta_1, \\ldots, \\beta_k)^T$ be a $k$-dimensional vector and let $f(\\boldsymbol{\\beta})$ denote a scalar function of $\\boldsymbol{\\beta}$. The first partial derivative of $f$ with respect to $\\boldsymbol{\\beta}$ is defined to be the $k$-dimensional vector of partial derivatives $\\partial f/\\partial \\beta_i$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f(\\boldsymbol{\\beta})}{\\partial \\boldsymbol{\\beta}} = \\frac{\\partial f}{\\partial \\boldsymbol{\\beta}} = \\begin{pmatrix}\n",
    "\\partial f/\\partial \\beta_1 \\\\\n",
    "\\partial f/\\partial \\beta_2 \\\\\n",
    "\\vdots \\\\\n",
    "\\partial f/\\partial \\beta_k\n",
    "\\end{pmatrix} \\tag{2.7.1}\n",
    "$$\n",
    "\n",
    "Also,\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial \\boldsymbol{\\beta}^T} = \\left(\\frac{\\partial f}{\\partial \\beta_1}, \\frac{\\partial f}{\\partial \\beta_2}, \\ldots, \\frac{\\partial f}{\\partial \\beta_k}\\right) \\tag{2.7.2}\n",
    "$$\n",
    "\n",
    "## Definition 2.7.2\n",
    "\n",
    "The second partial derivative of $f$ with respect to $\\boldsymbol{\\beta}$ is defined to be the $k \\times k$ matrix of partial derivatives $\\partial^2 f/\\partial \\beta_i \\partial \\beta_j = \\partial^2 f/\\partial \\beta_j \\partial \\beta_i$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial^2 f}{\\partial \\boldsymbol{\\beta} \\partial \\boldsymbol{\\beta}^T} = \\begin{pmatrix}\n",
    "\\partial^2 f/\\partial \\beta_1^2 & \\partial^2 f/\\partial \\beta_1 \\partial \\beta_2 & \\cdots & \\partial^2 f/\\partial \\beta_1 \\partial \\beta_k \\\\\n",
    "\\partial^2 f/\\partial \\beta_1 \\partial \\beta_2 & \\partial^2 f/\\partial \\beta_2^2 & \\cdots & \\partial^2 f/\\partial \\beta_2 \\partial \\beta_k \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\partial^2 f/\\partial \\beta_1 \\partial \\beta_k & \\partial^2 f/\\partial \\beta_2 \\partial \\beta_k & \\cdots & \\partial^2 f/\\partial \\beta_k^2\n",
    "\\end{pmatrix} \\tag{2.7.3}\n",
    "$$\n",
    "\n",
    "## Example 2.7.1\n",
    "\n",
    "Let $\\boldsymbol{\\beta} = (\\beta_1, \\beta_2)^T$ and let $f(\\boldsymbol{\\beta}) = \\beta_1^2 - 2\\beta_1\\beta_2$. Then,\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial \\boldsymbol{\\beta}^T} = (2\\beta_1 - 2\\beta_2, -2\\beta_1)\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\frac{\\partial^2 f}{\\partial \\boldsymbol{\\beta} \\partial \\boldsymbol{\\beta}^T} = \\begin{pmatrix}\n",
    "2 & -2 \\\\\n",
    "-2 & 0\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "## Result 2.7.1\n",
    "\n",
    "Let $f$ and $g$ represent scalar functions of a $k$-dimensional vector $\\boldsymbol{\\beta}$, and let $a$ and $b$ be real constants. Then\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial(af + bg)}{\\partial \\beta_j} &= a\\frac{\\partial f}{\\partial \\beta_j} + b\\frac{\\partial g}{\\partial \\beta_j} \\\\\n",
    "\\frac{\\partial(fg)}{\\partial \\beta_j} &= f\\frac{\\partial g}{\\partial \\beta_j} + g\\frac{\\partial f}{\\partial \\beta_j} \\\\\n",
    "\\frac{\\partial(f/g)}{\\partial \\beta_j} &= \\frac{1}{g^2}\\left(g\\frac{\\partial f}{\\partial \\beta_j} - f\\frac{\\partial g}{\\partial \\beta_j}\\right)\n",
    "\\end{align} \\tag{2.7.4}\n",
    "$$\n",
    "\n",
    "## Definition 2.7.3\n",
    "\n",
    "Let $\\mathbf{A} = \\{a_{ij}\\}$ be an $m \\times n$ matrix and let $f(\\mathbf{A})$ be a real function of $\\mathbf{A}$. The first partial derivative of $f$ with respect to $\\mathbf{A}$ is defined as the $m \\times n$ matrix of partial derivatives $\\partial f/\\partial a_{ij}$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f(\\mathbf{A})}{\\partial \\mathbf{A}} = \\{\\partial f/\\partial a_{ij}\\}, \\quad i = 1, \\ldots, m, \\quad j = 1, \\ldots, n\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\begin{pmatrix}\n",
    "\\partial f/\\partial a_{11} & \\partial f/\\partial a_{12} & \\cdots & \\partial f/\\partial a_{1n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\partial f/\\partial a_{m1} & \\partial f/\\partial a_{m2} & \\cdots & \\partial f/\\partial a_{mn}\n",
    "\\end{pmatrix} \\tag{2.7.5}\n",
    "$$\n",
    "\n",
    "The results that follow give rules for finding partial derivatives of vector or matrix functions of matrices and vectors.\n",
    "\n",
    "## Result 2.7.2\n",
    "\n",
    "Let $\\boldsymbol{\\beta} \\in \\mathbb{R}^n$ and let $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$. Then,\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{A}\\boldsymbol{\\beta}}{\\partial \\boldsymbol{\\beta}^T} = \\mathbf{A}, \\quad \\text{and} \\quad \\frac{\\partial \\boldsymbol{\\beta}^T \\mathbf{A}^T}{\\partial \\boldsymbol{\\beta}} = \\mathbf{A}^T \\tag{2.7.6}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "*This section presents the fundamental definitions and results for vector and matrix differentiation, which are essential tools in linear model theory and optimization.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f094f2",
   "metadata": {},
   "source": [
    "# 2.7 Vector and Matrix Differentiation\n",
    "\n",
    "We define derivatives of vectors, matrices, products of vectors and matrices, as well as scalar functions of vectors and matrices, results which commonly appear in linear model theory; see Magnus and Neudecker (1988) for more details. We assume throughout that all the derivatives exist and are continuous.\n",
    "\n",
    "## Definition 2.7.1\n",
    "\n",
    "Let $\\boldsymbol{\\beta} = (\\beta_1, \\ldots, \\beta_k)^T$ be a $k$-dimensional vector and let $f(\\boldsymbol{\\beta})$ denote a scalar function of $\\boldsymbol{\\beta}$. The first partial derivative of $f$ with respect to $\\boldsymbol{\\beta}$ is defined to be the $k$-dimensional vector of partial derivatives $\\partial f/\\partial \\beta_i$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f(\\boldsymbol{\\beta})}{\\partial \\boldsymbol{\\beta}} = \\frac{\\partial f}{\\partial \\boldsymbol{\\beta}} = \\begin{pmatrix}\n",
    "\\partial f/\\partial \\beta_1 \\\\\n",
    "\\partial f/\\partial \\beta_2 \\\\\n",
    "\\vdots \\\\\n",
    "\\partial f/\\partial \\beta_k\n",
    "\\end{pmatrix} \\tag{2.7.1}\n",
    "$$\n",
    "\n",
    "Also,\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial \\boldsymbol{\\beta}^T} = \\left(\\frac{\\partial f}{\\partial \\beta_1}, \\frac{\\partial f}{\\partial \\beta_2}, \\ldots, \\frac{\\partial f}{\\partial \\beta_k}\\right) \\tag{2.7.2}\n",
    "$$\n",
    "\n",
    "## Definition 2.7.2\n",
    "\n",
    "The second partial derivative of $f$ with respect to $\\boldsymbol{\\beta}$ is defined to be the $k \\times k$ matrix of partial derivatives $\\partial^2 f/\\partial \\beta_i \\partial \\beta_j = \\partial^2 f/\\partial \\beta_j \\partial \\beta_i$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial^2 f}{\\partial \\boldsymbol{\\beta} \\partial \\boldsymbol{\\beta}^T} = \\begin{pmatrix}\n",
    "\\partial^2 f/\\partial \\beta_1^2 & \\partial^2 f/\\partial \\beta_1 \\partial \\beta_2 & \\cdots & \\partial^2 f/\\partial \\beta_1 \\partial \\beta_k \\\\\n",
    "\\partial^2 f/\\partial \\beta_1 \\partial \\beta_2 & \\partial^2 f/\\partial \\beta_2^2 & \\cdots & \\partial^2 f/\\partial \\beta_2 \\partial \\beta_k \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\partial^2 f/\\partial \\beta_1 \\partial \\beta_k & \\partial^2 f/\\partial \\beta_2 \\partial \\beta_k & \\cdots & \\partial^2 f/\\partial \\beta_k^2\n",
    "\\end{pmatrix} \\tag{2.7.3}\n",
    "$$\n",
    "\n",
    "## Example 2.7.1\n",
    "\n",
    "Let $\\boldsymbol{\\beta} = (\\beta_1, \\beta_2)^T$ and let $f(\\boldsymbol{\\beta}) = \\beta_1^2 - 2\\beta_1\\beta_2$. Then,\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial \\boldsymbol{\\beta}^T} = (2\\beta_1 - 2\\beta_2, -2\\beta_1)\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\frac{\\partial^2 f}{\\partial \\boldsymbol{\\beta} \\partial \\boldsymbol{\\beta}^T} = \\begin{pmatrix}\n",
    "2 & -2 \\\\\n",
    "-2 & 0\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "## Result 2.7.1\n",
    "\n",
    "Let $f$ and $g$ represent scalar functions of a $k$-dimensional vector $\\boldsymbol{\\beta}$, and let $a$ and $b$ be real constants. Then\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial(af + bg)}{\\partial \\beta_j} &= a\\frac{\\partial f}{\\partial \\beta_j} + b\\frac{\\partial g}{\\partial \\beta_j} \\\\\n",
    "\\frac{\\partial(fg)}{\\partial \\beta_j} &= f\\frac{\\partial g}{\\partial \\beta_j} + g\\frac{\\partial f}{\\partial \\beta_j} \\\\\n",
    "\\frac{\\partial(f/g)}{\\partial \\beta_j} &= \\frac{1}{g^2}\\left(g\\frac{\\partial f}{\\partial \\beta_j} - f\\frac{\\partial g}{\\partial \\beta_j}\\right)\n",
    "\\end{align} \\tag{2.7.4}\n",
    "$$\n",
    "\n",
    "## Definition 2.7.3\n",
    "\n",
    "Let $\\mathbf{A} = \\{a_{ij}\\}$ be an $m \\times n$ matrix and let $f(\\mathbf{A})$ be a real function of $\\mathbf{A}$. The first partial derivative of $f$ with respect to $\\mathbf{A}$ is defined as the $m \\times n$ matrix of partial derivatives $\\partial f/\\partial a_{ij}$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f(\\mathbf{A})}{\\partial \\mathbf{A}} = \\{\\partial f/\\partial a_{ij}\\}, \\quad i = 1, \\ldots, m, \\quad j = 1, \\ldots, n\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\begin{pmatrix}\n",
    "\\partial f/\\partial a_{11} & \\partial f/\\partial a_{12} & \\cdots & \\partial f/\\partial a_{1n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\partial f/\\partial a_{m1} & \\partial f/\\partial a_{m2} & \\cdots & \\partial f/\\partial a_{mn}\n",
    "\\end{pmatrix} \\tag{2.7.5}\n",
    "$$\n",
    "\n",
    "The results that follow give rules for finding partial derivatives of vector or matrix functions of matrices and vectors.\n",
    "\n",
    "## Result 2.7.2\n",
    "\n",
    "Let $\\boldsymbol{\\beta} \\in \\mathbb{R}^n$ and let $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$. Then,\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{A}\\boldsymbol{\\beta}}{\\partial \\boldsymbol{\\beta}^T} = \\mathbf{A}, \\quad \\text{and} \\quad \\frac{\\partial \\boldsymbol{\\beta}^T \\mathbf{A}^T}{\\partial \\boldsymbol{\\beta}} = \\mathbf{A}^T \\tag{2.7.6}\n",
    "$$\n",
    "\n",
    "## Proof of Result 2.7.2\n",
    "\n",
    "We may write\n",
    "$\n",
    "\\mathbf{A}\\boldsymbol{\\beta} = \\begin{pmatrix}\n",
    "a_{11}\\beta_1 + \\cdots + a_{1n}\\beta_n \\\\\n",
    "a_{21}\\beta_1 + \\cdots + a_{2n}\\beta_n \\\\\n",
    "\\vdots \\\\\n",
    "a_{m1}\\beta_1 + \\cdots + a_{mn}\\beta_n\n",
    "\\end{pmatrix}\n",
    "$\n",
    "\n",
    "so that by Definition 2.7.2, $\\partial \\mathbf{A}\\boldsymbol{\\beta}/\\partial \\boldsymbol{\\beta}^T$ is given by\n",
    "\n",
    "$\n",
    "\\begin{pmatrix}\n",
    "\\partial(a_{11}\\beta_1 + \\cdots + a_{1n}\\beta_n)/\\partial\\beta_1 & \\cdots & \\partial(a_{11}\\beta_1 + \\cdots + a_{1n}\\beta_n)/\\partial\\beta_n \\\\\n",
    "\\partial(a_{21}\\beta_1 + \\cdots + a_{2n}\\beta_n)/\\partial\\beta_1 & \\cdots & \\partial(a_{21}\\beta_1 + \\cdots + a_{2n}\\beta_n)/\\partial\\beta_n \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\partial(a_{m1}\\beta_1 + \\cdots + a_{mn}\\beta_n)/\\partial\\beta_1 & \\cdots & \\partial(a_{m1}\\beta_1 + \\cdots + a_{mn}\\beta_n)/\\partial\\beta_n\n",
    "\\end{pmatrix}\n",
    "$\n",
    "\n",
    "$\n",
    "= \\begin{pmatrix}\n",
    "a_{11} & \\cdots & a_{1n} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "a_{m1} & \\cdots & a_{mn}\n",
    "\\end{pmatrix} = \\mathbf{A} \\tag{2.7.7}\n",
    "$\n",
    "\n",
    "That $\\partial \\boldsymbol{\\beta}^T \\mathbf{A}^T/\\partial \\boldsymbol{\\beta} = \\mathbf{A}^T$ follows by transposing both sides of (2.7.7).\n",
    "\n",
    "## Result 2.7.3\n",
    "\n",
    "Let $\\boldsymbol{\\beta} \\in \\mathbb{R}^n$ and $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$. Then,\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\boldsymbol{\\beta}^T \\mathbf{A}\\boldsymbol{\\beta}}{\\partial \\boldsymbol{\\beta}} &= (\\mathbf{A} + \\mathbf{A}^T)\\boldsymbol{\\beta} \\\\\n",
    "\\frac{\\partial \\boldsymbol{\\beta}^T \\mathbf{A}\\boldsymbol{\\beta}}{\\partial \\boldsymbol{\\beta}^T} &= \\boldsymbol{\\beta}^T(\\mathbf{A} + \\mathbf{A}^T) \\\\\n",
    "\\frac{\\partial^2 \\boldsymbol{\\beta}^T \\mathbf{A}\\boldsymbol{\\beta}}{\\partial \\boldsymbol{\\beta} \\partial \\boldsymbol{\\beta}^T} &= \\mathbf{A} + \\mathbf{A}^T\n",
    "\\end{align} \\tag{2.7.8}\n",
    "$\n",
    "\n",
    "Further, if $\\mathbf{A}$ is a symmetric matrix,\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\boldsymbol{\\beta}^T \\mathbf{A}\\boldsymbol{\\beta}}{\\partial \\boldsymbol{\\beta}} &= 2\\mathbf{A}\\boldsymbol{\\beta} \\\\\n",
    "\\frac{\\partial \\boldsymbol{\\beta}^T \\mathbf{A}\\boldsymbol{\\beta}}{\\partial \\boldsymbol{\\beta}^T} &= 2\\boldsymbol{\\beta}^T\\mathbf{A} \\\\\n",
    "\\frac{\\partial^2 \\boldsymbol{\\beta}^T \\mathbf{A}\\boldsymbol{\\beta}}{\\partial \\boldsymbol{\\beta} \\partial \\boldsymbol{\\beta}^T} &= 2\\mathbf{A}\n",
    "\\end{align} \\tag{2.7.9}\n",
    "$\n",
    "\n",
    "### Proof\n",
    "\n",
    "We prove the result for a symmetric matrix $\\mathbf{A}$. Clearly,\n",
    "\n",
    "$\n",
    "\\boldsymbol{\\beta}^T \\mathbf{A}\\boldsymbol{\\beta} = \\sum_{i,j=1}^n a_{ij}\\beta_i\\beta_j\n",
    "$\n",
    "\n",
    "so that\n",
    "\n",
    "$\n",
    "\\frac{\\partial \\boldsymbol{\\beta}^T \\mathbf{A}\\boldsymbol{\\beta}}{\\partial \\beta_r} = \\sum_{\\substack{j=1\\\\j \\neq r}}^n a_{rj}\\beta_j + \\sum_{\\substack{i=1\\\\i \\neq r}}^n a_{ir}\\beta_i + 2a_{rr}\\beta_r\n",
    "$\n",
    "\n",
    "$\n",
    "= 2\\sum_{j=1}^n a_{rj}\\beta_j \\quad \\text{(by symmetry of } \\mathbf{A}\\text{)}\n",
    "$\n",
    "\n",
    "$\n",
    "= 2\\mathbf{a}_r^T\\boldsymbol{\\beta}\n",
    "$\n",
    "\n",
    "where $\\mathbf{a}_r^T$ denotes the $r$th row vector of $\\mathbf{A}$. By Definition 2.7.3, we get\n",
    "\n",
    "$\n",
    "\\frac{\\partial \\boldsymbol{\\beta}^T \\mathbf{A}\\boldsymbol{\\beta}}{\\partial \\boldsymbol{\\beta}} = \\begin{pmatrix}\n",
    "\\partial \\boldsymbol{\\beta}^T \\mathbf{A}\\boldsymbol{\\beta}/\\partial \\beta_1 \\\\\n",
    "\\partial \\boldsymbol{\\beta}^T \\mathbf{A}\\boldsymbol{\\beta}/\\partial \\beta_2 \\\\\n",
    "\\vdots \\\\\n",
    "\\partial \\boldsymbol{\\beta}^T \\mathbf{A}\\boldsymbol{\\beta}/\\partial \\beta_n\n",
    "\\end{pmatrix} = 2\\begin{pmatrix}\n",
    "\\mathbf{a}_1^T \\\\\n",
    "\\mathbf{a}_2^T \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{a}_n^T\n",
    "\\end{pmatrix}\\boldsymbol{\\beta} = 2\\mathbf{A}\\boldsymbol{\\beta} \\tag{2.7.10}\n",
    "$\n",
    "\n",
    "The second result follows by transposing both sides of (2.7.10). To show the last result, we again take the first partial derivative of $\\partial \\boldsymbol{\\beta}^T \\mathbf{A}\\boldsymbol{\\beta}/\\partial \\boldsymbol{\\beta}^T = 2\\boldsymbol{\\beta}^T\\mathbf{A}$, and use Result 2.7.2.\n",
    "\n",
    "## Result 2.7.4\n",
    "\n",
    "Let $\\mathbf{C} \\in \\mathbb{R}^{m \\times n}$, $\\boldsymbol{\\alpha} \\in \\mathbb{R}^m$, and $\\boldsymbol{\\beta} \\in \\mathbb{R}^n$. Then,\n",
    "\n",
    "$\n",
    "\\frac{\\partial \\boldsymbol{\\alpha}^T \\mathbf{C}\\boldsymbol{\\beta}}{\\partial \\mathbf{C}} = \\boldsymbol{\\alpha}\\boldsymbol{\\beta}^T \\tag{2.7.11}\n",
    "$\n",
    "\n",
    "### Proof\n",
    "\n",
    "Since\n",
    "$\n",
    "\\boldsymbol{\\alpha}^T \\mathbf{C}\\boldsymbol{\\beta} = \\sum_{i=1}^m \\sum_{j=1}^n c_{ij}\\alpha_i\\beta_j\n",
    "$\n",
    "\n",
    "we have\n",
    "$\n",
    "\\frac{\\partial(\\boldsymbol{\\alpha}^T \\mathbf{C}\\boldsymbol{\\beta})}{\\partial c_{kl}} = \\alpha_k\\beta_l\n",
    "$\n",
    "\n",
    "which is the $(k,l)$th element of $\\boldsymbol{\\alpha}\\boldsymbol{\\beta}^T$, from which the result follows.\n",
    "\n",
    "## Additional Results\n",
    "\n",
    "We next give a few useful results without proof.\n",
    "\n",
    "### Result 2.7.5\n",
    "\n",
    "Let $\\mathbf{A}$ be an $n \\times n$ matrix. Then,\n",
    "\n",
    "$\n",
    "\\frac{\\partial \\text{tr}(\\mathbf{A})}{\\partial \\mathbf{A}} = \\mathbf{I}_n \\quad \\text{and} \\quad \\frac{\\partial |\\mathbf{A}|}{\\partial \\mathbf{A}} = \\text{Adj}(\\mathbf{A}) \\tag{2.7.12}\n",
    "$\n",
    "\n",
    "where $\\text{Adj}(\\mathbf{A})$ denotes the adjoint of $\\mathbf{A}$.\n",
    "\n",
    "### Result 2.7.6\n",
    "\n",
    "Suppose $\\mathbf{A}$ is an $n \\times n$ matrix with $|\\mathbf{A}| > 0$. Then\n",
    "\n",
    "$\n",
    "\\frac{\\partial \\ln |\\mathbf{A}|}{\\partial \\mathbf{A}} = (\\mathbf{A}^T)^{-1} \\tag{2.7.13}\n",
    "$\n",
    "\n",
    "### Result 2.7.7\n",
    "\n",
    "Let $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ and $\\mathbf{B} \\in \\mathbb{R}^{n \\times m}$. Then,\n",
    "\n",
    "$\n",
    "\\frac{\\partial \\text{tr}(\\mathbf{A}\\mathbf{B})}{\\partial \\mathbf{A}} = \\mathbf{B}^T \\tag{2.7.14}\n",
    "$\n",
    "\n",
    "### Result 2.7.8\n",
    "\n",
    "Let $\\boldsymbol{\\Omega}$ be a symmetric matrix, $\\mathbf{y} \\in \\mathbb{R}^n$, $\\boldsymbol{\\beta} \\in \\mathbb{R}^k$, and $\\mathbf{X} \\in \\mathbb{R}^{n \\times k}$. Then,\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\frac{\\partial (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T \\boldsymbol{\\Omega}(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})}{\\partial \\boldsymbol{\\beta}} &= -2\\mathbf{X}^T\\boldsymbol{\\Omega}(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}) \\\\\n",
    "\\frac{\\partial^2 (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T \\boldsymbol{\\Omega}(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})}{\\partial \\boldsymbol{\\beta} \\partial \\boldsymbol{\\beta}^T} &= 2\\mathbf{X}^T\\boldsymbol{\\Omega}\\mathbf{X}\n",
    "\\end{align} \\tag{2.7.15}\n",
    "$\n",
    "\n",
    "## Definition 2.7.4\n",
    "\n",
    "The next definition deals with partial derivatives of a matrix (or a vector) with respect to some scalar $\\theta$. We see that in this case, the partial derivative is itself a matrix or vector of the same dimension whose elements are the partial derivatives with respect to $\\theta$ of each element of that matrix or vector.\n",
    "\n",
    "Let $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ be a function of a scalar $\\theta$, then\n",
    "\n",
    "$\n",
    "\\frac{\\partial \\mathbf{A}}{\\partial \\theta} = \\{\\partial a_{ij}/\\partial \\theta\\}, \\quad i = 1, \\ldots, m, \\quad j = 1, \\ldots, n\n",
    "$\n",
    "\n",
    "$\n",
    "= \\begin{pmatrix}\n",
    "\\partial a_{11}/\\partial \\theta & \\partial a_{12}/\\partial \\theta & \\cdots & \\partial a_{1n}/\\partial \\theta \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\partial a_{m1}/\\partial \\theta & \\partial a_{m2}/\\partial \\theta & \\cdots & \\partial a_{mn}/\\partial \\theta\n",
    "\\end{pmatrix} \\tag{2.7.16}\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "*This section presents the fundamental definitions and results for vector and matrix differentiation, which are essential tools in linear model theory and optimization.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d2fc4c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sympy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14818/2689494051.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msympy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msympy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msymbols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMatrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mderive_by_array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sympy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from typing import Callable, Tuple, Union\n",
    "import sympy as sp\n",
    "from sympy import symbols, diff, Matrix, derive_by_array\n",
    "import warnings\n",
    "\n",
    "class VectorMatrixDifferentiation:\n",
    "    \"\"\"\n",
    "    Implementation of vector and matrix differentiation operations\n",
    "    as defined in Section 2.7\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def gradient_scalar_wrt_vector(f: Callable, beta: np.ndarray, h: float = 1e-8) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute the gradient of a scalar function f with respect to vector beta\n",
    "        using finite differences (Definition 2.7.1)\n",
    "        \n",
    "        Args:\n",
    "            f: Scalar function of vector\n",
    "            beta: k-dimensional vector\n",
    "            h: Step size for finite differences\n",
    "            \n",
    "        Returns:\n",
    "            k-dimensional gradient vector\n",
    "        \"\"\"\n",
    "        k = len(beta)\n",
    "        grad = np.zeros(k)\n",
    "        \n",
    "        for i in range(k):\n",
    "            beta_plus = beta.copy()\n",
    "            beta_minus = beta.copy()\n",
    "            beta_plus[i] += h\n",
    "            beta_minus[i] -= h\n",
    "            \n",
    "            grad[i] = (f(beta_plus) - f(beta_minus)) / (2 * h)\n",
    "        \n",
    "        return grad\n",
    "    \n",
    "    @staticmethod\n",
    "    def hessian_scalar_wrt_vector(f: Callable, beta: np.ndarray, h: float = 1e-6) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute the Hessian matrix of a scalar function f with respect to vector beta\n",
    "        using finite differences (Definition 2.7.2)\n",
    "        \n",
    "        Args:\n",
    "            f: Scalar function of vector\n",
    "            beta: k-dimensional vector\n",
    "            h: Step size for finite differences\n",
    "            \n",
    "        Returns:\n",
    "            k x k Hessian matrix\n",
    "        \"\"\"\n",
    "        k = len(beta)\n",
    "        hessian = np.zeros((k, k))\n",
    "        \n",
    "        for i in range(k):\n",
    "            for j in range(k):\n",
    "                # Compute second partial derivative using finite differences\n",
    "                beta_pp = beta.copy()\n",
    "                beta_pm = beta.copy()\n",
    "                beta_mp = beta.copy()\n",
    "                beta_mm = beta.copy()\n",
    "                \n",
    "                beta_pp[i] += h\n",
    "                beta_pp[j] += h\n",
    "                \n",
    "                beta_pm[i] += h\n",
    "                beta_pm[j] -= h\n",
    "                \n",
    "                beta_mp[i] -= h\n",
    "                beta_mp[j] += h\n",
    "                \n",
    "                beta_mm[i] -= h\n",
    "                beta_mm[j] -= h\n",
    "                \n",
    "                hessian[i, j] = (f(beta_pp) - f(beta_pm) - f(beta_mp) + f(beta_mm)) / (4 * h**2)\n",
    "        \n",
    "        return hessian\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative_A_beta_wrt_beta(A: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute ∂(Aβ)/∂β^T = A (Result 2.7.2)\n",
    "        \n",
    "        Args:\n",
    "            A: m x n matrix\n",
    "            \n",
    "        Returns:\n",
    "            Matrix A\n",
    "        \"\"\"\n",
    "        return A\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative_beta_AT_wrt_beta(A: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute ∂(β^T A^T)/∂β = A^T (Result 2.7.2)\n",
    "        \n",
    "        Args:\n",
    "            A: m x n matrix\n",
    "            \n",
    "        Returns:\n",
    "            Matrix A^T\n",
    "        \"\"\"\n",
    "        return A.T\n",
    "    \n",
    "    @staticmethod\n",
    "    def quadratic_form_derivatives(A: np.ndarray, beta: np.ndarray, \n",
    "                                 is_symmetric: bool = None) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Compute derivatives of quadratic form β^T A β (Result 2.7.3)\n",
    "        \n",
    "        Args:\n",
    "            A: n x n matrix\n",
    "            beta: n-dimensional vector\n",
    "            is_symmetric: Whether A is symmetric (auto-detect if None)\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (first_derivative, first_derivative_transpose, hessian)\n",
    "        \"\"\"\n",
    "        if is_symmetric is None:\n",
    "            is_symmetric = np.allclose(A, A.T)\n",
    "        \n",
    "        if is_symmetric:\n",
    "            # For symmetric matrix\n",
    "            first_deriv = 2 * A @ beta\n",
    "            first_deriv_T = 2 * beta.T @ A\n",
    "            hessian = 2 * A\n",
    "        else:\n",
    "            # For general matrix\n",
    "            first_deriv = (A + A.T) @ beta\n",
    "            first_deriv_T = beta.T @ (A + A.T)\n",
    "            hessian = A + A.T\n",
    "        \n",
    "        return first_deriv, first_deriv_T, hessian\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative_alpha_C_beta_wrt_C(alpha: np.ndarray, beta: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute ∂(α^T C β)/∂C = α β^T (Result 2.7.4)\n",
    "        \n",
    "        Args:\n",
    "            alpha: m-dimensional vector\n",
    "            beta: n-dimensional vector\n",
    "            \n",
    "        Returns:\n",
    "            m x n matrix α β^T\n",
    "        \"\"\"\n",
    "        return np.outer(alpha, beta)\n",
    "    \n",
    "    @staticmethod\n",
    "    def trace_derivative(A: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute ∂tr(A)/∂A = I (Result 2.7.5)\n",
    "        \n",
    "        Args:\n",
    "            A: n x n matrix\n",
    "            \n",
    "        Returns:\n",
    "            n x n identity matrix\n",
    "        \"\"\"\n",
    "        n = A.shape[0]\n",
    "        return np.eye(n)\n",
    "    \n",
    "    @staticmethod\n",
    "    def log_determinant_derivative(A: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute ∂ln|A|/∂A = (A^T)^{-1} (Result 2.7.6)\n",
    "        \n",
    "        Args:\n",
    "            A: n x n invertible matrix\n",
    "            \n",
    "        Returns:\n",
    "            (A^T)^{-1}\n",
    "        \"\"\"\n",
    "        if np.linalg.det(A) <= 0:\n",
    "            raise ValueError(\"Matrix must have positive determinant\")\n",
    "        \n",
    "        return np.linalg.inv(A.T)\n",
    "    \n",
    "    @staticmethod\n",
    "    def trace_product_derivative(A: np.ndarray, B: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute ∂tr(AB)/∂A = B^T (Result 2.7.7)\n",
    "        \n",
    "        Args:\n",
    "            A: m x n matrix\n",
    "            B: n x m matrix\n",
    "            \n",
    "        Returns:\n",
    "            B^T\n",
    "        \"\"\"\n",
    "        return B.T\n",
    "    \n",
    "    @staticmethod\n",
    "    def regression_derivatives(y: np.ndarray, X: np.ndarray, beta: np.ndarray, \n",
    "                             Omega: np.ndarray = None) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Compute derivatives for regression objective (y - Xβ)^T Ω (y - Xβ) (Result 2.7.8)\n",
    "        \n",
    "        Args:\n",
    "            y: n-dimensional response vector\n",
    "            X: n x k design matrix\n",
    "            beta: k-dimensional parameter vector\n",
    "            Omega: n x n symmetric weight matrix (default: identity)\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (first_derivative, hessian)\n",
    "        \"\"\"\n",
    "        if Omega is None:\n",
    "            Omega = np.eye(len(y))\n",
    "        \n",
    "        residual = y - X @ beta\n",
    "        first_deriv = -2 * X.T @ Omega @ residual\n",
    "        hessian = 2 * X.T @ Omega @ X\n",
    "        \n",
    "        return first_deriv, hessian\n",
    "\n",
    "\n",
    "class SymbolicDifferentiation:\n",
    "    \"\"\"\n",
    "    Symbolic computation of vector and matrix derivatives using SymPy\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def symbolic_gradient_example():\n",
    "        \"\"\"\n",
    "        Example 2.7.1: f(β) = β₁² - 2β₁β₂\n",
    "        \"\"\"\n",
    "        # Define symbols\n",
    "        beta1, beta2 = symbols('beta1 beta2')\n",
    "        beta = Matrix([beta1, beta2])\n",
    "        \n",
    "        # Define function\n",
    "        f = beta1**2 - 2*beta1*beta2\n",
    "        \n",
    "        # Compute gradient\n",
    "        gradient = Matrix([diff(f, beta1), diff(f, beta2)])\n",
    "        \n",
    "        # Compute Hessian\n",
    "        hessian = Matrix([\n",
    "            [diff(f, beta1, beta1), diff(f, beta1, beta2)],\n",
    "            [diff(f, beta2, beta1), diff(f, beta2, beta2)]\n",
    "        ])\n",
    "        \n",
    "        print(\"Example 2.7.1:\")\n",
    "        print(f\"f(β) = {f}\")\n",
    "        print(f\"∇f = {gradient}\")\n",
    "        print(f\"∇²f = {hessian}\")\n",
    "        \n",
    "        return f, gradient, hessian\n",
    "    \n",
    "    @staticmethod\n",
    "    def symbolic_quadratic_form():\n",
    "        \"\"\"\n",
    "        Symbolic computation of quadratic form derivatives\n",
    "        \"\"\"\n",
    "        # Define symbols\n",
    "        n = 3  # Example dimension\n",
    "        beta_symbols = [symbols(f'beta{i+1}') for i in range(n)]\n",
    "        beta = Matrix(beta_symbols)\n",
    "        \n",
    "        # Define a symbolic symmetric matrix\n",
    "        A = Matrix([\n",
    "            [symbols('a11'), symbols('a12'), symbols('a13')],\n",
    "            [symbols('a12'), symbols('a22'), symbols('a23')],\n",
    "            [symbols('a13'), symbols('a23'), symbols('a33')]\n",
    "        ])\n",
    "        \n",
    "        # Quadratic form\n",
    "        quad_form = beta.T @ A @ beta\n",
    "        quad_form = quad_form[0]  # Extract scalar\n",
    "        \n",
    "        # Compute gradient\n",
    "        gradient = Matrix([diff(quad_form, b) for b in beta_symbols])\n",
    "        \n",
    "        print(\"\\nSymbolic Quadratic Form:\")\n",
    "        print(f\"β^T A β = {quad_form}\")\n",
    "        print(f\"∇(β^T A β) = {gradient}\")\n",
    "        \n",
    "        return quad_form, gradient\n",
    "\n",
    "\n",
    "def demonstrate_examples():\n",
    "    \"\"\"\n",
    "    Demonstrate the implementations with concrete examples\n",
    "    \"\"\"\n",
    "    print(\"=== Vector and Matrix Differentiation Examples ===\\n\")\n",
    "    \n",
    "    # Initialize the class\n",
    "    vmd = VectorMatrixDifferentiation()\n",
    "    \n",
    "    # Example 2.7.1\n",
    "    print(\"1. Example 2.7.1: f(β) = β₁² - 2β₁β₂\")\n",
    "    \n",
    "    def f_example(beta):\n",
    "        return beta[0]**2 - 2*beta[0]*beta[1]\n",
    "    \n",
    "    beta_test = np.array([1.0, 2.0])\n",
    "    grad_numerical = vmd.gradient_scalar_wrt_vector(f_example, beta_test)\n",
    "    hess_numerical = vmd.hessian_scalar_wrt_vector(f_example, beta_test)\n",
    "    \n",
    "    # Analytical results\n",
    "    grad_analytical = np.array([2*beta_test[0] - 2*beta_test[1], -2*beta_test[0]])\n",
    "    hess_analytical = np.array([[2, -2], [-2, 0]])\n",
    "    \n",
    "    print(f\"β = {beta_test}\")\n",
    "    print(f\"Gradient (numerical): {grad_numerical}\")\n",
    "    print(f\"Gradient (analytical): {grad_analytical}\")\n",
    "    print(f\"Hessian (numerical):\\n{hess_numerical}\")\n",
    "    print(f\"Hessian (analytical):\\n{hess_analytical}\")\n",
    "    print(f\"Gradient error: {np.linalg.norm(grad_numerical - grad_analytical):.2e}\")\n",
    "    print(f\"Hessian error: {np.linalg.norm(hess_numerical - hess_analytical):.2e}\")\n",
    "    \n",
    "    # Result 2.7.2 demonstration\n",
    "    print(\"\\n2. Result 2.7.2: ∂(Aβ)/∂β^T = A\")\n",
    "    A = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "    result = vmd.derivative_A_beta_wrt_beta(A)\n",
    "    print(f\"A =\\n{A}\")\n",
    "    print(f\"∂(Aβ)/∂β^T =\\n{result}\")\n",
    "    print(f\"Verification: A == result? {np.allclose(A, result)}\")\n",
    "    \n",
    "    # Result 2.7.3 demonstration\n",
    "    print(\"\\n3. Result 2.7.3: Quadratic form derivatives\")\n",
    "    A_sym = np.array([[2, 1], [1, 3]])\n",
    "    beta = np.array([1, -1])\n",
    "    \n",
    "    first_deriv, first_deriv_T, hessian = vmd.quadratic_form_derivatives(A_sym, beta, is_symmetric=True)\n",
    "    \n",
    "    print(f\"A (symmetric) =\\n{A_sym}\")\n",
    "    print(f\"β = {beta}\")\n",
    "    print(f\"∂(β^T A β)/∂β = {first_deriv}\")\n",
    "    print(f\"∂(β^T A β)/∂β^T = {first_deriv_T}\")\n",
    "    print(f\"∂²(β^T A β)/∂β∂β^T =\\n{hessian}\")\n",
    "    \n",
    "    # Verify with analytical: 2Aβ and 2A\n",
    "    expected_grad = 2 * A_sym @ beta\n",
    "    expected_hess = 2 * A_sym\n",
    "    print(f\"Expected gradient: {expected_grad}\")\n",
    "    print(f\"Expected Hessian:\\n{expected_hess}\")\n",
    "    print(f\"Gradient match: {np.allclose(first_deriv, expected_grad)}\")\n",
    "    print(f\"Hessian match: {np.allclose(hessian, expected_hess)}\")\n",
    "    \n",
    "    # Result 2.7.4 demonstration\n",
    "    print(\"\\n4. Result 2.7.4: ∂(α^T C β)/∂C = α β^T\")\n",
    "    alpha = np.array([1, 2, 3])\n",
    "    beta = np.array([4, 5])\n",
    "    result = vmd.derivative_alpha_C_beta_wrt_C(alpha, beta)\n",
    "    expected = np.outer(alpha, beta)\n",
    "    \n",
    "    print(f\"α = {alpha}\")\n",
    "    print(f\"β = {beta}\")\n",
    "    print(f\"∂(α^T C β)/∂C =\\n{result}\")\n",
    "    print(f\"α β^T =\\n{expected}\")\n",
    "    print(f\"Match: {np.allclose(result, expected)}\")\n",
    "    \n",
    "    # Regression example (Result 2.7.8)\n",
    "    print(\"\\n5. Result 2.7.8: Regression derivatives\")\n",
    "    np.random.seed(42)\n",
    "    n, k = 10, 3\n",
    "    X = np.random.randn(n, k)\n",
    "    y = np.random.randn(n)\n",
    "    beta = np.random.randn(k)\n",
    "    \n",
    "    first_deriv, hessian = vmd.regression_derivatives(y, X, beta)\n",
    "    \n",
    "    print(f\"Design matrix X shape: {X.shape}\")\n",
    "    print(f\"Response vector y shape: {y.shape}\")\n",
    "    print(f\"Parameter vector β shape: {beta.shape}\")\n",
    "    print(f\"First derivative shape: {first_deriv.shape}\")\n",
    "    print(f\"Hessian shape: {hessian.shape}\")\n",
    "    \n",
    "    # Check if Hessian is positive definite (should be for X^T X)\n",
    "    eigenvals = np.linalg.eigvals(hessian)\n",
    "    print(f\"Hessian eigenvalues: {eigenvals}\")\n",
    "    print(f\"Positive definite: {np.all(eigenvals > 0)}\")\n",
    "\n",
    "\n",
    "def symbolic_examples():\n",
    "    \"\"\"\n",
    "    Run symbolic differentiation examples\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Symbolic Differentiation Examples ===\\n\")\n",
    "    \n",
    "    sym_diff = SymbolicDifferentiation()\n",
    "    sym_diff.symbolic_gradient_example()\n",
    "    sym_diff.symbolic_quadratic_form()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run numerical examples\n",
    "    demonstrate_examples()\n",
    "    \n",
    "    # Run symbolic examples\n",
    "    try:\n",
    "        symbolic_examples()\n",
    "    except ImportError:\n",
    "        print(\"\\nSymPy not available for symbolic examples\")\n",
    "    \n",
    "    print(\"\\n=== Implementation Complete ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ff5ca99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/radha/anaconda3/envs/cv37/lib/python3.7/site-packages/ipykernel_launcher.py:12: UserWarning: SymPy not available. Symbolic functionality will be disabled.\n",
      "  if sys.path[0] == \"\":\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Vector and Matrix Differentiation Examples ===\n",
      "\n",
      "1. Example 2.7.1: f(β) = β₁² - 2β₁β₂\n",
      "β = [1. 2.]\n",
      "Gradient (numerical): [-2.00000001 -1.99999999]\n",
      "Gradient (analytical): [-2. -2.]\n",
      "Hessian (numerical):\n",
      "[[ 2.00006678e+00 -2.00006678e+00]\n",
      " [-2.00006678e+00 -2.22044605e-04]]\n",
      "Hessian (analytical):\n",
      "[[ 2 -2]\n",
      " [-2  0]]\n",
      "Gradient error: 1.58e-08\n",
      "Hessian error: 2.50e-04\n",
      "\n",
      "2. Result 2.7.2: ∂(Aβ)/∂β^T = A\n",
      "A =\n",
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "∂(Aβ)/∂β^T =\n",
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "Verification: A == result? True\n",
      "\n",
      "3. Result 2.7.3: Quadratic form derivatives\n",
      "A (symmetric) =\n",
      "[[2 1]\n",
      " [1 3]]\n",
      "β = [ 1 -1]\n",
      "∂(β^T A β)/∂β = [ 2 -4]\n",
      "∂(β^T A β)/∂β^T = [ 2 -4]\n",
      "∂²(β^T A β)/∂β∂β^T =\n",
      "[[4 2]\n",
      " [2 6]]\n",
      "Expected gradient: [ 2 -4]\n",
      "Expected Hessian:\n",
      "[[4 2]\n",
      " [2 6]]\n",
      "Gradient match: True\n",
      "Hessian match: True\n",
      "\n",
      "4. Result 2.7.4: ∂(α^T C β)/∂C = α β^T\n",
      "α = [1 2 3]\n",
      "β = [4 5]\n",
      "∂(α^T C β)/∂C =\n",
      "[[ 4  5]\n",
      " [ 8 10]\n",
      " [12 15]]\n",
      "α β^T =\n",
      "[[ 4  5]\n",
      " [ 8 10]\n",
      " [12 15]]\n",
      "Match: True\n",
      "\n",
      "5. Result 2.7.8: Regression derivatives\n",
      "Design matrix X shape: (10, 3)\n",
      "Response vector y shape: (10,)\n",
      "Parameter vector β shape: (3,)\n",
      "First derivative shape: (3,)\n",
      "Hessian shape: (3, 3)\n",
      "Hessian eigenvalues: [ 9.69905828 21.58353438 17.82203363]\n",
      "Positive definite: True\n",
      "\n",
      "=== Symbolic Differentiation Examples ===\n",
      "\n",
      "SymPy not available. Cannot run symbolic examples.\n",
      "SymPy not available. Cannot run symbolic quadratic form example.\n",
      "\n",
      "=== Implementation Complete ===\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from typing import Callable, Tuple, Union\n",
    "import warnings\n",
    "\n",
    "# Try to import sympy, but make it optional\n",
    "try:\n",
    "    import sympy as sp\n",
    "    from sympy import symbols, diff, Matrix, derive_by_array\n",
    "    SYMPY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SYMPY_AVAILABLE = False\n",
    "    warnings.warn(\"SymPy not available. Symbolic functionality will be disabled.\")\n",
    "\n",
    "class VectorMatrixDifferentiation:\n",
    "    \"\"\"\n",
    "    Implementation of vector and matrix differentiation operations\n",
    "    as defined in Section 2.7\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def gradient_scalar_wrt_vector(f: Callable, beta: np.ndarray, h: float = 1e-8) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute the gradient of a scalar function f with respect to vector beta\n",
    "        using finite differences (Definition 2.7.1)\n",
    "        \n",
    "        Args:\n",
    "            f: Scalar function of vector\n",
    "            beta: k-dimensional vector\n",
    "            h: Step size for finite differences\n",
    "            \n",
    "        Returns:\n",
    "            k-dimensional gradient vector\n",
    "        \"\"\"\n",
    "        k = len(beta)\n",
    "        grad = np.zeros(k)\n",
    "        \n",
    "        for i in range(k):\n",
    "            beta_plus = beta.copy()\n",
    "            beta_minus = beta.copy()\n",
    "            beta_plus[i] += h\n",
    "            beta_minus[i] -= h\n",
    "            \n",
    "            grad[i] = (f(beta_plus) - f(beta_minus)) / (2 * h)\n",
    "        \n",
    "        return grad\n",
    "    \n",
    "    @staticmethod\n",
    "    def hessian_scalar_wrt_vector(f: Callable, beta: np.ndarray, h: float = 1e-6) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute the Hessian matrix of a scalar function f with respect to vector beta\n",
    "        using finite differences (Definition 2.7.2)\n",
    "        \n",
    "        Args:\n",
    "            f: Scalar function of vector\n",
    "            beta: k-dimensional vector\n",
    "            h: Step size for finite differences\n",
    "            \n",
    "        Returns:\n",
    "            k x k Hessian matrix\n",
    "        \"\"\"\n",
    "        k = len(beta)\n",
    "        hessian = np.zeros((k, k))\n",
    "        \n",
    "        for i in range(k):\n",
    "            for j in range(k):\n",
    "                # Compute second partial derivative using finite differences\n",
    "                beta_pp = beta.copy()\n",
    "                beta_pm = beta.copy()\n",
    "                beta_mp = beta.copy()\n",
    "                beta_mm = beta.copy()\n",
    "                \n",
    "                beta_pp[i] += h\n",
    "                beta_pp[j] += h\n",
    "                \n",
    "                beta_pm[i] += h\n",
    "                beta_pm[j] -= h\n",
    "                \n",
    "                beta_mp[i] -= h\n",
    "                beta_mp[j] += h\n",
    "                \n",
    "                beta_mm[i] -= h\n",
    "                beta_mm[j] -= h\n",
    "                \n",
    "                hessian[i, j] = (f(beta_pp) - f(beta_pm) - f(beta_mp) + f(beta_mm)) / (4 * h**2)\n",
    "        \n",
    "        return hessian\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative_A_beta_wrt_beta(A: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute ∂(Aβ)/∂β^T = A (Result 2.7.2)\n",
    "        \n",
    "        Args:\n",
    "            A: m x n matrix\n",
    "            \n",
    "        Returns:\n",
    "            Matrix A\n",
    "        \"\"\"\n",
    "        return A\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative_beta_AT_wrt_beta(A: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute ∂(β^T A^T)/∂β = A^T (Result 2.7.2)\n",
    "        \n",
    "        Args:\n",
    "            A: m x n matrix\n",
    "            \n",
    "        Returns:\n",
    "            Matrix A^T\n",
    "        \"\"\"\n",
    "        return A.T\n",
    "    \n",
    "    @staticmethod\n",
    "    def quadratic_form_derivatives(A: np.ndarray, beta: np.ndarray, \n",
    "                                 is_symmetric: bool = None) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Compute derivatives of quadratic form β^T A β (Result 2.7.3)\n",
    "        \n",
    "        Args:\n",
    "            A: n x n matrix\n",
    "            beta: n-dimensional vector\n",
    "            is_symmetric: Whether A is symmetric (auto-detect if None)\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (first_derivative, first_derivative_transpose, hessian)\n",
    "        \"\"\"\n",
    "        if is_symmetric is None:\n",
    "            is_symmetric = np.allclose(A, A.T)\n",
    "        \n",
    "        if is_symmetric:\n",
    "            # For symmetric matrix\n",
    "            first_deriv = 2 * A @ beta\n",
    "            first_deriv_T = 2 * beta.T @ A\n",
    "            hessian = 2 * A\n",
    "        else:\n",
    "            # For general matrix\n",
    "            first_deriv = (A + A.T) @ beta\n",
    "            first_deriv_T = beta.T @ (A + A.T)\n",
    "            hessian = A + A.T\n",
    "        \n",
    "        return first_deriv, first_deriv_T, hessian\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative_alpha_C_beta_wrt_C(alpha: np.ndarray, beta: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute ∂(α^T C β)/∂C = α β^T (Result 2.7.4)\n",
    "        \n",
    "        Args:\n",
    "            alpha: m-dimensional vector\n",
    "            beta: n-dimensional vector\n",
    "            \n",
    "        Returns:\n",
    "            m x n matrix α β^T\n",
    "        \"\"\"\n",
    "        return np.outer(alpha, beta)\n",
    "    \n",
    "    @staticmethod\n",
    "    def trace_derivative(A: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute ∂tr(A)/∂A = I (Result 2.7.5)\n",
    "        \n",
    "        Args:\n",
    "            A: n x n matrix\n",
    "            \n",
    "        Returns:\n",
    "            n x n identity matrix\n",
    "        \"\"\"\n",
    "        n = A.shape[0]\n",
    "        return np.eye(n)\n",
    "    \n",
    "    @staticmethod\n",
    "    def log_determinant_derivative(A: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute ∂ln|A|/∂A = (A^T)^{-1} (Result 2.7.6)\n",
    "        \n",
    "        Args:\n",
    "            A: n x n invertible matrix\n",
    "            \n",
    "        Returns:\n",
    "            (A^T)^{-1}\n",
    "        \"\"\"\n",
    "        if np.linalg.det(A) <= 0:\n",
    "            raise ValueError(\"Matrix must have positive determinant\")\n",
    "        \n",
    "        return np.linalg.inv(A.T)\n",
    "    \n",
    "    @staticmethod\n",
    "    def trace_product_derivative(A: np.ndarray, B: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute ∂tr(AB)/∂A = B^T (Result 2.7.7)\n",
    "        \n",
    "        Args:\n",
    "            A: m x n matrix\n",
    "            B: n x m matrix\n",
    "            \n",
    "        Returns:\n",
    "            B^T\n",
    "        \"\"\"\n",
    "        return B.T\n",
    "    \n",
    "    @staticmethod\n",
    "    def regression_derivatives(y: np.ndarray, X: np.ndarray, beta: np.ndarray, \n",
    "                             Omega: np.ndarray = None) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Compute derivatives for regression objective (y - Xβ)^T Ω (y - Xβ) (Result 2.7.8)\n",
    "        \n",
    "        Args:\n",
    "            y: n-dimensional response vector\n",
    "            X: n x k design matrix\n",
    "            beta: k-dimensional parameter vector\n",
    "            Omega: n x n symmetric weight matrix (default: identity)\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (first_derivative, hessian)\n",
    "        \"\"\"\n",
    "        if Omega is None:\n",
    "            Omega = np.eye(len(y))\n",
    "        \n",
    "        residual = y - X @ beta\n",
    "        first_deriv = -2 * X.T @ Omega @ residual\n",
    "        hessian = 2 * X.T @ Omega @ X\n",
    "        \n",
    "        return first_deriv, hessian\n",
    "\n",
    "\n",
    "class SymbolicDifferentiation:\n",
    "    \"\"\"\n",
    "    Symbolic computation of vector and matrix derivatives using SymPy\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def symbolic_gradient_example():\n",
    "        \"\"\"\n",
    "        Example 2.7.1: f(β) = β₁² - 2β₁β₂\n",
    "        \"\"\"\n",
    "        if not SYMPY_AVAILABLE:\n",
    "            print(\"SymPy not available. Cannot run symbolic examples.\")\n",
    "            return None, None, None\n",
    "            \n",
    "        # Define symbols\n",
    "        beta1, beta2 = symbols('beta1 beta2')\n",
    "        beta = Matrix([beta1, beta2])\n",
    "        \n",
    "        # Define function\n",
    "        f = beta1**2 - 2*beta1*beta2\n",
    "        \n",
    "        # Compute gradient\n",
    "        gradient = Matrix([diff(f, beta1), diff(f, beta2)])\n",
    "        \n",
    "        # Compute Hessian\n",
    "        hessian = Matrix([\n",
    "            [diff(f, beta1, beta1), diff(f, beta1, beta2)],\n",
    "            [diff(f, beta2, beta1), diff(f, beta2, beta2)]\n",
    "        ])\n",
    "        \n",
    "        print(\"Example 2.7.1:\")\n",
    "        print(f\"f(β) = {f}\")\n",
    "        print(f\"∇f = {gradient}\")\n",
    "        print(f\"∇²f = {hessian}\")\n",
    "        \n",
    "        return f, gradient, hessian\n",
    "    \n",
    "    @staticmethod\n",
    "    def symbolic_quadratic_form():\n",
    "        \"\"\"\n",
    "        Symbolic computation of quadratic form derivatives\n",
    "        \"\"\"\n",
    "        if not SYMPY_AVAILABLE:\n",
    "            print(\"SymPy not available. Cannot run symbolic quadratic form example.\")\n",
    "            return None, None\n",
    "            \n",
    "        # Define symbols\n",
    "        n = 3  # Example dimension\n",
    "        beta_symbols = [symbols(f'beta{i+1}') for i in range(n)]\n",
    "        beta = Matrix(beta_symbols)\n",
    "        \n",
    "        # Define a symbolic symmetric matrix\n",
    "        A = Matrix([\n",
    "            [symbols('a11'), symbols('a12'), symbols('a13')],\n",
    "            [symbols('a12'), symbols('a22'), symbols('a23')],\n",
    "            [symbols('a13'), symbols('a23'), symbols('a33')]\n",
    "        ])\n",
    "        \n",
    "        # Quadratic form\n",
    "        quad_form = beta.T @ A @ beta\n",
    "        quad_form = quad_form[0]  # Extract scalar\n",
    "        \n",
    "        # Compute gradient\n",
    "        gradient = Matrix([diff(quad_form, b) for b in beta_symbols])\n",
    "        \n",
    "        print(\"\\nSymbolic Quadratic Form:\")\n",
    "        print(f\"β^T A β = {quad_form}\")\n",
    "        print(f\"∇(β^T A β) = {gradient}\")\n",
    "        \n",
    "        return quad_form, gradient\n",
    "\n",
    "\n",
    "def demonstrate_examples():\n",
    "    \"\"\"\n",
    "    Demonstrate the implementations with concrete examples\n",
    "    \"\"\"\n",
    "    print(\"=== Vector and Matrix Differentiation Examples ===\\n\")\n",
    "    \n",
    "    # Initialize the class\n",
    "    vmd = VectorMatrixDifferentiation()\n",
    "    \n",
    "    # Example 2.7.1\n",
    "    print(\"1. Example 2.7.1: f(β) = β₁² - 2β₁β₂\")\n",
    "    \n",
    "    def f_example(beta):\n",
    "        return beta[0]**2 - 2*beta[0]*beta[1]\n",
    "    \n",
    "    beta_test = np.array([1.0, 2.0])\n",
    "    grad_numerical = vmd.gradient_scalar_wrt_vector(f_example, beta_test)\n",
    "    hess_numerical = vmd.hessian_scalar_wrt_vector(f_example, beta_test)\n",
    "    \n",
    "    # Analytical results\n",
    "    grad_analytical = np.array([2*beta_test[0] - 2*beta_test[1], -2*beta_test[0]])\n",
    "    hess_analytical = np.array([[2, -2], [-2, 0]])\n",
    "    \n",
    "    print(f\"β = {beta_test}\")\n",
    "    print(f\"Gradient (numerical): {grad_numerical}\")\n",
    "    print(f\"Gradient (analytical): {grad_analytical}\")\n",
    "    print(f\"Hessian (numerical):\\n{hess_numerical}\")\n",
    "    print(f\"Hessian (analytical):\\n{hess_analytical}\")\n",
    "    print(f\"Gradient error: {np.linalg.norm(grad_numerical - grad_analytical):.2e}\")\n",
    "    print(f\"Hessian error: {np.linalg.norm(hess_numerical - hess_analytical):.2e}\")\n",
    "    \n",
    "    # Result 2.7.2 demonstration\n",
    "    print(\"\\n2. Result 2.7.2: ∂(Aβ)/∂β^T = A\")\n",
    "    A = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "    result = vmd.derivative_A_beta_wrt_beta(A)\n",
    "    print(f\"A =\\n{A}\")\n",
    "    print(f\"∂(Aβ)/∂β^T =\\n{result}\")\n",
    "    print(f\"Verification: A == result? {np.allclose(A, result)}\")\n",
    "    \n",
    "    # Result 2.7.3 demonstration\n",
    "    print(\"\\n3. Result 2.7.3: Quadratic form derivatives\")\n",
    "    A_sym = np.array([[2, 1], [1, 3]])\n",
    "    beta = np.array([1, -1])\n",
    "    \n",
    "    first_deriv, first_deriv_T, hessian = vmd.quadratic_form_derivatives(A_sym, beta, is_symmetric=True)\n",
    "    \n",
    "    print(f\"A (symmetric) =\\n{A_sym}\")\n",
    "    print(f\"β = {beta}\")\n",
    "    print(f\"∂(β^T A β)/∂β = {first_deriv}\")\n",
    "    print(f\"∂(β^T A β)/∂β^T = {first_deriv_T}\")\n",
    "    print(f\"∂²(β^T A β)/∂β∂β^T =\\n{hessian}\")\n",
    "    \n",
    "    # Verify with analytical: 2Aβ and 2A\n",
    "    expected_grad = 2 * A_sym @ beta\n",
    "    expected_hess = 2 * A_sym\n",
    "    print(f\"Expected gradient: {expected_grad}\")\n",
    "    print(f\"Expected Hessian:\\n{expected_hess}\")\n",
    "    print(f\"Gradient match: {np.allclose(first_deriv, expected_grad)}\")\n",
    "    print(f\"Hessian match: {np.allclose(hessian, expected_hess)}\")\n",
    "    \n",
    "    # Result 2.7.4 demonstration\n",
    "    print(\"\\n4. Result 2.7.4: ∂(α^T C β)/∂C = α β^T\")\n",
    "    alpha = np.array([1, 2, 3])\n",
    "    beta = np.array([4, 5])\n",
    "    result = vmd.derivative_alpha_C_beta_wrt_C(alpha, beta)\n",
    "    expected = np.outer(alpha, beta)\n",
    "    \n",
    "    print(f\"α = {alpha}\")\n",
    "    print(f\"β = {beta}\")\n",
    "    print(f\"∂(α^T C β)/∂C =\\n{result}\")\n",
    "    print(f\"α β^T =\\n{expected}\")\n",
    "    print(f\"Match: {np.allclose(result, expected)}\")\n",
    "    \n",
    "    # Regression example (Result 2.7.8)\n",
    "    print(\"\\n5. Result 2.7.8: Regression derivatives\")\n",
    "    np.random.seed(42)\n",
    "    n, k = 10, 3\n",
    "    X = np.random.randn(n, k)\n",
    "    y = np.random.randn(n)\n",
    "    beta = np.random.randn(k)\n",
    "    \n",
    "    first_deriv, hessian = vmd.regression_derivatives(y, X, beta)\n",
    "    \n",
    "    print(f\"Design matrix X shape: {X.shape}\")\n",
    "    print(f\"Response vector y shape: {y.shape}\")\n",
    "    print(f\"Parameter vector β shape: {beta.shape}\")\n",
    "    print(f\"First derivative shape: {first_deriv.shape}\")\n",
    "    print(f\"Hessian shape: {hessian.shape}\")\n",
    "    \n",
    "    # Check if Hessian is positive definite (should be for X^T X)\n",
    "    eigenvals = np.linalg.eigvals(hessian)\n",
    "    print(f\"Hessian eigenvalues: {eigenvals}\")\n",
    "    print(f\"Positive definite: {np.all(eigenvals > 0)}\")\n",
    "\n",
    "\n",
    "def symbolic_examples():\n",
    "    \"\"\"\n",
    "    Run symbolic differentiation examples\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Symbolic Differentiation Examples ===\\n\")\n",
    "    \n",
    "    sym_diff = SymbolicDifferentiation()\n",
    "    sym_diff.symbolic_gradient_example()\n",
    "    sym_diff.symbolic_quadratic_form()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run numerical examples\n",
    "    demonstrate_examples()\n",
    "    \n",
    "    # Run symbolic examples\n",
    "    try:\n",
    "        symbolic_examples()\n",
    "    except ImportError:\n",
    "        print(\"\\nSymPy not available for symbolic examples\")\n",
    "    \n",
    "    print(\"\\n=== Implementation Complete ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d965dd",
   "metadata": {},
   "source": [
    "# 2.8 Special Operations on Matrices\n",
    "\n",
    "This section covers important matrix operations including the Kronecker product, vectorization, and direct sum operations that are fundamental in linear algebra and statistical computing.\n",
    "\n",
    "## Definition 2.8.1: Kronecker Product of Matrices\n",
    "\n",
    "Let $\\mathbf{A} = \\{a_{ij}\\}$ be an $m \\times n$ matrix and $\\mathbf{B} = \\{b_{ij}\\}$ be a $p \\times q$ matrix. The **Kronecker product** of $\\mathbf{A}$ and $\\mathbf{B}$ is denoted by $\\mathbf{A} \\otimes \\mathbf{B}$ and is defined to be the $mp \\times nq$ matrix\n",
    "\n",
    "$$\n",
    "\\mathbf{A} \\otimes \\mathbf{B} = \\begin{pmatrix}\n",
    "a_{11}\\mathbf{B} & a_{12}\\mathbf{B} & \\cdots & a_{1n}\\mathbf{B} \\\\\n",
    "a_{21}\\mathbf{B} & a_{22}\\mathbf{B} & \\cdots & a_{2n}\\mathbf{B} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "a_{m1}\\mathbf{B} & a_{m2}\\mathbf{B} & \\cdots & a_{mn}\\mathbf{B}\n",
    "\\end{pmatrix} \\tag{2.8.1}\n",
    "$$\n",
    "\n",
    "The matrix in (2.8.1) is a partitioned matrix whose $(i,j)$th entry is a $p \\times q$ submatrix $a_{ij}\\mathbf{B}$. The Kronecker product $\\mathbf{A} \\otimes \\mathbf{B}$ can be defined regardless of the dimensions of $\\mathbf{A}$ and $\\mathbf{B}$. The Kronecker product is also referred to in the literature as the **direct product** or the **tensor product**.\n",
    "\n",
    "## Example 2.8.1\n",
    "\n",
    "Consider two matrices $\\mathbf{A}$ and $\\mathbf{B}$ where\n",
    "\n",
    "$$\n",
    "\\mathbf{A} = \\begin{pmatrix}\n",
    "3 & 4 \\\\\n",
    "2 & 0\n",
    "\\end{pmatrix}, \\quad \\text{and} \\quad \\mathbf{B} = \\begin{pmatrix}\n",
    "-1 & 5 & -1 \\\\\n",
    "0 & 3 & 3\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Then,\n",
    "\n",
    "$$\n",
    "\\mathbf{A} \\otimes \\mathbf{B} = \\begin{pmatrix}\n",
    "-3 & 15 & -3 & -4 & 20 & -4 \\\\\n",
    "0 & 9 & 9 & 0 & 12 & 12 \\\\\n",
    "-2 & 10 & -2 & 0 & 0 & 0 \\\\\n",
    "0 & 6 & 6 & 0 & 0 & 0\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\mathbf{B} \\otimes \\mathbf{A} = \\begin{pmatrix}\n",
    "-3 & -4 & 15 & 20 & -3 & -4 \\\\\n",
    "-2 & 0 & 10 & 0 & -2 & 0 \\\\\n",
    "0 & 0 & 9 & 12 & 9 & 12 \\\\\n",
    "0 & 0 & 6 & 0 & 6 & 0\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "In general, $\\mathbf{A} \\otimes \\mathbf{B}$ is not equal to $\\mathbf{B} \\otimes \\mathbf{A}$. The elements in these two products are the same, except that they are in different positions.\n",
    "\n",
    "The definition $\\mathbf{A} \\otimes \\mathbf{B}$ extends naturally to more than two matrices:\n",
    "\n",
    "$$\n",
    "\\mathbf{A} \\otimes \\mathbf{B} \\otimes \\mathbf{C} = \\mathbf{A} \\otimes (\\mathbf{B} \\otimes \\mathbf{C}) \\quad \\text{and} \\quad \\bigotimes_{i=1}^k \\mathbf{A}_i = \\mathbf{A}_1 \\otimes \\mathbf{A}_2 \\otimes \\cdots \\otimes \\mathbf{A}_k \\tag{2.8.2}\n",
    "$$\n",
    "\n",
    "## Result 2.8.1: Properties of Kronecker Product\n",
    "\n",
    "Let $\\mathbf{A}$ be an $m \\times n$ matrix. Then:\n",
    "\n",
    "1. For a positive scalar $c$, we have $c \\otimes \\mathbf{A} = \\mathbf{A} \\otimes c = c\\mathbf{A}$.\n",
    "\n",
    "2. For any diagonal matrix $\\mathbf{D} = \\text{diag}(d_1, \\ldots, d_k)$, $\\mathbf{D} \\otimes \\mathbf{A} = \\text{diag}(d_1\\mathbf{A}, \\ldots, d_k\\mathbf{A})$.\n",
    "\n",
    "3. $\\mathbf{I} \\otimes \\mathbf{A} = \\text{diag}(\\mathbf{A}, \\mathbf{A}, \\ldots, \\mathbf{A})$.\n",
    "\n",
    "4. $\\mathbf{I}_m \\otimes \\mathbf{I}_p = \\mathbf{I}_{mp}$.\n",
    "\n",
    "5. For a $p \\times q$ matrix $\\mathbf{B}$, we have $(\\mathbf{A} \\otimes \\mathbf{B})^T = \\mathbf{A}^T \\otimes \\mathbf{B}^T$.\n",
    "\n",
    "6. $(\\mathbf{A} \\otimes \\mathbf{B})(\\mathbf{C} \\otimes \\mathbf{D}) = (\\mathbf{A}\\mathbf{C}) \\otimes (\\mathbf{B}\\mathbf{D})$, where we assume that relevant matrices are conformal for multiplication.\n",
    "\n",
    "7. $\\text{rank}(\\mathbf{A} \\otimes \\mathbf{B}) = \\text{rank}(\\mathbf{A}) \\cdot \\text{rank}(\\mathbf{B})$.\n",
    "\n",
    "8. $(\\mathbf{A} + \\mathbf{B}) \\otimes (\\mathbf{C} + \\mathbf{D}) = (\\mathbf{A} \\otimes \\mathbf{C}) + (\\mathbf{A} \\otimes \\mathbf{D}) + (\\mathbf{B} \\otimes \\mathbf{C}) + (\\mathbf{B} \\otimes \\mathbf{D})$.\n",
    "\n",
    "9. Suppose $\\mathbf{A}$ is an $n \\times n$ matrix, and $\\mathbf{B}$ is an $m \\times m$ matrix. The $nm$ eigenvalues of $\\mathbf{A} \\otimes \\mathbf{B}$ are products of the $n$ eigenvalues $\\lambda_i, i = 1, \\ldots, n$ of $\\mathbf{A}$ and the $m$ eigenvalues $\\gamma_j, j = 1, \\ldots, m$ of $\\mathbf{B}$.\n",
    "\n",
    "10. $|\\mathbf{A} \\otimes \\mathbf{B}| = |\\mathbf{A}|^m |\\mathbf{B}|^n = \\left(\\prod_{i=1}^n \\lambda_i\\right)^m \\left(\\prod_{j=1}^m \\gamma_j\\right)^n$.\n",
    "\n",
    "11. Provided all the inverses exist, $(\\mathbf{A} \\otimes \\mathbf{B})^{-1} = \\mathbf{A}^{-1} \\otimes \\mathbf{B}^{-1}$.\n",
    "\n",
    "## Definition 2.8.2: Vectorization of Matrices\n",
    "\n",
    "Given an $m \\times n$ matrix $\\mathbf{A}$ with columns $\\mathbf{a}_1, \\ldots, \\mathbf{a}_n$, we define $\\text{vec}(\\mathbf{A}) = (\\mathbf{a}_1^T, \\ldots, \\mathbf{a}_n^T)^T$ to be an $mn$-dimensional column vector.\n",
    "\n",
    "## Result 2.8.2: Properties of the vec Operator\n",
    "\n",
    "1. Given $m \\times n$ matrices $\\mathbf{A}$ and $\\mathbf{B}$, $\\text{vec}(\\mathbf{A} + \\mathbf{B}) = \\text{vec}(\\mathbf{A}) + \\text{vec}(\\mathbf{B})$.\n",
    "\n",
    "2. If $\\mathbf{A}$, $\\mathbf{B}$ and $\\mathbf{C}$ are respectively $m \\times n$, $n \\times p$ and $p \\times q$ matrices, then:\n",
    "\n",
    "   (i) $\\text{vec}(\\mathbf{A}\\mathbf{B}) = (\\mathbf{I}_p \\otimes \\mathbf{A})\\text{vec}(\\mathbf{B}) = (\\mathbf{B}^T \\otimes \\mathbf{I}_m)\\text{vec}(\\mathbf{A})$\n",
    "\n",
    "   (ii) $\\text{vec}(\\mathbf{A}\\mathbf{B}\\mathbf{C}) = (\\mathbf{C}^T \\otimes \\mathbf{A})\\text{vec}(\\mathbf{B})$\n",
    "\n",
    "   (iii) $\\text{vec}(\\mathbf{A}\\mathbf{B}\\mathbf{C}) = (\\mathbf{I}_q \\otimes \\mathbf{A}\\mathbf{B})\\text{vec}(\\mathbf{C}) = (\\mathbf{C}^T\\mathbf{B}^T \\otimes \\mathbf{I}_n)\\text{vec}(\\mathbf{A})$\n",
    "\n",
    "3. If $\\mathbf{A}$ is $m \\times n$ and $\\mathbf{B}$ is $n \\times m$, \n",
    "   $$\\text{vec}(\\mathbf{B}^T)^T\\text{vec}(\\mathbf{A}) = \\text{vec}(\\mathbf{A}^T)^T\\text{vec}(\\mathbf{B}) = \\text{tr}(\\mathbf{A}\\mathbf{B})$$\n",
    "\n",
    "4. If $\\mathbf{A}$, $\\mathbf{B}$ and $\\mathbf{C}$ are respectively $m \\times n$, $n \\times p$ and $p \\times m$ matrices,\n",
    "   $$\\begin{align}\n",
    "   \\text{tr}(\\mathbf{A}\\mathbf{B}\\mathbf{C}) &= \\text{vec}(\\mathbf{A}^T)^T(\\mathbf{C}^T \\otimes \\mathbf{I}_n)\\text{vec}(\\mathbf{B}) \\\\\n",
    "   &= \\text{vec}(\\mathbf{A}^T)^T(\\mathbf{I}_m \\otimes \\mathbf{B})\\text{vec}(\\mathbf{C}) \\\\\n",
    "   &= \\text{vec}(\\mathbf{B}^T)^T(\\mathbf{A} \\otimes \\mathbf{I}_p)\\text{vec}(\\mathbf{C}) \\\\\n",
    "   &= \\text{vec}(\\mathbf{B}^T)^T(\\mathbf{I}_n \\otimes \\mathbf{C})\\text{vec}(\\mathbf{A}) \\\\\n",
    "   &= \\text{vec}(\\mathbf{C}^T)^T(\\mathbf{B}^T \\otimes \\mathbf{I}_m)\\text{vec}(\\mathbf{A}) \\\\\n",
    "   &= \\text{vec}(\\mathbf{C}^T)^T(\\mathbf{I}_p \\otimes \\mathbf{A})\\text{vec}(\\mathbf{B})\n",
    "   \\end{align}$$\n",
    "\n",
    "## Definition 2.8.3: Direct Sum of Matrices\n",
    "\n",
    "The **direct sum** of two matrices $\\mathbf{A}$ and $\\mathbf{B}$ (which can be of any dimension) is defined as\n",
    "\n",
    "$$\n",
    "\\mathbf{A} \\oplus \\mathbf{B} = \\begin{pmatrix}\n",
    "\\mathbf{A} & \\mathbf{O} \\\\\n",
    "\\mathbf{O} & \\mathbf{B}\n",
    "\\end{pmatrix} \\tag{2.8.3}\n",
    "$$\n",
    "\n",
    "This operation extends naturally to more than two matrices:\n",
    "\n",
    "$$\n",
    "\\bigoplus_{i=1}^k \\mathbf{A}_i = \\mathbf{A}_1 \\oplus \\mathbf{A}_2 \\oplus \\cdots \\oplus \\mathbf{A}_k = \\begin{pmatrix}\n",
    "\\mathbf{A}_1 & \\mathbf{O} & \\cdots & \\mathbf{O} \\\\\n",
    "\\mathbf{O} & \\mathbf{A}_2 & \\cdots & \\mathbf{O} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\mathbf{O} & \\mathbf{O} & \\cdots & \\mathbf{A}_k\n",
    "\\end{pmatrix} \\tag{2.8.4}\n",
    "$$\n",
    "\n",
    "This definition applies to vectors as well.\n",
    "\n",
    "---\n",
    "\n",
    "*These special matrix operations are fundamental tools in multivariate statistics, quantum mechanics, signal processing, and many other areas of applied mathematics. The Kronecker product is particularly important in the analysis of structured matrices and tensor operations, while vectorization provides a bridge between matrix operations and vector spaces.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ea3b28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
