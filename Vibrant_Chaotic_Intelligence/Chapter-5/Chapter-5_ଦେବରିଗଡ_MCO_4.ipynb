{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f551d779",
   "metadata": {},
   "source": [
    "'''\n",
    " * Copyright (c) 2008 Radhamadhab Dalai\n",
    " *\n",
    " * Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    " * of this software and associated documentation files (the \"Software\"), to deal\n",
    " * in the Software without restriction, including without limitation the rights\n",
    " * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    " * copies of the Software, and to permit persons to whom the Software is\n",
    " * furnished to do so, subject to the following conditions:\n",
    " *\n",
    " * The above copyright notice and this permission notice shall be included in\n",
    " * all copies or substantial portions of the Software.\n",
    " *\n",
    " * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    " * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    " * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    " * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    " * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    " * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
    " * THE SOFTWARE.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1833040e",
   "metadata": {},
   "source": [
    "## Monte Carlo EM Algorithm (MCEM)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "A common difficulty with the implementation of the EM algorithm is that each \"E-step\" requires the computation of the expected log-likelihood, $ Q(\\theta | \\theta^{(t)}, x) $. Wei and Tanner (1990a, b) propose the Monte Carlo EM (MCEM) approach to address this challenge. This method involves simulating $ Z_1, \\dots, Z_m $ from the conditional distribution $ p(z | x, \\theta) $ and then maximizing the approximate complete-data log-likelihood:\n",
    "\n",
    "$$\n",
    "Q_m(\\theta | x) = \\frac{1}{m} \\sum_{i=1}^m \\log L_c(\\theta | x, z_i),\n",
    "\\tag{5.20}\n",
    "$$\n",
    "\n",
    "where $ L_c(\\theta | x, z) $ represents the complete-data likelihood.\n",
    "\n",
    "As $ m \\to \\infty $, $ Q_m(\\theta | x) $ converges to $ Q(\\theta | \\theta^{(t)}, x) $, and the MCEM algorithm approaches the regular EM algorithm. The authors suggest increasing $ m $ along with the iterations. While maximizing a sum like $ Q_m(\\theta | x) $ can be complex, closed-form solutions are often available in exponential family settings.\n",
    "\n",
    "---\n",
    "\n",
    "## Example 5.20: MCEM for Censored Data\n",
    "\n",
    "The EM solution for censored data (Example 5.17) can easily transition into an MCEM solution. For the EM sequence:\n",
    "\n",
    "$$\n",
    "\\theta^{(t+1)} = \\frac{n_m}{T_m} + \\frac{1}{n_m} \\mathbb{E}_{\\theta^{(t)}}[Z_i | x],\n",
    "$$\n",
    "\n",
    "the MCEM solution replaces \\( \\mathbb{E}_{\\theta^{(t)}}[Z_i | x] \\) with:\n",
    "\n",
    "$$\n",
    "\\frac{1}{M} \\sum_{i=1}^M Z_i, \\quad Z_i \\sim p(z | x, \\theta^{(t)}).\n",
    "$$\n",
    "\n",
    "The MCEM sequence is shown in the right panel of Figure 5.6. While the convergence of MCEM is slower than EM, the variability is controlled by the choice of $ M $. Increasing $ M $ brings the sequences closer together.\n",
    "\n",
    "---\n",
    "\n",
    "## Example 21: Genetic Linkage\n",
    "\n",
    "A classic example of the EM algorithm is the genetics problem (see Rao 1973, Dempster et al. 1977, Tanner 1996), where observations $ (n_1, n_2, n_3, n_4) $ are drawn from the multinomial distribution:\n",
    "\n",
    "$$\n",
    "\\text{Multinomial}\\left(n; \\frac{\\theta}{2}, \\frac{\\theta}{2}, \\frac{1-\\theta}{2}, \\frac{1-\\theta}{2}\\right).\n",
    "$$\n",
    "\n",
    "Estimation becomes easier if the first cell is split into two cells, creating the augmented model:\n",
    "\n",
    "$$\n",
    "(n_{11}, n_{12}, n_2, n_3, n_4) \\sim \\text{Multinomial}\\left(n; \\frac{\\theta}{4}, \\frac{\\theta}{4}, \\frac{\\theta}{2}, \\frac{1-\\theta}{2}, \\frac{1-\\theta}{2}\\right).\n",
    "$$\n",
    "\n",
    "This augmentation allows for simpler computation of the expected log-likelihood and facilitates the iterative application of the EM algorithm.\n",
    "# Monte Carlo EM Algorithm (MCEM)\n",
    "\n",
    "## Genetic Linkage (Continued)\n",
    "\n",
    "The complete-data likelihood function is:\n",
    "\n",
    "$$\n",
    "L_c(\\theta | x, z) = \\theta^{z_1 + z_2} (1 - \\theta)^{z_3 + z_4},\n",
    "$$\n",
    "\n",
    "as opposed to the observed-data likelihood function:\n",
    "\n",
    "$$\n",
    "L(\\theta | x) = (\\theta / 2)^{z_1 + z_2} ((1 - \\theta) / 2)^{z_3 + z_4}.\n",
    "$$\n",
    "\n",
    "The expected complete log-likelihood function is:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{\\theta^{(t)}} \\left[ (Z_1 + Z_2) \\log \\theta + (Z_3 + Z_4) \\log (1 - \\theta) \\right].\n",
    "$$\n",
    "\n",
    "Simplifying:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{\\theta^{(t)}} \\left[ (Z_1 + Z_2) \\right] = \\frac{z_1 \\theta}{2 + \\theta}, \\quad \n",
    "\\mathbb{E}_{\\theta^{(t)}} \\left[ (Z_3 + Z_4) \\right] = z_3 + z_4,\n",
    "$$\n",
    "\n",
    "we have:\n",
    "\n",
    "$$\n",
    "Q(\\theta | \\theta^{(t)}) = \\frac{z_1 \\theta}{2 + \\theta} \\log \\theta + (z_3 + z_4) \\log (1 - \\theta).\n",
    "$$\n",
    "\n",
    "Maximizing this leads to:\n",
    "\n",
    "$$\n",
    "\\theta^{(t+1)} = \\frac{\\mathbb{E}_{\\theta^{(t)}}[Z_1 + Z_2] + z_3 + z_4}{z_1 + z_2 + z_3 + z_4}.\n",
    "$$\n",
    "\n",
    "If we instead use the Monte Carlo EM algorithm, $ \\mathbb{E}_{\\theta^{(t)}}[Z_1 + Z_2] $ is replaced with the average:\n",
    "\n",
    "$$\n",
    "\\bar{Z}_m = \\frac{1}{m} \\sum_{i=1}^m Z_i,\n",
    "$$\n",
    "\n",
    "where the $ Z_i $ are simulated from the binomial distribution $ B(z_1, \\theta^{(t)}/(2 + \\theta^{(t)})) $. The maximum becomes:\n",
    "\n",
    "$$\n",
    "\\theta^{(t+1)} = \\frac{\\bar{Z}_m + z_3 + z_4}{\\bar{Z}_m + z_1 + z_2 + z_3 + z_4}.\n",
    "$$\n",
    "\n",
    "This example illustrates the Monte Carlo EM algorithm, though the regular EM algorithm also applies.\n",
    "\n",
    "---\n",
    "\n",
    "## Example 22: Capture-Recapture Models Revisited\n",
    "\n",
    "A generalization of a capture-recapture model assumes that an animal $ i $, $ i = 1, 2, \\ldots, n $, can be captured at time $ j $, $ j = 1, 2, \\ldots, t $, in one of $ m $ locations. The location is a multinomial random variable:\n",
    "\n",
    "$$\n",
    "H \\sim \\text{Multinomial}(\\theta_1, \\ldots, \\theta_m).\n",
    "$$\n",
    "\n",
    "If an animal is at location $ k $, the random variable $ X \\sim B(p_k) $, where $ p_k $ is the probability of capturing the animal in location $ k $.\n",
    "\n",
    "### Example Realization\n",
    "\n",
    "For $ t = 6 $, a typical realization for an animal might be:\n",
    "\n",
    "$$\n",
    "h = (4, 1, -, 3, 3, -), \\quad x = (1, 1, 0, 1, 1, 0),\n",
    "$$\n",
    "\n",
    "where $ h $ represents the location (or lack thereof) and $ x $ represents whether the animal was captured at each time point.\n",
    "\n",
    "### Monte Carlo EM Application\n",
    "\n",
    "The MCEM algorithm is particularly useful here, as the expectation in the E-step is computationally intensive due to the latent variables $ h $. The MCEM replaces the expectation with simulated averages, enabling iterative updates for the parameters $ \\theta_k $ and $ p_k $. See Dupuis (1995) for details on the implementation.\n",
    "\n",
    "## Capture-Recapture Models and MCEM Algorithm\n",
    "\n",
    "## Problem Description\n",
    "\n",
    "In a capture-recapture model, $ h $ denotes the sequence of observed locations and non-captures for animals. This leads to a missing data problem. If all of $ h $ were observed, the maximum likelihood estimation (MLE) would be trivial, with the MLEs being the cell means.\n",
    "\n",
    "### Random Variables\n",
    "\n",
    "- $ X_{ijk} = 1 $ if animal $ i $ is captured at time $ j $ in location $ k $, and $ 0 $ otherwise.\n",
    "- $ Y_{ijk} = I(H_{ijk} = k)I(X_{ijk} = 1) $ (the observed data).\n",
    "- $ Z_{ijk} = I(H_{ijk} = k)I(X_{ijk} = 0) $ (the missing data).\n",
    "\n",
    "### Likelihood Function\n",
    "\n",
    "The likelihood function is given by:\n",
    "\n",
    "$$\n",
    "L(\\theta_1, \\dots, \\theta_m, p_1, \\dots, p_m | x) = \\sum_{z} L(\\theta_1, \\dots, \\theta_m, p_1, \\dots, p_m | Y, X, Z),\n",
    "$$\n",
    "\n",
    "where the complete-data likelihood is:\n",
    "\n",
    "$$\n",
    "L(\\theta_1, \\dots, \\theta_m, p_1, \\dots, p_m | Y, X, Z) = \\prod_{i=1}^n \\prod_{j=1}^t \\prod_{k=1}^m \\theta_k^{Y_{ijk} + Z_{ijk}} p_k^{Y_{ijk}} (1 - p_k)^{Z_{ijk}}.\n",
    "$$\n",
    "\n",
    "The sum over $ z $ represents the expectation over all the states \\( h \\) that could have been visited.\n",
    "\n",
    "---\n",
    "\n",
    "## MCEM Algorithm for Capture-Recapture\n",
    "\n",
    "Since directly computing the expectation in the E-step is complicated, we use an EM strategy combined with MCEM for the expectation calculation.\n",
    "\n",
    "### Algorithm A.21: Capture-Recapture MCEM Algorithm\n",
    "\n",
    "1. **M-step**  \n",
    "   Update the probabilities $ \\theta_k $ as:\n",
    "   $$\n",
    "   \\theta_k = \\frac{\\sum_{i=1}^n \\sum_{j=1}^t (Y_{ijk} + Z_{ijk})}{\\sum_{i=1}^n \\sum_{j=1}^t \\sum_{k=1}^m (Y_{ijk} + Z_{ijk})}.\n",
    "   $$\n",
    "\n",
    "2. **Monte Carlo E-step**  \n",
    "   If $ Z_{ijk} = 0 $, for $ i = 1, \\dots, n $, generate:\n",
    "   $$\n",
    "   Z_{ijk} \\sim \\text{Multinomial}(\\theta_k).\n",
    "   $$\n",
    "   Then calculate:\n",
    "   $$\n",
    "   \\theta_k = \\frac{\\sum_{i=1}^n \\sum_{j=1}^t Y_{ijk}}{n}.\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "## Notes and Insights\n",
    "\n",
    "- Scherrer (1997) examines the performance of generalized versions of this algorithm and demonstrates that they outperform the conditional likelihood approach of Brownie et al. (1993).\n",
    "- While the MCEM algorithm offers flexibility, it does not preserve the EM monotonicity property and may encounter smoothness issues when the sample size used for Monte Carlo approximations is small.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a91e888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/50\n",
      "  Theta: [0.05072054 0.33981762 0.60946184]\n",
      "  P: [0.00918635 0.00146908 0.00098294]\n",
      "\n",
      "Iteration 11/50\n",
      "  Theta: [0.05035444 0.33249576 0.6171498 ]\n",
      "  P: [0.00925314 0.00150143 0.00097069]\n",
      "\n",
      "Iteration 21/50\n",
      "  Theta: [0.0500882  0.33236263 0.61754917]\n",
      "  P: [0.00930233 0.00150203 0.00097006]\n",
      "\n",
      "Iteration 31/50\n",
      "  Theta: [0.05532998 0.33855293 0.60611708]\n",
      "  P: [0.00842105 0.00147456 0.00098836]\n",
      "\n",
      "Iteration 41/50\n",
      "  Theta: [0.05278397 0.33141412 0.61580191]\n",
      "  P: [0.00882724 0.00150633 0.00097282]\n",
      "\n",
      "Iteration 50/50\n",
      "  Theta: [0.05634506 0.33750458 0.60615036]\n",
      "  P: [0.00826934 0.00147914 0.00098831]\n",
      "\n",
      "Final parameter estimates:\n",
      "  Theta (location probabilities): [0.05634506 0.33750458 0.60615036]\n",
      "  P (capture probabilities): [0.00826934 0.00147914 0.00098831]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multinomial\n",
    "\n",
    "# Simulated data: observed captures (Y) and initial parameters\n",
    "n = 10  # Number of animals\n",
    "t = 6   # Number of time points\n",
    "m = 3   # Number of locations\n",
    "\n",
    "# Example observed data: random initialization for demonstration\n",
    "np.random.seed(42)\n",
    "Y = np.random.randint(0, 2, (n, t, m))  # Observed data matrix (n x t x m)\n",
    "\n",
    "# Initial parameter guesses\n",
    "theta = np.random.dirichlet(np.ones(m))  # Initial probabilities for locations\n",
    "p = np.random.uniform(0.5, 0.9, m)       # Initial capture probabilities\n",
    "\n",
    "# MCEM algorithm parameters\n",
    "iterations = 50\n",
    "sample_size = 1000\n",
    "\n",
    "def capture_recapture_mcem(Y, theta, p, iterations, sample_size):\n",
    "    n, t, m = Y.shape\n",
    "    \n",
    "    for iter in range(iterations):\n",
    "        # Step 1: Monte Carlo E-step\n",
    "        # Simulate missing data Z using multinomial distributions\n",
    "        Z = np.zeros_like(Y, dtype=float)\n",
    "        for i in range(n):\n",
    "            for j in range(t):\n",
    "                observed = Y[i, j, :]  # Observed captures\n",
    "                remaining_prob = theta * (1 - p)  # Probability of non-capture\n",
    "                norm_factor = np.sum(remaining_prob)\n",
    "                if norm_factor > 0:\n",
    "                    Z[i, j, :] = multinomial.rvs(sample_size, remaining_prob / norm_factor)\n",
    "        \n",
    "        # Step 2: M-step\n",
    "        # Update theta and p based on the complete data (Y + Z)\n",
    "        complete_data = Y + Z\n",
    "        theta = np.sum(complete_data, axis=(0, 1)) / np.sum(complete_data)\n",
    "        p = np.sum(Y, axis=(0, 1)) / np.sum(complete_data, axis=(0, 1))\n",
    "        \n",
    "        # Print progress\n",
    "        if iter % 10 == 0 or iter == iterations - 1:\n",
    "            print(f\"Iteration {iter + 1}/{iterations}\")\n",
    "            print(f\"  Theta: {theta}\")\n",
    "            print(f\"  P: {p}\\n\")\n",
    "    \n",
    "    return theta, p\n",
    "\n",
    "# Run the algorithm\n",
    "final_theta, final_p = capture_recapture_mcem(Y, theta, p, iterations, sample_size)\n",
    "\n",
    "print(\"Final parameter estimates:\")\n",
    "print(\"  Theta (location probabilities):\", final_theta)\n",
    "print(\"  P (capture probabilities):\", final_p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86eadb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/50\n",
      "  Theta: [0.3332944955808186, 0.3329449558081859, 0.3337605486109955]\n",
      "  P: [0.0011985617259288853, 0.0014997750337449383, 0.0012467584280869738]\n",
      "\n",
      "Iteration 11/50\n",
      "  Theta: [0.3326120607866309, 0.33264535028878645, 0.33474258892458264]\n",
      "  P: [0.001201020867737577, 0.0015011258443832875, 0.001243100790612103]\n",
      "\n",
      "Iteration 21/50\n",
      "  Theta: [0.3336107458512958, 0.3329449558081859, 0.3334442983405183]\n",
      "  P: [0.001197425535099536, 0.0014997750337449383, 0.0012479408975190935]\n",
      "\n",
      "Iteration 31/50\n",
      "  Theta: [0.33698963032007856, 0.32716922718420743, 0.33584114249571395]\n",
      "  P: [0.0011854193420922652, 0.0015262515262515263, 0.0012390345442830947]\n",
      "\n",
      "Iteration 41/50\n",
      "  Theta: [0.34411358378135454, 0.32647014763894205, 0.3294162685797034]\n",
      "  P: [0.0011608783979878108, 0.0015295197308045274, 0.0012632004446465565]\n",
      "\n",
      "Iteration 50/50\n",
      "  Theta: [0.3520697747965179, 0.32696949017127447, 0.32096073503220757]\n",
      "  P: [0.0011346444780635401, 0.0015271838729383018, 0.001296478763677851]\n",
      "\n",
      "Final parameter estimates:\n",
      "  Theta (location probabilities): [0.3520697747965179, 0.32696949017127447, 0.32096073503220757]\n",
      "  P (capture probabilities): [0.0011346444780635401, 0.0015271838729383018, 0.001296478763677851]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Simulated data: observed captures (Y) and initial parameters\n",
    "n = 10  # Number of animals\n",
    "t = 6   # Number of time points\n",
    "m = 3   # Number of locations\n",
    "\n",
    "# Example observed data: random initialization for demonstration\n",
    "random.seed(42)\n",
    "Y = [[[random.randint(0, 1) for _ in range(m)] for _ in range(t)] for _ in range(n)]\n",
    "\n",
    "# Initial parameter guesses\n",
    "theta = [1.0 / m] * m  # Equal probabilities for locations\n",
    "p = [0.7 for _ in range(m)]  # Initial capture probabilities\n",
    "\n",
    "# MCEM algorithm parameters\n",
    "iterations = 50\n",
    "sample_size = 1000\n",
    "\n",
    "def multinomial_sample(probs, size):\n",
    "    \"\"\"Generate a multinomial sample.\"\"\"\n",
    "    total = sum(probs)\n",
    "    normalized_probs = [p / total for p in probs]\n",
    "    counts = [0] * len(probs)\n",
    "    \n",
    "    for _ in range(size):\n",
    "        r = random.random()\n",
    "        cumulative = 0\n",
    "        for i, prob in enumerate(normalized_probs):\n",
    "            cumulative += prob\n",
    "            if r <= cumulative:\n",
    "                counts[i] += 1\n",
    "                break\n",
    "    \n",
    "    return counts\n",
    "\n",
    "def capture_recapture_mcem(Y, theta, p, iterations, sample_size):\n",
    "    n, t, m = len(Y), len(Y[0]), len(Y[0][0])\n",
    "    \n",
    "    for iter in range(iterations):\n",
    "        # Step 1: Monte Carlo E-step\n",
    "        # Simulate missing data Z using multinomial distributions\n",
    "        Z = [[[0] * m for _ in range(t)] for _ in range(n)]\n",
    "        \n",
    "        for i in range(n):\n",
    "            for j in range(t):\n",
    "                observed = Y[i][j]\n",
    "                remaining_prob = [(theta[k] * (1 - p[k])) for k in range(m)]\n",
    "                norm_factor = sum(remaining_prob)\n",
    "                \n",
    "                if norm_factor > 0:\n",
    "                    Z[i][j] = multinomial_sample(remaining_prob, sample_size)\n",
    "        \n",
    "        # Step 2: M-step\n",
    "        # Update theta and p based on the complete data (Y + Z)\n",
    "        complete_data = [[[Y[i][j][k] + Z[i][j][k] for k in range(m)] for j in range(t)] for i in range(n)]\n",
    "        \n",
    "        # Update theta\n",
    "        theta = [0] * m\n",
    "        total_complete = sum(sum(sum(row) for row in animal) for animal in complete_data)\n",
    "        for k in range(m):\n",
    "            theta[k] = sum(complete_data[i][j][k] for i in range(n) for j in range(t)) / total_complete\n",
    "        \n",
    "        # Update p\n",
    "        p = [0] * m\n",
    "        for k in range(m):\n",
    "            observed_captures = sum(Y[i][j][k] for i in range(n) for j in range(t))\n",
    "            total_complete_k = sum(complete_data[i][j][k] for i in range(n) for j in range(t))\n",
    "            p[k] = observed_captures / total_complete_k if total_complete_k > 0 else 0\n",
    "        \n",
    "        # Print progress\n",
    "        if iter % 10 == 0 or iter == iterations - 1:\n",
    "            print(f\"Iteration {iter + 1}/{iterations}\")\n",
    "            print(f\"  Theta: {theta}\")\n",
    "            print(f\"  P: {p}\\n\")\n",
    "    \n",
    "    return theta, p\n",
    "\n",
    "# Run the algorithm\n",
    "final_theta, final_p = capture_recapture_mcem(Y, theta, p, iterations, sample_size)\n",
    "\n",
    "print(\"Final parameter estimates:\")\n",
    "print(\"  Theta (location probabilities):\", final_theta)\n",
    "print(\"  P (capture probabilities):\", final_p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1751bbae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/50\n",
      "  Theta: [0.3332944955808186, 0.3329449558081859, 0.3337605486109955]\n",
      "  P: [0.0011985617259288853, 0.0014997750337449383, 0.0012467584280869738]\n",
      "\n",
      "Iteration 11/50\n",
      "  Theta: [0.3326120607866309, 0.33264535028878645, 0.33474258892458264]\n",
      "  P: [0.001201020867737577, 0.0015011258443832875, 0.001243100790612103]\n",
      "\n",
      "Iteration 21/50\n",
      "  Theta: [0.3336107458512958, 0.3329449558081859, 0.3334442983405183]\n",
      "  P: [0.001197425535099536, 0.0014997750337449383, 0.0012479408975190935]\n",
      "\n",
      "Iteration 31/50\n",
      "  Theta: [0.33698963032007856, 0.32716922718420743, 0.33584114249571395]\n",
      "  P: [0.0011854193420922652, 0.0015262515262515263, 0.0012390345442830947]\n",
      "\n",
      "Iteration 41/50\n",
      "  Theta: [0.34411358378135454, 0.32647014763894205, 0.3294162685797034]\n",
      "  P: [0.0011608783979878108, 0.0015295197308045274, 0.0012632004446465565]\n",
      "\n",
      "Iteration 50/50\n",
      "  Theta: [0.3520697747965179, 0.32696949017127447, 0.32096073503220757]\n",
      "  P: [0.0011346444780635401, 0.0015271838729383018, 0.001296478763677851]\n",
      "\n",
      "\n",
      "Theta over iterations:\n",
      "  Parameter 1:\n",
      "                                                 *\n",
      "                                              *** \n",
      "                                       *   ***    \n",
      "                                        ***       \n",
      "                                     **           \n",
      "                      **** ******* **             \n",
      " *   *   * *  ** ** **    *       *               \n",
      "* *** * * * **  *  *                              \n",
      "       *                                          \n",
      "                                                  \n",
      "                                                  \n",
      "--------------------------------------------------\n",
      "  Parameter 2:\n",
      "                                                  \n",
      "                                                  \n",
      "                                                  \n",
      "                                                  \n",
      "                                                  \n",
      "             * ***                                \n",
      "    *      ** *   **                              \n",
      "**** ******         ****                          \n",
      "                        *** *  *******   ** **  * \n",
      "                           * **       ***  *  ** *\n",
      "                                                  \n",
      "--------------------------------------------------\n",
      "  Parameter 3:\n",
      "                                                  \n",
      "                                                  \n",
      "                                                  \n",
      "                                                  \n",
      "                                                  \n",
      "       *                     *                    \n",
      "* ***** ***         *    **** *  ****             \n",
      " *          **    ** ****      **    **           \n",
      "           *  ****                     **         \n",
      "                                         *****    \n",
      "                                              ****\n",
      "--------------------------------------------------\n",
      "\n",
      "P over iterations:\n",
      "  Parameter 1:\n",
      "                                                  \n",
      "                                                  \n",
      "                                                  \n",
      "                                                  \n",
      "                                                  \n",
      "                                                  \n",
      "                                                  \n",
      "                                                  \n",
      "                                                  \n",
      "*************************************             \n",
      "                                     *************\n",
      "--------------------------------------------------\n",
      "  Parameter 2:\n",
      "                                               *  \n",
      "**** ******         *************************** **\n",
      "    *      *********                              \n",
      "                                                  \n",
      "                                                  \n",
      "                                                  \n",
      "                                                  \n",
      "                                                  \n",
      "                                                  \n",
      "                                                  \n",
      "                                                  \n",
      "--------------------------------------------------\n",
      "  Parameter 3:\n",
      "                                                  \n",
      "                                                  \n",
      "                                                  \n",
      "                                                  \n",
      "                                                  \n",
      "                                                  \n",
      "                                                **\n",
      "           *  *****   **              **********  \n",
      "*********** **     ***  **************            \n",
      "                                                  \n",
      "                                                  \n",
      "--------------------------------------------------\n",
      "Final parameter estimates:\n",
      "  Theta (location probabilities): [0.3520697747965179, 0.32696949017127447, 0.32096073503220757]\n",
      "  P (capture probabilities): [0.0011346444780635401, 0.0015271838729383018, 0.001296478763677851]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Simulated data: observed captures (Y) and initial parameters\n",
    "n = 10  # Number of animals\n",
    "t = 6   # Number of time points\n",
    "m = 3   # Number of locations\n",
    "\n",
    "# Example observed data: random initialization for demonstration\n",
    "random.seed(42)\n",
    "Y = [[[random.randint(0, 1) for _ in range(m)] for _ in range(t)] for _ in range(n)]\n",
    "\n",
    "# Initial parameter guesses\n",
    "theta = [1.0 / m] * m  # Equal probabilities for locations\n",
    "p = [0.7 for _ in range(m)]  # Initial capture probabilities\n",
    "\n",
    "# MCEM algorithm parameters\n",
    "iterations = 50\n",
    "sample_size = 1000\n",
    "\n",
    "# To store parameter evolution\n",
    "theta_history = [[] for _ in range(m)]\n",
    "p_history = [[] for _ in range(m)]\n",
    "\n",
    "def multinomial_sample(probs, size):\n",
    "    \"\"\"Generate a multinomial sample.\"\"\"\n",
    "    total = sum(probs)\n",
    "    normalized_probs = [p / total for p in probs]\n",
    "    counts = [0] * len(probs)\n",
    "    \n",
    "    for _ in range(size):\n",
    "        r = random.random()\n",
    "        cumulative = 0\n",
    "        for i, prob in enumerate(normalized_probs):\n",
    "            cumulative += prob\n",
    "            if r <= cumulative:\n",
    "                counts[i] += 1\n",
    "                break\n",
    "    \n",
    "    return counts\n",
    "\n",
    "def capture_recapture_mcem(Y, theta, p, iterations, sample_size):\n",
    "    n, t, m = len(Y), len(Y[0]), len(Y[0][0])\n",
    "    \n",
    "    for iter in range(iterations):\n",
    "        # Step 1: Monte Carlo E-step\n",
    "        Z = [[[0] * m for _ in range(t)] for _ in range(n)]\n",
    "        \n",
    "        for i in range(n):\n",
    "            for j in range(t):\n",
    "                observed = Y[i][j]\n",
    "                remaining_prob = [(theta[k] * (1 - p[k])) for k in range(m)]\n",
    "                norm_factor = sum(remaining_prob)\n",
    "                \n",
    "                if norm_factor > 0:\n",
    "                    Z[i][j] = multinomial_sample(remaining_prob, sample_size)\n",
    "        \n",
    "        # Step 2: M-step\n",
    "        complete_data = [[[Y[i][j][k] + Z[i][j][k] for k in range(m)] for j in range(t)] for i in range(n)]\n",
    "        \n",
    "        # Update theta\n",
    "        theta = [0] * m\n",
    "        total_complete = sum(sum(sum(row) for row in animal) for animal in complete_data)\n",
    "        for k in range(m):\n",
    "            theta[k] = sum(complete_data[i][j][k] for i in range(n) for j in range(t)) / total_complete\n",
    "        \n",
    "        # Update p\n",
    "        p = [0] * m\n",
    "        for k in range(m):\n",
    "            observed_captures = sum(Y[i][j][k] for i in range(n) for j in range(t))\n",
    "            total_complete_k = sum(complete_data[i][j][k] for i in range(n) for j in range(t))\n",
    "            p[k] = observed_captures / total_complete_k if total_complete_k > 0 else 0\n",
    "        \n",
    "        # Save history for plotting\n",
    "        for k in range(m):\n",
    "            theta_history[k].append(theta[k])\n",
    "            p_history[k].append(p[k])\n",
    "        \n",
    "        # Print progress\n",
    "        if iter % 10 == 0 or iter == iterations - 1:\n",
    "            print(f\"Iteration {iter + 1}/{iterations}\")\n",
    "            print(f\"  Theta: {theta}\")\n",
    "            print(f\"  P: {p}\\n\")\n",
    "    \n",
    "    return theta, p\n",
    "\n",
    "# Run the algorithm\n",
    "final_theta, final_p = capture_recapture_mcem(Y, theta, p, iterations, sample_size)\n",
    "\n",
    "# Function to plot ASCII graphs\n",
    "def plot_ascii(data, label, iterations):\n",
    "    max_value = max(max(history) for history in data)\n",
    "    min_value = min(min(history) for history in data)\n",
    "    height = 10\n",
    "    width = iterations\n",
    "    scale = height / (max_value - min_value) if max_value != min_value else 1\n",
    "    \n",
    "    print(f\"\\n{label} over iterations:\")\n",
    "    for k, history in enumerate(data):\n",
    "        print(f\"  Parameter {k + 1}:\")\n",
    "        for y in range(height, -1, -1):\n",
    "            line = \"\"\n",
    "            for x in range(width):\n",
    "                value = (history[x] - min_value) * scale\n",
    "                line += \"*\" if int(value) == y else \" \"\n",
    "            print(line)\n",
    "        print(\"-\" * width)\n",
    "\n",
    "# Plot results\n",
    "plot_ascii(theta_history, \"Theta\", iterations)\n",
    "plot_ascii(p_history, \"P\", iterations)\n",
    "\n",
    "print(\"Final parameter estimates:\")\n",
    "print(\"  Theta (location probabilities):\", final_theta)\n",
    "print(\"  P (capture probabilities):\", final_p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f71e89d",
   "metadata": {},
   "source": [
    "### EM Standard Errors\n",
    "\n",
    "There are various algorithms and formulas available for obtaining standard errors from the EM algorithm (see Tanner, 1996, for a selection). However, the formulation by Oakes (1999) and its Monte Carlo version seem both simple and useful.\n",
    "\n",
    "Recall that the variance of the MLE, \\( \\hat{\\theta} \\), is approximated by:\n",
    "\n",
    "\\[\n",
    "\\text{Var}(\\hat{\\theta}) \\approx -\\mathbb{E}\\left[\\frac{\\partial^2}{\\partial \\theta^2} \\log L(\\theta \\mid x)\\right].\n",
    "\\]\n",
    "\n",
    "Oakes (1999) shows that this second derivative can be expressed in terms of the complete-data likelihood:\n",
    "\n",
    "\\[\n",
    "\\frac{\\partial^2}{\\partial \\theta^2} \\log L(\\theta \\mid x) = \n",
    "\\mathbb{E} \\left[\\frac{\\partial^2}{\\partial \\theta^2} \\log L(\\theta \\mid x, z)\\right] +\n",
    "\\text{Var}\\left[\\frac{\\partial}{\\partial \\theta} \\log L(\\theta \\mid x, z)\\right],\n",
    "\\]\n",
    "\n",
    "where the expectation is taken with respect to the distribution of the missing data. Thus, for the EM algorithm, we have a formula for the variance of the MLE.\n",
    "\n",
    "---\n",
    "\n",
    "### Advantages and Challenges\n",
    "\n",
    "The advantage of this expression is that it only involves the distribution of the complete data, which is often a reasonable distribution to work with. However, the mixed derivative may be difficult to compute. In terms of complexity, it compares favorably with other methods.\n",
    "\n",
    "---\n",
    "\n",
    "### Monte Carlo EM and Oakes' Identity\n",
    "\n",
    "For the Monte Carlo EM algorithm, the expression above cannot be used in its current form, as we want all expectations to be on the outside. By taking the derivative inside the expectation, Oakes' identity can be rewritten as:\n",
    "\n",
    "\\[\n",
    "\\frac{\\partial^2}{\\partial \\theta^2} \\log L(\\theta \\mid x) =\n",
    "\\mathbb{E}\\left[\\frac{\\partial^2}{\\partial \\theta^2} \\log L(\\theta \\mid x, z)\\right] -\n",
    "\\mathbb{E}\\left[\\left(\\frac{\\partial}{\\partial \\theta} \\log L(\\theta \\mid x, z)\\right)^2\\right] +\n",
    "\\mathbb{E}\\left[\\left(\\frac{\\partial}{\\partial \\theta} \\log L(\\theta \\mid x, z)\\right)\\right]^2.\n",
    "\\]\n",
    "\n",
    "This formulation is better suited for simulation since all expectations are now on the outside.\n",
    "\n",
    "---\n",
    "\n",
    "### Simplified Representation\n",
    "\n",
    "Equation (5.22) can be expressed in the rather pleasing form:\n",
    "\n",
    "\\[\n",
    "\\frac{\\partial^2}{\\partial \\theta^2} \\log L(\\theta \\mid x) =\n",
    "\\mathbb{E}\\left[\\frac{\\partial^2}{\\partial \\theta^2} \\log L(\\theta \\mid x, z)\\right] -\n",
    "\\text{Var}\\left[\\frac{\\partial}{\\partial \\theta} \\log L(\\theta \\mid x, z)\\right].\n",
    "\\]\n",
    "\n",
    "This expression facilitates the computation of standard errors in Monte Carlo EM methods while leveraging the distribution of the complete data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69894114",
   "metadata": {},
   "source": [
    "### Genetic Linkage Standard Errors\n",
    "\n",
    "The second derivative of the log-likelihood, as derived in the EM framework, can be expressed as:\n",
    "\n",
    "\\[\n",
    "\\frac{\\partial^2}{\\partial \\theta^2} \\log L(x) = \n",
    "\\mathbb{E}\\left[\\frac{\\partial^2}{\\partial \\theta^2} \\log L(\\theta \\mid x, z)\\right] + \n",
    "\\text{Var}\\left[\\frac{\\partial}{\\partial \\theta} \\log L(\\theta \\mid x, z)\\right] - \n",
    "\\text{Var}\\left(\\log L(\\theta \\mid x, z)\\right).\n",
    "\\]\n",
    "\n",
    "For Monte Carlo evaluation, this becomes:\n",
    "\n",
    "\\[\n",
    "\\frac{\\partial^2}{\\partial \\theta^2} \\log L(x) \\approx \n",
    "\\frac{1}{M} \\sum_{j=1}^M \\frac{\\partial^2}{\\partial \\theta^2} \\log L(\\theta \\mid x, z_j) - \n",
    "\\frac{1}{M} \\left[\\sum_{j=1}^M \\frac{\\partial}{\\partial \\theta} \\log L(\\theta \\mid x, z_j)\\right]^2 +\n",
    "\\frac{1}{M} \\sum_{j=1}^M \\left[\\frac{\\partial}{\\partial \\theta} \\log L(\\theta \\mid x, z_j)\\right]^2,\n",
    "\\]\n",
    "\n",
    "where \\( z_j \\), \\( j = 1, \\dots, M \\), are generated from the missing data distribution and have already been generated to perform MCEM.\n",
    "\n",
    "---\n",
    "\n",
    "### Example 5.23: Genetic Linkage Standard Errors\n",
    "\n",
    "For Example 5.21, the complete-data likelihood is given by:\n",
    "\n",
    "\\[\n",
    "L(x) = \\theta^{x_1 + x_2} (1 - \\theta)^{x_3 + x_4},\n",
    "\\]\n",
    "\n",
    "and applying the variance formula (5.22), we obtain:\n",
    "\n",
    "\\[\n",
    "\\frac{\\partial^2}{\\partial \\theta^2} \\log L(x) = \n",
    "\\frac{-x_1}{\\theta^2} + \\frac{-x_2}{(1 - \\theta)^2} + \\frac{(x_3 + x_4)A^2}{\\theta(1 - \\theta)},\n",
    "\\]\n",
    "\n",
    "where \\( A = \\mathbb{E}\\left[\\frac{\\theta}{\\theta + 1}\\right] \\). \n",
    "\n",
    "---\n",
    "\n",
    "### Practical Evaluation\n",
    "\n",
    "In practice, we evaluate the expectation at the converged value of the likelihood estimator. For these data, we obtain:\n",
    "\n",
    "\\[\n",
    "\\hat{\\theta} = 0.627 \\quad \\text{with a standard deviation of } 0.048.\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### Visualization\n",
    "\n",
    "The figure below illustrates the evolution of the EM estimate and its associated standard error over 25 iterations.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# Pseudo-code for plotting the graph in Jupyter Notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "iterations = list(range(1, 26))\n",
    "theta_estimates = [0.6 + 0.027 * (i / 25) for i in iterations]  # Simulated EM sequence\n",
    "std_deviation = [0.05 - 0.002 * (i / 25) for i in iterations]  # Simulated standard deviation sequence\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(iterations, theta_estimates, label=\"EM Estimate (θ)\", color=\"blue\")\n",
    "plt.fill_between(iterations, \n",
    "                 [t - s for t, s in zip(theta_estimates, std_deviation)], \n",
    "                 [t + s for t, s in zip(theta_estimates, std_deviation)], \n",
    "                 color=\"blue\", alpha=0.2, label=\"± 1 Standard Deviation\")\n",
    "\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Estimate\")\n",
    "plt.title(\"EM Sequence and Standard Deviation for Genetic Linkage Data\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9481826d",
   "metadata": {},
   "source": [
    "# 5.5 Notes\n",
    "\n",
    "## 5.5.1 Variations on EM\n",
    "\n",
    "The EM algorithm is a powerful optimization tool, but it has its limitations. Notably:\n",
    "\n",
    "1. **Difficult Computation in the E-step**: \n",
    "   - As discussed in Section 5.3.3, the E-step can become computationally challenging for complex likelihoods.\n",
    "2. **Multimodal Likelihoods**: \n",
    "   - EM converges to the maximum likelihood estimator for unimodal likelihoods but may get trapped in local maxima for multimodal likelihoods, making the results dependent on initial conditions.\n",
    "\n",
    "### Overcoming Initial Condition Dependence\n",
    "\n",
    "Several variations of EM have been proposed to address these issues. Below, we describe some key methods:\n",
    "\n",
    "---\n",
    "\n",
    "### **Stochastic EM (SEM)**\n",
    "\n",
    "Broniatowski et al. (1984) and Celeux and Diebolt (1985, 1992) introduced the **Stochastic EM (SEM)** algorithm, which modifies the standard EM algorithm:\n",
    "\n",
    "1. **Step 1**: Instead of computing the expected value, simulate the missing data \\( z \\) conditionally on the observations \\( x \\) and the current parameter value \\( \\theta^{(m)} \\).\n",
    "2. **Step 2**: Maximize the complete-data log-likelihood \\( H(x, z_m \\mid \\theta) \\).\n",
    "\n",
    "This stochastic step allows SEM to explore the likelihood surface more systematically, reducing its susceptibility to being trapped in local modes.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Convergence Challenges**\n",
    "\n",
    "- **Ergodicity**: The Markov chain \\( \\theta^{(m)} \\) generated by SEM is often ergodic, but the relationship between the stationary distribution and the maxima of the observed likelihood is rarely clear.\n",
    "- **Ergodic Average**: \n",
    "  \\[\n",
    "  \\bar{\\theta}^{(M)} = \\frac{1}{M} \\sum_{m=1}^M \\theta^{(m)},\n",
    "  \\]\n",
    "  is studied instead of the global mode:\n",
    "  \\[\n",
    "  \\hat{\\theta}^{(M)} = \\arg \\max_\\theta L(\\theta).\n",
    "  \\]\n",
    "\n",
    "---\n",
    "\n",
    "### **Simulated Annealing EM (SAEM)**\n",
    "\n",
    "To address the convergence problem in SEM, Celeux and Diebolt (1990) introduced **SAEM**:\n",
    "\n",
    "- Gradually reduce the randomness in the simulations during iterations.\n",
    "- End with a standard EM algorithm.\n",
    "\n",
    "SAEM connects to **Simulated Annealing** methods (see Section 5.2.3) and achieves convergence to the global mode under specific conditions.\n",
    "\n",
    "---\n",
    "\n",
    "### **Hybrid Approaches**\n",
    "\n",
    "1. **Celeux et al. (1996)**:\n",
    "   - Combine SEM and EM by using SEM to generate the starting point for EM.\n",
    "   \n",
    "2. **Lavielle and Moulines (1997)**:\n",
    "   - Similar to Celeux and Diebolt (1990), they propose convergence conditions equivalent to EM.\n",
    "\n",
    "3. **SAME Algorithm** (Doucet et al., 2002):\n",
    "   - **State Augmentation for Marginal Estimation (SAME)**:\n",
    "     - Integrates ideas from Sections 5.2.4 and 5.3.1 within an MCMC framework.\n",
    "\n",
    "---\n",
    "\n",
    "### **Expectation Conditional Maximization (ECM)**\n",
    "\n",
    "Meng and Rubin (1991, 1992), Liu and Rubin (1994), and Meng and van Dyk (1997) developed **ECM**, which takes advantage of Gibbs sampling:\n",
    "\n",
    "- Maximizes the complete-data likelihood along successive directions (conditional likelihoods).\n",
    "- This approach reduces the computational complexity of the E-step by focusing on subsets of parameters.\n",
    "\n",
    "---\n",
    "\n",
    "These methods enhance the robustness and flexibility of the EM algorithm, enabling it to handle more complex likelihood surfaces and data structures.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c594692",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot convert float NaN to integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_18082/3604792651.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Simulate missingness (randomly mask some data points as missing)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mmissingness\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_data\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.3\u001b[0m  \u001b[0;31m# 30% missing data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mobserved_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissingness\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m  \u001b[0;31m# Mark missing data as NaN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Step 2: Define the E-step (simulate missing data using conditional distribution)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot convert float NaN to integer"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Generate synthetic data (observed + missing values)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Assume we have 100 data points and a parameter theta\n",
    "theta_true = 0.7\n",
    "n_data = 100\n",
    "\n",
    "# Simulate data (binary outcome)\n",
    "observed_data = np.random.binomial(1, theta_true, n_data)\n",
    "\n",
    "# Simulate missingness (randomly mask some data points as missing)\n",
    "missingness = np.random.rand(n_data) < 0.3  # 30% missing data\n",
    "observed_data[missingness] = np.nan  # Mark missing data as NaN\n",
    "\n",
    "# Step 2: Define the E-step (simulate missing data using conditional distribution)\n",
    "def e_step(observed_data, theta):\n",
    "    \"\"\" Simulate missing data using conditional distribution. \"\"\"\n",
    "    simulated_data = np.copy(observed_data)\n",
    "    missing_indices = np.isnan(observed_data)\n",
    "    \n",
    "    # Simulate missing data (conditional on theta)\n",
    "    simulated_data[missing_indices] = np.random.binomial(1, theta, np.sum(missing_indices))\n",
    "    \n",
    "    return simulated_data\n",
    "\n",
    "# Step 3: Define the M-step (maximize the likelihood)\n",
    "def m_step(completed_data):\n",
    "    \"\"\" Maximize the likelihood to estimate theta. \"\"\"\n",
    "    # Maximum likelihood estimate for a binomial distribution: the sample mean\n",
    "    theta_estimate = np.nanmean(completed_data)  # Ignore NaNs when calculating the mean\n",
    "    return theta_estimate\n",
    "\n",
    "# Step 4: Run the SEM algorithm\n",
    "def sem_algorithm(observed_data, max_iter=100, theta_init=0.5):\n",
    "    theta = theta_init\n",
    "    theta_history = [theta]  # Keep track of theta estimates for plotting\n",
    "    \n",
    "    for iteration in range(max_iter):\n",
    "        # E-step: simulate missing data\n",
    "        simulated_data = e_step(observed_data, theta)\n",
    "        \n",
    "        # M-step: maximize the likelihood (estimate theta)\n",
    "        theta = m_step(simulated_data)\n",
    "        \n",
    "        # Track the theta estimates\n",
    "        theta_history.append(theta)\n",
    "    \n",
    "    return theta_history\n",
    "\n",
    "# Run the SEM algorithm\n",
    "theta_estimates = sem_algorithm(observed_data, max_iter=100)\n",
    "\n",
    "# Plot the convergence of theta estimates\n",
    "plt.plot(theta_estimates, label='Theta Estimates')\n",
    "plt.axhline(y=theta_true, color='r', linestyle='--', label='True Theta')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Theta Estimate')\n",
    "plt.legend()\n",
    "plt.title('Convergence of Stochastic EM Algorithm')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62c0889f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Theta Estimate = 0.5\n",
      "Iteration 1: Theta Estimate = 0.65\n",
      "Iteration 2: Theta Estimate = 0.71\n",
      "Iteration 3: Theta Estimate = 0.75\n",
      "Iteration 4: Theta Estimate = 0.65\n",
      "Iteration 5: Theta Estimate = 0.73\n",
      "Iteration 6: Theta Estimate = 0.79\n",
      "Iteration 7: Theta Estimate = 0.79\n",
      "Iteration 8: Theta Estimate = 0.78\n",
      "Iteration 9: Theta Estimate = 0.8\n",
      "Iteration 10: Theta Estimate = 0.81\n",
      "Iteration 11: Theta Estimate = 0.74\n",
      "Iteration 12: Theta Estimate = 0.74\n",
      "Iteration 13: Theta Estimate = 0.8\n",
      "Iteration 14: Theta Estimate = 0.77\n",
      "Iteration 15: Theta Estimate = 0.78\n",
      "Iteration 16: Theta Estimate = 0.77\n",
      "Iteration 17: Theta Estimate = 0.76\n",
      "Iteration 18: Theta Estimate = 0.74\n",
      "Iteration 19: Theta Estimate = 0.73\n",
      "Iteration 20: Theta Estimate = 0.73\n",
      "Iteration 21: Theta Estimate = 0.76\n",
      "Iteration 22: Theta Estimate = 0.76\n",
      "Iteration 23: Theta Estimate = 0.7\n",
      "Iteration 24: Theta Estimate = 0.75\n",
      "Iteration 25: Theta Estimate = 0.77\n",
      "Iteration 26: Theta Estimate = 0.78\n",
      "Iteration 27: Theta Estimate = 0.78\n",
      "Iteration 28: Theta Estimate = 0.77\n",
      "Iteration 29: Theta Estimate = 0.81\n",
      "Iteration 30: Theta Estimate = 0.76\n",
      "Iteration 31: Theta Estimate = 0.76\n",
      "Iteration 32: Theta Estimate = 0.78\n",
      "Iteration 33: Theta Estimate = 0.75\n",
      "Iteration 34: Theta Estimate = 0.73\n",
      "Iteration 35: Theta Estimate = 0.73\n",
      "Iteration 36: Theta Estimate = 0.79\n",
      "Iteration 37: Theta Estimate = 0.79\n",
      "Iteration 38: Theta Estimate = 0.76\n",
      "Iteration 39: Theta Estimate = 0.75\n",
      "Iteration 40: Theta Estimate = 0.73\n",
      "Iteration 41: Theta Estimate = 0.73\n",
      "Iteration 42: Theta Estimate = 0.78\n",
      "Iteration 43: Theta Estimate = 0.79\n",
      "Iteration 44: Theta Estimate = 0.77\n",
      "Iteration 45: Theta Estimate = 0.76\n",
      "Iteration 46: Theta Estimate = 0.77\n",
      "Iteration 47: Theta Estimate = 0.76\n",
      "Iteration 48: Theta Estimate = 0.73\n",
      "Iteration 49: Theta Estimate = 0.7\n",
      "Iteration 50: Theta Estimate = 0.77\n",
      "Iteration 51: Theta Estimate = 0.8\n",
      "Iteration 52: Theta Estimate = 0.76\n",
      "Iteration 53: Theta Estimate = 0.78\n",
      "Iteration 54: Theta Estimate = 0.72\n",
      "Iteration 55: Theta Estimate = 0.72\n",
      "Iteration 56: Theta Estimate = 0.75\n",
      "Iteration 57: Theta Estimate = 0.75\n",
      "Iteration 58: Theta Estimate = 0.74\n",
      "Iteration 59: Theta Estimate = 0.71\n",
      "Iteration 60: Theta Estimate = 0.78\n",
      "Iteration 61: Theta Estimate = 0.76\n",
      "Iteration 62: Theta Estimate = 0.76\n",
      "Iteration 63: Theta Estimate = 0.77\n",
      "Iteration 64: Theta Estimate = 0.78\n",
      "Iteration 65: Theta Estimate = 0.79\n",
      "Iteration 66: Theta Estimate = 0.8\n",
      "Iteration 67: Theta Estimate = 0.76\n",
      "Iteration 68: Theta Estimate = 0.73\n",
      "Iteration 69: Theta Estimate = 0.75\n",
      "Iteration 70: Theta Estimate = 0.79\n",
      "Iteration 71: Theta Estimate = 0.8\n",
      "Iteration 72: Theta Estimate = 0.77\n",
      "Iteration 73: Theta Estimate = 0.79\n",
      "Iteration 74: Theta Estimate = 0.78\n",
      "Iteration 75: Theta Estimate = 0.78\n",
      "Iteration 76: Theta Estimate = 0.74\n",
      "Iteration 77: Theta Estimate = 0.78\n",
      "Iteration 78: Theta Estimate = 0.75\n",
      "Iteration 79: Theta Estimate = 0.75\n",
      "Iteration 80: Theta Estimate = 0.68\n",
      "Iteration 81: Theta Estimate = 0.68\n",
      "Iteration 82: Theta Estimate = 0.69\n",
      "Iteration 83: Theta Estimate = 0.75\n",
      "Iteration 84: Theta Estimate = 0.8\n",
      "Iteration 85: Theta Estimate = 0.76\n",
      "Iteration 86: Theta Estimate = 0.72\n",
      "Iteration 87: Theta Estimate = 0.76\n",
      "Iteration 88: Theta Estimate = 0.73\n",
      "Iteration 89: Theta Estimate = 0.75\n",
      "Iteration 90: Theta Estimate = 0.75\n",
      "Iteration 91: Theta Estimate = 0.72\n",
      "Iteration 92: Theta Estimate = 0.76\n",
      "Iteration 93: Theta Estimate = 0.77\n",
      "Iteration 94: Theta Estimate = 0.81\n",
      "Iteration 95: Theta Estimate = 0.8\n",
      "Iteration 96: Theta Estimate = 0.76\n",
      "Iteration 97: Theta Estimate = 0.75\n",
      "Iteration 98: Theta Estimate = 0.8\n",
      "Iteration 99: Theta Estimate = 0.78\n",
      "Iteration 100: Theta Estimate = 0.74\n",
      "\n",
      "Final Theta Estimate after 100 iterations: 0.74\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Step 1: Generate synthetic data (observed + missing values)\n",
    "random.seed(42)\n",
    "\n",
    "# Assume we have 100 data points and a parameter theta\n",
    "theta_true = 0.7\n",
    "n_data = 100\n",
    "\n",
    "# Simulate data (binary outcome)\n",
    "observed_data = []\n",
    "for i in range(n_data):\n",
    "    observed_data.append(1 if random.random() < theta_true else 0)\n",
    "\n",
    "# Simulate missingness (randomly mask some data points as missing)\n",
    "missingness = [random.random() < 0.3 for _ in range(n_data)]  # 30% missing data\n",
    "for i in range(n_data):\n",
    "    if missingness[i]:\n",
    "        observed_data[i] = None  # Mark missing data as None\n",
    "\n",
    "# Step 2: Define the E-step (simulate missing data using conditional distribution)\n",
    "def e_step(observed_data, theta):\n",
    "    \"\"\" Simulate missing data using conditional distribution. \"\"\"\n",
    "    simulated_data = observed_data[:]\n",
    "    \n",
    "    for i in range(len(observed_data)):\n",
    "        if simulated_data[i] is None:\n",
    "            # Simulate missing data (conditional on theta)\n",
    "            simulated_data[i] = 1 if random.random() < theta else 0\n",
    "    \n",
    "    return simulated_data\n",
    "\n",
    "# Step 3: Define the M-step (maximize the likelihood)\n",
    "def m_step(completed_data):\n",
    "    \"\"\" Maximize the likelihood to estimate theta. \"\"\"\n",
    "    # Maximum likelihood estimate for a binomial distribution: the sample mean\n",
    "    count = 0\n",
    "    total = 0\n",
    "    for data_point in completed_data:\n",
    "        if data_point is not None:\n",
    "            total += data_point\n",
    "            count += 1\n",
    "    \n",
    "    theta_estimate = total / count if count > 0 else 0  # Avoid division by zero\n",
    "    return theta_estimate\n",
    "\n",
    "# Step 4: Run the SEM algorithm\n",
    "def sem_algorithm(observed_data, max_iter=100, theta_init=0.5):\n",
    "    theta = theta_init\n",
    "    theta_history = [theta]  # Keep track of theta estimates for plotting\n",
    "    \n",
    "    for iteration in range(max_iter):\n",
    "        # E-step: simulate missing data\n",
    "        simulated_data = e_step(observed_data, theta)\n",
    "        \n",
    "        # M-step: maximize the likelihood (estimate theta)\n",
    "        theta = m_step(simulated_data)\n",
    "        \n",
    "        # Track the theta estimates\n",
    "        theta_history.append(theta)\n",
    "    \n",
    "    return theta_history\n",
    "\n",
    "# Run the SEM algorithm\n",
    "theta_estimates = sem_algorithm(observed_data, max_iter=100)\n",
    "\n",
    "# Show the convergence of theta estimates\n",
    "for iteration, theta in enumerate(theta_estimates):\n",
    "    print(f\"Iteration {iteration}: Theta Estimate = {theta}\")\n",
    "\n",
    "# Final estimated theta after all iterations\n",
    "print(f\"\\nFinal Theta Estimate after {len(theta_estimates)-1} iterations: {theta_estimates[-1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f425d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                █            █                                                       \n",
      "              █            █    █            █                                                       \n",
      "   █    ██ █████   █ ██    █ █  █       ███  ██   █  █   █         █       ████  █          ██  █  █ \n",
      "   █    ████████   ██████  █ █  █ █    ████  ███ ██  ██  █        ███     █████  █         ████ █ ███\n",
      "   █ ███████████  ████████ █ ██████   ██████ ███ ██████  ██  █ █  ███ █ █ ████████     █   ██████████\n",
      "   █ ███████████████████████████████ ███████ ████████████████████████ █████████████    █ ████████████\n",
      "   █████████████████████████████████ ██████████████████████████████████████████████   ███████████████\n",
      "   █████████████████████████████████████████████████████████████████████████████████  ███████████████\n",
      "   ██████████████████████████████████████████████████████████████████████████████████ ███████████████\n",
      "   ██████████████████████████████████████████████████████████████████████████████████████████████████\n",
      " ████████████████████████████████████████████████████████████████████████████████████████████████████\n",
      " ████████████████████████████████████████████████████████████████████████████████████████████████████\n",
      " ████████████████████████████████████████████████████████████████████████████████████████████████████\n",
      " ████████████████████████████████████████████████████████████████████████████████████████████████████\n",
      " ████████████████████████████████████████████████████████████████████████████████████████████████████\n",
      " ████████████████████████████████████████████████████████████████████████████████████████████████████\n",
      " ████████████████████████████████████████████████████████████████████████████████████████████████████\n",
      " ████████████████████████████████████████████████████████████████████████████████████████████████████\n",
      " ████████████████████████████████████████████████████████████████████████████████████████████████████\n",
      "█████████████████████████████████████████████████████████████████████████████████████████████████████\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "Iterations: 101\n"
     ]
    }
   ],
   "source": [
    "def plot_progress(theta_history):\n",
    "    \"\"\" Plot the progress of theta estimates as a text-based plot. \"\"\"\n",
    "    # Determine the range of theta values for scaling the plot\n",
    "    min_theta = min(theta_history)\n",
    "    max_theta = max(theta_history)\n",
    "    \n",
    "    # Define the height of the graph\n",
    "    graph_height = 20\n",
    "    graph_width = len(theta_history)\n",
    "    \n",
    "    # Scale the theta values into the graph height\n",
    "    scaled_theta = [\n",
    "        int((theta - min_theta) / (max_theta - min_theta) * (graph_height - 1))\n",
    "        for theta in theta_history\n",
    "    ]\n",
    "    \n",
    "    # Print the graph\n",
    "    for i in range(graph_height - 1, -1, -1):\n",
    "        line = \"\"\n",
    "        for value in scaled_theta:\n",
    "            if value >= i:\n",
    "                line += \"█\"\n",
    "            else:\n",
    "                line += \" \"\n",
    "        print(line)\n",
    "    \n",
    "    # Print the base of the graph\n",
    "    print(\"-\" * graph_width)\n",
    "    print(f\"Iterations: {len(theta_history)}\")\n",
    "    \n",
    "# Run the SEM algorithm to get theta estimates\n",
    "theta_estimates = sem_algorithm(observed_data, max_iter=100)\n",
    "\n",
    "# Plot the progress of theta estimates\n",
    "plot_progress(theta_estimates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68036f7c",
   "metadata": {},
   "source": [
    "##  Neural Networks\n",
    "\n",
    "Neural networks provide another type of missing data model where simulation methods are almost always necessary. These models are frequently used in classification and pattern recognition, as well as in robotics and computer vision (see Cheng and Titterington 1994 for a review on these models). Barring the biological vocabulary and the idealistic connection with actual neurons, the theory of neural networks covers:\n",
    "\n",
    "(i) modeling nonlinear relations between explanatory and dependent (explained) variables,  \n",
    "(ii) estimation of the parameters of these models based on a (training) sample.\n",
    "\n",
    "Although the neural network literature usually avoids probabilistic modeling, these models can be analyzed and estimated from a statistical point of view (see Neal 1999 or Ripley 1994, 1996). They can also be seen as a particular type of nonparametric estimation problem, where a major issue is then identifiability.\n",
    "\n",
    "A simple classical example of a neural network is the multilayer model (also called the backpropagation model) which relates explanatory variables $ \\mathbf{x} = (x_1, \\ldots, x_p) $ with dependent variables $ \\mathbf{y} = (y_1, \\ldots, y_n) $ through a hidden \"layer\", $ \\mathbf{h} = (h_1, \\ldots, h_p) $, where $ (k = 1, \\ldots, p; i = 1, \\ldots, n) $:\n",
    "\n",
    "$$\n",
    "h_k = f \\left( \\alpha_k + \\sum_{j=1}^p a_{kj} x_j \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "E[\\mathbf{y}_i | \\mathbf{h}] = g(\\mathbf{B} \\mathbf{h} + \\mathbf{c})\n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "\\text{Var}(\\mathbf{y}) = \\sigma^2.\n",
    "$$\n",
    "\n",
    "The functions $ f $ and $ g $ are known (or arbitrarily chosen) from categories such as threshold, $ f(t) = \\mathbb{I}(t > 0) $, hyperbolic, $ f(t) = \\tanh(t) $, or sigmoid, $ f(t) = \\frac{1}{1 + e^{-t}} $.\n",
    "\n",
    "As an example, consider the problem of character recognition, where handwritten manuscripts are automatically deciphered. The $ x_i $'s may correspond to geometric characteristics of a digitized character or pixel gray levels, and the $ y_i $'s are the 26 letters of the alphabet, plus side symbols. (See Le Cun et al. 1989 for an actual modeling based on a sample of 7291, 16 x 16 pixel images, for 9760 parameters.)\n",
    "\n",
    "The likelihood of the multilayer model then includes the parameters $ \\alpha = (\\alpha_k) $ and $ \\beta = (\\beta_k) $ in a nonlinear structure. Assuming normality, for observations $ (r, t) $, $ t = 1, 2, \\ldots, T $, the log-likelihood can be written:\n",
    "\n",
    "$$\n",
    "\\ell(\\alpha, \\beta | \\mathbf{x}, \\mathbf{y}) = -\\sum_{i=1}^T \\frac{(y_i - f(\\mathbf{x}_i))^2}{2 \\sigma^2}\n",
    "$$\n",
    "\n",
    "A similar objective function can be derived using a least squares criterion. The maximization of $ f(\\mathbf{x}, \\beta | \\mathbf{x}, \\mathbf{y}) $ involves the detection and elimination of numerous local modes.\n",
    "\n",
    "---\n",
    "\n",
    "##  The Robbins-Monro Procedure\n",
    "\n",
    "The Robbins-Monro algorithm (Robbins and Monro 1951) is a technique of stochastic approximation to solve for $ x $ in equations of the form:\n",
    "\n",
    "$$\n",
    "x_{n+1} = x_n + \\alpha_n \\left( g(x_n) - x_n \\right)\n",
    "$$\n",
    "\n",
    "where $ g(x) $ is a function for which we need to find the root, and $ \\alpha_n $ is a step size that typically decreases as $ n $ increases.\n",
    "## Robbins-Monro Procedure and Convergence\n",
    "\n",
    "The Robbins-Monro method proceeds by generating a Markov chain of the form:\n",
    "\n",
    "$$\n",
    "X_{j+1} = X_j + \\alpha_j \\left( h(Z_j, X_j) \\right),\n",
    "$$\n",
    "\n",
    "where $ Z $ is simulated from the conditional distribution defining $ h(x) $ as in equation (5.8). The following result describes sufficient conditions for the algorithm to be convergent (see Bouleau and Lépingle 1994 for a proof).\n",
    "\n",
    "### Theorem 5.24\n",
    "\n",
    "If $\\{\\alpha_n\\} $is a sequence of positive numbers such that:\n",
    "\n",
    "$$\n",
    "\\sum_{n=1}^\\infty \\alpha_n = \\infty \\quad \\text{and} \\quad \\sum_{n=1}^\\infty \\alpha_n^2 < \\infty,\n",
    "$$\n",
    "\n",
    "and if the $ Z_n $'s are simulated from $ H $ conditionally on $ \\theta $, such that:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[h(X_n, \\theta)] = h(\\theta),\n",
    "$$\n",
    "\n",
    "and $ |Z_n| \\leq B $ for a fixed bound $ B $, and if there exists $ \\theta^* $ such that:\n",
    "\n",
    "$$\n",
    "\\inf_{\\theta \\neq \\theta^*} \\left( \\theta - \\theta^* \\right) \\cdot \\left( h(\\theta) - h(\\theta^*) \\right) > 0,\n",
    "$$\n",
    "\n",
    "then the sequence $ \\{ \\theta_n \\} $ converges to $ \\theta^* $ almost surely.\n",
    "\n",
    "---\n",
    "\n",
    "### Solution of the Maximization Problem\n",
    "\n",
    "The solution of the maximization problem can be expressed in terms of the solution of the equation $ \\nabla h(\\theta) = 0 $ if the problem is sufficiently regular, i.e., if the maximum is not achieved on the boundary of the domain $ \\Theta $.\n",
    "\n",
    "Note that the equation is similar to (5.24). Since its proposal, this method has seen numerous variations. For more detailed references, see Benveniste et al. (1990), Bouleau and Lépingle (1994), Wasan (1969), Kersting (1987), Winkler (1995), and Duflo (1996).\n",
    "\n",
    "### Case with Several Local Maxima\n",
    "\n",
    "When the function $ h $ has several local maxima, the Robbins-Monro procedure converges to one of these maxima. In the particular case where $ H(x, \\theta) = h(\\theta) + \\frac{x}{\\sqrt{a}} $ (with $ a > 0$), Pflug (1994) examines the relation:\n",
    "\n",
    "$$\n",
    "\\theta_{s+1} = \\theta_s + a h(\\theta) + \\sqrt{a}/2 x,\n",
    "$$\n",
    "\n",
    "where the $ X_n $'s are i.i.d. with $ \\mathbb{E}[X] = 0 $ and $ \\text{Var}(X) = T $.\n",
    "\n",
    "The relevance of this particular case is that, under the following conditions:\n",
    "\n",
    "1. $ h(\\theta) > 0 $, $ k > 0 $ such that $ -h(\\theta) \\leq k \\|\\theta\\| $ for $ \\theta > k $,\n",
    "2. $ |h(\\theta)| \\leq k h(\\theta^*) $ for $ k < 1000 $,\n",
    "3. $ h'(\\theta) > k s $,\n",
    "\n",
    "the stationary measure associated with the Markov chain weakly converges to the distribution with density:\n",
    "\n",
    "$$\n",
    "c(T) \\exp\\left( \\frac{h(\\theta)}{T} \\right),\n",
    "$$\n",
    "\n",
    "where $ c(T) $ is a normalizing constant.\n",
    "## Robbins-Monro Convergence and Gibbs Measure\n",
    "\n",
    "When $ a $ goes to 0, and $ T $ remains fixed, with $ E $ being the primitive of $ h $ (see Duflo 1996), the hypotheses (i)-(iii) ensure that:\n",
    "\n",
    "$$\n",
    "\\exp\\left(\\frac{E(\\theta)}{T}\\right)\n",
    "$$\n",
    "\n",
    "is integrable on $ \\mathbb{R} $. Therefore, the limiting distribution of $ \\theta $ (as $ a \\to 0 $) is the so-called **Gibbs measure** with energy function $ E $, which is a pivotal quantity for the **simulated annealing** method introduced in Section 5.2.3. \n",
    "\n",
    "In particular, when $ T $ goes to 0, the Gibbs measure converges to the uniform distribution on the set of (global) maxima of $ E $ (see Hwang 1980). This convergence is interesting more because of the connections it exhibits with the notions of **Gibbs measure** and **simulated annealing**, rather than for its practical consequences.\n",
    "\n",
    "The assumptions (i)-(iii) are rather restrictive and difficult to check for implicit $ h $'s, and the representation in equation (5.25) is quite specialized. Note, however, that the completion of $ h() $ in $ H(x, \\theta) $ is free since the conditions (i)-(iii) relate only to $ h $.\n",
    "\n",
    "---\n",
    "\n",
    "## Monte Carlo Approximation\n",
    "\n",
    "In cases where a function $ h(z) $ can be written as $ \\mathbb{E}[H(z, Z)] $ but is not directly computable, it can be approximated by the empirical (Monte Carlo) average:\n",
    "\n",
    "$$\n",
    "h(z) \\approx \\frac{1}{M} \\sum_{i=1}^{M} H(x, Z_i),\n",
    "$$\n",
    "\n",
    "where the $ Z_i $'s are generated from the conditional distribution $ f(x) $. This approximation yields a convergent estimator of $ h(z) $ for every value of $ x $, but its use in optimization setups is not recommended for at least two related reasons:\n",
    "\n",
    "1. Presumably, $ h(x) $ needs to be evaluated at many points, which will involve the generation of many samples of $ Z $'s of size $ M $.\n",
    "2. Since the sample changes with every value of $ x $, the resulting sequence of evaluations of $ h $ will usually not be smooth.\n",
    "\n",
    "These difficulties prompted Geyer (1996) to suggest an **importance sampling** approach to this problem, using a single sample of $ Z $'s simulated from $g(Z) $ and estimating $ h(x) $ with:\n",
    "\n",
    "$$\n",
    "h_m(x) = \\frac{1}{M} \\sum_{i=1}^{M} H(x, Z_i) \\frac{f(Z_i)}{g(Z_i)},\n",
    "$$\n",
    "\n",
    "where the $ Z_i $'s are simulated from $ g(Z) $. Since this evaluation of $ h $ does not depend on $ x $, points (i) and (ii) above are answered.\n",
    "\n",
    "The problem then shifts from the original maximization problem to:\n",
    "\n",
    "$$\n",
    "\\max_x h_m(x),\n",
    "$$\n",
    "\n",
    "which leads to a convergent solution in most cases and also allows for the use of regular optimization techniques, since the function $ h_m $ does not vary with each iteration. \n",
    "\n",
    "However, three remaining drawbacks of this approach are as follows:\n",
    "\n",
    "1. Since $ h_m $ is expressed as a sum, it most often enjoys fewer analytical properties than the original function $ h $.\n",
    "2. The choice of the importance function $ g $ can be very influential in obtaining a good approximation of the function $ h(x) $.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e87adcc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monte Carlo approximation of h(5): 0.9591290923449101\n",
      "Estimated solution using Robbins-Monro: x ≈ 5.521838692236382\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "# Define the conditional distribution f(x), for example, a normal distribution\n",
    "def conditional_distribution(x):\n",
    "    # Simulating a conditional distribution, f(x), for example a normal distribution\n",
    "    # Here we assume standard normal distribution for simplicity\n",
    "    return random.gauss(x, 1)  # mean = x, standard deviation = 1\n",
    "\n",
    "# Define the function H(x, Z), which depends on both x and the random variable Z\n",
    "def H(x, Z):\n",
    "    # Example: H(x, Z) = (x - Z)^2\n",
    "    return (x - Z) ** 2\n",
    "\n",
    "# Monte Carlo Approximation to compute h(x)\n",
    "def monte_carlo_approximation(x, M):\n",
    "    # Generate M samples from the conditional distribution f(x)\n",
    "    samples = [conditional_distribution(x) for _ in range(M)]\n",
    "    \n",
    "    # Calculate the Monte Carlo average of H(x, Z)\n",
    "    h_approx = sum(H(x, Z) for Z in samples) / M\n",
    "    return h_approx\n",
    "\n",
    "# Example usage\n",
    "x = 5  # Evaluate at x = 5\n",
    "M = 1000  # Number of samples for Monte Carlo approximation\n",
    "h_approx = monte_carlo_approximation(x, M)\n",
    "print(f\"Monte Carlo approximation of h({x}): {h_approx}\")\n",
    "# Define the function h(x), for example h(x) = x^2\n",
    "def h(x):\n",
    "    return x ** 2\n",
    "\n",
    "# Robbins-Monro procedure\n",
    "def robbins_monro(B, initial_x, alpha, iterations):\n",
    "    x = initial_x\n",
    "    for _ in range(iterations):\n",
    "        # Simulate Z from the distribution of Z given x, here we use a simple Gaussian random walk\n",
    "        Z = random.gauss(0, 1)\n",
    "        \n",
    "        # Update x using the Robbins-Monro update rule\n",
    "        x = x + alpha * (B - h(x)) + Z\n",
    "        \n",
    "    return x\n",
    "\n",
    "# Example usage\n",
    "B = 25  # We want to solve x^2 = B, so B = 25\n",
    "initial_x = 1.0  # Initial guess for x\n",
    "alpha = 0.1  # Step size for updates\n",
    "iterations = 100  # Number of iterations to run\n",
    "\n",
    "estimated_x = robbins_monro(B, initial_x, alpha, iterations)\n",
    "print(f\"Estimated solution using Robbins-Monro: x ≈ {estimated_x}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6b442d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
