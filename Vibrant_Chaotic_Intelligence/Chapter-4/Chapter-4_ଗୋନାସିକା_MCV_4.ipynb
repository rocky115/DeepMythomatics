{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0a86ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " * Copyright (c) 2008 Radhamadhab Dalai\n",
    " *\n",
    " * Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    " * of this software and associated documentation files (the \"Software\"), to deal\n",
    " * in the Software without restriction, including without limitation the rights\n",
    " * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    " * copies of the Software, and to permit persons to whom the Software is\n",
    " * furnished to do so, subject to the following conditions:\n",
    " *\n",
    " * The above copyright notice and this permission notice shall be included in\n",
    " * all copies or substantial portions of the Software.\n",
    " *\n",
    " * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    " * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    " * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    " * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    " * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    " * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
    " * THE SOFTWARE.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea9b926",
   "metadata": {},
   "source": [
    "##  Controlling Monte Carlo Variance\n",
    "\n",
    "The control variate method for Monte Carlo variance reduction can be expressed using the following estimator:\n",
    "\n",
    "$$\n",
    "\\hat{\\theta} = \\frac{1}{m} \\sum_{i=1}^{m} h(X_i) + \\beta \\left( h_0(X_i) - \\mathbb{E}[h_0(X)] \\right)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ h(X_i) $ is the function to estimate,\n",
    "- $ h_0(X_i) $ is the control variate,\n",
    "- $ \\beta $ is the coefficient to minimize variance,\n",
    "- $ \\mathbb{E}[h_0(X)] $ is the expectation of $ h_0(X) $.\n",
    "\n",
    "The variance of this estimator is given by:\n",
    "\n",
    "$$\n",
    "\\text{var}(\\hat{\\theta}) = \\text{var}(d_1) + \\beta^2 \\text{var}(d_3) + 2\\beta \\text{cov}(d_1, d_3)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ d_1 $ is the original estimator based on the function $ h(X) $,\n",
    "- $ d_3 $ is the estimator based on the control variate $ h_0(X) $.\n",
    "\n",
    "If we assume that $ P(X > \\mu) = 1/2 $, we can choose the optimal value for $ \\beta $ based on the covariance structure and known probabilities.\n",
    "\n",
    "### Example : Logistic Regression\n",
    "\n",
    "Consider the logistic regression model:\n",
    "\n",
    "$$\n",
    "P(Y = 1 | X) = \\frac{\\exp(\\theta^T X)}{1 + \\exp(\\theta^T X)}\n",
    "$$\n",
    "\n",
    "The likelihood associated with a sample $ (x_1, Y_1), (x_2, Y_2), \\dots, (x_n, Y_n) $ is:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta) = \\prod_{i=1}^{n} \\left( \\exp(\\theta^T x_i) \\right)^{y_i} \\left( 1 + \\exp(\\theta^T x_i) \\right)^{-1}\n",
    "$$\n",
    "\n",
    "When $ \\theta $ has a conjugate prior, the posterior distribution is of the same form as the prior, with new parameters replacing the original ones. Specifically, if the prior is a normal distribution, the posterior for $ \\theta $ is given by:\n",
    "\n",
    "$$\n",
    "p(\\theta | y) \\propto \\exp\\left(\\theta^T \\sum_{i=1}^{n} y_i x_i \\right) \\left( 1 + \\exp(\\theta^T x_i) \\right)^{-n}\n",
    "$$\n",
    "\n",
    "This implies that the expectation of the posterior distribution $ E[\\theta] $ can be derived from the generated variables $ \\theta_i $, $ i = 1, 2, \\dots, m $, and that:\n",
    "\n",
    "$$\n",
    "E[\\theta | X] = \\frac{\\sum_{i=1}^{n} y_i x_i + C}{n(1 + 1)}\n",
    "$$\n",
    "\n",
    "\n",
    "Therefore, the posterior expectation of the function:\n",
    "\n",
    "$$\n",
    "\\eta(\\theta) = \\frac{\\exp(\\theta^T X)}{1 + \\exp(\\theta^T X)}\n",
    "$$\n",
    "\n",
    "is known and equal to:\n",
    "\n",
    "$$\n",
    "\\frac{\\sum_{i=1}^{n} y_i x_i + C}{n(1 + 1)}\n",
    "$$\n",
    "\n",
    "under the prior distribution $\\mathcal{N}(0, A) $. Unfortunately, a control variate version of this estimator is not available, since the optimal constant $ \\beta^* $ (or even its sign) cannot be evaluated, except by the regression of the estimators $ \\theta_i $ upon the sum:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{n} \\frac{\\exp(\\theta^T x_i)}{1 + \\exp(\\theta^T x_i)}\n",
    "$$\n",
    "\n",
    "Thus, the fact that the posterior mean of $ \\eta(\\theta) $ is known does not help us to establish a control variate estimator. This information can be used in a more informal way to study convergence of $ \\hat{\\theta} $ (see, for instance, Robert 1993).\n",
    "\n",
    "In conclusion, the technique of control variates is manageable only in very specific cases: the control function $ h $ must be available, as well as the optimal weight. See, however, Brooks and Gelman (1998b) for a general approach based on the score function (whose expectation is null under general regularity conditions).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b40582",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Logistic function (sigmoid)\n",
    "def logistic(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Control variate function (here we assume itâ€™s a known value or we estimate it)\n",
    "def control_variate(x, theta, c=0):\n",
    "    return logistic(np.dot(x, theta))  # Example control variate based on logistic regression\n",
    "\n",
    "# Generate synthetic data (for illustration)\n",
    "def generate_data(n, p, theta_true):\n",
    "    X = np.random.randn(n, p)  # n samples, p features\n",
    "    y = np.random.binomial(1, logistic(np.dot(X, theta_true)))  # Binary response based on logistic model\n",
    "    return X, y\n",
    "\n",
    "# Function to compute the control variate estimator\n",
    "def control_variate_estimator(X, y, theta_true, n_iter=1000, theta_init=None):\n",
    "    n, p = X.shape  # X should be a matrix of size (n, p)\n",
    "    \n",
    "    if theta_init is None:\n",
    "        theta_init = np.zeros(p)  # Initial guess for theta\n",
    "\n",
    "    # Ridge regression to avoid singular matrix issues (regularization)\n",
    "    lambda_reg = 1e-5  # Small regularization parameter to prevent singular matrix\n",
    "    theta_d1 = np.linalg.inv(X.T @ X + lambda_reg * np.eye(p)) @ X.T @ y  # Regularized OLS\n",
    "\n",
    "    # Control variate method: using known or estimated control variate\n",
    "    theta_d3 = np.linalg.inv(X.T @ X + lambda_reg * np.eye(p)) @ X.T @ (y - control_variate(X, theta_true))  # Adjusted estimator\n",
    "\n",
    "    # Calculating the optimal weight (beta) based on covariance\n",
    "    covariance = np.cov(control_variate(X, theta_true), y)[0][1]  # Covariance between estimator and control variate\n",
    "    var_h0 = np.var(control_variate(X, theta_true))  # Variance of control variate\n",
    "\n",
    "    beta_optimal = covariance / var_h0  # Optimal weight\n",
    "\n",
    "    # Calculate control variate estimator (d2)\n",
    "    estimator_d2 = theta_d1 + beta_optimal * (theta_d3 - np.mean(control_variate(X, theta_true)))\n",
    "\n",
    "    return np.linalg.norm(theta_d1), np.linalg.norm(estimator_d2), np.linalg.norm(theta_d3), beta_optimal\n",
    "\n",
    "# Parameters\n",
    "n = 100  # number of samples\n",
    "p = 10   # number of features\n",
    "theta_true = np.random.randn(p)  # True theta values\n",
    "\n",
    "# To store results for plotting\n",
    "iterations = list(range(1, 101))\n",
    "theta_d1_vals = []\n",
    "theta_d2_vals = []\n",
    "\n",
    "for i in iterations:\n",
    "    if i > p:  # Ensure we have more samples than features\n",
    "        X, y = generate_data(i, p, theta_true)  # Generate data for each iteration\n",
    "        theta_d1, theta_d2, theta_d3, beta_optimal = control_variate_estimator(X, y, theta_true)\n",
    "        theta_d1_vals.append(theta_d1)\n",
    "        theta_d2_vals.append(theta_d2)\n",
    "\n",
    "# Plot the results\n",
    "plt.plot(iterations, theta_d1_vals, label=\"Standard Estimator (d1)\", color='red')\n",
    "plt.plot(iterations, theta_d2_vals, label=\"Control Variate Estimator (d2)\", color='blue')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Estimator Norm\")\n",
    "plt.title(\"Convergence of Estimators (Control Variates)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4749df2e",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_26053/3022809081.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Ensure we have more samples than features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta_true\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Generate data for each iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mtheta_d1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta_d2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta_d3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta_optimal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontrol_variate_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0mtheta_d1_vals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta_d1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mtheta_d2_vals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta_d2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_26053/3022809081.py\u001b[0m in \u001b[0;36mcontrol_variate_estimator\u001b[0;34m(X, y, theta_true, n_iter, theta_init)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mtheta_d3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mp\u001b[0m  \u001b[0;31m# Placeholder for theta_d3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mtheta_d3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_transpose_X\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcontrol_variates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mX_transpose_X\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m# Covariance and variance for optimal weight calculation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_26053/3022809081.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mtheta_d3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mp\u001b[0m  \u001b[0;31m# Placeholder for theta_d3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mtheta_d3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_transpose_X\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcontrol_variates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mX_transpose_X\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m# Covariance and variance for optimal weight calculation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "# Logistic function (sigmoid)\n",
    "def logistic(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "# Control variate function\n",
    "def control_variate(x, theta, c=0):\n",
    "    return logistic(sum(x_i * theta_i for x_i, theta_i in zip(x, theta)))  # Example control variate based on logistic regression\n",
    "\n",
    "# Generate synthetic data (for illustration)\n",
    "def generate_data(n, p, theta_true):\n",
    "    X = [[random.gauss(0, 1) for _ in range(p)] for _ in range(n)]  # n samples, p features\n",
    "    y = [random.randint(0, 1) if logistic(sum(x_i * theta_i for x_i, theta_i in zip(X_i, theta_true))) > 0.5 else 0 for X_i in X]\n",
    "    return X, y\n",
    "\n",
    "# Function to compute the control variate estimator\n",
    "def control_variate_estimator(X, y, theta_true, n_iter=1000, theta_init=None):\n",
    "    n = len(X)  # Number of samples\n",
    "    p = len(X[0])  # Number of features\n",
    "    \n",
    "    if theta_init is None:\n",
    "        theta_init = [0] * p  # Initial guess for theta\n",
    "    \n",
    "    # Regularized OLS (using the pseudo-inverse approach)\n",
    "    X_transpose = [list(i) for i in zip(*X)]  # Transpose of X\n",
    "    X_transpose_X = [[sum(X_transpose[i][k] * X[k][j] for k in range(n)) for j in range(p)] for i in range(p)]\n",
    "    X_transpose_y = [sum(X_transpose[i][j] * y[j] for j in range(n)) for i in range(p)]\n",
    "    \n",
    "    # Adding small regularization term to prevent singular matrix\n",
    "    lambda_reg = 1e-5\n",
    "    for i in range(p):\n",
    "        X_transpose_X[i][i] += lambda_reg  # Regularization on diagonal\n",
    "    \n",
    "    # Solving for theta_d1 using inverse of X_transpose_X\n",
    "    theta_d1 = [0] * p  # Placeholder for theta_d1\n",
    "    for i in range(p):\n",
    "        theta_d1[i] = sum(X_transpose_X[i][j] * X_transpose_y[j] for j in range(p)) / X_transpose_X[i][i]\n",
    "\n",
    "    # Control variate method (adjusting for control variate)\n",
    "    control_variates = [control_variate(X_i, theta_true) for X_i in X]\n",
    "    theta_d3 = [0] * p  # Placeholder for theta_d3\n",
    "    for i in range(p):\n",
    "        theta_d3[i] = sum(X_transpose_X[i][j] * (y[j] - control_variates[j]) for j in range(n)) / X_transpose_X[i][i]\n",
    "    \n",
    "    # Covariance and variance for optimal weight calculation\n",
    "    covariance = sum((control_variates[i] - sum(control_variates) / n) * (y[i] - sum(y) / n) for i in range(n)) / n\n",
    "    var_h0 = sum((control_variates[i] - sum(control_variates) / n)**2 for i in range(n)) / n\n",
    "    beta_optimal = covariance / var_h0 if var_h0 != 0 else 0  # Handle division by zero\n",
    "    \n",
    "    # Final control variate estimator (d2)\n",
    "    estimator_d2 = [theta_d1[i] + beta_optimal * (theta_d3[i] - sum(control_variates) / n) for i in range(p)]\n",
    "    \n",
    "    # Return norms (magnitude) of the estimators\n",
    "    norm_d1 = math.sqrt(sum(t**2 for t in theta_d1))\n",
    "    norm_d2 = math.sqrt(sum(t**2 for t in estimator_d2))\n",
    "    norm_d3 = math.sqrt(sum(t**2 for t in theta_d3))\n",
    "    \n",
    "    return norm_d1, norm_d2, norm_d3, beta_optimal\n",
    "\n",
    "# Parameters\n",
    "n = 100  # number of samples\n",
    "p = 10   # number of features\n",
    "theta_true = [random.gauss(0, 1) for _ in range(p)]  # True theta values\n",
    "\n",
    "# To store results for plotting\n",
    "iterations = list(range(1, 101))\n",
    "theta_d1_vals = []\n",
    "theta_d2_vals = []\n",
    "\n",
    "for i in iterations:\n",
    "    if i > p:  # Ensure we have more samples than features\n",
    "        X, y = generate_data(i, p, theta_true)  # Generate data for each iteration\n",
    "        theta_d1, theta_d2, theta_d3, beta_optimal = control_variate_estimator(X, y, theta_true)\n",
    "        theta_d1_vals.append(theta_d1)\n",
    "        theta_d2_vals.append(theta_d2)\n",
    "\n",
    "# Plot the results (Using Python's built-in matplotlib)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(iterations, theta_d1_vals, label=\"Standard Estimator (d1)\", color='red')\n",
    "plt.plot(iterations, theta_d2_vals, label=\"Control Variate Estimator (d2)\", color='blue')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Estimator Norm\")\n",
    "plt.title(\"Convergence of Estimators (Control Variates)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4454909f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "# Logistic function (sigmoid)\n",
    "def logistic(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "# Control variate function\n",
    "def control_variate(x, theta, c=0):\n",
    "    return logistic(sum(x_i * theta_i for x_i, theta_i in zip(x, theta)))  # Example control variate based on logistic regression\n",
    "\n",
    "# Generate synthetic data (for illustration)\n",
    "def generate_data(n, p, theta_true):\n",
    "    X = [[random.gauss(0, 1) for _ in range(p)] for _ in range(n)]  # n samples, p features\n",
    "    y = [random.randint(0, 1) if logistic(sum(x_i * theta_i for x_i, theta_i in zip(X_i, theta_true))) > 0.5 else 0 for X_i in X]\n",
    "    return X, y\n",
    "\n",
    "# Function to compute the control variate estimator\n",
    "# Function to compute the control variate estimator\n",
    "# Function to compute the control variate estimator\n",
    "def control_variate_estimator(X, y, theta_true, n_iter=1000, theta_init=None):\n",
    "    n = len(X)  # Number of samples\n",
    "    p = len(X[0])  # Number of features\n",
    "    \n",
    "    if theta_init is None:\n",
    "        theta_init = [0] * p  # Initial guess for theta\n",
    "    \n",
    "    # Regularized OLS (using the pseudo-inverse approach)\n",
    "    X_transpose = [list(i) for i in zip(*X)]  # Transpose of X\n",
    "    X_transpose_X = [[sum(X_transpose[i][k] * X[k][j] for k in range(n)) for j in range(p)] for i in range(p)]\n",
    "    X_transpose_y = [sum(X_transpose[i][j] * y[j] for j in range(n)) for i in range(p)]\n",
    "    \n",
    "    # Adding small regularization term to prevent singular matrix\n",
    "    lambda_reg = 1e-5\n",
    "    for i in range(p):\n",
    "        X_transpose_X[i][i] += lambda_reg  # Regularization on diagonal\n",
    "    \n",
    "    # Solving for theta_d1 using inverse of X_transpose_X\n",
    "    theta_d1 = [0] * p  # Placeholder for theta_d1\n",
    "    for i in range(p):\n",
    "        theta_d1[i] = sum(X_transpose_X[i][j] * X_transpose_y[j] for j in range(p)) / X_transpose_X[i][i]\n",
    "\n",
    "    # Control variate method (adjusting for control variate)\n",
    "    control_variates = [control_variate(X_i, theta_true) for X_i in X]\n",
    "    theta_d3 = [0] * p  # Placeholder for theta_d3\n",
    "    for i in range(p):\n",
    "        # The summation should loop over samples, not features.\n",
    "        theta_d3[i] = sum(X_transpose[i][j] * (y[j] - control_variates[j]) for j in range(n)) / X_transpose_X[i][i]\n",
    "    \n",
    "    # Covariance and variance for optimal weight calculation\n",
    "    covariance = sum((control_variates[i] - sum(control_variates) / n) * (y[i] - sum(y) / n) for i in range(n)) / n\n",
    "    var_h0 = sum((control_variates[i] - sum(control_variates) / n)**2 for i in range(n)) / n\n",
    "    beta_optimal = covariance / var_h0 if var_h0 != 0 else 0  # Handle division by zero\n",
    "    \n",
    "    # Final control variate estimator (d2)\n",
    "    estimator_d2 = [theta_d1[i] + beta_optimal * (theta_d3[i] - sum(control_variates) / n) for i in range(p)]\n",
    "    \n",
    "    # Return norms (magnitude) of the estimators\n",
    "    norm_d1 = math.sqrt(sum(t**2 for t in theta_d1))\n",
    "    norm_d2 = math.sqrt(sum(t**2 for t in estimator_d2))\n",
    "    norm_d3 = math.sqrt(sum(t**2 for t in theta_d3))\n",
    "    \n",
    "    return norm_d1, norm_d2, norm_d3, beta_optimal\n",
    "\n",
    "\n",
    "# Parameters\n",
    "n = 100  # number of samples\n",
    "p = 10   # number of features\n",
    "theta_true = [random.gauss(0, 1) for _ in range(p)]  # True theta values\n",
    "\n",
    "# To store results for plotting\n",
    "iterations = list(range(1, 101))\n",
    "theta_d1_vals = []\n",
    "theta_d2_vals = []\n",
    "\n",
    "for i in iterations:\n",
    "    if i >= p:  # Ensure we have more samples than features\n",
    "        X, y = generate_data(i, p, theta_true)  # Generate data for each iteration\n",
    "        theta_d1, theta_d2, theta_d3, beta_optimal = control_variate_estimator(X, y, theta_true)\n",
    "        theta_d1_vals.append(theta_d1)\n",
    "        theta_d2_vals.append(theta_d2)\n",
    "\n",
    "# Plot the results (Using Python's built-in matplotlib)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(iterations, theta_d1_vals, label=\"Standard Estimator (d1)\", color='red')\n",
    "plt.plot(iterations, theta_d2_vals, label=\"Control Variate Estimator (d2)\", color='blue')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Estimator Norm\")\n",
    "plt.title(\"Convergence of Estimators (Control Variates)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "31b64490",
   "metadata": {},
   "source": [
    "## Monitoring Importance Sampling Convergence\n",
    "\n",
    "#### Monitoring Importance Sampling Convergence\n",
    "\n",
    "With reference to convergence control for simulation methods, importance sampling methods can be implemented in a monitored way; that is, in parallel with other evaluation methods. These methods may involve alternative instrumental distributions or other techniques, such as:\n",
    "\n",
    "- Standard Monte Carlo\n",
    "- Markov Chain Monte Carlo (MCMC)\n",
    "- Riemann sums\n",
    "- Rao-Blackwellization\n",
    "\n",
    "The respective samples provide separate evaluations of $ E[h(X)] $, which can be calculated using:\n",
    "\n",
    "- Equation $ (3.8) $\n",
    "- Equation $ (3.11) $\n",
    "- Alternative estimators (as in Section $ 3.3.3 $)\n",
    "\n",
    "The **convergence criterion** is to stop the simulation when most estimators are close enough to each other. While this empirical method is not completely foolproof, it generally prevents pseudo-convergences when the instrumental distributions are sufficiently different.\n",
    "\n",
    "However, there are some trade-offs:\n",
    "- **Conservative nature**: This approach is only as fast as the slowest estimator.\n",
    "- **Variance issues**: It may highlight instrumental distributions with variance problems.\n",
    "\n",
    "From a computational perspective, implementing this control method efficiently requires **parallel programming**. This ensures equitable weighting for each distribution. Specifically:\n",
    "- A distribution $ f $ with larger variance compared to another distribution $ g $ can compensate for its drawback by a lower computation time, producing a larger sample within the same time frame.\n",
    "\n",
    "**Note:**  \n",
    "This feature does not necessarily require a truly parallel implementation. It can also be reproduced through cyclic allocation of uniform random variables to each distribution involved.\n",
    "\n",
    "## Accept-Reject with Loose Bounds\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "###  Accept-Reject with Loose Bounds\n",
    "\n",
    "For the **Accept-Reject algorithm** (Section 4.4), interesting results emerge when we assume the following condition:  \n",
    "$$\n",
    "a \\leq M \\frac{g(x)}{1 + \\epsilon},\n",
    "$$\n",
    "where the bound $ M $ is not tight.\n",
    "\n",
    "#### Variance of the Estimator\n",
    "Assuming $ \\mathbb{E}_f[h(X)] = 0 $, the variance of the estimator $ \\tilde{\\theta} $ from Equation (3.16) is given by:  \n",
    "$$\n",
    "\\mathrm{var}(\\tilde{\\theta}) = \\frac{\\mathbb{E}_t\\left[t^{-2}\\right]}{M - 1} \\mathbb{E}_s[h^2(X)],\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\mathbb{E}_t[t^{-2}] = \\text{Li}(1 - p),\n",
    "$$\n",
    "and $ \\text{Li}(z) $ denotes the **dilogarithm function**, defined as:  \n",
    "$$\n",
    "\\text{Li}(z) = \\sum_{k=1}^\\infty \\frac{z^k}{k^2},\n",
    "$$\n",
    "(see Abramowitz and Stegun 1964, formula 27.7, for tabulated values). Alternatively, it can be written as:\n",
    "$$\n",
    "\\text{Li}(z) = -\\int_0^z \\frac{\\log(u)}{1 - u} \\, \\mathrm{d}u.\n",
    "$$\n",
    "(Refer to Problem 4.19 for derivation.)\n",
    "\n",
    "#### Variance Bound\n",
    "The bound on the variance of $ \\tilde{\\theta} $ is:\n",
    "$$\n",
    "\\mathrm{var}(\\tilde{\\theta}) \\leq \\frac{\\text{Li}(1 - p)}{M - 1}.\n",
    "$$\n",
    "\n",
    "#### Comparison to the Usual Estimator\n",
    "The estimator $ \\tilde{\\theta} $ uniformly dominates the usual Accept-Reject estimator $ \\hat{\\theta} $ (from Equation 3.15) as long as:\n",
    "$$\n",
    "\\epsilon \\cdot \\left\\{ \\frac{\\log(p) - \\text{Li}(1 - p)}{p \\, \\text{Li}(1 - p)} \\right\\} < 1. \\tag{4.20}\n",
    "$$\n",
    "\n",
    "This result establishes the **advantage of recycling rejected variables** for computing integrals, since Equation (4.20) does not depend on the function $ h $. However, there are limitations:\n",
    "- The assumption $ \\mathbb{E}_f[h(X)] = 0 $ is restrictive because the sum of weights for $ \\tilde{\\theta} $ does not equal $ 1 $, meaning $ \\tilde{\\theta} $ does not correctly estimate constant functions (except for $ h = 0 $).\n",
    "- Consequently, $ \\tilde{\\theta} $ dominates $ \\hat{\\theta} $ only for non-constant functions, making a uniform comparison between the two estimators impossible.\n",
    "\n",
    "#### Graphical Analysis (Fig.12)\n",
    "Fig.12 shows the graphs of the left-hand side of Equation (4.20) for $ \\epsilon = 0.1, 0.2, \\dots, 0.9 $. Key observations:\n",
    "- Domination (where the curve is less than 1) occurs for larger values of $ p $.  \n",
    "  - This is counterintuitive because smaller values of $ p $ lead to higher rejection rates, larger rejected subsamples, and smaller variance of $ \\tilde{\\theta} $ for appropriate density functions $ g $.\n",
    "- The curves are correctly ordered in $ \\epsilon $, with larger values of $ \\epsilon $ leading to wider domination zones.\n",
    "\n",
    "### Summary\n",
    "The loose-bound Accept-Reject method demonstrates significant advantages in recycling rejected samples to improve estimation efficiency, particularly when the bound $ M $ is not tight and appropriate assumptions are met.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235e3c7b",
   "metadata": {},
   "source": [
    "## Partitioning\n",
    "\n",
    "###  Partitioning\n",
    "\n",
    "When sufficient information about the function $ f $ is available, **stratified sampling** can be employed. This technique (refer to Hammersley and Handscomb, 1964, or Rubinstein, 1981) divides the integration domain into a partition $ \\mathcal{X} = \\bigcup X_p $, allowing separate evaluations of the integrals in each region.\n",
    "\n",
    "#### Decomposition of the Integral\n",
    "The integral  \n",
    "$$\n",
    "\\int h(x) f(x) \\, \\mathrm{d}x\n",
    "$$\n",
    "is expressed as:\n",
    "$$\n",
    "\\int h(x) f(x) \\, \\mathrm{d}x = \\sum_{p} \\int_{X_p} h(x) f(x) \\, \\mathrm{d}x,\n",
    "$$\n",
    "where the weights $ \\alpha_p $ are the probabilities of the regions $ X_p $, and the $ f_p $'s are the restrictions of $ f $ to these regions:\n",
    "$$\n",
    "f_p(x) = \\frac{f(x)}{\\alpha_p}, \\quad x \\in X_p.\n",
    "$$\n",
    "\n",
    "Samples of size $ n_p $ are generated from the $ f_p $'s to evaluate each integral $ J_p $ separately using a regular estimator $ \\hat{J}_p $.\n",
    "\n",
    "#### Variance of the Stratified Estimator\n",
    "The variance of the resulting stratified estimator:\n",
    "$$\n",
    "\\hat{J} = \\sum_{p} \\alpha_p \\hat{J}_p,\n",
    "$$\n",
    "is given by:\n",
    "$$\n",
    "\\mathrm{Var}(\\hat{J}) = \\sum_{p} \\alpha_p^2 \\mathrm{Var}(\\hat{J}_p).\n",
    "$$\n",
    "\n",
    "#### Comparison to Standard Monte Carlo\n",
    "The variance of the stratified estimator is generally **much smaller** than the variance of a standard Monte Carlo estimator based on a sample of size $ n = \\sum_{p} n_p $.\n",
    "\n",
    "#### Optimal Sample Allocation\n",
    "The optimal choice of the $ n_p $'s is such that:\n",
    "$$\n",
    "n_p \\propto \\alpha_p \\sqrt{\\int_{X_p} (h(x) - J_p)^2 f(x) \\, \\mathrm{d}x}.\n",
    "$$\n",
    "\n",
    "#### Region Selection\n",
    "If the regions $ X_p $ can be chosen, the variance of the stratified estimator can be further reduced by selecting $ X_p $'s such that the variance factors:\n",
    "$$\n",
    "\\int_{X_p} (h(x) - J_p)^2 f(x) \\, \\mathrm{d}x,\n",
    "$$\n",
    "are similar across regions.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Insights\n",
    "- Stratified sampling reduces variance by leveraging knowledge of $ f $ and partitioning the domain $ \\mathcal{X} $.\n",
    "- Proper allocation of sample sizes $ n_p $ and region selection $ X_p $ are crucial to achieving optimal variance reduction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22fb1207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stratified Sampling Estimate: 0.3251986419078344\n",
      "Variance of Estimate: 7.693177456233189e-06\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def stratified_sampling(h, f, bounds, strata, samples_per_stratum):\n",
    "    \"\"\"\n",
    "    Perform stratified sampling to approximate the integral of h(x) f(x) over a given domain.\n",
    "\n",
    "    Parameters:\n",
    "    - h: Function h(x).\n",
    "    - f: Function f(x).\n",
    "    - bounds: Tuple (a, b) defining the integration domain.\n",
    "    - strata: List of tuples [(a1, b1), (a2, b2), ...] defining the strata within the domain.\n",
    "    - samples_per_stratum: List of integers specifying the number of samples for each stratum.\n",
    "\n",
    "    Returns:\n",
    "    - estimate: The stratified sampling estimate of the integral.\n",
    "    - variance: The variance of the stratified estimator.\n",
    "    \"\"\"\n",
    "    estimates = []\n",
    "    variances = []\n",
    "\n",
    "    # Ensure the strata and samples_per_stratum align\n",
    "    assert len(strata) == len(samples_per_stratum), \"Mismatch between strata and sample counts.\"\n",
    "\n",
    "    total_samples = sum(samples_per_stratum)\n",
    "\n",
    "    # Loop over each stratum\n",
    "    for (a, b), n_samples in zip(strata, samples_per_stratum):\n",
    "        # Generate random samples within the stratum\n",
    "        samples = np.random.uniform(a, b, n_samples)\n",
    "\n",
    "        # Compute h(x) * f(x) for samples\n",
    "        evaluations = h(samples) * f(samples)\n",
    "\n",
    "        # Estimate the integral for this stratum\n",
    "        weight = (b - a) / (bounds[1] - bounds[0])  # Weight of the stratum\n",
    "        stratum_estimate = weight * np.mean(evaluations)\n",
    "        stratum_variance = (weight ** 2) * np.var(evaluations) / n_samples\n",
    "\n",
    "        estimates.append(stratum_estimate)\n",
    "        variances.append(stratum_variance)\n",
    "\n",
    "    # Combine stratum estimates\n",
    "    estimate = np.sum(estimates)\n",
    "    variance = np.sum(variances)\n",
    "\n",
    "    return estimate, variance\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Define the function h(x) and f(x)\n",
    "    def h(x):\n",
    "        return x ** 2\n",
    "\n",
    "    def f(x):\n",
    "        return np.exp(-x)\n",
    "\n",
    "    # Define the bounds of integration and strata\n",
    "    bounds = (0, 2)\n",
    "    strata = [(0, 1), (1, 2)]  # Two strata: [0, 1] and [1, 2]\n",
    "\n",
    "    # Define the number of samples per stratum\n",
    "    samples_per_stratum = [500, 500]\n",
    "\n",
    "    # Perform stratified sampling\n",
    "    estimate, variance = stratified_sampling(h, f, bounds, strata, samples_per_stratum)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Stratified Sampling Estimate: {estimate}\")\n",
    "    print(f\"Variance of Estimate: {variance}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3a599a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stratified Sampling Estimate: 0.3226720619545693\n",
      "Variance of Estimate: 8.198372639646675e-06\n"
     ]
    }
   ],
   "source": [
    "def stratified_sampling(h, f, bounds, strata, samples_per_stratum):\n",
    "    \"\"\"\n",
    "    Perform stratified sampling to approximate the integral of h(x) f(x) over a given domain.\n",
    "\n",
    "    Parameters:\n",
    "    - h: Function h(x).\n",
    "    - f: Function f(x).\n",
    "    - bounds: Tuple (a, b) defining the integration domain.\n",
    "    - strata: List of tuples [(a1, b1), (a2, b2), ...] defining the strata within the domain.\n",
    "    - samples_per_stratum: List of integers specifying the number of samples for each stratum.\n",
    "\n",
    "    Returns:\n",
    "    - estimate: The stratified sampling estimate of the integral.\n",
    "    - variance: The variance of the stratified estimator.\n",
    "    \"\"\"\n",
    "    estimates = []\n",
    "    variances = []\n",
    "\n",
    "    # Ensure the strata and samples_per_stratum align\n",
    "    if len(strata) != len(samples_per_stratum):\n",
    "        raise ValueError(\"Mismatch between strata and sample counts.\")\n",
    "\n",
    "    total_samples = sum(samples_per_stratum)\n",
    "\n",
    "    # Loop over each stratum\n",
    "    for (a, b), n_samples in zip(strata, samples_per_stratum):\n",
    "        # Generate random samples within the stratum manually\n",
    "        samples = []\n",
    "        for _ in range(n_samples):\n",
    "            samples.append(a + (b - a) * random_uniform())\n",
    "\n",
    "        # Compute h(x) * f(x) for samples\n",
    "        evaluations = [(h(x) * f(x)) for x in samples]\n",
    "\n",
    "        # Estimate the integral for this stratum\n",
    "        weight = (b - a) / (bounds[1] - bounds[0])  # Weight of the stratum\n",
    "        stratum_mean = sum(evaluations) / n_samples\n",
    "        stratum_variance = (weight ** 2) * (sum((val - stratum_mean) ** 2 for val in evaluations) / (n_samples - 1)) / n_samples\n",
    "\n",
    "        estimates.append(weight * stratum_mean)\n",
    "        variances.append(stratum_variance)\n",
    "\n",
    "    # Combine stratum estimates\n",
    "    estimate = sum(estimates)\n",
    "    variance = sum(variances)\n",
    "\n",
    "    return estimate, variance\n",
    "\n",
    "def random_uniform():\n",
    "    \"\"\"\n",
    "    Generate a uniform random number between 0 and 1 using a basic linear congruential generator.\n",
    "    \"\"\"\n",
    "    global seed\n",
    "    seed = (1103515245 * seed + 12345) % (2**31)\n",
    "    return seed / (2**31)\n",
    "\n",
    "# Initialize the random seed\n",
    "def set_seed(s):\n",
    "    global seed\n",
    "    seed = s\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Set a fixed random seed for reproducibility\n",
    "    set_seed(12345)\n",
    "\n",
    "    # Define the function h(x) and f(x)\n",
    "    def h(x):\n",
    "        return x ** 2\n",
    "\n",
    "    def f(x):\n",
    "        return math.exp(-x)\n",
    "\n",
    "    # Define the bounds of integration and strata\n",
    "    bounds = (0, 2)\n",
    "    strata = [(0, 1), (1, 2)]  # Two strata: [0, 1] and [1, 2]\n",
    "\n",
    "    # Define the number of samples per stratum\n",
    "    samples_per_stratum = [500, 500]\n",
    "\n",
    "    # Perform stratified sampling\n",
    "    estimate, variance = stratified_sampling(h, f, bounds, strata, samples_per_stratum)\n",
    "\n",
    "    # Print results\n",
    "    print(\"Stratified Sampling Estimate:\", estimate)\n",
    "    print(\"Variance of Estimate:\", variance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09b58141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stratified Sampling Estimate: 0.3226720619545693\n",
      "Variance of Estimate: 8.198372639646675e-06\n",
      "Stratum Contributions:\n",
      "Stratum [0, 1]: 0.079299\n",
      "Stratum [1, 2]: 0.243373\n",
      "\n",
      "Bar Plot Not Supported in This Mode: Use External Plotting Tools.\n"
     ]
    }
   ],
   "source": [
    "def stratified_sampling(h, f, bounds, strata, samples_per_stratum):\n",
    "    \"\"\"\n",
    "    Perform stratified sampling to approximate the integral of h(x) f(x) over a given domain.\n",
    "\n",
    "    Parameters:\n",
    "    - h: Function h(x).\n",
    "    - f: Function f(x).\n",
    "    - bounds: Tuple (a, b) defining the integration domain.\n",
    "    - strata: List of tuples [(a1, b1), (a2, b2), ...] defining the strata within the domain.\n",
    "    - samples_per_stratum: List of integers specifying the number of samples for each stratum.\n",
    "\n",
    "    Returns:\n",
    "    - estimate: The stratified sampling estimate of the integral.\n",
    "    - variance: The variance of the stratified estimator.\n",
    "    - stratum_estimates: List of individual stratum estimates.\n",
    "    \"\"\"\n",
    "    estimates = []\n",
    "    variances = []\n",
    "    stratum_estimates = []\n",
    "\n",
    "    # Ensure the strata and samples_per_stratum align\n",
    "    if len(strata) != len(samples_per_stratum):\n",
    "        raise ValueError(\"Mismatch between strata and sample counts.\")\n",
    "\n",
    "    total_samples = sum(samples_per_stratum)\n",
    "\n",
    "    # Loop over each stratum\n",
    "    for (a, b), n_samples in zip(strata, samples_per_stratum):\n",
    "        # Generate random samples within the stratum manually\n",
    "        samples = []\n",
    "        for _ in range(n_samples):\n",
    "            samples.append(a + (b - a) * random_uniform())\n",
    "\n",
    "        # Compute h(x) * f(x) for samples\n",
    "        evaluations = [(h(x) * f(x)) for x in samples]\n",
    "\n",
    "        # Estimate the integral for this stratum\n",
    "        weight = (b - a) / (bounds[1] - bounds[0])  # Weight of the stratum\n",
    "        stratum_mean = sum(evaluations) / n_samples\n",
    "        stratum_variance = (weight ** 2) * (sum((val - stratum_mean) ** 2 for val in evaluations) / (n_samples - 1)) / n_samples\n",
    "\n",
    "        estimates.append(weight * stratum_mean)\n",
    "        variances.append(stratum_variance)\n",
    "        stratum_estimates.append(weight * stratum_mean)\n",
    "\n",
    "    # Combine stratum estimates\n",
    "    estimate = sum(estimates)\n",
    "    variance = sum(variances)\n",
    "\n",
    "    return estimate, variance, stratum_estimates\n",
    "\n",
    "def random_uniform():\n",
    "    \"\"\"\n",
    "    Generate a uniform random number between 0 and 1 using a basic linear congruential generator.\n",
    "    \"\"\"\n",
    "    global seed\n",
    "    seed = (1103515245 * seed + 12345) % (2**31)\n",
    "    return seed / (2**31)\n",
    "\n",
    "# Initialize the random seed\n",
    "def set_seed(s):\n",
    "    global seed\n",
    "    seed = s\n",
    "\n",
    "# Plotting function to visualize stratum contributions\n",
    "def plot_strata_contributions(strata, stratum_estimates):\n",
    "    \"\"\"\n",
    "    Plot the contributions of each stratum to the integral.\n",
    "\n",
    "    Parameters:\n",
    "    - strata: List of tuples defining the strata.\n",
    "    - stratum_estimates: List of estimates for each stratum.\n",
    "    \"\"\"\n",
    "    x_labels = [f\"[{a}, {b}]\" for a, b in strata]\n",
    "    y_values = stratum_estimates\n",
    "\n",
    "    # Compute positions for the bars\n",
    "    positions = range(len(strata))\n",
    "\n",
    "    print(\"Stratum Contributions:\")\n",
    "    for label, estimate in zip(x_labels, y_values):\n",
    "        print(f\"Stratum {label}: {estimate:.6f}\")\n",
    "\n",
    "    # Plot bar graph\n",
    "    print(\"\\nBar Plot Not Supported in This Mode: Use External Plotting Tools.\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Set a fixed random seed for reproducibility\n",
    "    set_seed(12345)\n",
    "\n",
    "    # Define the function h(x) and f(x)\n",
    "    def h(x):\n",
    "        return x ** 2\n",
    "\n",
    "    def f(x):\n",
    "        return math.exp(-x)\n",
    "\n",
    "    # Define the bounds of integration and strata\n",
    "    bounds = (0, 2)\n",
    "    strata = [(0, 1), (1, 2)]  # Two strata: [0, 1] and [1, 2]\n",
    "\n",
    "    # Define the number of samples per stratum\n",
    "    samples_per_stratum = [500, 500]\n",
    "\n",
    "    # Perform stratified sampling\n",
    "    estimate, variance, stratum_estimates = stratified_sampling(h, f, bounds, strata, samples_per_stratum)\n",
    "\n",
    "    # Print results\n",
    "    print(\"Stratified Sampling Estimate:\", estimate)\n",
    "    print(\"Variance of Estimate:\", variance)\n",
    "\n",
    "    # Plot stratum contributions\n",
    "    plot_strata_contributions(strata, stratum_estimates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db490b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def stratified_sampling(h, f, bounds, strata, samples_per_stratum):\n",
    "    \"\"\"\n",
    "    Perform stratified sampling to approximate the integral of h(x) f(x) over a given domain.\n",
    "\n",
    "    Parameters:\n",
    "    - h: Function h(x).\n",
    "    - f: Function f(x).\n",
    "    - bounds: Tuple (a, b) defining the integration domain.\n",
    "    - strata: List of tuples [(a1, b1), (a2, b2), ...] defining the strata within the domain.\n",
    "    - samples_per_stratum: List of integers specifying the number of samples for each stratum.\n",
    "\n",
    "    Returns:\n",
    "    - estimate: The stratified sampling estimate of the integral.\n",
    "    - variance: The variance of the stratified estimator.\n",
    "    - stratum_estimates: List of individual stratum estimates.\n",
    "    \"\"\"\n",
    "    estimates = []\n",
    "    variances = []\n",
    "    stratum_estimates = []\n",
    "\n",
    "    # Ensure the strata and samples_per_stratum align\n",
    "    if len(strata) != len(samples_per_stratum):\n",
    "        raise ValueError(\"Mismatch between strata and sample counts.\")\n",
    "\n",
    "    total_samples = sum(samples_per_stratum)\n",
    "\n",
    "    # Loop over each stratum\n",
    "    for (a, b), n_samples in zip(strata, samples_per_stratum):\n",
    "        # Generate random samples within the stratum manually\n",
    "        samples = []\n",
    "        for _ in range(n_samples):\n",
    "            samples.append(a + (b - a) * random_uniform())\n",
    "\n",
    "        # Compute h(x) * f(x) for samples\n",
    "        evaluations = [(h(x) * f(x)) for x in samples]\n",
    "\n",
    "        # Estimate the integral for this stratum\n",
    "        weight = (b - a) / (bounds[1] - bounds[0])  # Weight of the stratum\n",
    "        stratum_mean = sum(evaluations) / n_samples\n",
    "        stratum_variance = (weight ** 2) * (sum((val - stratum_mean) ** 2 for val in evaluations) / (n_samples - 1)) / n_samples\n",
    "\n",
    "        estimates.append(weight * stratum_mean)\n",
    "        variances.append(stratum_variance)\n",
    "        stratum_estimates.append(weight * stratum_mean)\n",
    "\n",
    "    # Combine stratum estimates\n",
    "    estimate = sum(estimates)\n",
    "    variance = sum(variances)\n",
    "\n",
    "    return estimate, variance, stratum_estimates\n",
    "\n",
    "def random_uniform():\n",
    "    \"\"\"\n",
    "    Generate a uniform random number between 0 and 1 using a basic linear congruential generator.\n",
    "    \"\"\"\n",
    "    global seed\n",
    "    seed = (1103515245 * seed + 12345) % (2**31)\n",
    "    return seed / (2**31)\n",
    "\n",
    "# Initialize the random seed\n",
    "def set_seed(s):\n",
    "    global seed\n",
    "    seed = s\n",
    "\n",
    "# Plotting function to visualize stratum contributions\n",
    "def plot_strata_contributions(strata, stratum_estimates):\n",
    "    \"\"\"\n",
    "    Plot the contributions of each stratum to the integral.\n",
    "\n",
    "    Parameters:\n",
    "    - strata: List of tuples defining the strata.\n",
    "    - stratum_estimates: List of estimates for each stratum.\n",
    "    \"\"\"\n",
    "    x_labels = [f\"[{a}, {b}]\" for a, b in strata]\n",
    "    y_values = stratum_estimates\n",
    "\n",
    "    # Create bar plot using seaborn\n",
    "    sns.barplot(x=x_labels, y=y_values, palette=\"Blues_d\")\n",
    "    plt.xlabel(\"Stratum\")\n",
    "    plt.ylabel(\"Contribution to Integral\")\n",
    "    plt.title(\"Stratum Contributions\")\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Set a fixed random seed for reproducibility\n",
    "    set_seed(12345)\n",
    "\n",
    "    # Define the function h(x) and f(x)\n",
    "    def h(x):\n",
    "        return x ** 2\n",
    "\n",
    "    def f(x):\n",
    "        return math.exp(-x)\n",
    "\n",
    "    # Define the bounds of integration and strata\n",
    "    bounds = (0, 2)\n",
    "    strata = [(0, 1), (1, 2)]  # Two strata: [0, 1] and [1, 2]\n",
    "\n",
    "    # Define the number of samples per stratum\n",
    "    samples_per_stratum = [500, 500]\n",
    "\n",
    "    # Perform stratified sampling\n",
    "    estimate, variance, stratum_estimates = stratified_sampling(h, f, bounds, strata, samples_per_stratum)\n",
    "\n",
    "    # Print results\n",
    "    print(\"Stratified Sampling Estimate:\", estimate)\n",
    "    print(\"Variance of Estimate:\", variance)\n",
    "\n",
    "    # Plot stratum contributions\n",
    "    plot_strata_contributions(strata, stratum_estimates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc08b42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
