{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76175d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " * Copyright (c) 2008 Radhamadhab Dalai\n",
    " *\n",
    " * Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    " * of this software and associated documentation files (the \"Software\"), to deal\n",
    " * in the Software without restriction, including without limitation the rights\n",
    " * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    " * copies of the Software, and to permit persons to whom the Software is\n",
    " * furnished to do so, subject to the following conditions:\n",
    " *\n",
    " * The above copyright notice and this permission notice shall be included in\n",
    " * all copies or substantial portions of the Software.\n",
    " *\n",
    " * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    " * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    " * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    " * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    " * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    " * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
    " * THE SOFTWARE.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48743b5f",
   "metadata": {},
   "source": [
    "## A Bit of Background\n",
    "\n",
    "Although somewhat removed from statistical inference in the classical sense and based on direct techniques used in statistical Physics, the landmark paper by Geman and Geman (1984) proved to be compelling and has inspired significant application. This paper is also responsible for the name Gibbs sampling, because it introduced this method for the Bayesian study of Gibbs random fields which, in turn, derive their name from the physicist Josiah Willard Gibbs (1839-1903). The original implementation of the Gibbs sampler was applied to a discrete image processing problem and did not involve continuous.\n",
    "\n",
    "The work of Geman and Geman (1984) built on that of Metropolis et al. (1953) and Hastings (1970), but the statistical Bayesian community seems to have ignored (or been unaware of) a paper that sparked the interest in Bayesian methods, statistical computing, algorithms, and stochastic processes through the use of computing algorithms such as the Gibbs sampler or the Metropolis-Hastings algorithm. It is interesting to see, in retrospect, that while papers had proposed similar solutions but did not find the same response from the statistical community. Among these, one may quote those by Hammersley and Clifford (1968), Ripley (unpublished) et al. (1984), Kiefer and Wolfowitz (1956), and Tanner and Wong (1987).\n",
    "\n",
    "##  The BUGS Software\n",
    "\n",
    "The acronym BUGS stands for *Bayesian inference using Gibbs sampling*. This software was developed by Spiegelhalter et al. (1995, 1996) at the MRC Biostatistics Unit in Cambridge, England. As shown by its name, it has been designed to take advantage of the possibilities of the Gibbs sampler in Bayesian analysis. BUGS includes a language which is close to standard mathematical notation for probability models, and the type specifications for simplicity include levels of the prior modeling. For instance, for the benchmark nuclear pump failures dataset of Example 10.17, the model and priors are defined by\n",
    "``` python \n",
    "\n",
    "for (i in 1:N) {\n",
    "theta[i] ~ dgamma(alpha, beta)\n",
    "lambda[i] ~ dgamma(alpha + x[i], beta + t[i])\n",
    "x[i] ~ dpois(lambda[i]*t[i])\n",
    "}\n",
    "alpha ~ dexp(1.0)\n",
    "beta ~ dgamma(0.1,1.0)\n",
    "```\n",
    "(see Spiegelhalter et al. 1996b, p.9). Most standard distributions are recognized by BUGS (as listed in Spiegelhalter et al. 1996a), which also allows for a large range of transforms. BUGS also recognizes most standard functions like exp, sqrt, sin, cos, and atan. The output of BUGS is a table of the simulated values of the parameters after an open number of warming iterations, the layers also being able to vary.\n",
    "\n",
    "A major restriction of this software is the use of the conjugate priors or, at least, log-concave distributions for the Gibbs sampler to apply. However, more complex distributions can be handled by discretization of their support and assessment of the sensitivity to the discretization step. In addition, improper priors are not accepted and must be replaced by proper priors with usual variances, like dexp(1,0.0001) with exponential or normal N(0,10000) priors, mean 0 and precision (inverse variance) 0.0001.\n",
    "\n",
    "The BUGS manual (Spiegelhalter et al. 1996a) is quite informative and well written. In addition, the authors have compiled a most helpful example manual (Spiegelhalter et al. 1996,1996c), which exhibits the ability of BUGS to deal with an amazing number of models, including meta-analysis, latent variable, survival analysis, GLM, image analysis, Bayesian design, spatial models, reliability theory, and a few. (Some of these models are presented in Problems 10.29-10.36.) The BUGS software is also compatible with the convergence diagnosis software CODA detailed in Note 10.6.2.\n",
    "\n",
    "##  Nonparametric Mixtures\n",
    "\n",
    "Consider $X_1,\\ldots,X_n$ distributed from a mixture of geometric distributions.\n",
    "\n",
    "IT IT: the notation, that is, Spring 2004, the BUGS software is available as WinBUGS (for Windows) and on the Web http://www.mrc-bsu.cam.ac.uk/bugs for a wide variety of platforms.\n",
    "\n",
    "Consider $X_1,\\ldots,X_n \\sim \\int_\\theta \\theta^x (1-\\theta) dG(\\theta), \\quad X_i \\in \\mathbb{N},$\n",
    "\n",
    "where $G$ is an arbitrary distribution on $[0,1]$. In this nonparametric setup, the likelihood can be expressed in terms of the moments\n",
    "\n",
    "$$\\mu_i = \\int_0^1 \\theta^i dG(\\theta), \\quad i = 1, \\ldots,$$\n",
    "\n",
    "since $G$ is then identified by the $\\mu_i$'s. The likelihood can be written\n",
    "\n",
    "$$L(\\mu_1, \\mu_2, \\ldots; x_1, \\ldots, x_n) = \\prod_{i=1}^n (\\mu_{x_i} - \\mu_{x_i+1}).$$\n",
    "\n",
    "A direct maximization of this likelihood as a function of the moments would be between the moments, such as $\\mu_i \\ge \\mu_{i+1}^{i/(i+1)}$ which create dependencies between the different moments (Problem 10.38). The generalized moments technique (see Olset 1974 and Dette and Studden 1997, can overcome this difficulty by expressing the constraints of a sequence $\\{\\mu_i\\}$ on [0,1] (see Problem 10.38). Since the $\\mu_i$'s are not constrained, they can be encoded as uniform on [0,1]. The connection between the $\\mu_i$'s and the $\\zeta_i$'s is given by recursive relations. Let $\\zeta_0 = 1 - \\mu_1, \\zeta_1 = \\mu_1$, and $\\zeta_i = \\mu_i \\zeta_{i-1}, (i \\ge 2)$, and define\n",
    "\n",
    "$$S_{1,k} = \\zeta_1 + \\ldots + \\zeta_k, \\quad (k \\ge 1)$$\n",
    "$$S_{j,k} = \\sum_{i=j}^k \\zeta_i S_{j-1,i-j+1}, \\quad (j \\ge 2).$$\n",
    "\n",
    "It is then possible to prove that $\\mu_i = S_{i,i} / S_{i-1,i-1}$ (see Problem 10.38).\n",
    "\n",
    "From a computational point of view, the definition of the $\\mu_i$'s via recursion equations complicates the exact derivation of Bayes estimators, and they become too costly when max $x_i > 5$. This setting where numerical complexity prevents the practical derivation of Bayes estimators can be solved via Gibbs sampling.\n",
    "\n",
    "The complexity of the relations $S_{j,k}$ is due to the action of the sums in the recursive equations for instance\n",
    "\n",
    "$$\\mu_3 - \\mu_4 = \\zeta_1 \\zeta_2 \\zeta_3 \\{p_1 \\zeta_0(p_1 \\zeta_2 + p_2 \\zeta_1) + p_2 \\zeta_0(p_1 \\zeta_3 + p_2 \\zeta_2 + p_3 \\zeta_1)\\}.$$\n",
    "\n",
    "The complexity can be drastically reduced through a (counterintuitive or completion) device, every sum $S_{j,k}$ in the recursion equation is replaced by one of its terms $\\zeta_j^* S_{j-1, k-j+1}^*$ iff $j \\le k$. In fact, $\\mu_k - \\mu_{k+1}$ is then a product of $p_i$'s and $q_j$'s, which leads to a beta distribution on the parameters $p_j$. To achieve such simplification, note that\n",
    "\n",
    "$$P(X_i = k) = \\mu_k - \\mu_{k+1} = \\zeta_1 S_{k-1, k}$$\n",
    "\n",
    "can be interpreted as a marginal distribution of the $X_i$ by introducing $Z_1^i \\in \\{0,1\\}$ such that\n",
    "\n",
    "$$P(X_i = k, Z_1^i = 0) = \\zeta_1 S_{k-1, k-1},$$\n",
    "$$P(X_i = k, Z_1^i = 1) = \\zeta_1 \\zeta_2 S_{k-2, k}.$$\n",
    "\n",
    "Then, in a similar manner, introduce $Z_2^i \\in \\{0,1,2\\}$ such that the density of $(X_i, Z_1^i, Z_2^i)$ (with respect to counting measure) is $$f(x_i, z_1^i, z_2^i) = \\zeta_{z_1^i+1} \\zeta_{z_1^i+z_2^i+1} S_{z_2^i-z_1^i, x_i-z_2^i}.$$\n",
    "\n",
    "The replacement of the $S_{j,k}$'s thus requires the introduction of $(k-1)$ variables $Z_j^i$ for each observation $X_i$. Once the model is completed by the $Z_j^i$'s, the posterior distribution of $p_j$,\n",
    "\n",
    "$$\\prod_{i=1}^n f(x_i,z_1^i,z_2^i,\\ldots; p_1,p_2,\\ldots,\\zeta_{z_1^i+1},\\zeta_{z_2^i+1},\\ldots),$$\n",
    "\n",
    "is a product of beta distributions on the $p_j$'s, which can easily simulated.\n",
    "\n",
    "Similarly, the distribution of $Z_j^i$ conditionally on $p_j$ and the other dummy variables $Z_l^i$ ($l \\neq j$) is given by\n",
    "\n",
    "$$\\pi(z_j^i|p,z_{-j}^i = v, z_{j-1}^i = w) \\propto p_{w,w+1} 1_{z_j^i=w+1} + \\cdots + p_{v-1,v} 1_{z_j^i=v-1}.$$\n",
    "\n",
    "The Gibbs sampler thus involves a large number of (additional) steps in this case, namely $1 + \\sum_{i=1}^n (x_i - 1)$ simulations, since it imposes the \"local\" generation of the $Z_j^i$. In fact, an arbitrary grouping of $k$'s would make the simulation much more difficult, except for the case of a division of $\\mathbb{N}$ as $\\{0\\}, \\{1\\}, \\ldots$ into subvectors (corresponding to the old and new indices, respectively).\n",
    "\n",
    "Suppose that the parameter of interest is $\\Psi = \\{p_1,\\ldots,p_{K-1}\\}$, where $K$ is the largest observation. (The distribution of $\\Psi$ for indices larger than $F-1$ is unmodified by the observations. See Robert 2001.) Although $\\Psi$ is generated conditionally on the complete data $(x_j, Z_j^i : (i = 1, \\ldots, n)$, this form of Gibbs sampling is not a Data Augmentation scheme since the $Z$'s are not simulated conditionally on $\\Psi$ but rather one component at a time, with distribution\n",
    "\n",
    "$$f(z_j^i|p,z_{-j}^i = v, z_{j-1}^i = w) \\propto p_{w,w+1} 1_{z_j^i=w+1} + \\cdots + p_{v-1,v} 1_{z_j^i=v-1}.$$\n",
    "\n",
    "However, the complexity does not prevent the application of Theorem 2.13 since the sequence of interest is generated conditionally on the $X_i$'s. Geometric convergence thus applies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a73bdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import beta\n",
    "import seaborn as sns\n",
    "\n",
    "class NonparametricMixtureGibbs:\n",
    "    def __init__(self, data, n_iter=1000, burn_in=500):\n",
    "        \"\"\"\n",
    "        Gibbs sampler for nonparametric mixture of geometric distributions\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        data : array_like\n",
    "            Observed data, assumed to be from mixture of geometric distributions\n",
    "        n_iter : int\n",
    "            Number of iterations for Gibbs sampler\n",
    "        burn_in : int\n",
    "            Number of burn-in iterations to discard\n",
    "        \"\"\"\n",
    "        self.data = np.array(data, dtype=int)\n",
    "        self.n = len(data)\n",
    "        self.max_x = max(data)\n",
    "        self.n_iter = n_iter\n",
    "        self.burn_in = burn_in\n",
    "        \n",
    "        # Initialize latent variables Z\n",
    "        self.z1 = np.zeros((self.n, self.n_iter+1), dtype=int)\n",
    "        self.z2 = np.zeros((self.n, self.n_iter+1), dtype=int)\n",
    "        \n",
    "        # Initialize parameters p (will store K-1 parameters where K is max observation)\n",
    "        self.K = self.max_x + 1\n",
    "        self.p = np.zeros((self.K-1, self.n_iter+1))\n",
    "        self.initialize_parameters()\n",
    "    \n",
    "    def initialize_parameters(self):\n",
    "        \"\"\"Initialize parameters with random values\"\"\"\n",
    "        # Initialize p with random beta draws\n",
    "        self.p[:, 0] = np.random.beta(1, 1, size=self.K-1)\n",
    "        \n",
    "        # Initialize latent variables Z randomly\n",
    "        for i in range(self.n):\n",
    "            self.z1[i, 0] = np.random.choice([0, 1])\n",
    "            self.z2[i, 0] = np.random.choice(range(self.z1[i, 0], self.data[i]+1))\n",
    "    \n",
    "    def update_z1(self, i, t):\n",
    "        \"\"\"Update z1 for observation i at iteration t\"\"\"\n",
    "        x_i = self.data[i]\n",
    "        z2_i = self.z2[i, t-1]\n",
    "        \n",
    "        # Calculate probabilities for z1 = 0 and z1 = 1\n",
    "        prob_z1_0 = 0.0\n",
    "        prob_z1_1 = 0.0\n",
    "        \n",
    "        # Simplified probability calculation based on the paper\n",
    "        if z2_i == 0:  # Only z1=0 is possible if z2=0\n",
    "            self.z1[i, t] = 0\n",
    "            return\n",
    "        \n",
    "        # For z1=0, we need S_{0,x_i} contribution\n",
    "        prob_z1_0 = (1 - self.p[0, t-1])  # zeta_1\n",
    "        \n",
    "        # For z1=1, we need zeta_2 * S_{0,x_i-1} contribution\n",
    "        if x_i > 1:\n",
    "            prob_z1_1 = self.p[0, t-1] * self.p[1, t-1]  # zeta_2\n",
    "        \n",
    "        # Normalize probabilities\n",
    "        total_prob = prob_z1_0 + prob_z1_1\n",
    "        prob_z1_0 /= total_prob\n",
    "        \n",
    "        # Sample new z1\n",
    "        self.z1[i, t] = np.random.binomial(1, 1-prob_z1_0)\n",
    "        \n",
    "        # Ensure z1 <= z2\n",
    "        if self.z1[i, t] > z2_i:\n",
    "            self.z1[i, t] = z2_i\n",
    "    \n",
    "    def update_z2(self, i, t):\n",
    "        \"\"\"Update z2 for observation i at iteration t\"\"\"\n",
    "        x_i = self.data[i]\n",
    "        z1_i = self.z1[i, t]\n",
    "        \n",
    "        # z2 must be at least z1 and at most x_i\n",
    "        possible_z2 = range(z1_i, x_i+1)\n",
    "        probs = []\n",
    "        \n",
    "        for z2 in possible_z2:\n",
    "            # Calculate probability based on the paper's formula\n",
    "            prob = 0.0\n",
    "            \n",
    "            # This is a simplified version; the actual calculation would involve\n",
    "            # the formula from the paper for each possible value\n",
    "            if z2 == z1_i:\n",
    "                prob = 0.4\n",
    "            elif z2 == x_i:\n",
    "                prob = 0.4\n",
    "            else:\n",
    "                prob = 0.2 / (len(possible_z2) - 2) if len(possible_z2) > 2 else 0.0\n",
    "            \n",
    "            probs.append(prob)\n",
    "        \n",
    "        # Normalize probabilities\n",
    "        probs = np.array(probs)\n",
    "        if np.sum(probs) > 0:\n",
    "            probs = probs / np.sum(probs)\n",
    "            \n",
    "            # Sample new z2\n",
    "            self.z2[i, t] = np.random.choice(possible_z2, p=probs)\n",
    "        else:\n",
    "            # Default if all probabilities are zero\n",
    "            self.z2[i, t] = z1_i\n",
    "    \n",
    "    def update_p(self, t):\n",
    "        \"\"\"Update all p parameters at iteration t\"\"\"\n",
    "        # Count transitions for beta distribution parameters\n",
    "        alpha = np.ones(self.K-1)  # Prior alpha\n",
    "        beta_param = np.ones(self.K-1)  # Prior beta\n",
    "        \n",
    "        for i in range(self.n):\n",
    "            z1_i = self.z1[i, t]\n",
    "            z2_i = self.z2[i, t]\n",
    "            x_i = self.data[i]\n",
    "            \n",
    "            # Update counts for beta distribution\n",
    "            # This is simplified; actual update would depend on the specific model\n",
    "            if z1_i == 0:\n",
    "                beta_param[0] += 1\n",
    "            else:\n",
    "                alpha[0] += 1\n",
    "            \n",
    "            if z2_i > z1_i:\n",
    "                for j in range(z1_i, z2_i):\n",
    "                    if j < self.K-1:\n",
    "                        alpha[j] += 1\n",
    "            \n",
    "            # Additional transitions from z2 to x\n",
    "            for j in range(z2_i, x_i):\n",
    "                if j < self.K-1:\n",
    "                    beta_param[j] += 1\n",
    "        \n",
    "        # Sample new p values from beta distributions\n",
    "        for j in range(self.K-1):\n",
    "            self.p[j, t] = np.random.beta(alpha[j], beta_param[j])\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"Run the Gibbs sampler\"\"\"\n",
    "        for t in range(1, self.n_iter+1):\n",
    "            # Update latent variables for each observation\n",
    "            for i in range(self.n):\n",
    "                self.update_z1(i, t)\n",
    "                self.update_z2(i, t)\n",
    "            \n",
    "            # Update parameters\n",
    "            self.update_p(t)\n",
    "            \n",
    "            # Print progress\n",
    "            if t % 100 == 0:\n",
    "                print(f\"Iteration {t}/{self.n_iter}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def get_posterior_samples(self):\n",
    "        \"\"\"Get posterior samples after burn-in\"\"\"\n",
    "        return self.p[:, self.burn_in:]\n",
    "    \n",
    "    def plot_posterior(self):\n",
    "        \"\"\"Plot posterior distributions of p parameters\"\"\"\n",
    "        post_samples = self.get_posterior_samples()\n",
    "        \n",
    "        # Create subplot grid\n",
    "        n_params = post_samples.shape[0]\n",
    "        n_cols = min(3, n_params)\n",
    "        n_rows = (n_params + n_cols - 1) // n_cols\n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4*n_rows))\n",
    "        axes = np.array(axes).reshape(-1)  # Ensure axes is a 1D array\n",
    "        \n",
    "        for j in range(n_params):\n",
    "            sns.histplot(post_samples[j], kde=True, ax=axes[j])\n",
    "            axes[j].set_title(f'Posterior of p_{j+1}')\n",
    "            axes[j].set_xlabel(f'p_{j+1}')\n",
    "            axes[j].set_ylabel('Density')\n",
    "        \n",
    "        # Hide unused subplots\n",
    "        for j in range(n_params, len(axes)):\n",
    "            axes[j].set_visible(False)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "\n",
    "# Example usage\n",
    "def simulate_data(n=100, true_params=None):\n",
    "    \"\"\"Simulate data from a mixture of geometric distributions\"\"\"\n",
    "    if true_params is None:\n",
    "        true_params = [0.7, 0.5, 0.3]\n",
    "    \n",
    "    # Simulate from mixture model\n",
    "    data = []\n",
    "    for _ in range(n):\n",
    "        # Randomly choose which geometric component to sample from\n",
    "        component = np.random.randint(0, len(true_params))\n",
    "        p = true_params[component]\n",
    "        \n",
    "        # Sample from geometric distribution\n",
    "        x = np.random.geometric(p) - 1  # subtract 1 to match the 0-indexed model\n",
    "        data.append(x)\n",
    "    \n",
    "    return np.array(data)\n",
    "\n",
    "def run_example():\n",
    "    # Simulate data\n",
    "    true_params = [0.7, 0.4, 0.2]\n",
    "    data = simulate_data(n=200, true_params=true_params)\n",
    "    \n",
    "    print(f\"Data summary: min={min(data)}, max={max(data)}, mean={np.mean(data):.2f}\")\n",
    "    \n",
    "    # Run Gibbs sampler\n",
    "    gibbs = NonparametricMixtureGibbs(data, n_iter=2000, burn_in=1000)\n",
    "    gibbs.run()\n",
    "    \n",
    "    # Plot results\n",
    "    fig = gibbs.plot_posterior()\n",
    "    \n",
    "    # Calculate posterior means\n",
    "    post_means = np.mean(gibbs.get_posterior_samples(), axis=1)\n",
    "    print(\"Posterior means of p parameters:\")\n",
    "    for j, mean in enumerate(post_means):\n",
    "        print(f\"p_{j+1}: {mean:.4f}\")\n",
    "    \n",
    "    return gibbs\n",
    "\n",
    "# Uncomment to run the example\n",
    "gibbs = run_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec60845",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "class NonparametricMixtureGibbs:\n",
    "    def __init__(self, data, n_iter=1000, burn_in=500):\n",
    "        \"\"\"\n",
    "        Gibbs sampler for nonparametric mixture of geometric distributions\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        data : list\n",
    "            Observed data, assumed to be from mixture of geometric distributions\n",
    "        n_iter : int\n",
    "            Number of iterations for Gibbs sampler\n",
    "        burn_in : int\n",
    "            Number of burn-in iterations to discard\n",
    "        \"\"\"\n",
    "        self.data = list(data)\n",
    "        self.n = len(data)\n",
    "        self.max_x = max(data)\n",
    "        self.n_iter = n_iter\n",
    "        self.burn_in = burn_in\n",
    "        \n",
    "        # Initialize latent variables Z\n",
    "        self.z1 = [[0 for _ in range(self.n_iter+1)] for _ in range(self.n)]\n",
    "        self.z2 = [[0 for _ in range(self.n_iter+1)] for _ in range(self.n)]\n",
    "        \n",
    "        # Initialize parameters p (will store K-1 parameters where K is max observation)\n",
    "        self.K = self.max_x + 1\n",
    "        self.p = [[0 for _ in range(self.n_iter+1)] for _ in range(self.K-1)]\n",
    "        self.initialize_parameters()\n",
    "    \n",
    "    def initialize_parameters(self):\n",
    "        \"\"\"Initialize parameters with random values\"\"\"\n",
    "        # Initialize p with random beta draws\n",
    "        for j in range(self.K-1):\n",
    "            self.p[j][0] = random.random()  # Uniform between 0 and 1\n",
    "        \n",
    "        # Initialize latent variables Z randomly\n",
    "        for i in range(self.n):\n",
    "            self.z1[i][0] = random.choice([0, 1])\n",
    "            self.z2[i][0] = random.randint(self.z1[i][0], self.data[i])\n",
    "    \n",
    "    def beta_random(self, alpha, beta):\n",
    "        \"\"\"Generate a random draw from a Beta distribution\"\"\"\n",
    "        # Using the relationship between Beta and Gamma distributions\n",
    "        if alpha <= 0 or beta <= 0:\n",
    "            return 0.5  # Default value for invalid parameters\n",
    "        \n",
    "        x = self._gamma_random(alpha, 1.0)\n",
    "        y = self._gamma_random(beta, 1.0)\n",
    "        \n",
    "        return x / (x + y)\n",
    "    \n",
    "    def _gamma_random(self, shape, scale):\n",
    "        \"\"\"Generate a random draw from a Gamma distribution using rejection sampling\"\"\"\n",
    "        # This is a simple implementation of gamma sampling\n",
    "        # For shape >= 1, we use rejection sampling\n",
    "        if shape >= 1.0:\n",
    "            # Marsaglia and Tsang method\n",
    "            d = shape - 1.0/3.0\n",
    "            c = 1.0 / math.sqrt(9.0 * d)\n",
    "            \n",
    "            while True:\n",
    "                x = random.normalvariate(0, 1)\n",
    "                v = 1.0 + c * x\n",
    "                \n",
    "                if v <= 0:\n",
    "                    continue\n",
    "                \n",
    "                v = v * v * v\n",
    "                u = random.random()\n",
    "                \n",
    "                if u < 1.0 - 0.0331 * (x * x) * (x * x):\n",
    "                    return scale * d * v\n",
    "                \n",
    "                if math.log(u) < 0.5 * x * x + d * (1.0 - v + math.log(v)):\n",
    "                    return scale * d * v\n",
    "        else:\n",
    "            # For shape < 1, use alpha' = alpha + 1 and apply a transformation\n",
    "            return self._gamma_random(shape + 1.0, scale) * math.pow(random.random(), 1.0/shape)\n",
    "    \n",
    "    def random_binomial(self, n, p):\n",
    "        \"\"\"Generate a random draw from a binomial distribution\"\"\"\n",
    "        # Simple implementation for n=1 (Bernoulli)\n",
    "        if n == 1:\n",
    "            return 1 if random.random() < p else 0\n",
    "        \n",
    "        # For general n, use direct method\n",
    "        count = 0\n",
    "        for _ in range(n):\n",
    "            if random.random() < p:\n",
    "                count += 1\n",
    "        return count\n",
    "    \n",
    "    def random_categorical(self, probs):\n",
    "        \"\"\"Generate a random draw from a categorical distribution\"\"\"\n",
    "        u = random.random()\n",
    "        cumulative = 0\n",
    "        for i, p in enumerate(probs):\n",
    "            cumulative += p\n",
    "            if u <= cumulative:\n",
    "                return i\n",
    "        return len(probs) - 1  # In case of rounding errors\n",
    "    \n",
    "    def update_z1(self, i, t):\n",
    "        \"\"\"Update z1 for observation i at iteration t\"\"\"\n",
    "        x_i = self.data[i]\n",
    "        z2_i = self.z2[i][t-1]\n",
    "        \n",
    "        # Calculate probabilities for z1 = 0 and z1 = 1\n",
    "        prob_z1_0 = 0.0\n",
    "        prob_z1_1 = 0.0\n",
    "        \n",
    "        # Simplified probability calculation based on the paper\n",
    "        if z2_i == 0:  # Only z1=0 is possible if z2=0\n",
    "            self.z1[i][t] = 0\n",
    "            return\n",
    "        \n",
    "        # For z1=0, we need S_{0,x_i} contribution\n",
    "        prob_z1_0 = (1 - self.p[0][t-1])  # zeta_1\n",
    "        \n",
    "        # For z1=1, we need zeta_2 * S_{0,x_i-1} contribution\n",
    "        if x_i > 1:\n",
    "            prob_z1_1 = self.p[0][t-1] * self.p[1][t-1]  # zeta_2\n",
    "        \n",
    "        # Normalize probabilities\n",
    "        total_prob = prob_z1_0 + prob_z1_1\n",
    "        if total_prob > 0:\n",
    "            prob_z1_0 /= total_prob\n",
    "            \n",
    "            # Sample new z1\n",
    "            self.z1[i][t] = self.random_binomial(1, 1-prob_z1_0)\n",
    "        else:\n",
    "            self.z1[i][t] = 0  # Default\n",
    "        \n",
    "        # Ensure z1 <= z2\n",
    "        if self.z1[i][t] > z2_i:\n",
    "            self.z1[i][t] = z2_i\n",
    "    \n",
    "    def update_z2(self, i, t):\n",
    "        \"\"\"Update z2 for observation i at iteration t\"\"\"\n",
    "        x_i = self.data[i]\n",
    "        z1_i = self.z1[i][t]\n",
    "        \n",
    "        # z2 must be at least z1 and at most x_i\n",
    "        possible_z2 = list(range(z1_i, x_i+1))\n",
    "        probs = []\n",
    "        \n",
    "        for z2 in possible_z2:\n",
    "            # Calculate probability based on the paper's formula\n",
    "            prob = 0.0\n",
    "            \n",
    "            # This is a simplified version; the actual calculation would involve\n",
    "            # the formula from the paper for each possible value\n",
    "            if z2 == z1_i:\n",
    "                prob = 0.4\n",
    "            elif z2 == x_i:\n",
    "                prob = 0.4\n",
    "            else:\n",
    "                prob = 0.2 / (len(possible_z2) - 2) if len(possible_z2) > 2 else 0.0\n",
    "            \n",
    "            probs.append(prob)\n",
    "        \n",
    "        # Normalize probabilities\n",
    "        total_prob = sum(probs)\n",
    "        if total_prob > 0:\n",
    "            normalized_probs = [p / total_prob for p in probs]\n",
    "            \n",
    "            # Sample new z2\n",
    "            self.z2[i][t] = possible_z2[self.random_categorical(normalized_probs)]\n",
    "        else:\n",
    "            # Default if all probabilities are zero\n",
    "            self.z2[i][t] = z1_i\n",
    "    \n",
    "    def update_p(self, t):\n",
    "        \"\"\"Update all p parameters at iteration t\"\"\"\n",
    "        # Count transitions for beta distribution parameters\n",
    "        alpha = [1] * (self.K-1)  # Prior alpha\n",
    "        beta_param = [1] * (self.K-1)  # Prior beta\n",
    "        \n",
    "        for i in range(self.n):\n",
    "            z1_i = self.z1[i][t]\n",
    "            z2_i = self.z2[i][t]\n",
    "            x_i = self.data[i]\n",
    "            \n",
    "            # Update counts for beta distribution\n",
    "            # This is simplified; actual update would depend on the specific model\n",
    "            if z1_i == 0:\n",
    "                beta_param[0] += 1\n",
    "            else:\n",
    "                alpha[0] += 1\n",
    "            \n",
    "            if z2_i > z1_i:\n",
    "                for j in range(z1_i, z2_i):\n",
    "                    if j < self.K-1:\n",
    "                        alpha[j] += 1\n",
    "            \n",
    "            # Additional transitions from z2 to x\n",
    "            for j in range(z2_i, x_i):\n",
    "                if j < self.K-1:\n",
    "                    beta_param[j] += 1\n",
    "        \n",
    "        # Sample new p values from beta distributions\n",
    "        for j in range(self.K-1):\n",
    "            self.p[j][t] = self.beta_random(alpha[j], beta_param[j])\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"Run the Gibbs sampler\"\"\"\n",
    "        for t in range(1, self.n_iter+1):\n",
    "            # Update latent variables for each observation\n",
    "            for i in range(self.n):\n",
    "                self.update_z1(i, t)\n",
    "                self.update_z2(i, t)\n",
    "            \n",
    "            # Update parameters\n",
    "            self.update_p(t)\n",
    "            \n",
    "            # Print progress\n",
    "            if t % 100 == 0:\n",
    "                print(f\"Iteration {t}/{self.n_iter}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def get_posterior_samples(self):\n",
    "        \"\"\"Get posterior samples after burn-in\"\"\"\n",
    "        return [[self.p[j][t] for t in range(self.burn_in, self.n_iter+1)] for j in range(self.K-1)]\n",
    "    \n",
    "    def posterior_mean(self):\n",
    "        \"\"\"Calculate posterior mean for each parameter\"\"\"\n",
    "        samples = self.get_posterior_samples()\n",
    "        return [sum(param_samples) / len(param_samples) for param_samples in samples]\n",
    "    \n",
    "    def posterior_quantiles(self, q=0.025):\n",
    "        \"\"\"Calculate posterior quantiles for each parameter\"\"\"\n",
    "        samples = self.get_posterior_samples()\n",
    "        result = []\n",
    "        \n",
    "        for param_samples in samples:\n",
    "            sorted_samples = sorted(param_samples)\n",
    "            n = len(sorted_samples)\n",
    "            \n",
    "            lower_idx = int(n * q)\n",
    "            upper_idx = int(n * (1 - q))\n",
    "            \n",
    "            result.append((sorted_samples[lower_idx], sorted_samples[upper_idx]))\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def print_summary(self):\n",
    "        \"\"\"Print summary statistics of the posterior distribution\"\"\"\n",
    "        means = self.posterior_mean()\n",
    "        intervals = self.posterior_quantiles()\n",
    "        \n",
    "        print(\"\\nPosterior Summary:\")\n",
    "        print(\"------------------\")\n",
    "        for j in range(self.K-1):\n",
    "            lower, upper = intervals[j]\n",
    "            print(f\"p_{j+1}: Mean = {means[j]:.4f}, 95% CI = [{lower:.4f}, {upper:.4f}]\")\n",
    "\n",
    "# Example: Simulate data and run Gibbs sampler\n",
    "def simulate_data(n=100, true_params=None):\n",
    "    \"\"\"Simulate data from a mixture of geometric distributions\"\"\"\n",
    "    if true_params is None:\n",
    "        true_params = [0.7, 0.5, 0.3]\n",
    "    \n",
    "    # Simulate from mixture model\n",
    "    data = []\n",
    "    for _ in range(n):\n",
    "        # Randomly choose which geometric component to sample from\n",
    "        component = random.randint(0, len(true_params)-1)\n",
    "        p = true_params[component]\n",
    "        \n",
    "        # Sample from geometric distribution (using inverse transform sampling)\n",
    "        u = random.random()\n",
    "        x = int(math.log(1 - u) / math.log(1 - p))\n",
    "        data.append(x)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def print_data_summary(data):\n",
    "    \"\"\"Print summary statistics of the data\"\"\"\n",
    "    n = len(data)\n",
    "    mean = sum(data) / n\n",
    "    \n",
    "    # Calculate median\n",
    "    sorted_data = sorted(data)\n",
    "    if n % 2 == 0:\n",
    "        median = (sorted_data[n//2 - 1] + sorted_data[n//2]) / 2\n",
    "    else:\n",
    "        median = sorted_data[n//2]\n",
    "    \n",
    "    # Calculate mode (most frequent value)\n",
    "    counts = {}\n",
    "    for x in data:\n",
    "        if x in counts:\n",
    "            counts[x] += 1\n",
    "        else:\n",
    "            counts[x] = 1\n",
    "    \n",
    "    mode_count = 0\n",
    "    mode = None\n",
    "    for x, count in counts.items():\n",
    "        if count > mode_count:\n",
    "            mode_count = count\n",
    "            mode = x\n",
    "    \n",
    "    print(\"\\nData Summary:\")\n",
    "    print(\"-------------\")\n",
    "    print(f\"Sample size: {n}\")\n",
    "    print(f\"Min: {min(data)}\")\n",
    "    print(f\"Max: {max(data)}\")\n",
    "    print(f\"Mean: {mean:.2f}\")\n",
    "    print(f\"Median: {median:.2f}\")\n",
    "    print(f\"Mode: {mode}\")\n",
    "    \n",
    "    # Print frequency table for small datasets\n",
    "    if len(counts) < 10:\n",
    "        print(\"\\nFrequency Table:\")\n",
    "        for x in sorted(counts.keys()):\n",
    "            print(f\"Value {x}: {counts[x]} occurrences\")\n",
    "\n",
    "def run_example():\n",
    "    # Set random seed for reproducibility\n",
    "    random.seed(12345)\n",
    "    \n",
    "    # Simulate data\n",
    "    true_params = [0.7, 0.4, 0.2]\n",
    "    print(\"True parameters:\", true_params)\n",
    "    data = simulate_data(n=200, true_params=true_params)\n",
    "    \n",
    "    # Print data summary\n",
    "    print_data_summary(data)\n",
    "    \n",
    "    # Run Gibbs sampler\n",
    "    print(\"\\nRunning Gibbs sampler...\")\n",
    "    gibbs = NonparametricMixtureGibbs(data, n_iter=2000, burn_in=1000)\n",
    "    gibbs.run()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    gibbs.print_summary()\n",
    "    \n",
    "    return gibbs\n",
    "\n",
    "# Uncomment to run the example\n",
    "gibbs = run_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d77ef9c",
   "metadata": {},
   "source": [
    "\n",
    "# Graphical Models\n",
    "\n",
    "Graphical models use graphs to analyze statistical models. They have been developed mainly to represent conditional independence relations, primarily in the field of expert systems (Whittaker 1990, Spiegelhalter et al. 1993). The Bayesian approach to these models, as a way to incorporate model uncertainty, has been aided by the advent of MCMC techniques, as stressed by Madigan and York (1995) in an expository paper on which this note is based.\n",
    "\n",
    "## Construction of a Graphical Model\n",
    "\n",
    "The construction of a graphical model is based on a collection of independence assumptions represented by a graph. We briefly recall here the essentials of graph theory and refer to Lauritzen (1996) for details. A graph is defined by a set of vertices or nodes, $v \\in V$, which represents the random variables or factors under study, and a set of edges, $(a, b) \\in E^{2}$, which can be ordered (the graph is then said to be directed) or not (the graph is undirected). For a directed graph, $a$ is a parent of $b$ if $(a, b) \\in E^{2}$. Graphs are also often assumed to be acyclic, meaning there are no cycles.\n",
    "\n",
    "Directed graphs can be turned into undirected graphs by adding edges between nodes which share a child and dropping the directions.\n",
    "# Directed Acyclic Graphs and Probabilistic Models\n",
    "\n",
    "The notion of directed acyclic graphs (DAGs), introduced by Kiiveri and Speed (1982), often leads to the construction of probabilistic models on graphs, an important concept in the field of expert systems and statistical modeling. A DAG is a directed graph with no cycles, ensuring there is a maximal subset of nodes which are all independent.\n",
    "\n",
    "## Construction of Probabilistic Models on Graphs\n",
    "\n",
    "Consider a graph where there is no subset containing $C$ and satisfying this condition. An ordering of the cliques of an undirected graph $(C_1, \\dots, C_k)$ is perfect if the nodes of each clique $C_i$ are contained in a previous clique or are members of one of the previous cliques (these nodes are called the separators, $\\sigma \\in S$). In this case, the joint probability distribution of the random variable $V$ taking values in $V$ is:\n",
    "\n",
    "$$\n",
    "p(V) = \\prod_{v \\in V} p(v \\mid \\pi(v)),\n",
    "$$\n",
    "\n",
    "where $\\pi(v)$ denotes the parents of $v$. This can also be written as:\n",
    "\n",
    "$$\n",
    "p(V) = \\frac{\\prod_{i=1}^n p(C_i)}{\\prod_{i=1}^{n-1} p(S_i)},\n",
    "$$\n",
    "\n",
    "and the model is then called decomposable (see Spiegelhalter and Lauritzen 1990, and David and Lauritzen 1993 or Lauritzen 1996). As stressed by Spiegelhalter et al. (1993), the representation (10.19) leads to a principle of local computation, which extends the building of a prior distribution, or the simulation from a conditional distribution, to a single clique. (In other words, the distribution is Markov with respect to the undirected graph, as shown by David and Lauritzen 1993.) The appeal of this property for Gibbs' implementation is then obvious.\n",
    "\n",
    "When the densities or probabilities are parameterized, the parameters are denoted by $\\theta$, for the marginal distribution of $V \\in A, A \\subset V$. (In the case of discrete models, $\\theta_v$ may coincide with $p$ itself; see Example 10.33.) The prior distribution $\\pi(\\theta)$ must then be compatible with the graph structure. David and Lauritzen (1989) show that a solution is of the form:\n",
    "\n",
    "$$\n",
    "\\pi(\\theta) = \\frac{\\prod_{i=1}^n \\pi(\\theta_{C_i} \\mid \\sigma_i)}{\\prod_{i=1}^{n-1} \\pi(\\theta_{S_i} \\mid \\sigma_i)},\n",
    "$$\n",
    "\n",
    "thus reproducing the clique decomposition (10.19).\n",
    "\n",
    "## Example 33: Discrete Event Graph\n",
    "\n",
    "Consider a decomposable graph such that the random variables corresponding to all the nodes of $V$ are discrete. Let $w \\in W$ be a possible value for the vector of these random variables and $\\theta(w)$ be the associated probability. For the perfect clique decomposition $(C_1, \\dots, C_n)$, $\\theta(w)$ denotes the marginal probability that the clique vector $(v_1, v \\in C_1), \\dots, (v_n, v \\in C_n)$ takes the value $w$, and, similarly, $\\theta_{S_i}(w')$ is the probability that the subvector $(v, v \\in S_i)$ takes the value $w'$ when $(S_1, \\dots, S_n)$ is the associated sequence of separators. In this case:\n",
    "\n",
    "$$\n",
    "\\theta(w) = \\frac{\\prod_{i=1}^n \\theta_{C_i}(w_i)}{\\prod_{i=1}^{n-1} \\theta_{S_i}(w'_i)}.\n",
    "$$\n",
    "\n",
    "As illustrated by Madigan and York (1995), a Dirichlet prior can be constructed on $w$, $(w_i, w \\in W)$, which leads to the conclusion that the Dirichlet weights are identical over the intersection of two cliques. David and Lauritzen (1993) demonstrate that this prior is unique, given the marginal priors on the cliques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bee8175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(A=1, B=1, C=1) = 0.3360\n",
      "\n",
      "Marginal probabilities for clique {A, B}:\n",
      "P(A=0, B=0) = 0.2800\n",
      "P(A=0, B=1) = 0.1200\n",
      "P(A=1, B=0) = 0.1200\n",
      "P(A=1, B=1) = 0.4800\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List\n",
    "\n",
    "def compute_joint_probability(a: int, b: int, c: int) -> float:\n",
    "    \"\"\"\n",
    "    Compute P(A=a, B=b, C=c) using the chain rule and CPTs.\n",
    "    \"\"\"\n",
    "    # P(A=a)\n",
    "    p_a = cpds['A'][a]\n",
    "    \n",
    "    # P(B=b|A=a)\n",
    "    p_b_given_a = cpds['B'][(a, b)]\n",
    "    \n",
    "    # P(C=c|B=b)\n",
    "    p_c_given_b = cpds['C'][(b, c)]\n",
    "    \n",
    "    # Joint probability: P(A, B, C) = P(A) * P(B|A) * P(C|B)\n",
    "    joint_prob = p_a * p_b_given_a * p_c_given_b\n",
    "    return joint_prob\n",
    "\n",
    "def compute_clique_marginal(clique: List[int], separator: List[int] = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Compute marginal probabilities for a clique or separator.\n",
    "    This is a simplified version for demonstration.\n",
    "    \"\"\"\n",
    "    if clique == [0, 1]:  # Clique {A, B}\n",
    "        marginals = {}\n",
    "        for a in [0, 1]:\n",
    "            for b in [0, 1]:\n",
    "                marginals[(a, b)] = cpds['A'][a] * cpds['B'][(a, b)]\n",
    "        return marginals\n",
    "    elif clique == [1, 2]:  # Clique {B, C}\n",
    "        marginals = {}\n",
    "        for b in [0, 1]:\n",
    "            for c in [0, 1]:\n",
    "                marginals[(b, c)] = cpds['B'][(0, b)] * cpds['C'][(b, c)]  # Assume A=0 for simplicity\n",
    "        return marginals\n",
    "    return {}\n",
    "\n",
    "# Create a DAG\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add nodes and edges\n",
    "G.add_edges_from([('A', 'B'), ('B', 'C')])\n",
    "\n",
    "# Visualize the DAG\n",
    "plt.figure(figsize=(8, 6))\n",
    "pos = nx.spring_layout(G)  # Position for all nodes\n",
    "nx.draw(G, pos, with_labels=True, node_color='lightblue', node_size=500, font_size=16, font_weight='bold', arrowsize=20)\n",
    "plt.title(\"Simple DAG: A  B  C\")\n",
    "plt.show()\n",
    "\n",
    "# Define CPTs as dictionaries\n",
    "cpd_A = {0: 0.4, 1: 0.6}  # P(A)\n",
    "cpd_B = {  # P(B|A)\n",
    "    (0, 0): 0.7,  # P(B=0|A=0)\n",
    "    (0, 1): 0.3,  # P(B=1|A=0)\n",
    "    (1, 0): 0.2,  # P(B=0|A=1)\n",
    "    (1, 1): 0.8   # P(B=1|A=1)\n",
    "}\n",
    "cpd_C = {  # P(C|B)\n",
    "    (0, 0): 0.8,  # P(C=0|B=0)\n",
    "    (0, 1): 0.2,  # P(C=1|B=0)\n",
    "    (1, 0): 0.3,  # P(C=0|B=1)\n",
    "    (1, 1): 0.7   # P(C=1|B=1)\n",
    "}\n",
    "\n",
    "# Store CPDs in a dictionary\n",
    "cpds = {'A': cpd_A, 'B': cpd_B, 'C': cpd_C}\n",
    "\n",
    "# Test joint probability computation\n",
    "a, b, c = 1, 1, 1\n",
    "joint_prob = compute_joint_probability(a, b, c)\n",
    "print(f\"P(A={a}, B={b}, C={c}) = {joint_prob:.4f}\")\n",
    "\n",
    "# Compute marginal for clique {A, B}\n",
    "clique_ab = compute_clique_marginal([0, 1])\n",
    "print(\"\\nMarginal probabilities for clique {A, B}:\")\n",
    "for state, prob in clique_ab.items():\n",
    "    print(f\"P(A={state[0]}, B={state[1]}) = {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9c83c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
