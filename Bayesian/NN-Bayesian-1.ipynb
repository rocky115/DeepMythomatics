{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8be3973",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " * Copyright (c) 2018 Radhamadhab Dalai\n",
    " *\n",
    " * Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    " * of this software and associated documentation files (the \"Software\"), to deal\n",
    " * in the Software without restriction, including without limitation the rights\n",
    " * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    " * copies of the Software, and to permit persons to whom the Software is\n",
    " * furnished to do so, subject to the following conditions:\n",
    " *\n",
    " * The above copyright notice and this permission notice shall be included in\n",
    " * all copies or substantial portions of the Software.\n",
    " *\n",
    " * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    " * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    " * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    " * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    " * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    " * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
    " * THE SOFTWARE.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d66d97a",
   "metadata": {},
   "source": [
    "## Bayesian Classification Theory\n",
    "\n",
    "Bayes' rule is a fundamental theorem in probability used for classification:\n",
    "\n",
    "$$\n",
    "p(c_k | x) = \\frac{p(x | c_k) \\cdot p(c_k)}{p(x)}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\n",
    "p(x) = \\sum_{i=1}^{M} p(x | c_i) \\cdot p(c_i)\n",
    "$$\n",
    "\n",
    "Here, \\( M \\) is the number of classes or groups, \\( x = [x_1, \\ldots, x_d]^T \\) is a vector of feature values, and \\( p(c_k | x) \\) is the conditional probability that a given vector \\( x \\) belongs to the class \\( c_k \\). It is assumed that all possible events fall into exactly one of \\( M \\) classes or groups \\( \\{c_1, \\ldots, c_M\\} \\).\n",
    "\n",
    "### Naive Bayesian Classification\n",
    "\n",
    "In Naive Bayesian classification, the goal is to assign the feature vector \\( x \\) to the class \\( c_k \\) with the highest conditional probability \\( p(c_k | x) \\). To estimate \\( p(c_k | x) \\), we need to estimate \\( p(x | c_k) \\), \\( p(c_k) \\), and \\( p(x) \\). Estimating \\( p(x | c_k) \\) is challenging due to the vector \\( x \\) containing \\( d \\) components \\( x_1, \\ldots, x_d \\). \n",
    "\n",
    "A common strategy is to assume that the distribution of \\( x \\) conditional on \\( c_k \\) can be decomposed as:\n",
    "\n",
    "$$\n",
    "p(x | c_k) = \\prod_{i=1}^{d} p(x_i | c_k)\n",
    "$$\n",
    "\n",
    "This assumption implies that the occurrence of a particular value of \\( x_i \\) is statistically independent of the occurrence of any other \\( x_j \\), \\( j \\neq i \\), given the class \\( c_k \\). \n",
    "\n",
    "Under this assumption, Bayes' rule simplifies to:\n",
    "\n",
    "$$\n",
    "p(c_k | x) = \\frac{p(c_k) \\cdot \\prod_{i=1}^{d} p(x_i | c_k)}{p(x)}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\n",
    "p(x) = \\sum_{l=1}^{M} p(c_l) \\cdot \\prod_{i=1}^{d} p(x_i | c_l)\n",
    "$$\n",
    "\n",
    "Thus, the estimation formula for the posterior probability is:\n",
    "\n",
    "$$\n",
    "\\hat{p}(c_k | x) = \\frac{\\hat{p}(c_k) \\cdot \\prod_{i=1}^{d} \\hat{p}(x_i | c_k)}{\\hat{p}(x)}\n",
    "$$\n",
    "\n",
    "This estimate can then be used for classification.\n",
    "\n",
    "### Bayesian Classification Theory\n",
    "\n",
    "Bayesian decision theory provides a fundamental probability model for classification procedures. Consider an \\( M \\)-group classification problem where each object has an associated attribute vector of dimension \\( d \\). Let \\( x \\in \\mathbb{R}^d \\) be an attribute vector, and \\( w_j \\) denote the membership variable that takes the value 1 if an object belongs to group \\( j \\). Define \\( p(\\omega_j) \\) as the prior probability of group \\( j \\) and \\( f(x | \\omega_j) \\) as the probability density function.\n",
    "\n",
    "According to Bayes' rule:\n",
    "\n",
    "$$\n",
    "p(\\omega_j | x) = \\frac{f(x | \\omega_j) \\cdot p(\\omega_j)}{f(x)}\n",
    "$$\n",
    "\n",
    "where \\( p(\\omega_j | x) \\) is the posterior probability of group \\( j \\) and \\( f(x) \\) is the probability density function:\n",
    "\n",
    "$$\n",
    "f(x) = \\sum_{j=1}^{M} f(x | \\omega_j) \\cdot p(\\omega_j)\n",
    "$$\n",
    "\n",
    "If an object with feature vector \\( x \\) is observed and classified into group \\( j \\), the probability of classification error is given by:\n",
    "\n",
    "$$\n",
    "p(\\text{Error} | x) = 1 - p(\\omega_j | x)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5225b1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create and train the Naive Bayes classifier\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9933af73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "# Custom Bayesian Classification Implementation\n",
    "import numpy as np\n",
    "\n",
    "def calculate_probabilities(X, y):\n",
    "    classes = np.unique(y)\n",
    "    probs = {}\n",
    "    for cls in classes:\n",
    "        X_cls = X[y == cls]\n",
    "        probs[cls] = {\n",
    "            'prior': len(X_cls) / len(X),\n",
    "            'mean': np.mean(X_cls, axis=0),\n",
    "            'var': np.var(X_cls, axis=0)\n",
    "        }\n",
    "    return probs\n",
    "\n",
    "def gaussian_pdf(x, mean, var):\n",
    "    coef = 1 / np.sqrt(2 * np.pi * var)\n",
    "    exponent = np.exp(- (x - mean) ** 2 / (2 * var))\n",
    "    return coef * exponent\n",
    "\n",
    "def predict(X, probs):\n",
    "    predictions = []\n",
    "    for x in X:\n",
    "        class_probs = {}\n",
    "        for cls, values in probs.items():\n",
    "            prior = values['prior']\n",
    "            mean = values['mean']\n",
    "            var = values['var']\n",
    "            likelihood = np.prod(gaussian_pdf(x, mean, var))\n",
    "            class_probs[cls] = prior * likelihood\n",
    "        predictions.append(max(class_probs, key=class_probs.get))\n",
    "    return np.array(predictions)\n",
    "\n",
    "# Example usage:\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Convert to binary classification for simplicity\n",
    "y = (y == 0).astype(int)\n",
    "\n",
    "# Split dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Calculate probabilities from training data\n",
    "probs = calculate_probabilities(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = predict(X_test, probs)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea58b958",
   "metadata": {},
   "source": [
    "The purpose of Bayesian classification rule is to minimize the probability of total\n",
    "classification error (misclassification rate):\n",
    "\n",
    "$$\n",
    "\\text{Decide } \\omega_k \\text{ if } p(\\omega_k | x) = \\max\\{p(\\omega_1|x), \\ldots, p(\\omega_M|x)\\}.\n",
    "\\tag{7.7.9}\n",
    "$$\n",
    "\n",
    "Let \\( c_{ij} \\) be the cost of misclassifying to group \\( i \\) when it actually belongs to group \\( j \\). The expected cost associated with assigning to group \\( i \\) is\n",
    "\n",
    "$$\n",
    "C_i(x) = \\sum_{j=1}^{M} c_{ij} p(\\omega_j | x), \\quad i = 1, \\ldots, M.\n",
    "\\tag{7.7.10}\n",
    "$$\n",
    "\n",
    "\\( C_i \\) is also known as the conditional risk function. The optimal Bayesian decision\n",
    "rule that minimizes the overall expected cost can be represented as\n",
    "\n",
    "$$\n",
    "\\text{Decide } \\omega_k \\text{ for } x \\text{ if } C_k(x) = \\min\\{C_1(x), \\ldots, C_M(x)\\}.\n",
    "\\tag{7.7.11}\n",
    "$$\n",
    "\n",
    "For binary classification with the two classes of \\( \\omega_1 \\) and \\( \\omega_2 \\), we should assign to class +1 if\n",
    "\n",
    "$$\n",
    "c_{12}(x) p(\\omega_2) f(x|\\omega_2) < c_{21}(x) p(\\omega_1) f(x|\\omega_1)\n",
    "\\tag{7.7.12}\n",
    "$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\n",
    "\\frac{f(x|\\omega_1)}{f(x|\\omega_2)} > \\frac{c_{21}(x) p(\\omega_2)}{c_{12}(x) p(\\omega_1)}\n",
    "\\tag{7.7.13}\n",
    "$$\n",
    "\n",
    "Otherwise, we should assign to class −1.\n",
    "\n",
    "At this point, we discuss the relationship between Bayes classification and neural\n",
    "network classification. To this end, let \\( C \\) be a random variable which specifies the\n",
    "class membership, i.e., \\( C = c_k \\) denotes membership in the \\( k \\)th class. The goal of\n",
    "classification is then to determine \\( C \\) given \\( x \\in X \\). To minimize the probability of\n",
    "classification error, the optimal decision rule corresponds to choosing the class \\( c_k \\)\n",
    "which maximizes the posterior probability \\( p(c_k | x) \\).\n",
    "\n",
    "Define \\( y = [y_1, \\ldots, y_m]^T \\in \\mathbb{R}^M \\), where \\( M \\) is the number of classes. Let \\( e_k = [0, \\ldots, 0, 1, 0, \\ldots, 0]^T \\in \\mathbb{R}^M \\) be the \\( M \\times 1 \\) basic vector whose \\( k \\)th element equals 1 and others equal zero. Hence, if \\( y = e_k \\) then \\( x \\) belongs to class \\( k \\). This implies that \\( y \\) and \\( c_k \\) relate the same information if and only if \\( y = e_k \\). The advantage of this representation in [151] is: the \\( k \\)th component of the least squares estimate \\( \\hat{y} = E\\{y|x\\} \\) can be written as\n",
    "\n",
    "$$\n",
    "\\hat{y}_k = E\\{y_k | x\\} = \\sum y_k p(y_k | x) = p(y_k = 1 | x) = p(c_k | x).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "638187c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 53.33%\n",
      "Predicted class for the new sample: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Example dataset\n",
    "# X is the feature matrix with shape (n_samples, n_features)\n",
    "# y is the label vector with shape (n_samples,)\n",
    "# Here we generate a simple dataset for demonstration purposes\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 4)  # 100 samples, 4 features\n",
    "y = np.random.randint(0, 2, 100)  # 100 samples, binary classification (0 or 1)\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize the Gaussian Naive Bayes classifier\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Train the classifier on the training data\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set\n",
    "y_pred = gnb.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Predict the class of a new sample\n",
    "new_sample = np.array([[0.5, 0.5, 0.5, 0.5]])\n",
    "predicted_class = gnb.predict(new_sample)\n",
    "print(f'Predicted class for the new sample: {predicted_class[0]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51e4d11",
   "metadata": {},
   "source": [
    "# Sparse Bayesian Learning\n",
    "### Sparse Bayesian Learning\n",
    "\n",
    "Suppose we are given a sample of \\( N \\) “training” pairs \\(\\{x_n , t_n \\}_{n=1}^N \\), where \\( t = [t_1 , \\ldots , t_N ]^T \\) is a target vector expressed as the sum of an approximation vector \\( y = [y(x_1) , \\ldots , y(x_N )]^T \\) and an “error” vector \\(\\epsilon = [\\epsilon_1 , \\ldots , \\epsilon_N ]^T \\):\n",
    "\n",
    "$$ t = y + \\epsilon = \\Phi w + \\epsilon $$\n",
    "\n",
    "where \\( w \\) is the parameter vector and \\( \\Phi = [\\phi_1 , \\ldots , \\phi_M ] \\) is the \\( N \\times M \\) “design” matrix whose columns comprise the complete set of \\( M \\) “basis vectors.”\n",
    "\n",
    "In the sparse Bayesian framework, the errors are assumed to be modeled probabilistically as independent zero-mean Gaussians, with variance \\(\\sigma^2\\), i.e.,\n",
    "\n",
    "$$ p(\\epsilon) = \\prod_{n=1}^N \\mathcal{N}(\\epsilon_n | 0, \\sigma^2) $$\n",
    "\n",
    "The above error model implies a multivariate Gaussian likelihood for the target vector \\( t \\):\n",
    "\n",
    "$$ p(t|w, \\sigma^2) = (2\\pi)^{-\\frac{N}{2}} \\sigma^{-N} \\exp \\left( -\\frac{1}{2\\sigma^2} \\|t - y\\|^2 \\right) $$\n",
    "\n",
    "By [146], given hyperparameters \\(\\alpha = [\\alpha_1, \\ldots, \\alpha_M]^T\\), the posterior parameter distribution conditioned on the data is given by combining the likelihood and prior within Bayes’ rule:\n",
    "\n",
    "$$ p(w|t, \\alpha, \\sigma^2) = \\frac{p(t|w, \\sigma^2) p(w|\\alpha)}{p(t|\\alpha, \\sigma^2)} $$\n",
    "\n",
    "and is Gaussian \\(\\mathcal{N}(\\mu, \\Sigma)\\) with\n",
    "\n",
    "$$ \\Sigma = (A + \\sigma^{-2} \\Phi^T \\Phi)^{-1} $$\n",
    "\n",
    "$$ \\mu = \\sigma^{-2} \\Sigma \\Phi^T t $$\n",
    "\n",
    "where \\( A = \\text{Diag}(\\alpha_1 , \\ldots , \\alpha_M) \\) is an \\( M \\times M \\) diagonal matrix.\n",
    "\n",
    "To find the solution vector \\(\\alpha = \\alpha_{MP}\\), Tipping et al. [146] proposed to use sparse Bayesian learning for formulating the (local) maximization with respect to \\(\\alpha\\) of its logarithm\n",
    "\n",
    "$$ L(\\alpha) = \\log p(t|\\alpha, \\sigma^2) = \\log \\int p(t|w, \\sigma^2) p(w|\\alpha) \\, dw $$\n",
    "\n",
    "$$ = -\\frac{N}{2} \\log(2\\pi) + \\log |C| + t^T C^{-1} t $$\n",
    "\n",
    "with\n",
    "\n",
    "$$ C = \\sigma^2 I + \\Phi A^{-1} \\Phi^T $$\n",
    "\n",
    "Considering the dependence of \\( L(\\alpha) \\) on a single hyperparameter \\(\\alpha_i\\), \\( i \\in \\{1, \\ldots, M\\}\\), then \\( C \\) in the above equation can be decomposed as\n",
    "\n",
    "$$ C = \\sigma^2 I + \\sum_{m \\neq i} \\alpha_m^{-1} \\phi_m \\phi_m^T + \\alpha_i^{-1} \\phi_i \\phi_i^T $$\n",
    "\n",
    "$$ = C_{-i} + \\alpha_i^{-1} \\phi_i \\phi_i^T $$\n",
    "\n",
    "where \\( C_{-i} \\) is \\( C \\) with the contribution of basis vector \\(i\\) removed.\n",
    "\n",
    "Then, the matrix determinant \\(|C|\\) and the inverse \\(C^{-1}\\) in the loss \\(L(\\alpha)\\) can be obtained as [146]:\n",
    "\n",
    "$$ |C| = |C_{-i}| \\cdot \\left( 1 + \\alpha_i^{-1} \\phi_i^T C_{-i}^{-1} \\phi_i \\right) $$\n",
    "\n",
    "and\n",
    "\n",
    "$$ C^{-1} = C_{-i}^{-1} - \\frac{C_{-i}^{-1} \\phi_i \\phi_i^T C_{-i}^{-1}}{\\alpha_i^{-1} + \\phi_i^T C_{-i}^{-1} \\phi_i} $$\n",
    "\n",
    "Hence, we have\n",
    "\n",
    "$$ L(\\alpha) = -\\frac{N}{2} \\log(2\\pi) + \\log |C_{-i}| + t^T C_{-i}^{-1} t - \\log \\alpha_i + \\log(\\alpha_i + s_i) - \\frac{q_i^2}{\\alpha_i + s_i} $$\n",
    "\n",
    "where\n",
    "\n",
    "$$ s_i = \\phi_i^T C_{-i}^{-1} \\phi_i $$\n",
    "\n",
    "$$ q_i = \\phi_i^T C_{-i}^{-1} t $$\n",
    "\n",
    "- Sparsity factor \\( s_i \\) can be seen to be a measure of the extent that basis vector \\(\\phi_i\\) “overlaps” those already present in the model.\n",
    "- Quality factor \\( q_i \\) can be written as \\( q_i = \\sigma^{-2} \\phi_i^T (t - y_{-i}) \\), and is thus a measure of the alignment of \\(\\phi_i\\) with the error of the model with that vector excluded.\n",
    "\n",
    "Analysis of sparse Bayesian learning [30] shows that \\( L(\\alpha) \\) has a unique maximum with respect to \\(\\alpha_i\\):\n",
    "\n",
    "$$ \\alpha_i = \\begin{cases} \n",
    "\\frac{s_i^2}{q_i^2 - s_i}, & \\text{if } q_i^2 > s_i \\\\\n",
    "\\infty, & \\text{if } q_i^2 \\leq s_i \n",
    "\\end{cases} $$\n",
    "\n",
    "This result implies the following:\n",
    "- If \\(\\phi_i\\) is “in the model” (i.e., \\(\\alpha_i < \\infty\\)) yet \\(q_i^2 \\leq s_i\\), then \\(\\phi_i\\) may be deleted (i.e., set \\(\\alpha_i = \\infty\\)).\n",
    "- If \\(\\phi_i\\) is excluded from the model (i.e., \\(\\alpha_i = \\infty\\)) and \\(q_i^2 > s_i\\), \\(\\phi_i\\) may be added (i.e., set \\(\\alpha_i = \\frac{s_i}{q_i^2 - s_i}\\)).\n",
    "\n",
    "Through the above analysis, Tipping et al. [146] proposed the following sequential sparse Bayesian learning algorithm:\n",
    "\n",
    "1. If regression initialize \\(\\sigma^2\\) to some sensible value (e.g., \\(\\sigma^2 = \\text{var}(t) \\times 0.1\\)).\n",
    "2. Initialize with a single basis vector \\(\\phi_i\\), setting\n",
    "\n",
    "$$ \\alpha_i = \\frac{\\| \\phi_i \\|^2}{(\\phi_i^T t)^2 / (\\| \\phi_i \\|^2 \\sigma^2) - 1} $$\n",
    "\n",
    "All other \\(\\alpha_m\\) are notionally set to infinity.\n",
    "\n",
    "3. Explicitly compute \\(\\Sigma\\) and \\(\\mu\\) (which are scalars initially), along with initial values of \\(s_m\\) and \\(q_m\\) for all \\(M\\) bases \\(\\phi_m\\).\n",
    "\n",
    "4. Recompute/update \\(\\Sigma\\), \\(\\mu\\):\n",
    "\n",
    "$$ \\Sigma = (\\Phi^T B \\Phi + A)^{-1} $$\n",
    "\n",
    "$$ \\mu = \\Sigma \\Phi^T B t $$\n",
    "\n",
    "where \\( A = \\text{Diag}(\\alpha_1 , \\ldots , \\alpha_M) \\) and \\( B = \\sigma^{-2}I \\).\n",
    "\n",
    "5. Select a candidate basis vector \\(\\phi_i\\) from the set of all \\(M\\).\n",
    "\n",
    "6. Compute \\(\\theta_i = q_i^2 - s_i\\).\n",
    "\n",
    "7. If \\(\\theta_i > 0\\) and \\(\\alpha_i < \\infty\\) (i.e., \\(\\phi_i\\) is in the model), re-estimate \\(\\alpha_i\\).\n",
    "\n",
    "8. If \\(\\theta_i > 0\\) and \\(\\alpha_i = \\infty\\), add \\(\\phi_i\\) to the model with updated \\(\\alpha_i\\).\n",
    "\n",
    "9. If \\(\\theta_i \\leq 0\\) and \\(\\alpha_i < \\infty\\), then delete \\(\\phi_i\\) from the model and set \\(\\alpha_i = \\infty\\).\n",
    "\n",
    "10. In regression and estimating the noise level, update \\(\\sigma = \\frac{\\| t - y \\|^2}{N - M + \\sum_m \\alpha_m \\Sigma_{mm}}\\), where \\( y \\approx \\Phi \\mu_{MP} \\) and \\(\\Sigma_{mm}\\) is the \\((m, m)\\)-th diagonal element.\n",
    "\n",
    "11. Compute\n",
    "\n",
    "$$ S_m = \\phi_m^T B \\phi_m - \\phi_m^T B \\Phi \\Sigma \\Phi\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c888daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/radha/anaconda3/envs/cv37/lib/python3.7/site-packages/ipykernel_launcher.py:36: RuntimeWarning: invalid value encountered in subtract\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1 is different from 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5911/52835332.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0msbl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparseBayesianLearning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigma2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0msbl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msbl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"True weights:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_5911/52835332.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, Phi)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPhi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mPhi\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m# Example usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1 is different from 10)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SparseBayesianLearning:\n",
    "    def __init__(self, sigma2=1.0):\n",
    "        self.sigma2 = sigma2\n",
    "        self.alpha = None\n",
    "        self.mu = None\n",
    "        self.Sigma = None\n",
    "\n",
    "    def fit(self, Phi, t, max_iter=500, tol=1e-6):\n",
    "        N, M = Phi.shape\n",
    "        self.alpha = np.full(M, np.inf)\n",
    "        self.alpha[0] = 1.0\n",
    "        self.mu = np.zeros(M)\n",
    "        \n",
    "        for _ in range(max_iter):\n",
    "            # Compute Sigma and mu\n",
    "            A_inv = np.diag(1 / self.alpha)\n",
    "            S_inv = np.linalg.inv(Phi.T @ Phi / self.sigma2 + A_inv)\n",
    "            self.Sigma = self.sigma2 * S_inv\n",
    "            self.mu = self.Sigma @ (Phi.T @ t / self.sigma2)\n",
    "            \n",
    "            # Update alpha\n",
    "            gamma = 1 - self.alpha * np.diag(self.Sigma)\n",
    "            self.alpha = gamma / self.mu ** 2\n",
    "\n",
    "            # Prune small alpha values\n",
    "            keep = self.alpha < 1e9\n",
    "            if np.sum(keep) == 0:\n",
    "                break\n",
    "            self.alpha = self.alpha[keep]\n",
    "            Phi = Phi[:, keep]\n",
    "            self.mu = self.mu[keep]\n",
    "\n",
    "            # Convergence check\n",
    "            if np.max(np.abs(gamma - self.alpha * self.mu ** 2)) < tol:\n",
    "                break\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, Phi):\n",
    "        return Phi @ self.mu\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate some synthetic data\n",
    "    N, M = 100, 10\n",
    "    np.random.seed(0)\n",
    "    X = np.random.randn(N, M)\n",
    "    w_true = np.random.randn(M)\n",
    "    y = X @ w_true + np.random.randn(N) * 0.1\n",
    "\n",
    "    # Fit SBL model\n",
    "    sbl = SparseBayesianLearning(sigma2=0.1)\n",
    "    sbl.fit(X, y)\n",
    "    y_pred = sbl.predict(X)\n",
    "\n",
    "    print(\"True weights:\", w_true)\n",
    "    print(\"Estimated weights:\", sbl.mu)\n",
    "    print(\"Predicted values:\", y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc933e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True weights: [ 0.55596268  0.89247389 -0.42231482  0.10471403  0.22805333  0.20147995\n",
      "  0.54077359 -1.81807763 -0.04932407  0.2390336 ]\n",
      "Estimated weights: [0.04594617]\n",
      "Predicted values: [ 0.08105144  0.00661825 -0.1173001   0.00711924 -0.04817699 -0.04114326\n",
      " -0.03089698  0.03349892 -0.05353417 -0.01852444  0.08652356  0.08580717\n",
      "  0.01729531 -0.03537469 -0.06851757 -0.00313544 -0.02288268  0.04230993\n",
      " -0.01626466 -0.05940182 -0.01696249  0.04181923 -0.04392201 -0.06543014\n",
      " -0.02928779  0.02394093 -0.05462353  0.01833465 -0.00507892 -0.02885822\n",
      " -0.0600299  -0.00614307 -0.03306306  0.03433044  0.04909391 -0.03510948\n",
      "  0.03209143  0.01288522 -0.07802145  0.02606481 -0.02750585 -0.06646272\n",
      "  0.0427072  -0.02144976 -0.01815926 -0.04845613 -0.02377802 -0.09264615\n",
      "  0.03544315 -0.04732158  0.01758509  0.04113378 -0.01022788 -0.10290122\n",
      "  0.00980859  0.00753189  0.09240953  0.02084954 -0.08967792  0.0249631\n",
      " -0.07123628 -0.08005459  0.03108078 -0.03157876 -0.00415271  0.01318216\n",
      " -0.04285587 -0.02507337 -0.02717269  0.01518873 -0.06638946 -0.03238843\n",
      " -0.01777523 -0.02321928 -0.05410754  0.03550237 -0.02073564 -0.0229279\n",
      "  0.0684302  -0.03482126  0.06483795 -0.02446587 -0.0005121   0.01492651\n",
      "  0.01100791 -0.1186696   0.05303033  0.04968508  0.06094583 -0.02973551\n",
      " -0.06716101 -0.03756067 -0.05966968  0.07318171  0.01680935 -0.04353347\n",
      " -0.01679567 -0.03022918  0.01651785  0.02026377]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SparseBayesianLearning:\n",
    "    def __init__(self, sigma2=1.0):\n",
    "        self.sigma2 = sigma2\n",
    "        self.alpha = None\n",
    "        self.mu = None\n",
    "        self.Sigma = None\n",
    "\n",
    "    def fit(self, Phi, t, max_iter=500, tol=1e-6):\n",
    "        N, M = Phi.shape\n",
    "        self.alpha = np.full(M, np.inf)\n",
    "        self.alpha[0] = 1.0\n",
    "        self.mu = np.zeros(M)\n",
    "\n",
    "        # Initializing the active set to contain the first basis vector\n",
    "        active_set = [0]\n",
    "\n",
    "        for _ in range(max_iter):\n",
    "            # Extract active basis functions\n",
    "            Phi_active = Phi[:, active_set]\n",
    "            A_inv = np.diag(1 / self.alpha[active_set])\n",
    "            S_inv = np.linalg.inv(Phi_active.T @ Phi_active / self.sigma2 + A_inv)\n",
    "            self.Sigma = self.sigma2 * S_inv\n",
    "            self.mu = self.Sigma @ (Phi_active.T @ t / self.sigma2)\n",
    "            \n",
    "            # Update alpha\n",
    "            gamma = 1 - np.diag(self.Sigma) * self.alpha[active_set]\n",
    "            self.alpha[active_set] = gamma / self.mu ** 2\n",
    "\n",
    "            # Prune small alpha values\n",
    "            keep = self.alpha[active_set] < 1e9\n",
    "            if np.sum(keep) == 0:\n",
    "                break\n",
    "            active_set = [i for i, k in zip(active_set, keep) if k]\n",
    "            self.alpha = np.full(M, np.inf)\n",
    "            self.alpha[active_set] = gamma / self.mu ** 2\n",
    "\n",
    "            # Convergence check\n",
    "            if np.max(np.abs(gamma - self.alpha[active_set] * self.mu ** 2)) < tol:\n",
    "                break\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, Phi):\n",
    "        # Extract active basis functions for prediction\n",
    "        active_set = np.where(self.alpha < 1e9)[0]\n",
    "        Phi_active = Phi[:, active_set]\n",
    "        return Phi_active @ self.mu\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate some synthetic data\n",
    "    N, M = 100, 10\n",
    "    np.random.seed(0)\n",
    "    X = np.random.randn(N, M)\n",
    "    w_true = np.random.randn(M)\n",
    "    y = X @ w_true + np.random.randn(N) * 0.1\n",
    "\n",
    "    # Fit SBL model\n",
    "    sbl = SparseBayesianLearning(sigma2=0.1)\n",
    "    sbl.fit(X, y)\n",
    "    y_pred = sbl.predict(X)\n",
    "\n",
    "    print(\"True weights:\", w_true)\n",
    "    print(\"Estimated weights:\", sbl.mu)\n",
    "    print(\"Predicted values:\", y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5e10bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated weights:\n",
      "[ 1.3990157   0.92274868  0.05144812 -0.64339411  0.70301962  0.38327377\n",
      "  0.89031089  0.65747169  1.07162577 -0.53273178]\n",
      "Hyperparameters:\n",
      "[  0.51088519   1.17427923 363.42015907   2.41502917   2.02289376\n",
      "   6.80180289   1.26137757   2.31264318   0.8706797    3.52206255]\n",
      "Noise variance:\n",
      "0.010677310972015039\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sbl(X, t, max_iter=100, tol=1e-6):\n",
    "    \"\"\"\n",
    "    Sparse Bayesian Learning (SBL) for regression.\n",
    "    \n",
    "    Parameters:\n",
    "    X : numpy array of shape (N, M)\n",
    "        The design matrix where N is the number of samples and M is the number of features (basis functions).\n",
    "    t : numpy array of shape (N,)\n",
    "        The target vector.\n",
    "    max_iter : int\n",
    "        The maximum number of iterations.\n",
    "    tol : float\n",
    "        The convergence tolerance.\n",
    "    \n",
    "    Returns:\n",
    "    mu : numpy array of shape (M,)\n",
    "        The estimated weights.\n",
    "    alpha : numpy array of shape (M,)\n",
    "        The hyperparameters.\n",
    "    sigma2 : float\n",
    "        The noise variance.\n",
    "    \"\"\"\n",
    "    \n",
    "    N, M = X.shape\n",
    "    alpha = np.ones(M) * 1e-6\n",
    "    sigma2 = np.var(t) * 0.1\n",
    "\n",
    "    for iteration in range(max_iter):\n",
    "        # Compute A, Sigma, and mu\n",
    "        A = np.diag(alpha)\n",
    "        Sigma_inv = A + (1 / sigma2) * np.dot(X.T, X)\n",
    "        Sigma = np.linalg.inv(Sigma_inv)\n",
    "        mu = (1 / sigma2) * np.dot(Sigma, np.dot(X.T, t))\n",
    "\n",
    "        # Compute gamma\n",
    "        gamma = 1 - alpha * np.diag(Sigma)\n",
    "\n",
    "        # Update alpha and sigma2\n",
    "        alpha_new = gamma / (mu ** 2)\n",
    "        sigma2_new = np.sum((t - np.dot(X, mu)) ** 2) / (N - np.sum(gamma))\n",
    "\n",
    "        # Check for convergence\n",
    "        if np.max(np.abs(alpha_new - alpha)) < tol and np.abs(sigma2_new - sigma2) < tol:\n",
    "            break\n",
    "\n",
    "        alpha = alpha_new\n",
    "        sigma2 = sigma2_new\n",
    "\n",
    "    return mu, alpha, sigma2\n",
    "\n",
    "# Example usage\n",
    "np.random.seed(42)\n",
    "N = 100\n",
    "M = 10\n",
    "X = np.random.randn(N, M)\n",
    "w_true = np.random.randn(M)\n",
    "t = np.dot(X, w_true) + np.random.randn(N) * 0.1\n",
    "\n",
    "mu, alpha, sigma2 = sbl(X, t)\n",
    "\n",
    "print(f'Estimated weights:\\n{mu}')\n",
    "print(f'Hyperparameters:\\n{alpha}')\n",
    "print(f'Noise variance:\\n{sigma2}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a8d67a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
